#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¿«é€ŸéªŒè¯ virtual_vram.py v0.3 ä¿®å¤
"""

import sys
import torch
import torch.nn as nn

# å¯¼å…¥ä¿®å¤åçš„ virtual_vram
sys.path.insert(0, '/mnt/user-data/outputs')
from virtual_vram import VirtualVRAMConfig, virtual_vram


def test_basic_functionality():
    """æµ‹è¯•åŸºæœ¬åŠŸèƒ½"""
    print("="*70)
    print("æµ‹è¯• 1: åŸºæœ¬åŠŸèƒ½éªŒè¯")
    print("="*70)
    
    if not torch.cuda.is_available():
        print("âŒ éœ€è¦ CUDA")
        return False
    
    # ç®€å•æ¨¡å‹
    model = nn.Sequential(
        nn.Linear(256, 512),
        nn.ReLU(),
        nn.Linear(512, 256),
    ).cuda()
    
    x = torch.randn(16, 256, device="cuda", requires_grad=True)
    
    # æµ‹è¯•é…ç½®
    config = VirtualVRAMConfig(
        enabled=True,
        min_tensor_bytes=1 << 10,  # 1 KB
        compress=True,
        compress_dtype="int8",
        stream_prefetch=True,  # v0.3: å·²ä¿®å¤
        verbose=True,
    )
    
    print("\n[Baseline]")
    y = model(x)
    loss = y.mean()
    loss.backward()
    
    baseline_grad_norm = sum(p.grad.norm().item() for p in model.parameters() if p.grad is not None)
    print(f"  grad_norm = {baseline_grad_norm:.4f}")
    
    model.zero_grad()
    x.grad = None
    
    print("\n[Virtual VRAM v0.3]")
    with virtual_vram(config):
        y = model(x)
        loss = y.mean()
        loss.backward()
    
    vvram_grad_norm = sum(p.grad.norm().item() for p in model.parameters() if p.grad is not None)
    has_nan = any(torch.isnan(p.grad).any() for p in model.parameters() if p.grad is not None)
    
    print(f"  grad_norm = {vvram_grad_norm:.4f}")
    print(f"  has_nan = {has_nan}")
    
    # æ£€æŸ¥
    if has_nan:
        print("\nâŒ æµ‹è¯•å¤±è´¥ï¼šä»ç„¶æœ‰ NaNï¼")
        return False
    elif abs(vvram_grad_norm - baseline_grad_norm) / baseline_grad_norm > 0.1:
        print(f"\nâš ï¸  è­¦å‘Šï¼šæ¢¯åº¦å·®å¼‚è¾ƒå¤§ ({abs(vvram_grad_norm - baseline_grad_norm) / baseline_grad_norm:.1%})")
        return True  # å¯èƒ½æ˜¯é‡åŒ–è¯¯å·®ï¼Œä¸ç®—å¤±è´¥
    else:
        print("\nâœ… æµ‹è¯•é€šè¿‡ï¼šæ—  NaNï¼Œæ¢¯åº¦ä¸€è‡´")
        return True


def test_all_modes():
    """æµ‹è¯•æ‰€æœ‰æ¨¡å¼"""
    print("\n" + "="*70)
    print("æµ‹è¯• 2: æ‰€æœ‰æ¨¡å¼éªŒè¯")
    print("="*70)
    
    if not torch.cuda.is_available():
        print("âŒ éœ€è¦ CUDA")
        return False
    
    torch.manual_seed(42)
    
    model = nn.Sequential(
        nn.Linear(128, 256),
        nn.ReLU(),
        nn.Linear(256, 128),
    ).cuda()
    
    x = torch.randn(8, 128, device="cuda")
    
    # æµ‹è¯•æ¨¡å¼
    modes = [
        ("plain (åŒæ­¥)", VirtualVRAMConfig(
            enabled=True, min_tensor_bytes=1<<10,
            compress=False, stream_prefetch=False)),
        ("plain (å¼‚æ­¥)", VirtualVRAMConfig(
            enabled=True, min_tensor_bytes=1<<10,
            compress=False, stream_prefetch=True)),
        ("fp16 (åŒæ­¥)", VirtualVRAMConfig(
            enabled=True, min_tensor_bytes=1<<10,
            compress=True, compress_dtype="float16", stream_prefetch=False)),
        ("fp16 (å¼‚æ­¥)", VirtualVRAMConfig(
            enabled=True, min_tensor_bytes=1<<10,
            compress=True, compress_dtype="float16", stream_prefetch=True)),
        ("int8 (åŒæ­¥)", VirtualVRAMConfig(
            enabled=True, min_tensor_bytes=1<<10,
            compress=True, compress_dtype="int8", stream_prefetch=False)),
        ("int8 (å¼‚æ­¥)", VirtualVRAMConfig(
            enabled=True, min_tensor_bytes=1<<10,
            compress=True, compress_dtype="int8", stream_prefetch=True)),
    ]
    
    all_passed = True
    
    for name, cfg in modes:
        model.zero_grad()
        
        try:
            with virtual_vram(cfg):
                y = model(x)
                loss = y.mean()
                loss.backward()
            
            grad_norm = sum(p.grad.norm().item() for p in model.parameters() if p.grad is not None)
            has_nan = any(torch.isnan(p.grad).any() or torch.isinf(p.grad).any()
                         for p in model.parameters() if p.grad is not None)
            
            status = "âœ…" if not has_nan else "âŒ"
            print(f"  {name:15s}: grad_norm={grad_norm:.4f}  {status}")
            
            if has_nan:
                all_passed = False
                
        except Exception as e:
            print(f"  {name:15s}: âŒ å¼‚å¸¸ - {e}")
            all_passed = False
    
    return all_passed


def main():
    print("â•”" + "="*68 + "â•—")
    print("â•‘" + " "*15 + "Virtual VRAM v0.3 å¿«é€ŸéªŒè¯" + " "*15 + "â•‘")
    print("â•š" + "="*68 + "â•")
    print()
    
    results = []
    
    # æµ‹è¯• 1
    try:
        results.append(("åŸºæœ¬åŠŸèƒ½", test_basic_functionality()))
    except Exception as e:
        print(f"\nâŒ æµ‹è¯• 1 å¤±è´¥: {e}")
        results.append(("åŸºæœ¬åŠŸèƒ½", False))
    
    # æµ‹è¯• 2
    try:
        results.append(("æ‰€æœ‰æ¨¡å¼", test_all_modes()))
    except Exception as e:
        print(f"\nâŒ æµ‹è¯• 2 å¤±è´¥: {e}")
        results.append(("æ‰€æœ‰æ¨¡å¼", False))
    
    # æ€»ç»“
    print("\n" + "="*70)
    print("æ€»ç»“")
    print("="*70)
    
    for name, passed in results:
        status = "âœ… PASS" if passed else "âŒ FAIL"
        print(f"  {name:15s}: {status}")
    
    all_passed = all(r[1] for r in results)
    print()
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼v0.3 ä¿®å¤ç”Ÿæ•ˆã€‚")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥ã€‚")
    
    return 0 if all_passed else 1


if __name__ == "__main__":
    sys.exit(main())
