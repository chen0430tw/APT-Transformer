#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
éªŒè¯ Virtual VRAM v0.3 ä¿®å¤æ•ˆæœ
"""

import torch
import torch.nn as nn
from virtual_vram import VirtualVRAMConfig, virtual_vram


def test_gradient_convergence():
    """æµ‹è¯•æ¢¯åº¦æ”¶æ•›æ€§ï¼ˆä¿®å¤å‰ä¼š NaNï¼‰"""
    print("="*70)
    print("æµ‹è¯• 1: æ¢¯åº¦æ”¶æ•›æ€§ï¼ˆæ ¸å¿ƒä¿®å¤éªŒè¯ï¼‰")
    print("="*70)
    
    if not torch.cuda.is_available():
        print("éœ€è¦ CUDA")
        return
    
    # å›ºå®šéšæœºç§å­
    torch.manual_seed(42)
    torch.cuda.manual_seed(42)
    
    # ç®€å• Transformer å—
    model = nn.Sequential(
        nn.Linear(512, 512),
        nn.LayerNorm(512),
        nn.GELU(),
        nn.Linear(512, 512),
    ).cuda()
    
    # å›ºå®šè¾“å…¥
    x = torch.randn(8, 128, 512, device="cuda", requires_grad=True)
    
    # ä¿å­˜åˆå§‹æƒé‡
    init_state = {k: v.clone() for k, v in model.state_dict().items()}
    
    def run_test(config_name, vvram_config):
        """è¿è¡Œä¸€æ¬¡æµ‹è¯•"""
        # æ¢å¤åˆå§‹çŠ¶æ€
        model.load_state_dict(init_state)
        model.zero_grad(set_to_none=True)
        
        x_copy = x.clone().detach().requires_grad_(True)
        
        if vvram_config is None:
            # Baseline
            y = model(x_copy)
            loss = y.mean()
            loss.backward()
        else:
            # Virtual VRAM
            with virtual_vram(vvram_config):
                y = model(x_copy)
                loss = y.mean()
                loss.backward()
        
        torch.cuda.synchronize()
        
        # æ£€æŸ¥æ¢¯åº¦
        grad_norm = sum(p.grad.float().norm().item() 
                       for p in model.parameters() if p.grad is not None)
        has_nan = any(torch.isnan(p.grad).any() or torch.isinf(p.grad).any()
                     for p in model.parameters() if p.grad is not None)
        num_grads = sum(1 for p in model.parameters() if p.grad is not None)
        
        print(f"  {config_name:20s}: loss={loss.item():.6f}  "
              f"grad_norm={grad_norm:.4f}  #grads={num_grads}  "
              f"NaN={'âŒ' if has_nan else 'âœ…'}")
        
        return loss.item(), grad_norm, has_nan
    
    # æµ‹è¯•é…ç½®
    print("\nã€ä¿®å¤å‰çš„é—®é¢˜é…ç½®ã€‘")
    
    # Baseline
    loss_base, grad_base, nan_base = run_test("baseline", None)
    
    # ä¿®å¤ååº”è¯¥ä¸ NaN çš„é…ç½®
    configs = [
        ("plain (åŒæ­¥)", VirtualVRAMConfig(
            enabled=True, 
            min_tensor_bytes=1<<10,
            compress=False,
            stream_prefetch=False,  # åŒæ­¥æ¨¡å¼
            track_dependencies=True,
        )),
        ("plain (å¼‚æ­¥+ä¾èµ–)", VirtualVRAMConfig(
            enabled=True,
            min_tensor_bytes=1<<10,
            compress=False,
            stream_prefetch=True,  # å¼‚æ­¥ä½†æœ‰ä¾èµ–é—­ç¯
            track_dependencies=True,
        )),
        ("fp16 (åŒæ­¥)", VirtualVRAMConfig(
            enabled=True,
            min_tensor_bytes=1<<10,
            compress=True,
            compress_dtype="float16",
            stream_prefetch=False,
            track_dependencies=True,
        )),
        ("fp16 (å¼‚æ­¥+ä¾èµ–)", VirtualVRAMConfig(
            enabled=True,
            min_tensor_bytes=1<<10,
            compress=True,
            compress_dtype="float16",
            stream_prefetch=True,
            track_dependencies=True,
        )),
        ("int8 (åŒæ­¥)", VirtualVRAMConfig(
            enabled=True,
            min_tensor_bytes=1<<10,
            compress=True,
            compress_dtype="int8",
            stream_prefetch=False,
            track_dependencies=True,
        )),
        ("int8 (å¼‚æ­¥+ä¾èµ–)", VirtualVRAMConfig(
            enabled=True,
            min_tensor_bytes=1<<10,
            compress=True,
            compress_dtype="int8",
            stream_prefetch=True,
            track_dependencies=True,
        )),
    ]
    
    print("\nã€ä¿®å¤åé…ç½®ã€‘")
    results = {}
    for name, cfg in configs:
        loss, grad, nan = run_test(name, cfg)
        results[name] = (loss, grad, nan)
    
    # æ€»ç»“
    print("\n" + "="*70)
    print("æ€»ç»“:")
    print("="*70)
    
    all_passed = True
    for name, (loss, grad, nan) in results.items():
        # æ£€æŸ¥ï¼š1) ä¸ NaN  2) loss ä¸ baseline ä¸€è‡´
        loss_match = abs(loss - loss_base) < 1e-4
        no_nan = not nan
        passed = loss_match and no_nan
        all_passed = all_passed and passed
        
        status = "âœ… PASS" if passed else "âŒ FAIL"
        print(f"  {name:20s}: {status}  "
              f"(loss_match={loss_match}, no_nan={no_nan})")
    
    print()
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼NaN é—®é¢˜å·²ä¿®å¤ã€‚")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œéœ€è¦è¿›ä¸€æ­¥æ£€æŸ¥ã€‚")
    
    return all_passed


def test_dependency_tracking():
    """æµ‹è¯•ä¾èµ–è¿½è¸ª"""
    print("\n" + "="*70)
    print("æµ‹è¯• 2: ä¾èµ–è¿½è¸ªï¼ˆå®¡è®¡ stream ä¾èµ–é—­ç¯ï¼‰")
    print("="*70)
    
    if not torch.cuda.is_available():
        print("éœ€è¦ CUDA")
        return
    
    model = nn.Sequential(
        nn.Linear(256, 512),
        nn.ReLU(),
        nn.Linear(512, 256),
    ).cuda()
    
    x = torch.randn(16, 256, device="cuda")
    
    # æµ‹è¯•å¼‚æ­¥ prefetch + ä¾èµ–è¿½è¸ª
    config = VirtualVRAMConfig(
        enabled=True,
        min_tensor_bytes=1<<10,
        compress=True,
        compress_dtype="int8",
        stream_prefetch=True,
        track_dependencies=True,
        verbose=False,
    )
    
    with virtual_vram(config) as mgr:
        y = model(x)
        loss = y.mean()
        loss.backward()
    
    print(f"  dependency_edges = {mgr.stats.dependency_edges}")
    print(f"  loop_breaks = {mgr.stats.loop_breaks}")
    print(f"  offload_count = {mgr.stats.offload_count}")
    print(f"  restore_count = {mgr.stats.restore_count}")
    
    if mgr.stats.loop_breaks > 0:
        print("\n  âš ï¸  æ£€æµ‹åˆ°æ–­ç¯ï¼è¿™ä¼šå¯¼è‡´ NaNã€‚")
        return False
    elif mgr.stats.dependency_edges > 0:
        print("\n  âœ… ä¾èµ–é—­ç¯æ­£å¸¸å»ºç«‹ã€‚")
        return True
    else:
        print("\n  â„¹ï¸  æœªä½¿ç”¨å¼‚æ­¥ prefetchï¼Œæ— éœ€ä¾èµ–è¾¹ã€‚")
        return True


def test_restore_semantics():
    """æµ‹è¯• restore è¯­ä¹‰ï¼ˆdetachï¼‰"""
    print("\n" + "="*70)
    print("æµ‹è¯• 3: Restore è¯­ä¹‰ï¼ˆsaved tensor åº”è¯¥ detachï¼‰")
    print("="*70)
    
    if not torch.cuda.is_available():
        print("éœ€è¦ CUDA")
        return
    
    class CustomModule(nn.Module):
        def __init__(self):
            super().__init__()
            self.linear = nn.Linear(128, 128)
        
        def forward(self, x):
            # è¿™é‡Œä¼šä¿å­˜ x ä½œä¸º saved tensor
            y = self.linear(x)
            return y
    
    model = CustomModule().cuda()
    x = torch.randn(8, 128, device="cuda", requires_grad=True)
    
    config = VirtualVRAMConfig(
        enabled=True,
        min_tensor_bytes=1<<10,
        compress=True,
        compress_dtype="int8",
        stream_prefetch=False,
        verbose=False,
    )
    
    with virtual_vram(config):
        y = model(x)
        loss = y.mean()
        
        # æ£€æŸ¥ backward æ˜¯å¦æ­£å¸¸
        try:
            loss.backward()
            print("  âœ… Backward æ­£å¸¸å®Œæˆï¼ˆæ—  requires_grad å¼‚å¸¸ï¼‰")
            
            # æ£€æŸ¥æ¢¯åº¦
            has_grad = x.grad is not None
            has_nan = torch.isnan(x.grad).any() if has_grad else False
            
            print(f"  âœ… è¾“å…¥æ¢¯åº¦å­˜åœ¨: {has_grad}")
            print(f"  âœ… è¾“å…¥æ¢¯åº¦æ—  NaN: {not has_nan}")
            
            return has_grad and not has_nan
            
        except Exception as e:
            print(f"  âŒ Backward å¤±è´¥: {e}")
            return False


def main():
    """ä¸»æµ‹è¯•æµç¨‹"""
    print("â•”" + "="*68 + "â•—")
    print("â•‘" + " "*15 + "Virtual VRAM v0.3 ä¿®å¤éªŒè¯" + " "*15 + "â•‘")
    print("â•š" + "="*68 + "â•")
    print()
    
    if not torch.cuda.is_available():
        print("âŒ éœ€è¦ CUDA")
        return
    
    results = {}
    
    # æµ‹è¯• 1: æ¢¯åº¦æ”¶æ•›æ€§
    results['gradient'] = test_gradient_convergence()
    
    # æµ‹è¯• 2: ä¾èµ–è¿½è¸ª
    results['dependency'] = test_dependency_tracking()
    
    # æµ‹è¯• 3: Restore è¯­ä¹‰
    results['restore'] = test_restore_semantics()
    
    # æ€»ç»“
    print("\n" + "="*70)
    print("æœ€ç»ˆæ€»ç»“")
    print("="*70)
    
    for name, passed in results.items():
        status = "âœ… PASS" if passed else "âŒ FAIL"
        print(f"  {name:15s}: {status}")
    
    all_passed = all(results.values())
    print()
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰æ ¸å¿ƒä¿®å¤å·²éªŒè¯é€šè¿‡ï¼")
        print()
        print("ä¿®å¤å†…å®¹:")
        print("  1. âœ… Stream ä¾èµ–é—­ç¯ï¼šä½¿ç”¨ CUDA event å»ºç«‹ä¾èµ–è¾¹")
        print("  2. âœ… Restore è¯­ä¹‰ä¿®æ­£ï¼šæ¢å¤çš„ tensor ä¸€å¾‹ detach")
        print("  3. âœ… ä¾èµ–è¿½è¸ªå®¡è®¡ï¼šå¯å®¡è®¡çš„æ¬è¿è·¯å¾„")
    else:
        print("âš ï¸  éƒ¨åˆ†æµ‹è¯•æœªé€šè¿‡ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒè¯•ã€‚")


if __name__ == "__main__":
    main()
