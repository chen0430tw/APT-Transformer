"""
Multimodal Training Plugin for APT
å¤šæ¨¡æ€è®­ç»ƒæ’ä»¶ - æ”¯æŒæ–‡æœ¬+å›¾åƒ+éŸ³é¢‘çš„è”åˆè®­ç»ƒ
"""

import torch
import torch.nn as nn
from typing import Dict, Any, Optional, List, Tuple
from torchvision import transforms
from PIL import Image
import torchaudio
from torch.utils.data import Dataset, DataLoader


class MultimodalTrainingPlugin:
    """
    å¤šæ¨¡æ€è®­ç»ƒæ’ä»¶
    
    æ”¯æŒçš„æ¨¡æ€:
    1. æ–‡æœ¬ (Text) - ä½¿ç”¨APTåŸç”Ÿæ”¯æŒ
    2. å›¾åƒ (Image) - ä½¿ç”¨è§†è§‰ç¼–ç å™¨
    3. éŸ³é¢‘ (Audio) - ä½¿ç”¨éŸ³é¢‘ç¼–ç å™¨
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.name = "multimodal-training"
        self.version = "1.0.0"
        self.config = config
        
        self.modalities = config.get('modalities', ['text', 'image'])
        self.vision_encoder = config.get('vision_encoder', 'clip')
        self.audio_encoder = config.get('audio_encoder', 'wav2vec2')
        
        # åˆå§‹åŒ–ç¼–ç å™¨
        self.encoders = {}
        self._init_encoders()
    
    # ==================== ç¼–ç å™¨åˆå§‹åŒ– ====================
    
    def _init_encoders(self):
        """åˆå§‹åŒ–å„æ¨¡æ€ç¼–ç å™¨"""
        if 'image' in self.modalities:
            self._init_vision_encoder()
        
        if 'audio' in self.modalities:
            self._init_audio_encoder()
    
    def _init_vision_encoder(self):
        """åˆå§‹åŒ–è§†è§‰ç¼–ç å™¨"""
        print("ğŸ–¼ï¸ åˆå§‹åŒ–è§†è§‰ç¼–ç å™¨...")
        
        if self.vision_encoder == 'clip':
            try:
                from transformers import CLIPVisionModel, CLIPImageProcessor
                
                model_name = self.config.get('clip_model', 'openai/clip-vit-base-patch32')
                self.encoders['vision_model'] = CLIPVisionModel.from_pretrained(model_name)
                self.encoders['vision_processor'] = CLIPImageProcessor.from_pretrained(model_name)
                
                print(f"âœ… CLIPè§†è§‰ç¼–ç å™¨åŠ è½½å®Œæˆ: {model_name}")
                
            except ImportError:
                print("âŒ éœ€è¦å®‰è£…transformersåº“")
                raise
        
        elif self.vision_encoder == 'vit':
            try:
                from transformers import ViTModel, ViTImageProcessor
                
                model_name = self.config.get('vit_model', 'google/vit-base-patch16-224')
                self.encoders['vision_model'] = ViTModel.from_pretrained(model_name)
                self.encoders['vision_processor'] = ViTImageProcessor.from_pretrained(model_name)
                
                print(f"âœ… ViTè§†è§‰ç¼–ç å™¨åŠ è½½å®Œæˆ: {model_name}")
                
            except ImportError:
                print("âŒ éœ€è¦å®‰è£…transformersåº“")
                raise
        
        else:
            # è‡ªå®šä¹‰ç®€å•CNNç¼–ç å™¨
            self.encoders['vision_model'] = SimpleCNNEncoder(
                output_dim=self.config.get('vision_dim', 768)
            )
            print("âœ… ç®€å•CNNç¼–ç å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _init_audio_encoder(self):
        """åˆå§‹åŒ–éŸ³é¢‘ç¼–ç å™¨"""
        print("ğŸµ åˆå§‹åŒ–éŸ³é¢‘ç¼–ç å™¨...")
        
        if self.audio_encoder == 'wav2vec2':
            try:
                from transformers import Wav2Vec2Model, Wav2Vec2Processor
                
                model_name = self.config.get('wav2vec2_model', 'facebook/wav2vec2-base')
                self.encoders['audio_model'] = Wav2Vec2Model.from_pretrained(model_name)
                self.encoders['audio_processor'] = Wav2Vec2Processor.from_pretrained(model_name)
                
                print(f"âœ… Wav2Vec2éŸ³é¢‘ç¼–ç å™¨åŠ è½½å®Œæˆ: {model_name}")
                
            except ImportError:
                print("âŒ éœ€è¦å®‰è£…transformersåº“")
                raise
        
        else:
            # è‡ªå®šä¹‰ç®€å•éŸ³é¢‘ç¼–ç å™¨
            self.encoders['audio_model'] = SimpleAudioEncoder(
                output_dim=self.config.get('audio_dim', 768)
            )
            print("âœ… ç®€å•éŸ³é¢‘ç¼–ç å™¨åˆå§‹åŒ–å®Œæˆ")
    
    # ==================== æ•°æ®å¤„ç† ====================
    
    def process_text(self, text: str, tokenizer: Any) -> torch.Tensor:
        """å¤„ç†æ–‡æœ¬æ•°æ®"""
        return tokenizer(
            text,
            return_tensors='pt',
            padding=True,
            truncation=True,
            max_length=512
        )
    
    def process_image(self, image: Image.Image) -> torch.Tensor:
        """
        å¤„ç†å›¾åƒæ•°æ®
        
        Args:
            image: PILå›¾åƒ
            
        Returns:
            å›¾åƒç‰¹å¾å¼ é‡
        """
        if 'vision_processor' in self.encoders:
            # ä½¿ç”¨é¢„è®­ç»ƒçš„processor
            inputs = self.encoders['vision_processor'](
                images=image,
                return_tensors='pt'
            )
            
            with torch.no_grad():
                outputs = self.encoders['vision_model'](**inputs)
                # è·å–å›¾åƒç‰¹å¾
                image_features = outputs.last_hidden_state.mean(dim=1)  # [1, hidden_dim]
            
            return image_features
        
        else:
            # ä½¿ç”¨è‡ªå®šä¹‰ç¼–ç å™¨
            transform = transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], 
                                   std=[0.229, 0.224, 0.225])
            ])
            
            image_tensor = transform(image).unsqueeze(0)  # [1, 3, 224, 224]
            
            with torch.no_grad():
                image_features = self.encoders['vision_model'](image_tensor)
            
            return image_features
    
    def process_audio(self, audio_path: str) -> torch.Tensor:
        """
        å¤„ç†éŸ³é¢‘æ•°æ®
        
        Args:
            audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„
            
        Returns:
            éŸ³é¢‘ç‰¹å¾å¼ é‡
        """
        # åŠ è½½éŸ³é¢‘
        waveform, sample_rate = torchaudio.load(audio_path)
        
        if 'audio_processor' in self.encoders:
            # ä½¿ç”¨é¢„è®­ç»ƒçš„processor
            inputs = self.encoders['audio_processor'](
                waveform.squeeze().numpy(),
                sampling_rate=sample_rate,
                return_tensors='pt'
            )
            
            with torch.no_grad():
                outputs = self.encoders['audio_model'](**inputs)
                # è·å–éŸ³é¢‘ç‰¹å¾
                audio_features = outputs.last_hidden_state.mean(dim=1)  # [1, hidden_dim]
            
            return audio_features
        
        else:
            # ä½¿ç”¨è‡ªå®šä¹‰ç¼–ç å™¨
            # é‡é‡‡æ ·åˆ°16kHz
            if sample_rate != 16000:
                resampler = torchaudio.transforms.Resample(sample_rate, 16000)
                waveform = resampler(waveform)
            
            with torch.no_grad():
                audio_features = self.encoders['audio_model'](waveform)
            
            return audio_features
    
    # ==================== å¤šæ¨¡æ€èåˆ ====================
    
    def create_multimodal_model(
        self,
        base_model: nn.Module,
        fusion_method: str = 'concatenate'
    ) -> nn.Module:
        """
        åˆ›å»ºå¤šæ¨¡æ€èåˆæ¨¡å‹
        
        Args:
            base_model: åŸºç¡€APTæ¨¡å‹
            fusion_method: èåˆæ–¹æ³• (concatenate/add/attention)
            
        Returns:
            å¤šæ¨¡æ€æ¨¡å‹
        """
        print(f"ğŸ”— åˆ›å»ºå¤šæ¨¡æ€æ¨¡å‹ (èåˆæ–¹æ³•: {fusion_method})...")
        
        return MultimodalFusionModel(
            base_model=base_model,
            vision_encoder=self.encoders.get('vision_model'),
            audio_encoder=self.encoders.get('audio_model'),
            fusion_method=fusion_method,
            hidden_dim=self.config.get('hidden_dim', 768)
        )
    
    # ==================== æ•°æ®åŠ è½½ ====================
    
    def create_multimodal_dataloader(
        self,
        text_data: List[str],
        image_data: Optional[List[str]] = None,
        audio_data: Optional[List[str]] = None,
        batch_size: int = 32
    ) -> DataLoader:
        """
        åˆ›å»ºå¤šæ¨¡æ€æ•°æ®åŠ è½½å™¨
        
        Args:
            text_data: æ–‡æœ¬æ•°æ®åˆ—è¡¨
            image_data: å›¾åƒè·¯å¾„åˆ—è¡¨
            audio_data: éŸ³é¢‘è·¯å¾„åˆ—è¡¨
            batch_size: æ‰¹æ¬¡å¤§å°
            
        Returns:
            DataLoader
        """
        print("ğŸ“¦ åˆ›å»ºå¤šæ¨¡æ€æ•°æ®åŠ è½½å™¨...")
        
        class MultimodalDataset(Dataset):
            def __init__(self, texts, images, audios, plugin):
                self.texts = texts
                self.images = images
                self.audios = audios
                self.plugin = plugin
            
            def __len__(self):
                return len(self.texts)
            
            def __getitem__(self, idx):
                item = {'text': self.texts[idx]}
                
                if self.images:
                    image = Image.open(self.images[idx]).convert('RGB')
                    item['image'] = self.plugin.process_image(image)
                
                if self.audios:
                    item['audio'] = self.plugin.process_audio(self.audios[idx])
                
                return item
        
        dataset = MultimodalDataset(text_data, image_data, audio_data, self)
        
        return DataLoader(
            dataset,
            batch_size=batch_size,
            shuffle=True,
            num_workers=self.config.get('num_workers', 4)
        )
    
    # ==================== è®­ç»ƒæµç¨‹ ====================
    
    def train_multimodal(
        self,
        model: nn.Module,
        dataloader: DataLoader,
        optimizer: torch.optim.Optimizer,
        num_epochs: int = 3,
        device: str = 'cuda'
    ):
        """
        å¤šæ¨¡æ€è®­ç»ƒæµç¨‹
        
        Args:
            model: å¤šæ¨¡æ€æ¨¡å‹
            dataloader: æ•°æ®åŠ è½½å™¨
            optimizer: ä¼˜åŒ–å™¨
            num_epochs: è®­ç»ƒè½®æ•°
            device: è®¾å¤‡
        """
        print(f"ğŸ‹ï¸ å¼€å§‹å¤šæ¨¡æ€è®­ç»ƒ ({num_epochs} epochs)...")
        
        model.to(device)
        model.train()
        
        for epoch in range(num_epochs):
            epoch_loss = 0
            
            for batch_idx, batch in enumerate(dataloader):
                # å°†æ•°æ®ç§»åˆ°è®¾å¤‡
                for key in batch:
                    if isinstance(batch[key], torch.Tensor):
                        batch[key] = batch[key].to(device)
                
                # å‰å‘ä¼ æ’­
                outputs = model(**batch)
                loss = outputs['loss'] if isinstance(outputs, dict) else outputs[0]
                
                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                epoch_loss += loss.item()
                
                if batch_idx % 100 == 0:
                    print(f"  Epoch {epoch+1}/{num_epochs} | "
                          f"Batch {batch_idx}/{len(dataloader)} | "
                          f"Loss: {loss.item():.4f}")
            
            avg_loss = epoch_loss / len(dataloader)
            print(f"ğŸ“Š Epoch {epoch+1} å¹³å‡æŸå¤±: {avg_loss:.4f}")
        
        print("âœ… å¤šæ¨¡æ€è®­ç»ƒå®Œæˆ!")
    
    # ==================== è¯„ä¼°ä¸æ¨ç† ====================
    
    def evaluate_multimodal(
        self,
        model: nn.Module,
        dataloader: DataLoader,
        device: str = 'cuda'
    ) -> Dict[str, float]:
        """
        å¤šæ¨¡æ€æ¨¡å‹è¯„ä¼°
        
        Args:
            model: å¤šæ¨¡æ€æ¨¡å‹
            dataloader: æ•°æ®åŠ è½½å™¨
            device: è®¾å¤‡
            
        Returns:
            è¯„ä¼°æŒ‡æ ‡å­—å…¸
        """
        print("ğŸ“Š å¼€å§‹å¤šæ¨¡æ€è¯„ä¼°...")
        
        model.to(device)
        model.eval()
        
        total_loss = 0
        total_correct = 0
        total_samples = 0
        
        with torch.no_grad():
            for batch in dataloader:
                # å°†æ•°æ®ç§»åˆ°è®¾å¤‡
                for key in batch:
                    if isinstance(batch[key], torch.Tensor):
                        batch[key] = batch[key].to(device)
                
                # å‰å‘ä¼ æ’­
                outputs = model(**batch)
                loss = outputs.get('loss', 0)
                logits = outputs.get('logits')
                
                total_loss += loss.item() if isinstance(loss, torch.Tensor) else loss
                
                # è®¡ç®—å‡†ç¡®ç‡ï¼ˆå¦‚æœæœ‰æ ‡ç­¾ï¼‰
                if 'labels' in batch and logits is not None:
                    predictions = logits.argmax(dim=-1)
                    correct = (predictions == batch['labels']).sum().item()
                    total_correct += correct
                    total_samples += batch['labels'].size(0)
        
        metrics = {
            'loss': total_loss / len(dataloader),
        }
        
        if total_samples > 0:
            metrics['accuracy'] = total_correct / total_samples
        
        print(f"âœ… è¯„ä¼°å®Œæˆ: {metrics}")
        return metrics
    
    def inference_multimodal(
        self,
        model: nn.Module,
        text: str = None,
        image: Image.Image = None,
        audio_path: str = None,
        device: str = 'cuda'
    ) -> torch.Tensor:
        """
        å¤šæ¨¡æ€æ¨ç†
        
        Args:
            model: å¤šæ¨¡æ€æ¨¡å‹
            text: æ–‡æœ¬è¾“å…¥
            image: å›¾åƒè¾“å…¥
            audio_path: éŸ³é¢‘è·¯å¾„
            device: è®¾å¤‡
            
        Returns:
            æ¨¡å‹è¾“å‡º
        """
        model.to(device)
        model.eval()
        
        inputs = {}
        
        if text:
            # å¤„ç†æ–‡æœ¬
            inputs['text'] = self.process_text(text, model.tokenizer)
        
        if image:
            # å¤„ç†å›¾åƒ
            inputs['image'] = self.process_image(image).to(device)
        
        if audio_path:
            # å¤„ç†éŸ³é¢‘
            inputs['audio'] = self.process_audio(audio_path).to(device)
        
        with torch.no_grad():
            outputs = model(**inputs)
        
        return outputs
    
    # ==================== æ’ä»¶é’©å­ ====================
    
    def on_training_start(self, context: Dict[str, Any]):
        """è®­ç»ƒå¼€å§‹æ—¶çš„é’©å­"""
        if context.get('enable_multimodal'):
            print("ğŸŒˆ å¯ç”¨å¤šæ¨¡æ€è®­ç»ƒæ¨¡å¼")
            
            # åˆ›å»ºå¤šæ¨¡æ€æ¨¡å‹
            base_model = context.get('model')
            multimodal_model = self.create_multimodal_model(base_model)
            context['model'] = multimodal_model
    
    def on_batch_start(self, context: Dict[str, Any]):
        """æ‰¹æ¬¡å¼€å§‹æ—¶çš„é’©å­"""
        batch = context.get('batch')
        
        # ç¡®ä¿æ‰€æœ‰æ¨¡æ€æ•°æ®éƒ½åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
        device = context.get('device', 'cuda')
        for key in batch:
            if isinstance(batch[key], torch.Tensor):
                batch[key] = batch[key].to(device)
    
    def on_epoch_end(self, context: Dict[str, Any]):
        """è®­ç»ƒè½®æ¬¡ç»“æŸæ—¶çš„é’©å­"""
        epoch = context.get('epoch', 0)
        metrics = context.get('metrics', {})
        
        print(f"ğŸ“Š Epoch {epoch} - å¤šæ¨¡æ€è®­ç»ƒæŒ‡æ ‡: {metrics}")


# ==================== è¾…åŠ©ç±» ====================

class SimpleCNNEncoder(nn.Module):
    """ç®€å•çš„CNNå›¾åƒç¼–ç å™¨"""
    
    def __init__(self, output_dim: int = 768):
        super().__init__()
        self.conv_layers = nn.Sequential(
            nn.Conv2d(3, 64, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(64, 128, 3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),
            nn.Conv2d(128, 256, 3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool2d((1, 1))
        )
        self.fc = nn.Linear(256, output_dim)
    
    def forward(self, x):
        x = self.conv_layers(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x


class SimpleAudioEncoder(nn.Module):
    """ç®€å•çš„éŸ³é¢‘ç¼–ç å™¨"""
    
    def __init__(self, output_dim: int = 768):
        super().__init__()
        self.conv_layers = nn.Sequential(
            nn.Conv1d(1, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool1d(2),
            nn.Conv1d(64, 128, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.AdaptiveAvgPool1d(1)
        )
        self.fc = nn.Linear(128, output_dim)
    
    def forward(self, x):
        if x.dim() == 2:
            x = x.unsqueeze(1)  # [batch, 1, time]
        x = self.conv_layers(x)
        x = x.squeeze(-1)
        x = self.fc(x)
        return x


class MultimodalFusionModel(nn.Module):
    """å¤šæ¨¡æ€èåˆæ¨¡å‹"""
    
    def __init__(
        self,
        base_model: nn.Module,
        vision_encoder: Optional[nn.Module] = None,
        audio_encoder: Optional[nn.Module] = None,
        fusion_method: str = 'concatenate',
        hidden_dim: int = 768
    ):
        super().__init__()
        self.base_model = base_model
        self.vision_encoder = vision_encoder
        self.audio_encoder = audio_encoder
        self.fusion_method = fusion_method
        
        # èåˆå±‚
        if fusion_method == 'concatenate':
            # è®¡ç®—èåˆåçš„ç»´åº¦
            fusion_dim = hidden_dim
            if vision_encoder:
                fusion_dim += hidden_dim
            if audio_encoder:
                fusion_dim += hidden_dim
            
            self.fusion_layer = nn.Linear(fusion_dim, hidden_dim)
        
        elif fusion_method == 'attention':
            self.fusion_attention = nn.MultiheadAttention(
                hidden_dim, num_heads=8, batch_first=True
            )
        
        # è¾“å‡ºå±‚
        vocab_size = getattr(base_model.config, 'vocab_size', 50257)
        self.output_layer = nn.Linear(hidden_dim, vocab_size)
    
    def forward(
        self,
        text: torch.Tensor,
        image: Optional[torch.Tensor] = None,
        audio: Optional[torch.Tensor] = None,
        labels: Optional[torch.Tensor] = None
    ):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            text: æ–‡æœ¬åµŒå…¥ [batch, seq_len, hidden]
            image: å›¾åƒç‰¹å¾ [batch, hidden]
            audio: éŸ³é¢‘ç‰¹å¾ [batch, hidden]
            labels: æ ‡ç­¾ï¼ˆå¯é€‰ï¼‰
        """
        # è·å–æ–‡æœ¬ç‰¹å¾
        if hasattr(self.base_model, 'get_text_features'):
            text_features = self.base_model.get_text_features(text)
        else:
            # å‡è®¾textå·²ç»æ˜¯ç‰¹å¾
            text_features = text
        
        # æ”¶é›†æ‰€æœ‰æ¨¡æ€ç‰¹å¾
        features = [text_features]
        
        if image is not None and self.vision_encoder:
            features.append(image)
        
        if audio is not None and self.audio_encoder:
            features.append(audio)
        
        # èåˆ
        if self.fusion_method == 'concatenate':
            # æ‹¼æ¥èåˆ
            fused = torch.cat(features, dim=-1)
            fused = self.fusion_layer(fused)
        
        elif self.fusion_method == 'add':
            # åŠ æ³•èåˆ
            fused = sum(features) / len(features)
        
        elif self.fusion_method == 'attention':
            # æ³¨æ„åŠ›èåˆ
            features_stacked = torch.stack(features, dim=1)  # [batch, num_modalities, hidden]
            fused, _ = self.fusion_attention(
                features_stacked, features_stacked, features_stacked
            )
            fused = fused.mean(dim=1)  # [batch, hidden]
        
        # è¾“å‡º
        logits = self.output_layer(fused)
        
        # è®¡ç®—æŸå¤±ï¼ˆå¦‚æœæä¾›äº†æ ‡ç­¾ï¼‰
        loss = None
        if labels is not None:
            loss_fct = nn.CrossEntropyLoss()
            loss = loss_fct(logits.view(-1, logits.size(-1)), labels.view(-1))
        
        return {
            'logits': logits,
            'loss': loss
        }


# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸŒˆ å¤šæ¨¡æ€è®­ç»ƒæ’ä»¶ (Multimodal Training Plugin)")
    print("=" * 60)
    
    # é…ç½®
    config = {
        'modalities': ['text', 'image', 'audio'],
        'vision_encoder': 'clip',
        'audio_encoder': 'wav2vec2',
        'clip_model': 'openai/clip-vit-base-patch32',
        'wav2vec2_model': 'facebook/wav2vec2-base',
        'hidden_dim': 768,
        'num_workers': 4
    }
    
    plugin = MultimodalTrainingPlugin(config)
    
    print("\nâœ… å¤šæ¨¡æ€è®­ç»ƒæ’ä»¶åˆå§‹åŒ–å®Œæˆ!")
    print("\nğŸ’¡ æ’ä»¶åŠŸèƒ½:")
    print("1. âœ¨ æ”¯æŒæ–‡æœ¬+å›¾åƒ+éŸ³é¢‘çš„è”åˆè®­ç»ƒ")
    print("2. ğŸ–¼ï¸ ä½¿ç”¨CLIP/ViTä½œä¸ºè§†è§‰ç¼–ç å™¨")
    print("3. ğŸµ ä½¿ç”¨Wav2Vec2ä½œä¸ºéŸ³é¢‘ç¼–ç å™¨")
    print("4. ğŸ”— æ”¯æŒå¤šç§èåˆæ–¹æ³•(concatenate/add/attention)")
    print("5. ğŸ“¦ æä¾›å®Œæ•´çš„æ•°æ®åŠ è½½å’Œè®­ç»ƒæµç¨‹")
    print("6. ğŸ“Š æ”¯æŒæ¨¡å‹è¯„ä¼°å’Œæ¨ç†")
    
    print("\nğŸ“ ä½¿ç”¨ç¤ºä¾‹:")
    print("""
    # åˆ›å»ºå¤šæ¨¡æ€æ•°æ®åŠ è½½å™¨
    dataloader = plugin.create_multimodal_dataloader(
        text_data=["è¿™æ˜¯ä¸€å¼ å›¾ç‰‡", "è¿™æ˜¯ä¸€æ®µéŸ³é¢‘"],
        image_data=["image1.jpg", "image2.jpg"],
        audio_data=["audio1.wav", "audio2.wav"],
        batch_size=2
    )
    
    # åˆ›å»ºå¤šæ¨¡æ€æ¨¡å‹
    multimodal_model = plugin.create_multimodal_model(
        base_model=apt_model,
        fusion_method='attention'
    )
    
    # è®­ç»ƒ
    plugin.train_multimodal(
        model=multimodal_model,
        dataloader=dataloader,
        optimizer=optimizer,
        num_epochs=3
    )
    """)
    
    print("\n" + "=" * 60)
