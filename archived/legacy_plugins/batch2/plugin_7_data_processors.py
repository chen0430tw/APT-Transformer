"""
Data Processors Plugin for APT
æ•°æ®å¤„ç†å™¨æ’ä»¶ - æä¾›é«˜çº§æ•°æ®é¢„å¤„ç†å’Œå¢å¼ºåŠŸèƒ½
"""

import torch
import numpy as np
from typing import Dict, Any, List, Optional, Callable, Union
import json
import random
from collections import Counter
import re


class DataProcessorsPlugin:
    """
    æ•°æ®å¤„ç†å™¨æ’ä»¶
    
    æä¾›å¤šç§æ•°æ®å¤„ç†åŠŸèƒ½:
    1. æ–‡æœ¬æ¸…æ´—å’Œæ ‡å‡†åŒ–
    2. æ•°æ®å¢å¼º (Data Augmentation)
    3. æ•°æ®å¹³è¡¡ (Data Balancing)
    4. ç‰¹å¾å·¥ç¨‹ (Feature Engineering)
    5. æ•°æ®è´¨é‡æ£€æŸ¥ (Data Quality Check)
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.name = "data-processors"
        self.version = "1.0.0"
        self.config = config
        
        # é…ç½®å‚æ•°
        self.enable_cleaning = config.get('enable_cleaning', True)
        self.enable_augmentation = config.get('enable_augmentation', True)
        self.augmentation_ratio = config.get('augmentation_ratio', 0.2)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'processed_samples': 0,
            'augmented_samples': 0,
            'cleaned_samples': 0,
            'filtered_samples': 0
        }
        
        print(f"âœ… æ•°æ®å¤„ç†å™¨æ’ä»¶åˆå§‹åŒ–å®Œæˆ")
    
    # ==================== æ–‡æœ¬æ¸…æ´— ====================
    
    def clean_text(self, text: str) -> str:
        """
        æ¸…æ´—æ–‡æœ¬æ•°æ®
        
        åŠŸèƒ½:
        - å»é™¤å¤šä½™ç©ºæ ¼
        - ç»Ÿä¸€æ ‡ç‚¹ç¬¦å·
        - å»é™¤ç‰¹æ®Šå­—ç¬¦
        - ä¿®æ­£å¸¸è§é”™è¯¯
        """
        if not text:
            return ""
        
        # å»é™¤å¤šä½™ç©ºæ ¼
        text = re.sub(r'\s+', ' ', text)
        text = text.strip()
        
        # ç»Ÿä¸€å¼•å·
        text = text.replace('"', '"').replace('"', '"')
        text = text.replace(''', "'").replace(''', "'")
        
        # å»é™¤é›¶å®½å­—ç¬¦
        text = re.sub(r'[\u200b\u200c\u200d\ufeff]', '', text)
        
        # ä¿®æ­£å¸¸è§æ‹¼å†™é”™è¯¯ï¼ˆå¯æ‰©å±•ï¼‰
        corrections = {
            'teh': 'the',
            'recieve': 'receive',
            'occured': 'occurred',
        }
        
        for wrong, correct in corrections.items():
            text = re.sub(r'\b' + wrong + r'\b', correct, text, flags=re.IGNORECASE)
        
        return text
    
    def normalize_text(self, text: str, lowercase: bool = False) -> str:
        """
        æ ‡å‡†åŒ–æ–‡æœ¬
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            lowercase: æ˜¯å¦è½¬å°å†™
        """
        text = self.clean_text(text)
        
        if lowercase:
            text = text.lower()
        
        # æ ‡å‡†åŒ–æ•°å­—ï¼ˆå¯é€‰ï¼‰
        if self.config.get('normalize_numbers', False):
            text = re.sub(r'\d+', '<NUM>', text)
        
        # æ ‡å‡†åŒ–URLï¼ˆå¯é€‰ï¼‰
        if self.config.get('normalize_urls', False):
            text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '<URL>', text)
        
        return text
    
    def remove_duplicates(self, texts: List[str]) -> List[str]:
        """å»é™¤é‡å¤æ–‡æœ¬"""
        seen = set()
        unique_texts = []
        
        for text in texts:
            normalized = self.normalize_text(text, lowercase=True)
            if normalized not in seen:
                seen.add(normalized)
                unique_texts.append(text)
        
        removed = len(texts) - len(unique_texts)
        print(f"ğŸ“Š å»é™¤é‡å¤: {removed} æ¡ ({removed/len(texts)*100:.1f}%)")
        
        return unique_texts
    
    # ==================== æ•°æ®å¢å¼º ====================
    
    def augment_text(self, text: str, methods: List[str] = None) -> List[str]:
        """
        æ–‡æœ¬æ•°æ®å¢å¼º
        
        æ”¯æŒçš„æ–¹æ³•:
        - synonym_replacement: åŒä¹‰è¯æ›¿æ¢
        - random_insertion: éšæœºæ’å…¥
        - random_swap: éšæœºäº¤æ¢
        - random_deletion: éšæœºåˆ é™¤
        - back_translation: å›è¯‘ï¼ˆéœ€è¦ç¿»è¯‘æ¨¡å‹ï¼‰
        """
        if methods is None:
            methods = ['synonym_replacement', 'random_swap']
        
        augmented = [text]  # ä¿ç•™åŸæ–‡
        
        for method in methods:
            if method == 'synonym_replacement':
                aug_text = self._synonym_replacement(text)
                if aug_text != text:
                    augmented.append(aug_text)
            
            elif method == 'random_insertion':
                aug_text = self._random_insertion(text)
                if aug_text != text:
                    augmented.append(aug_text)
            
            elif method == 'random_swap':
                aug_text = self._random_swap(text)
                if aug_text != text:
                    augmented.append(aug_text)
            
            elif method == 'random_deletion':
                aug_text = self._random_deletion(text)
                if aug_text != text:
                    augmented.append(aug_text)
        
        return augmented
    
    def _synonym_replacement(self, text: str, n: int = 2) -> str:
        """åŒä¹‰è¯æ›¿æ¢"""
        words = text.split()
        
        # ç®€å•çš„åŒä¹‰è¯å­—å…¸ï¼ˆå®é™…åº”ç”¨ä¸­åº”ä½¿ç”¨WordNetç­‰ï¼‰
        synonyms = {
            'good': ['great', 'excellent', 'fine'],
            'bad': ['poor', 'terrible', 'awful'],
            'big': ['large', 'huge', 'enormous'],
            'small': ['tiny', 'little', 'mini'],
            'happy': ['joyful', 'pleased', 'delighted'],
            'sad': ['unhappy', 'sorrowful', 'depressed'],
        }
        
        # éšæœºæ›¿æ¢nä¸ªè¯
        replaceable_indices = [i for i, word in enumerate(words) if word.lower() in synonyms]
        
        if not replaceable_indices:
            return text
        
        replace_count = min(n, len(replaceable_indices))
        indices_to_replace = random.sample(replaceable_indices, replace_count)
        
        for idx in indices_to_replace:
            word = words[idx].lower()
            if word in synonyms:
                words[idx] = random.choice(synonyms[word])
        
        return ' '.join(words)
    
    def _random_insertion(self, text: str, n: int = 1) -> str:
        """éšæœºæ’å…¥"""
        words = text.split()
        
        for _ in range(n):
            # éšæœºé€‰æ‹©ä¸€ä¸ªè¯
            random_word = random.choice(words)
            # éšæœºæ’å…¥ä½ç½®
            random_idx = random.randint(0, len(words))
            words.insert(random_idx, random_word)
        
        return ' '.join(words)
    
    def _random_swap(self, text: str, n: int = 2) -> str:
        """éšæœºäº¤æ¢"""
        words = text.split()
        
        if len(words) < 2:
            return text
        
        for _ in range(n):
            idx1, idx2 = random.sample(range(len(words)), 2)
            words[idx1], words[idx2] = words[idx2], words[idx1]
        
        return ' '.join(words)
    
    def _random_deletion(self, text: str, p: float = 0.1) -> str:
        """éšæœºåˆ é™¤"""
        words = text.split()
        
        if len(words) == 1:
            return text
        
        new_words = [word for word in words if random.random() > p]
        
        # ç¡®ä¿è‡³å°‘ä¿ç•™ä¸€ä¸ªè¯
        if len(new_words) == 0:
            return random.choice(words)
        
        return ' '.join(new_words)
    
    def augment_dataset(
        self,
        data: List[Dict[str, Any]],
        text_key: str = 'text',
        augmentation_factor: float = 0.5
    ) -> List[Dict[str, Any]]:
        """
        å¯¹æ•´ä¸ªæ•°æ®é›†è¿›è¡Œå¢å¼º
        
        Args:
            data: æ•°æ®åˆ—è¡¨
            text_key: æ–‡æœ¬å­—æ®µçš„é”®å
            augmentation_factor: å¢å¼ºå› å­ (0-1)ï¼Œè¡¨ç¤ºå¢å¼ºåæ•°æ®é‡å åŸæ•°æ®çš„æ¯”ä¾‹
        """
        print(f"ğŸ”„ å¼€å§‹æ•°æ®å¢å¼º (å¢å¼ºå› å­: {augmentation_factor})...")
        
        augmented_data = data.copy()
        num_to_augment = int(len(data) * augmentation_factor)
        
        # éšæœºé€‰æ‹©è¦å¢å¼ºçš„æ ·æœ¬
        samples_to_augment = random.sample(data, num_to_augment)
        
        for sample in samples_to_augment:
            text = sample[text_key]
            augmented_texts = self.augment_text(text)
            
            # æ·»åŠ å¢å¼ºæ ·æœ¬ï¼ˆé™¤äº†åŸæ–‡ï¼‰
            for aug_text in augmented_texts[1:]:
                new_sample = sample.copy()
                new_sample[text_key] = aug_text
                augmented_data.append(new_sample)
        
        self.stats['augmented_samples'] += len(augmented_data) - len(data)
        
        print(f"âœ… æ•°æ®å¢å¼ºå®Œæˆ: {len(data)} -> {len(augmented_data)} (+{len(augmented_data) - len(data)})")
        
        return augmented_data
    
    # ==================== æ•°æ®å¹³è¡¡ ====================
    
    def balance_dataset(
        self,
        data: List[Dict[str, Any]],
        label_key: str = 'label',
        method: str = 'oversample'
    ) -> List[Dict[str, Any]]:
        """
        æ•°æ®å¹³è¡¡
        
        Args:
            data: æ•°æ®åˆ—è¡¨
            label_key: æ ‡ç­¾å­—æ®µçš„é”®å
            method: å¹³è¡¡æ–¹æ³• (oversample/undersample/smote)
        """
        print(f"âš–ï¸ å¼€å§‹æ•°æ®å¹³è¡¡ (æ–¹æ³•: {method})...")
        
        # ç»Ÿè®¡å„ç±»åˆ«æ ·æœ¬æ•°
        label_counts = Counter([item[label_key] for item in data])
        print(f"ğŸ“Š åŸå§‹åˆ†å¸ƒ: {dict(label_counts)}")
        
        if method == 'oversample':
            balanced_data = self._oversample(data, label_key, label_counts)
        elif method == 'undersample':
            balanced_data = self._undersample(data, label_key, label_counts)
        else:
            print(f"âš ï¸ æœªçŸ¥çš„å¹³è¡¡æ–¹æ³•: {method}")
            return data
        
        # ç»Ÿè®¡å¹³è¡¡åçš„åˆ†å¸ƒ
        new_label_counts = Counter([item[label_key] for item in balanced_data])
        print(f"ğŸ“Š å¹³è¡¡ååˆ†å¸ƒ: {dict(new_label_counts)}")
        
        return balanced_data
    
    def _oversample(
        self,
        data: List[Dict[str, Any]],
        label_key: str,
        label_counts: Counter
    ) -> List[Dict[str, Any]]:
        """è¿‡é‡‡æ · - å¤åˆ¶å°‘æ•°ç±»æ ·æœ¬"""
        max_count = max(label_counts.values())
        
        balanced_data = data.copy()
        
        # æŒ‰æ ‡ç­¾åˆ†ç»„
        grouped = {}
        for item in data:
            label = item[label_key]
            if label not in grouped:
                grouped[label] = []
            grouped[label].append(item)
        
        # å¯¹æ¯ä¸ªç±»åˆ«è¿›è¡Œè¿‡é‡‡æ ·
        for label, samples in grouped.items():
            current_count = len(samples)
            need_count = max_count - current_count
            
            if need_count > 0:
                # éšæœºå¤åˆ¶æ ·æœ¬
                oversampled = random.choices(samples, k=need_count)
                balanced_data.extend(oversampled)
        
        return balanced_data
    
    def _undersample(
        self,
        data: List[Dict[str, Any]],
        label_key: str,
        label_counts: Counter
    ) -> List[Dict[str, Any]]:
        """æ¬ é‡‡æ · - åˆ é™¤å¤šæ•°ç±»æ ·æœ¬"""
        min_count = min(label_counts.values())
        
        # æŒ‰æ ‡ç­¾åˆ†ç»„
        grouped = {}
        for item in data:
            label = item[label_key]
            if label not in grouped:
                grouped[label] = []
            grouped[label].append(item)
        
        # å¯¹æ¯ä¸ªç±»åˆ«è¿›è¡Œæ¬ é‡‡æ ·
        balanced_data = []
        for label, samples in grouped.items():
            # éšæœºé€‰æ‹©min_countä¸ªæ ·æœ¬
            undersampled = random.sample(samples, min_count)
            balanced_data.extend(undersampled)
        
        return balanced_data
    
    # ==================== ç‰¹å¾å·¥ç¨‹ ====================
    
    def extract_features(
        self,
        text: str,
        include_stats: bool = True,
        include_ngrams: bool = True
    ) -> Dict[str, Any]:
        """
        æå–æ–‡æœ¬ç‰¹å¾
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            include_stats: æ˜¯å¦åŒ…å«ç»Ÿè®¡ç‰¹å¾
            include_ngrams: æ˜¯å¦åŒ…å«n-gramç‰¹å¾
        """
        features = {}
        
        words = text.split()
        
        if include_stats:
            # ç»Ÿè®¡ç‰¹å¾
            features['length'] = len(text)
            features['word_count'] = len(words)
            features['avg_word_length'] = np.mean([len(w) for w in words]) if words else 0
            features['sentence_count'] = len(re.split(r'[.!?]+', text))
            features['unique_word_ratio'] = len(set(words)) / len(words) if words else 0
            
            # æ ‡ç‚¹ç¬¦å·ç»Ÿè®¡
            features['punctuation_count'] = sum(1 for c in text if c in ',.!?;:')
            features['uppercase_ratio'] = sum(1 for c in text if c.isupper()) / len(text) if text else 0
        
        if include_ngrams:
            # n-gramç‰¹å¾
            features['bigrams'] = self._extract_ngrams(words, 2)
            features['trigrams'] = self._extract_ngrams(words, 3)
        
        return features
    
    def _extract_ngrams(self, words: List[str], n: int) -> List[str]:
        """æå–n-gram"""
        ngrams = []
        for i in range(len(words) - n + 1):
            ngram = ' '.join(words[i:i+n])
            ngrams.append(ngram)
        return ngrams
    
    def add_features_to_dataset(
        self,
        data: List[Dict[str, Any]],
        text_key: str = 'text'
    ) -> List[Dict[str, Any]]:
        """ä¸ºæ•°æ®é›†æ·»åŠ ç‰¹å¾"""
        print("ğŸ”§ æå–ç‰¹å¾...")
        
        enhanced_data = []
        for item in data:
            new_item = item.copy()
            features = self.extract_features(item[text_key])
            new_item['features'] = features
            enhanced_data.append(new_item)
        
        print(f"âœ… ç‰¹å¾æå–å®Œæˆ")
        return enhanced_data
    
    # ==================== æ•°æ®è´¨é‡æ£€æŸ¥ ====================
    
    def check_quality(
        self,
        data: List[Dict[str, Any]],
        text_key: str = 'text',
        min_length: int = 10,
        max_length: int = 10000
    ) -> Dict[str, Any]:
        """
        æ•°æ®è´¨é‡æ£€æŸ¥
        
        æ£€æŸ¥é¡¹:
        - ç©ºæ–‡æœ¬
        - è¿‡çŸ­/è¿‡é•¿æ–‡æœ¬
        - é‡å¤æ–‡æœ¬
        - å¼‚å¸¸å­—ç¬¦
        """
        print("ğŸ” å¼€å§‹æ•°æ®è´¨é‡æ£€æŸ¥...")
        
        issues = {
            'empty': [],
            'too_short': [],
            'too_long': [],
            'duplicates': [],
            'unusual_chars': []
        }
        
        seen_texts = {}
        
        for idx, item in enumerate(data):
            text = item.get(text_key, '')
            
            # æ£€æŸ¥ç©ºæ–‡æœ¬
            if not text or not text.strip():
                issues['empty'].append(idx)
                continue
            
            # æ£€æŸ¥é•¿åº¦
            if len(text) < min_length:
                issues['too_short'].append(idx)
            elif len(text) > max_length:
                issues['too_long'].append(idx)
            
            # æ£€æŸ¥é‡å¤
            text_normalized = self.normalize_text(text, lowercase=True)
            if text_normalized in seen_texts:
                issues['duplicates'].append((idx, seen_texts[text_normalized]))
            else:
                seen_texts[text_normalized] = idx
            
            # æ£€æŸ¥å¼‚å¸¸å­—ç¬¦
            if self._has_unusual_chars(text):
                issues['unusual_chars'].append(idx)
        
        # æ‰“å°æŠ¥å‘Š
        print("\nğŸ“‹ è´¨é‡æ£€æŸ¥æŠ¥å‘Š:")
        print(f"  æ€»æ ·æœ¬æ•°: {len(data)}")
        print(f"  ç©ºæ–‡æœ¬: {len(issues['empty'])}")
        print(f"  è¿‡çŸ­æ–‡æœ¬ (<{min_length}å­—ç¬¦): {len(issues['too_short'])}")
        print(f"  è¿‡é•¿æ–‡æœ¬ (>{max_length}å­—ç¬¦): {len(issues['too_long'])}")
        print(f"  é‡å¤æ–‡æœ¬: {len(issues['duplicates'])}")
        print(f"  å¼‚å¸¸å­—ç¬¦: {len(issues['unusual_chars'])}")
        
        return issues
    
    def _has_unusual_chars(self, text: str) -> bool:
        """æ£€æŸ¥æ˜¯å¦åŒ…å«å¼‚å¸¸å­—ç¬¦"""
        # æ£€æŸ¥æ˜¯å¦åŒ…å«è¿‡å¤šçš„éASCIIå­—ç¬¦
        non_ascii_ratio = sum(1 for c in text if ord(c) > 127) / len(text) if text else 0
        
        # å¦‚æœéASCIIå­—ç¬¦è¶…è¿‡80%ï¼Œå¯èƒ½æ˜¯å¼‚å¸¸
        if non_ascii_ratio > 0.8:
            return True
        
        # æ£€æŸ¥æ˜¯å¦åŒ…å«æ§åˆ¶å­—ç¬¦
        control_chars = [c for c in text if ord(c) < 32 and c not in '\n\r\t']
        if control_chars:
            return True
        
        return False
    
    def filter_by_quality(
        self,
        data: List[Dict[str, Any]],
        issues: Dict[str, Any],
        remove_types: List[str] = None
    ) -> List[Dict[str, Any]]:
        """æ ¹æ®è´¨é‡é—®é¢˜è¿‡æ»¤æ•°æ®"""
        if remove_types is None:
            remove_types = ['empty', 'too_short', 'unusual_chars']
        
        print(f"ğŸ§¹ æ ¹æ®è´¨é‡é—®é¢˜è¿‡æ»¤æ•°æ® (ç§»é™¤ç±»å‹: {remove_types})...")
        
        # æ”¶é›†è¦ç§»é™¤çš„ç´¢å¼•
        indices_to_remove = set()
        for issue_type in remove_types:
            if issue_type in issues:
                if issue_type == 'duplicates':
                    # å¯¹äºé‡å¤ï¼Œåªç§»é™¤åå‡ºç°çš„é‚£ä¸ª
                    indices_to_remove.update([dup[0] for dup in issues[issue_type]])
                else:
                    indices_to_remove.update(issues[issue_type])
        
        # è¿‡æ»¤æ•°æ®
        filtered_data = [item for idx, item in enumerate(data) if idx not in indices_to_remove]
        
        removed = len(data) - len(filtered_data)
        self.stats['filtered_samples'] += removed
        
        print(f"âœ… è¿‡æ»¤å®Œæˆ: {len(data)} -> {len(filtered_data)} (ç§»é™¤ {removed} æ¡)")
        
        return filtered_data
    
    # ==================== æ‰¹å¤„ç†ç®¡é“ ====================
    
    def process_pipeline(
        self,
        data: List[Dict[str, Any]],
        text_key: str = 'text',
        label_key: str = 'label',
        steps: List[str] = None
    ) -> List[Dict[str, Any]]:
        """
        æ•°æ®å¤„ç†ç®¡é“
        
        Args:
            data: åŸå§‹æ•°æ®
            text_key: æ–‡æœ¬å­—æ®µé”®å
            label_key: æ ‡ç­¾å­—æ®µé”®å
            steps: å¤„ç†æ­¥éª¤åˆ—è¡¨
        """
        if steps is None:
            steps = ['clean', 'quality_check', 'augment', 'balance']
        
        print("=" * 60)
        print("ğŸ”„ æ•°æ®å¤„ç†ç®¡é“å¯åŠ¨")
        print(f"ğŸ“Š åˆå§‹æ ·æœ¬æ•°: {len(data)}")
        print(f"ğŸ› ï¸ å¤„ç†æ­¥éª¤: {' -> '.join(steps)}")
        print("=" * 60)
        
        processed_data = data
        
        for step in steps:
            print(f"\nâ–¶ï¸ æ‰§è¡Œæ­¥éª¤: {step}")
            
            if step == 'clean':
                # æ¸…æ´—æ–‡æœ¬
                for item in processed_data:
                    item[text_key] = self.clean_text(item[text_key])
                self.stats['cleaned_samples'] = len(processed_data)
            
            elif step == 'quality_check':
                # è´¨é‡æ£€æŸ¥å¹¶è¿‡æ»¤
                issues = self.check_quality(processed_data, text_key)
                processed_data = self.filter_by_quality(processed_data, issues)
            
            elif step == 'remove_duplicates':
                # å»é‡
                texts = [item[text_key] for item in processed_data]
                unique_texts = self.remove_duplicates(texts)
                processed_data = [item for item in processed_data if item[text_key] in unique_texts]
            
            elif step == 'augment':
                # æ•°æ®å¢å¼º
                processed_data = self.augment_dataset(
                    processed_data,
                    text_key,
                    augmentation_factor=self.augmentation_ratio
                )
            
            elif step == 'balance':
                # æ•°æ®å¹³è¡¡
                if label_key in processed_data[0]:
                    processed_data = self.balance_dataset(
                        processed_data,
                        label_key,
                        method='oversample'
                    )
            
            elif step == 'extract_features':
                # ç‰¹å¾æå–
                processed_data = self.add_features_to_dataset(processed_data, text_key)
        
        self.stats['processed_samples'] = len(processed_data)
        
        print("\n" + "=" * 60)
        print("âœ… æ•°æ®å¤„ç†ç®¡é“å®Œæˆ")
        print(f"ğŸ“Š æœ€ç»ˆæ ·æœ¬æ•°: {len(processed_data)}")
        print(f"ğŸ“ˆ å¤„ç†ç»Ÿè®¡: {self.stats}")
        print("=" * 60)
        
        return processed_data
    
    # ==================== æ’ä»¶é’©å­ ====================
    
    def on_data_load(self, context: Dict[str, Any]):
        """æ•°æ®åŠ è½½æ—¶çš„é’©å­"""
        data = context.get('data', [])
        
        if self.config.get('auto_process', False):
            print("ğŸ”„ è‡ªåŠ¨å¯åŠ¨æ•°æ®å¤„ç†...")
            processed_data = self.process_pipeline(data)
            context['data'] = processed_data
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
        return self.stats


# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ› ï¸ æ•°æ®å¤„ç†å™¨æ’ä»¶ (Data Processors Plugin)")
    print("=" * 60)
    
    # é…ç½®
    config = {
        'enable_cleaning': True,
        'enable_augmentation': True,
        'augmentation_ratio': 0.3,
        'normalize_numbers': False,
        'normalize_urls': True,
        'auto_process': False
    }
    
    plugin = DataProcessorsPlugin(config)
    
    # ç¤ºä¾‹æ•°æ®
    sample_data = [
        {'text': 'This is a  good   example.', 'label': 0},
        {'text': 'Another great sample here!', 'label': 1},
        {'text': 'Bad  quality text...', 'label': 0},
        {'text': 'This is a good example.', 'label': 0},  # é‡å¤
        {'text': 'x', 'label': 1},  # å¤ªçŸ­
    ]
    
    print("\nğŸ“ ç¤ºä¾‹æ•°æ®:")
    for i, item in enumerate(sample_data):
        print(f"  {i+1}. {item}")
    
    # è¿è¡Œå¤„ç†ç®¡é“
    processed = plugin.process_pipeline(
        sample_data,
        steps=['clean', 'quality_check', 'remove_duplicates', 'augment', 'balance']
    )
    
    print("\nğŸ“ å¤„ç†åæ•°æ®æ ·æœ¬:")
    for i, item in enumerate(processed[:5]):
        print(f"  {i+1}. {item['text'][:50]}... (label: {item.get('label', 'N/A')})")
    
    print("\nğŸ’¡ æ’ä»¶åŠŸèƒ½:")
    print("1. ğŸ§¹ æ–‡æœ¬æ¸…æ´—å’Œæ ‡å‡†åŒ–")
    print("2. ğŸ”„ å¤šç§æ•°æ®å¢å¼ºæ–¹æ³•")
    print("3. âš–ï¸ æ•°æ®å¹³è¡¡ (è¿‡é‡‡æ ·/æ¬ é‡‡æ ·)")
    print("4. ğŸ”§ è‡ªåŠ¨ç‰¹å¾æå–")
    print("5. ğŸ” æ•°æ®è´¨é‡æ£€æŸ¥")
    print("6. ğŸ”— å®Œæ•´çš„å¤„ç†ç®¡é“")
