"""
Advanced Debugging Plugin for APT
é«˜çº§è°ƒè¯•æ’ä»¶ - æä¾›å…¨æ–¹ä½çš„è®­ç»ƒè°ƒè¯•å’Œè¯Šæ–­å·¥å…·
"""

import torch
import torch.nn as nn
import numpy as np
from typing import Dict, Any, List, Optional, Callable
import time
import json
import os
from collections import defaultdict
import matplotlib.pyplot as plt
import traceback
import sys
from contextlib import contextmanager


class AdvancedDebuggingPlugin:
    """
    é«˜çº§è°ƒè¯•æ’ä»¶
    
    æä¾›åŠŸèƒ½:
    1. æ¢¯åº¦ç›‘æ§å’Œåˆ†æ
    2. æ¿€æ´»å€¼ç»Ÿè®¡
    3. å†…å­˜ä½¿ç”¨è¿½è¸ª
    4. æ€§èƒ½åˆ†æ (Profiling)
    5. å¼‚å¸¸æ£€æµ‹å’Œè¯Šæ–­
    6. è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–
    7. å‚æ•°å˜åŒ–è¿½è¸ª
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.name = "advanced-debugging"
        self.version = "1.0.0"
        self.config = config
        
        # è°ƒè¯•æ¨¡å¼
        self.debug_level = config.get('debug_level', 'normal')  # minimal/normal/verbose
        self.log_interval = config.get('log_interval', 100)
        
        # ç›‘æ§é€‰é¡¹
        self.monitor_gradients = config.get('monitor_gradients', True)
        self.monitor_activations = config.get('monitor_activations', True)
        self.monitor_memory = config.get('monitor_memory', True)
        self.monitor_performance = config.get('monitor_performance', True)
        
        # å­˜å‚¨è·¯å¾„
        self.output_dir = config.get('output_dir', './debug_output')
        os.makedirs(self.output_dir, exist_ok=True)
        
        # ç›‘æ§æ•°æ®
        self.gradient_stats = defaultdict(list)
        self.activation_stats = defaultdict(list)
        self.memory_stats = []
        self.performance_stats = []
        self.parameter_history = defaultdict(list)
        
        # å¼‚å¸¸è®°å½•
        self.anomalies = []
        
        # é’©å­å¥æŸ„
        self.hooks = []
        
        print(f"ğŸ› é«˜çº§è°ƒè¯•æ’ä»¶åˆå§‹åŒ–å®Œæˆ (è°ƒè¯•çº§åˆ«: {self.debug_level})")
    
    # ==================== æ¢¯åº¦ç›‘æ§ ====================
    
    def register_gradient_hooks(self, model: nn.Module):
        """æ³¨å†Œæ¢¯åº¦ç›‘æ§é’©å­"""
        if not self.monitor_gradients:
            return
        
        print("ğŸ“Š æ³¨å†Œæ¢¯åº¦ç›‘æ§é’©å­...")
        
        def gradient_hook(module, grad_input, grad_output):
            """æ¢¯åº¦é’©å­å‡½æ•°"""
            module_name = self._get_module_name(model, module)
            
            if grad_output[0] is not None:
                grad = grad_output[0]
                
                stats = {
                    'mean': grad.abs().mean().item(),
                    'std': grad.std().item(),
                    'max': grad.abs().max().item(),
                    'min': grad.abs().min().item(),
                    'norm': grad.norm().item(),
                }
                
                self.gradient_stats[module_name].append(stats)
                
                # æ£€æµ‹æ¢¯åº¦å¼‚å¸¸
                self._check_gradient_anomaly(module_name, stats)
        
        # ä¸ºæ‰€æœ‰æ¨¡å—æ³¨å†Œé’©å­
        for name, module in model.named_modules():
            if len(list(module.children())) == 0:  # åªç›‘æ§å¶å­æ¨¡å—
                hook = module.register_full_backward_hook(gradient_hook)
                self.hooks.append(hook)
        
        print(f"âœ… å·²æ³¨å†Œ {len(self.hooks)} ä¸ªæ¢¯åº¦ç›‘æ§é’©å­")
    
    def _check_gradient_anomaly(self, module_name: str, stats: Dict[str, float]):
        """æ£€æµ‹æ¢¯åº¦å¼‚å¸¸"""
        threshold = self.config.get('gradient_threshold', 10.0)
        
        # æ£€æµ‹æ¢¯åº¦çˆ†ç‚¸
        if stats['max'] > threshold:
            anomaly = {
                'type': 'gradient_explosion',
                'module': module_name,
                'value': stats['max'],
                'threshold': threshold,
                'timestamp': time.time()
            }
            self.anomalies.append(anomaly)
            print(f"âš ï¸ æ¢¯åº¦çˆ†ç‚¸è­¦å‘Š: {module_name} (max={stats['max']:.4f})")
        
        # æ£€æµ‹æ¢¯åº¦æ¶ˆå¤±
        if stats['mean'] < 1e-7:
            anomaly = {
                'type': 'gradient_vanishing',
                'module': module_name,
                'value': stats['mean'],
                'timestamp': time.time()
            }
            self.anomalies.append(anomaly)
            print(f"âš ï¸ æ¢¯åº¦æ¶ˆå¤±è­¦å‘Š: {module_name} (mean={stats['mean']:.2e})")
    
    def get_gradient_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ¢¯åº¦åˆ†ææŠ¥å‘Š"""
        report = {}
        
        for module_name, stats_list in self.gradient_stats.items():
            if not stats_list:
                continue
            
            # è®¡ç®—ç»Ÿè®¡é‡
            means = [s['mean'] for s in stats_list]
            maxs = [s['max'] for s in stats_list]
            norms = [s['norm'] for s in stats_list]
            
            report[module_name] = {
                'avg_mean': np.mean(means),
                'avg_max': np.mean(maxs),
                'avg_norm': np.mean(norms),
                'trend': 'increasing' if means[-1] > means[0] else 'decreasing',
                'num_samples': len(stats_list)
            }
        
        return report
    
    # ==================== æ¿€æ´»å€¼ç›‘æ§ ====================
    
    def register_activation_hooks(self, model: nn.Module):
        """æ³¨å†Œæ¿€æ´»å€¼ç›‘æ§é’©å­"""
        if not self.monitor_activations:
            return
        
        print("ğŸ“Š æ³¨å†Œæ¿€æ´»å€¼ç›‘æ§é’©å­...")
        
        def activation_hook(module, input, output):
            """æ¿€æ´»å€¼é’©å­å‡½æ•°"""
            module_name = self._get_module_name(model, module)
            
            if isinstance(output, torch.Tensor):
                stats = {
                    'mean': output.mean().item(),
                    'std': output.std().item(),
                    'max': output.max().item(),
                    'min': output.min().item(),
                    'sparsity': (output == 0).float().mean().item(),
                }
                
                self.activation_stats[module_name].append(stats)
                
                # æ£€æµ‹æ¿€æ´»å¼‚å¸¸
                self._check_activation_anomaly(module_name, stats)
        
        # ä¸ºæ‰€æœ‰æ¨¡å—æ³¨å†Œé’©å­
        hook_count = 0
        for name, module in model.named_modules():
            if len(list(module.children())) == 0:  # åªç›‘æ§å¶å­æ¨¡å—
                hook = module.register_forward_hook(activation_hook)
                self.hooks.append(hook)
                hook_count += 1
        
        print(f"âœ… å·²æ³¨å†Œ {hook_count} ä¸ªæ¿€æ´»å€¼ç›‘æ§é’©å­")
    
    def _check_activation_anomaly(self, module_name: str, stats: Dict[str, float]):
        """æ£€æµ‹æ¿€æ´»å€¼å¼‚å¸¸"""
        # æ£€æµ‹æ­»ç¥ç»å…ƒ (è¿‡é«˜çš„ç¨€ç–åº¦)
        if stats['sparsity'] > 0.9:
            anomaly = {
                'type': 'dead_neurons',
                'module': module_name,
                'sparsity': stats['sparsity'],
                'timestamp': time.time()
            }
            self.anomalies.append(anomaly)
            
            if self.debug_level == 'verbose':
                print(f"âš ï¸ æ­»ç¥ç»å…ƒè­¦å‘Š: {module_name} (ç¨€ç–åº¦={stats['sparsity']:.2%})")
        
        # æ£€æµ‹æ¿€æ´»å€¼é¥±å’Œ
        if abs(stats['mean']) > 10:
            anomaly = {
                'type': 'activation_saturation',
                'module': module_name,
                'mean': stats['mean'],
                'timestamp': time.time()
            }
            self.anomalies.append(anomaly)
            
            if self.debug_level == 'verbose':
                print(f"âš ï¸ æ¿€æ´»é¥±å’Œè­¦å‘Š: {module_name} (mean={stats['mean']:.2f})")
    
    # ==================== å†…å­˜ç›‘æ§ ====================
    
    def track_memory(self, step: int):
        """è¿½è¸ªå†…å­˜ä½¿ç”¨"""
        if not self.monitor_memory or not torch.cuda.is_available():
            return
        
        memory_stats = {
            'step': step,
            'allocated': torch.cuda.memory_allocated() / 1024**3,  # GB
            'reserved': torch.cuda.memory_reserved() / 1024**3,  # GB
            'max_allocated': torch.cuda.max_memory_allocated() / 1024**3,  # GB
            'timestamp': time.time()
        }
        
        self.memory_stats.append(memory_stats)
        
        # æ£€æµ‹å†…å­˜æ³„æ¼
        if len(self.memory_stats) > 10:
            recent_allocated = [s['allocated'] for s in self.memory_stats[-10:]]
            if recent_allocated[-1] > recent_allocated[0] * 1.5:
                print(f"âš ï¸ å¯èƒ½çš„å†…å­˜æ³„æ¼: {recent_allocated[0]:.2f}GB -> {recent_allocated[-1]:.2f}GB")
    
    def get_memory_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆå†…å­˜ä½¿ç”¨æŠ¥å‘Š"""
        if not self.memory_stats:
            return {}
        
        allocated = [s['allocated'] for s in self.memory_stats]
        
        return {
            'peak_memory_gb': max(allocated),
            'avg_memory_gb': np.mean(allocated),
            'current_memory_gb': allocated[-1],
            'memory_trend': 'increasing' if allocated[-1] > allocated[0] else 'stable',
        }
    
    # ==================== æ€§èƒ½åˆ†æ ====================
    
    @contextmanager
    def profile_section(self, name: str):
        """æ€§èƒ½åˆ†æä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        if not self.monitor_performance:
            yield
            return
        
        start_time = time.time()
        start_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0
        
        try:
            yield
        finally:
            end_time = time.time()
            end_memory = torch.cuda.memory_allocated() if torch.cuda.is_available() else 0
            
            stats = {
                'name': name,
                'duration': end_time - start_time,
                'memory_delta': (end_memory - start_memory) / 1024**2,  # MB
                'timestamp': time.time()
            }
            
            self.performance_stats.append(stats)
            
            if self.debug_level == 'verbose':
                print(f"â±ï¸ {name}: {stats['duration']:.3f}s, "
                      f"Memory: {stats['memory_delta']:+.1f}MB")
    
    def get_performance_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½åˆ†ææŠ¥å‘Š"""
        if not self.performance_stats:
            return {}
        
        # æŒ‰åç§°åˆ†ç»„
        grouped = defaultdict(list)
        for stat in self.performance_stats:
            grouped[stat['name']].append(stat)
        
        report = {}
        for name, stats in grouped.items():
            durations = [s['duration'] for s in stats]
            report[name] = {
                'total_time': sum(durations),
                'avg_time': np.mean(durations),
                'min_time': min(durations),
                'max_time': max(durations),
                'call_count': len(durations)
            }
        
        return report
    
    # ==================== å‚æ•°è¿½è¸ª ====================
    
    def track_parameters(self, model: nn.Module, step: int):
        """è¿½è¸ªæ¨¡å‹å‚æ•°å˜åŒ–"""
        for name, param in model.named_parameters():
            if param.requires_grad:
                stats = {
                    'step': step,
                    'mean': param.data.mean().item(),
                    'std': param.data.std().item(),
                    'norm': param.data.norm().item(),
                    'grad_norm': param.grad.norm().item() if param.grad is not None else 0,
                }
                
                self.parameter_history[name].append(stats)
    
    def get_parameter_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆå‚æ•°å˜åŒ–æŠ¥å‘Š"""
        report = {}
        
        for param_name, history in self.parameter_history.items():
            if len(history) < 2:
                continue
            
            norms = [h['norm'] for h in history]
            grad_norms = [h['grad_norm'] for h in history]
            
            report[param_name] = {
                'initial_norm': norms[0],
                'final_norm': norms[-1],
                'norm_change': norms[-1] - norms[0],
                'avg_grad_norm': np.mean(grad_norms),
                'max_grad_norm': max(grad_norms),
            }
        
        return report
    
    # ==================== å¼‚å¸¸è¯Šæ–­ ====================
    
    def diagnose_training(self, loss_history: List[float]) -> Dict[str, Any]:
        """è¯Šæ–­è®­ç»ƒé—®é¢˜"""
        diagnosis = {
            'status': 'healthy',
            'issues': [],
            'recommendations': []
        }
        
        if not loss_history:
            return diagnosis
        
        # æ£€æŸ¥æŸå¤±æ˜¯å¦ä¸ºNaNæˆ–Inf
        if any(np.isnan(loss) or np.isinf(loss) for loss in loss_history):
            diagnosis['status'] = 'critical'
            diagnosis['issues'].append('Loss contains NaN or Inf values')
            diagnosis['recommendations'].append('Reduce learning rate or check data quality')
        
        # æ£€æŸ¥æŸå¤±æ˜¯å¦ä¸ä¸‹é™
        if len(loss_history) > 100:
            recent_losses = loss_history[-100:]
            if max(recent_losses) - min(recent_losses) < 0.01:
                diagnosis['status'] = 'warning'
                diagnosis['issues'].append('Loss not decreasing')
                diagnosis['recommendations'].append('Increase learning rate or check model capacity')
        
        # æ£€æŸ¥æŸå¤±æ˜¯å¦éœ‡è¡
        if len(loss_history) > 10:
            recent_losses = loss_history[-10:]
            variance = np.var(recent_losses)
            if variance > np.mean(recent_losses):
                diagnosis['status'] = 'warning'
                diagnosis['issues'].append('Loss is oscillating')
                diagnosis['recommendations'].append('Reduce learning rate or use gradient clipping')
        
        # æ£€æŸ¥æ¢¯åº¦é—®é¢˜
        gradient_report = self.get_gradient_report()
        for module_name, stats in gradient_report.items():
            if stats['avg_max'] > 10:
                diagnosis['issues'].append(f'Gradient explosion in {module_name}')
                diagnosis['recommendations'].append('Use gradient clipping')
            elif stats['avg_mean'] < 1e-7:
                diagnosis['issues'].append(f'Gradient vanishing in {module_name}')
                diagnosis['recommendations'].append('Use residual connections or different activation')
        
        return diagnosis
    
    # ==================== å¯è§†åŒ– ====================
    
    def visualize_gradients(self, save_path: Optional[str] = None):
        """å¯è§†åŒ–æ¢¯åº¦ç»Ÿè®¡"""
        if not self.gradient_stats:
            print("âš ï¸ æ²¡æœ‰æ¢¯åº¦æ•°æ®å¯è§†åŒ–")
            return
        
        fig, axes = plt.subplots(2, 2, figsize=(15, 10))
        fig.suptitle('Gradient Statistics', fontsize=16)
        
        # é€‰æ‹©å‡ ä¸ªä»£è¡¨æ€§çš„å±‚
        sample_modules = list(self.gradient_stats.keys())[:4]
        
        for idx, module_name in enumerate(sample_modules):
            if idx >= 4:
                break
            
            row, col = idx // 2, idx % 2
            ax = axes[row, col]
            
            stats_list = self.gradient_stats[module_name]
            means = [s['mean'] for s in stats_list]
            
            ax.plot(means, label='Mean Gradient')
            ax.set_title(f'{module_name}')
            ax.set_xlabel('Step')
            ax.set_ylabel('Gradient Magnitude')
            ax.legend()
            ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_path:
            plt.savefig(save_path)
            print(f"âœ… æ¢¯åº¦å¯è§†åŒ–å·²ä¿å­˜åˆ°: {save_path}")
        else:
            plt.savefig(os.path.join(self.output_dir, 'gradients.png'))
            print(f"âœ… æ¢¯åº¦å¯è§†åŒ–å·²ä¿å­˜åˆ°: {self.output_dir}/gradients.png")
        
        plt.close()
    
    def visualize_memory(self, save_path: Optional[str] = None):
        """å¯è§†åŒ–å†…å­˜ä½¿ç”¨"""
        if not self.memory_stats:
            print("âš ï¸ æ²¡æœ‰å†…å­˜æ•°æ®å¯è§†åŒ–")
            return
        
        steps = [s['step'] for s in self.memory_stats]
        allocated = [s['allocated'] for s in self.memory_stats]
        reserved = [s['reserved'] for s in self.memory_stats]
        
        plt.figure(figsize=(12, 6))
        plt.plot(steps, allocated, label='Allocated', marker='o', markersize=3)
        plt.plot(steps, reserved, label='Reserved', marker='s', markersize=3)
        plt.xlabel('Training Step')
        plt.ylabel('Memory (GB)')
        plt.title('GPU Memory Usage')
        plt.legend()
        plt.grid(True, alpha=0.3)
        
        if save_path:
            plt.savefig(save_path)
            print(f"âœ… å†…å­˜å¯è§†åŒ–å·²ä¿å­˜åˆ°: {save_path}")
        else:
            plt.savefig(os.path.join(self.output_dir, 'memory.png'))
            print(f"âœ… å†…å­˜å¯è§†åŒ–å·²ä¿å­˜åˆ°: {self.output_dir}/memory.png")
        
        plt.close()
    
    # ==================== æŠ¥å‘Šç”Ÿæˆ ====================
    
    def generate_full_report(self, save_path: Optional[str] = None) -> Dict[str, Any]:
        """ç”Ÿæˆå®Œæ•´çš„è°ƒè¯•æŠ¥å‘Š"""
        print("ğŸ“‹ ç”Ÿæˆå®Œæ•´è°ƒè¯•æŠ¥å‘Š...")
        
        report = {
            'timestamp': time.time(),
            'debug_level': self.debug_level,
            'gradient_report': self.get_gradient_report(),
            'memory_report': self.get_memory_report(),
            'performance_report': self.get_performance_report(),
            'parameter_report': self.get_parameter_report(),
            'anomalies': self.anomalies,
            'anomaly_summary': {
                'total': len(self.anomalies),
                'by_type': {}
            }
        }
        
        # ç»Ÿè®¡å¼‚å¸¸ç±»å‹
        for anomaly in self.anomalies:
            anomaly_type = anomaly['type']
            report['anomaly_summary']['by_type'][anomaly_type] = \
                report['anomaly_summary']['by_type'].get(anomaly_type, 0) + 1
        
        # ä¿å­˜æŠ¥å‘Š
        if save_path is None:
            save_path = os.path.join(self.output_dir, 'debug_report.json')
        
        with open(save_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, indent=2, ensure_ascii=False)
        
        print(f"âœ… è°ƒè¯•æŠ¥å‘Šå·²ä¿å­˜åˆ°: {save_path}")
        
        # ç”Ÿæˆå¯è§†åŒ–
        self.visualize_gradients()
        if self.memory_stats:
            self.visualize_memory()
        
        return report
    
    # ==================== è¾…åŠ©æ–¹æ³• ====================
    
    def _get_module_name(self, model: nn.Module, target_module: nn.Module) -> str:
        """è·å–æ¨¡å—åç§°"""
        for name, module in model.named_modules():
            if module is target_module:
                return name
        return "unknown"
    
    def cleanup(self):
        """æ¸…ç†é’©å­"""
        for hook in self.hooks:
            hook.remove()
        self.hooks.clear()
        print("ğŸ§¹ å·²æ¸…ç†æ‰€æœ‰é’©å­")
    
    # ==================== æ’ä»¶é’©å­ ====================
    
    def on_training_start(self, context: Dict[str, Any]):
        """è®­ç»ƒå¼€å§‹æ—¶çš„é’©å­"""
        model = context.get('model')
        
        if model:
            self.register_gradient_hooks(model)
            self.register_activation_hooks(model)
        
        print(f"ğŸ› è°ƒè¯•æ’ä»¶å·²å¯åŠ¨ (ç›‘æ§: æ¢¯åº¦={self.monitor_gradients}, "
              f"æ¿€æ´»={self.monitor_activations}, å†…å­˜={self.monitor_memory})")
    
    def on_batch_end(self, context: Dict[str, Any]):
        """æ‰¹æ¬¡ç»“æŸæ—¶çš„é’©å­"""
        step = context.get('step', 0)
        
        # è¿½è¸ªå†…å­˜
        if step % self.log_interval == 0:
            self.track_memory(step)
        
        # è¿½è¸ªå‚æ•°
        if self.config.get('track_parameters', False):
            model = context.get('model')
            if model and step % self.log_interval == 0:
                self.track_parameters(model, step)
    
    def on_training_end(self, context: Dict[str, Any]):
        """è®­ç»ƒç»“æŸæ—¶çš„é’©å­"""
        print("\n" + "=" * 60)
        print("ğŸ“Š ç”Ÿæˆæœ€ç»ˆè°ƒè¯•æŠ¥å‘Š...")
        
        # ç”Ÿæˆå®Œæ•´æŠ¥å‘Š
        report = self.generate_full_report()
        
        # æ‰“å°æ‘˜è¦
        print("\nğŸ“‹ è°ƒè¯•æ‘˜è¦:")
        print(f"  æ€»å¼‚å¸¸æ•°: {len(self.anomalies)}")
        
        if self.anomalies:
            print("  å¼‚å¸¸ç±»å‹åˆ†å¸ƒ:")
            anomaly_types = {}
            for anomaly in self.anomalies:
                anomaly_type = anomaly['type']
                anomaly_types[anomaly_type] = anomaly_types.get(anomaly_type, 0) + 1
            
            for anomaly_type, count in anomaly_types.items():
                print(f"    - {anomaly_type}: {count}")
        
        # æ¸…ç†
        self.cleanup()
        
        print("=" * 60)


# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================

if __name__ == "__main__":
    print("=" * 60)
    print("ğŸ› é«˜çº§è°ƒè¯•æ’ä»¶ (Advanced Debugging Plugin)")
    print("=" * 60)
    
    # é…ç½®
    config = {
        'debug_level': 'verbose',
        'log_interval': 10,
        'monitor_gradients': True,
        'monitor_activations': True,
        'monitor_memory': True,
        'monitor_performance': True,
        'track_parameters': True,
        'gradient_threshold': 10.0,
        'output_dir': './debug_output'
    }
    
    plugin = AdvancedDebuggingPlugin(config)
    
    print("\nğŸ’¡ æ’ä»¶åŠŸèƒ½:")
    print("1. ğŸ“Š æ¢¯åº¦ç›‘æ§å’Œå¼‚å¸¸æ£€æµ‹")
    print("2. ğŸ” æ¿€æ´»å€¼ç»Ÿè®¡å’Œåˆ†æ")
    print("3. ğŸ’¾ å†…å­˜ä½¿ç”¨è¿½è¸ª")
    print("4. â±ï¸ æ€§èƒ½åˆ†æå’Œprofiling")
    print("5. ğŸš¨ å¼‚å¸¸æ£€æµ‹å’Œè¯Šæ–­")
    print("6. ğŸ“ˆ è®­ç»ƒè¿‡ç¨‹å¯è§†åŒ–")
    print("7. ğŸ“ å®Œæ•´çš„è°ƒè¯•æŠ¥å‘Š")
    
    print("\nğŸ“ ä½¿ç”¨ç¤ºä¾‹:")
    print("""
    # åœ¨è®­ç»ƒå¼€å§‹æ—¶æ³¨å†Œé’©å­
    plugin.on_training_start({'model': model})
    
    # åœ¨è®­ç»ƒå¾ªç¯ä¸­
    for step, batch in enumerate(dataloader):
        # æ€§èƒ½åˆ†æ
        with plugin.profile_section('forward_pass'):
            outputs = model(batch)
        
        with plugin.profile_section('backward_pass'):
            loss.backward()
        
        # æ‰¹æ¬¡ç»“æŸ
        plugin.on_batch_end({'step': step, 'model': model})
    
    # è®­ç»ƒç»“æŸæ—¶ç”ŸæˆæŠ¥å‘Š
    plugin.on_training_end({})
    """)
    
    # æ¼”ç¤ºè¯Šæ–­åŠŸèƒ½
    print("\nğŸ” æ¼”ç¤ºè¯Šæ–­åŠŸèƒ½:")
    
    # æ¨¡æ‹Ÿä¸€äº›æŸå¤±å€¼
    loss_history = [2.5, 2.3, 2.1, float('nan'), 1.8]
    diagnosis = plugin.diagnose_training(loss_history)
    
    print(f"\nè¯Šæ–­ç»“æœ: {diagnosis['status']}")
    if diagnosis['issues']:
        print("å‘ç°çš„é—®é¢˜:")
        for issue in diagnosis['issues']:
            print(f"  - {issue}")
        print("å»ºè®®:")
        for rec in diagnosis['recommendations']:
            print(f"  - {rec}")
    
    print("\n" + "=" * 60)
