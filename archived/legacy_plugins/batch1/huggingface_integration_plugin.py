"""
HuggingFace Integration Plugin for APT Model
æä¾›ä¸HuggingFace Hubçš„å®Œæ•´é›†æˆåŠŸèƒ½
"""

import torch
from pathlib import Path
from typing import Optional, Dict, Any
from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments
from datasets import load_dataset, Dataset
from huggingface_hub import HfApi, create_repo, upload_folder, login

# å‡è®¾æ’ä»¶åŸºç±»å·²å­˜åœ¨
# from plugins.plugin_system import APTPlugin


class HuggingFaceIntegrationPlugin:
    """
    HuggingFaceé›†æˆæ’ä»¶
    
    åŠŸèƒ½:
    1. ä»HuggingFace Hubå¯¼å…¥/å¯¼å‡ºæ¨¡å‹
    2. åŠ è½½HuggingFaceæ•°æ®é›†
    3. ä½¿ç”¨HF Trainerè®­ç»ƒAPTæ¨¡å‹
    4. ä¸Šä¼ æ¨¡å‹åˆ°HuggingFace Hub
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.name = "huggingface-integration"
        self.version = "1.0.0"
        self.config = config
        self.api = HfApi()
        
    # ==================== æ¨¡å‹å¯¼å…¥/å¯¼å‡º ====================
    
    def export_to_huggingface(
        self,
        model: torch.nn.Module,
        tokenizer: Any,
        repo_name: str,
        private: bool = False,
        commit_message: str = "Upload APT model"
    ):
        """
        å¯¼å‡ºAPTæ¨¡å‹åˆ°HuggingFace Hub
        
        Args:
            model: APTæ¨¡å‹å®ä¾‹
            tokenizer: åˆ†è¯å™¨
            repo_name: ä»“åº“å (æ ¼å¼: username/model-name)
            private: æ˜¯å¦ä¸ºç§æœ‰ä»“åº“
            commit_message: æäº¤æ¶ˆæ¯
        """
        print(f"ğŸš€ æ­£åœ¨å°†æ¨¡å‹å¯¼å‡ºåˆ° HuggingFace Hub: {repo_name}")
        
        # 1. åˆ›å»ºæœ¬åœ°ä¿å­˜ç›®å½•
        save_dir = Path(f"./hf_export/{repo_name.split('/')[-1]}")
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # 2. ä¿å­˜æ¨¡å‹å’Œåˆ†è¯å™¨
        torch.save(model.state_dict(), save_dir / "pytorch_model.bin")
        tokenizer.save_pretrained(save_dir)
        
        # 3. åˆ›å»ºmodel_card
        self._create_model_card(save_dir, repo_name)
        
        # 4. åˆ›å»ºä»“åº“å¹¶ä¸Šä¼ 
        try:
            create_repo(repo_name, private=private, exist_ok=True)
            upload_folder(
                repo_id=repo_name,
                folder_path=str(save_dir),
                commit_message=commit_message
            )
            print(f"âœ… æ¨¡å‹æˆåŠŸä¸Šä¼ åˆ°: https://huggingface.co/{repo_name}")
        except Exception as e:
            print(f"âŒ ä¸Šä¼ å¤±è´¥: {e}")
            raise
    
    def import_from_huggingface(
        self,
        repo_name: str,
        local_dir: Optional[str] = None
    ) -> tuple:
        """
        ä»HuggingFace Hubå¯¼å…¥æ¨¡å‹
        
        Args:
            repo_name: ä»“åº“å
            local_dir: æœ¬åœ°ä¿å­˜ç›®å½•
            
        Returns:
            (model, tokenizer) å…ƒç»„
        """
        print(f"ğŸ“¥ æ­£åœ¨ä» HuggingFace Hub å¯¼å…¥æ¨¡å‹: {repo_name}")
        
        try:
            model = AutoModelForCausalLM.from_pretrained(repo_name)
            tokenizer = AutoTokenizer.from_pretrained(repo_name)
            
            if local_dir:
                model.save_pretrained(local_dir)
                tokenizer.save_pretrained(local_dir)
                print(f"âœ… æ¨¡å‹å·²ä¿å­˜åˆ°: {local_dir}")
            
            return model, tokenizer
        except Exception as e:
            print(f"âŒ å¯¼å…¥å¤±è´¥: {e}")
            raise
    
    # ==================== æ•°æ®é›†åŠ è½½ ====================
    
    def load_hf_dataset(
        self,
        dataset_name: str,
        split: Optional[str] = None,
        **kwargs
    ) -> Dataset:
        """
        åŠ è½½HuggingFaceæ•°æ®é›†
        
        Args:
            dataset_name: æ•°æ®é›†åç§°
            split: æ•°æ®é›†åˆ†å‰² (train/test/validation)
            **kwargs: å…¶ä»–å‚æ•°ä¼ é€’ç»™load_dataset
            
        Returns:
            Datasetå¯¹è±¡
        """
        print(f"ğŸ“š æ­£åœ¨åŠ è½½ HuggingFace æ•°æ®é›†: {dataset_name}")
        
        try:
            dataset = load_dataset(dataset_name, split=split, **kwargs)
            print(f"âœ… æ•°æ®é›†åŠ è½½æˆåŠŸ! æ ·æœ¬æ•°: {len(dataset)}")
            return dataset
        except Exception as e:
            print(f"âŒ åŠ è½½å¤±è´¥: {e}")
            raise
    
    def convert_to_apt_format(self, hf_dataset: Dataset) -> list:
        """
        å°†HuggingFaceæ•°æ®é›†è½¬æ¢ä¸ºAPTæ ¼å¼
        
        Args:
            hf_dataset: HuggingFaceæ•°æ®é›†
            
        Returns:
            APTæ ¼å¼çš„æ•°æ®åˆ—è¡¨
        """
        apt_data = []
        for item in hf_dataset:
            # æ ¹æ®å®é™…éœ€æ±‚è½¬æ¢æ ¼å¼
            apt_item = {
                'text': item.get('text', ''),
                'label': item.get('label', None),
                # å¯ä»¥æ·»åŠ æ›´å¤šå­—æ®µ
            }
            apt_data.append(apt_item)
        
        return apt_data
    
    # ==================== HF Traineré›†æˆ ====================
    
    def train_with_hf_trainer(
        self,
        model: torch.nn.Module,
        tokenizer: Any,
        train_dataset: Dataset,
        eval_dataset: Optional[Dataset] = None,
        output_dir: str = "./hf_trainer_output",
        **training_args_kwargs
    ):
        """
        ä½¿ç”¨HuggingFace Trainerè®­ç»ƒAPTæ¨¡å‹
        
        Args:
            model: APTæ¨¡å‹
            tokenizer: åˆ†è¯å™¨
            train_dataset: è®­ç»ƒæ•°æ®é›†
            eval_dataset: éªŒè¯æ•°æ®é›†
            output_dir: è¾“å‡ºç›®å½•
            **training_args_kwargs: TrainingArgumentsçš„å‚æ•°
        """
        print("ğŸ‹ï¸ ä½¿ç”¨ HuggingFace Trainer å¼€å§‹è®­ç»ƒ...")
        
        # è®¾ç½®é»˜è®¤è®­ç»ƒå‚æ•°
        default_args = {
            'output_dir': output_dir,
            'num_train_epochs': 3,
            'per_device_train_batch_size': 8,
            'per_device_eval_batch_size': 8,
            'warmup_steps': 500,
            'weight_decay': 0.01,
            'logging_dir': f'{output_dir}/logs',
            'logging_steps': 100,
            'save_strategy': 'epoch',
            'evaluation_strategy': 'epoch' if eval_dataset else 'no',
        }
        default_args.update(training_args_kwargs)
        
        training_args = TrainingArguments(**default_args)
        
        # åˆ›å»ºTrainer
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
        )
        
        # å¼€å§‹è®­ç»ƒ
        trainer.train()
        
        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        trainer.save_model()
        tokenizer.save_pretrained(output_dir)
        print(f"âœ… è®­ç»ƒå®Œæˆ! æ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}")
    
    # ==================== å·¥å…·æ–¹æ³• ====================
    
    def _create_model_card(self, save_dir: Path, repo_name: str):
        """åˆ›å»ºæ¨¡å‹å¡ç‰‡"""
        model_card = f"""---
language: 
- zh
- en
tags:
- apt-model
- text-generation
- autopoietic-attention
license: apache-2.0
---

# {repo_name}

## æ¨¡å‹æè¿°

è¿™æ˜¯ä¸€ä¸ªåŸºäºAPT (Autopoietic Transformer) æ¶æ„çš„è¯­è¨€æ¨¡å‹ã€‚

### ç‰¹ç‚¹
- ğŸ§  è‡ªç”Ÿæˆæ³¨æ„åŠ›æœºåˆ¶
- ğŸ›¡ï¸ DBC-DACæ¢¯åº¦ç¨³å®š
- ğŸŒ å®Œæ•´çš„ä¸­æ–‡æ”¯æŒ
- âš¡ æ··åˆç²¾åº¦è®­ç»ƒ

## ä½¿ç”¨æ–¹æ³•

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

model = AutoModelForCausalLM.from_pretrained("{repo_name}")
tokenizer = AutoTokenizer.from_pretrained("{repo_name}")

text = "ä½ å¥½ï¼Œä¸–ç•Œ"
inputs = tokenizer(text, return_tensors="pt")
outputs = model.generate(**inputs, max_length=50)
print(tokenizer.decode(outputs[0]))
```

## è®­ç»ƒä¿¡æ¯

ç”±APTæ¡†æ¶è®­ç»ƒ: https://github.com/your-repo/apt-model

## è®¸å¯è¯

Apache 2.0
"""
        
        with open(save_dir / "README.md", "w", encoding="utf-8") as f:
            f.write(model_card)
    
    def login_to_hub(self, token: Optional[str] = None):
        """ç™»å½•åˆ°HuggingFace Hub"""
        if token:
            login(token=token)
        else:
            login()  # ä½¿ç”¨ç¼“å­˜çš„token
        print("âœ… å·²ç™»å½•åˆ° HuggingFace Hub")
    
    # ==================== æ’ä»¶é’©å­ ====================
    
    def on_training_end(self, context: Dict[str, Any]):
        """è®­ç»ƒç»“æŸæ—¶è‡ªåŠ¨ä¸Šä¼ åˆ°HuggingFace Hub"""
        if self.config.get('auto_upload', False):
            repo_name = self.config.get('repo_name')
            if repo_name:
                self.export_to_huggingface(
                    model=context['model'],
                    tokenizer=context['tokenizer'],
                    repo_name=repo_name,
                    private=self.config.get('private', False)
                )


# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================

if __name__ == "__main__":
    # é…ç½®
    config = {
        'auto_upload': True,
        'repo_name': 'username/apt-chinese-base',
        'private': False,
    }
    
    plugin = HuggingFaceIntegrationPlugin(config)
    
    # ç¤ºä¾‹1: åŠ è½½HuggingFaceæ•°æ®é›†
    dataset = plugin.load_hf_dataset("wikitext", split="train")
    
    # ç¤ºä¾‹2: å¯¼å‡ºæ¨¡å‹åˆ°HuggingFace Hub
    # plugin.login_to_hub("your_token")
    # plugin.export_to_huggingface(model, tokenizer, "username/my-apt-model")
    
    # ç¤ºä¾‹3: ä»HuggingFaceå¯¼å…¥æ¨¡å‹
    # model, tokenizer = plugin.import_from_huggingface("gpt2")
    
    print("âœ… HuggingFace Integration Plugin ç¤ºä¾‹å®Œæˆ!")
