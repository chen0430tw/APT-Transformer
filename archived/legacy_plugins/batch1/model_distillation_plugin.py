"""
Model Distillation Plugin for APT
çŸ¥è¯†è’¸é¦æ’ä»¶ - å°†å¤§æ¨¡å‹çš„çŸ¥è¯†è½¬ç§»åˆ°å°æ¨¡å‹
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Dict, Any, Tuple
from torch.utils.data import DataLoader


class ModelDistillationPlugin:
    """
    æ¨¡å‹è’¸é¦æ’ä»¶
    
    æ”¯æŒä¸‰ç§è’¸é¦æ–¹æ³•:
    1. å“åº”è’¸é¦ (Response Distillation) - è’¸é¦è¾“å‡ºlogits
    2. ç‰¹å¾è’¸é¦ (Feature Distillation) - è’¸é¦ä¸­é—´å±‚ç‰¹å¾
    3. å…³ç³»è’¸é¦ (Relation Distillation) - è’¸é¦æ ·æœ¬é—´çš„å…³ç³»
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.name = "model-distillation"
        self.version = "1.0.0"
        self.config = config
        
        # è’¸é¦å‚æ•°
        self.temperature = config.get('temperature', 4.0)
        self.alpha = config.get('alpha', 0.5)  # è’¸é¦æŸå¤±æƒé‡
        self.beta = config.get('beta', 0.5)   # çœŸå®æ ‡ç­¾æŸå¤±æƒé‡
        
        self.distill_type = config.get('distill_type', 'response')  # response/feature/relation
    
    # ==================== å“åº”è’¸é¦ ====================
    
    def response_distillation_loss(
        self,
        student_logits: torch.Tensor,
        teacher_logits: torch.Tensor,
        labels: Optional[torch.Tensor] = None,
        temperature: Optional[float] = None
    ) -> torch.Tensor:
        """
        å“åº”è’¸é¦æŸå¤± (æœ€å¸¸ç”¨)
        
        ä½¿ç”¨KLæ•£åº¦è¡¡é‡å­¦ç”Ÿå’Œæ•™å¸ˆè¾“å‡ºåˆ†å¸ƒçš„å·®å¼‚
        
        Args:
            student_logits: å­¦ç”Ÿæ¨¡å‹çš„logits [batch, seq_len, vocab]
            teacher_logits: æ•™å¸ˆæ¨¡å‹çš„logits [batch, seq_len, vocab]
            labels: çœŸå®æ ‡ç­¾ (å¯é€‰)
            temperature: æ¸©åº¦å‚æ•°,ç”¨äºè½¯åŒ–æ¦‚ç‡åˆ†å¸ƒ
            
        Returns:
            è’¸é¦æŸå¤±
        """
        T = temperature or self.temperature
        
        # ä½¿ç”¨æ¸©åº¦å‚æ•°è½¯åŒ–åˆ†å¸ƒ
        student_log_probs = F.log_softmax(student_logits / T, dim=-1)
        teacher_probs = F.softmax(teacher_logits / T, dim=-1)
        
        # KLæ•£åº¦æŸå¤± (è¡¡é‡ä¸¤ä¸ªåˆ†å¸ƒçš„å·®å¼‚)
        distill_loss = F.kl_div(
            student_log_probs,
            teacher_probs,
            reduction='batchmean'
        ) * (T ** 2)  # æ¸©åº¦çš„å¹³æ–¹ç”¨äºç¼©æ”¾
        
        # å¦‚æœæœ‰çœŸå®æ ‡ç­¾,ç»“åˆäº¤å‰ç†µæŸå¤±
        if labels is not None:
            ce_loss = F.cross_entropy(
                student_logits.view(-1, student_logits.size(-1)),
                labels.view(-1),
                ignore_index=-100
            )
            # åŠ æƒç»„åˆ
            total_loss = self.alpha * distill_loss + self.beta * ce_loss
            return total_loss
        
        return distill_loss
    
    # ==================== ç‰¹å¾è’¸é¦ ====================
    
    def feature_distillation_loss(
        self,
        student_features: torch.Tensor,
        teacher_features: torch.Tensor,
        normalize: bool = True
    ) -> torch.Tensor:
        """
        ç‰¹å¾è’¸é¦æŸå¤±
        
        è®©å­¦ç”Ÿæ¨¡å‹çš„ä¸­é—´å±‚ç‰¹å¾æ¥è¿‘æ•™å¸ˆæ¨¡å‹
        
        Args:
            student_features: å­¦ç”Ÿæ¨¡å‹ç‰¹å¾ [batch, seq_len, hidden_dim]
            teacher_features: æ•™å¸ˆæ¨¡å‹ç‰¹å¾ [batch, seq_len, hidden_dim]
            normalize: æ˜¯å¦å½’ä¸€åŒ–ç‰¹å¾
            
        Returns:
            ç‰¹å¾è’¸é¦æŸå¤±
        """
        if normalize:
            student_features = F.normalize(student_features, p=2, dim=-1)
            teacher_features = F.normalize(teacher_features, p=2, dim=-1)
        
        # MSEæŸå¤±
        feature_loss = F.mse_loss(student_features, teacher_features)
        
        return feature_loss
    
    def multi_layer_feature_distillation(
        self,
        student_features_list: list,
        teacher_features_list: list,
        layer_weights: Optional[list] = None
    ) -> torch.Tensor:
        """
        å¤šå±‚ç‰¹å¾è’¸é¦
        
        Args:
            student_features_list: å­¦ç”Ÿæ¨¡å‹å¤šå±‚ç‰¹å¾åˆ—è¡¨
            teacher_features_list: æ•™å¸ˆæ¨¡å‹å¤šå±‚ç‰¹å¾åˆ—è¡¨
            layer_weights: å„å±‚æƒé‡
            
        Returns:
            æ€»ç‰¹å¾æŸå¤±
        """
        if layer_weights is None:
            layer_weights = [1.0] * len(student_features_list)
        
        total_loss = 0
        for s_feat, t_feat, weight in zip(
            student_features_list, teacher_features_list, layer_weights
        ):
            layer_loss = self.feature_distillation_loss(s_feat, t_feat)
            total_loss += weight * layer_loss
        
        return total_loss / len(student_features_list)
    
    # ==================== å…³ç³»è’¸é¦ ====================
    
    def relation_distillation_loss(
        self,
        student_outputs: torch.Tensor,
        teacher_outputs: torch.Tensor
    ) -> torch.Tensor:
        """
        å…³ç³»è’¸é¦æŸå¤±
        
        ä¿æŒæ ·æœ¬é—´çš„ç›¸å¯¹å…³ç³»(ç›¸ä¼¼åº¦)
        
        Args:
            student_outputs: å­¦ç”Ÿæ¨¡å‹è¾“å‡º [batch, hidden]
            teacher_outputs: æ•™å¸ˆæ¨¡å‹è¾“å‡º [batch, hidden]
            
        Returns:
            å…³ç³»è’¸é¦æŸå¤±
        """
        # è®¡ç®—æ ·æœ¬é—´çš„ç›¸ä¼¼åº¦çŸ©é˜µ
        student_sim = self._compute_similarity_matrix(student_outputs)
        teacher_sim = self._compute_similarity_matrix(teacher_outputs)
        
        # L2æŸå¤±
        relation_loss = F.mse_loss(student_sim, teacher_sim)
        
        return relation_loss
    
    def _compute_similarity_matrix(self, features: torch.Tensor) -> torch.Tensor:
        """è®¡ç®—ç‰¹å¾çš„ç›¸ä¼¼åº¦çŸ©é˜µ"""
        # å½’ä¸€åŒ–
        features = F.normalize(features, p=2, dim=-1)
        
        # è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ
        similarity = torch.matmul(features, features.transpose(-2, -1))
        
        return similarity
    
    # ==================== æ³¨æ„åŠ›è’¸é¦ ====================
    
    def attention_distillation_loss(
        self,
        student_attention: torch.Tensor,
        teacher_attention: torch.Tensor
    ) -> torch.Tensor:
        """
        æ³¨æ„åŠ›è’¸é¦æŸå¤±
        
        è®©å­¦ç”Ÿæ¨¡å‹å­¦ä¹ æ•™å¸ˆæ¨¡å‹çš„æ³¨æ„åŠ›æ¨¡å¼
        
        Args:
            student_attention: å­¦ç”Ÿæ³¨æ„åŠ›æƒé‡ [batch, heads, seq, seq]
            teacher_attention: æ•™å¸ˆæ³¨æ„åŠ›æƒé‡ [batch, heads, seq, seq]
            
        Returns:
            æ³¨æ„åŠ›è’¸é¦æŸå¤±
        """
        # MSEæŸå¤±
        attention_loss = F.mse_loss(student_attention, teacher_attention)
        
        return attention_loss
    
    # ==================== è’¸é¦è®­ç»ƒæµç¨‹ ====================
    
    def distill_training_step(
        self,
        student_model: nn.Module,
        teacher_model: nn.Module,
        batch: Dict[str, torch.Tensor],
        optimizer: torch.optim.Optimizer
    ) -> Dict[str, float]:
        """
        å•æ­¥è’¸é¦è®­ç»ƒ
        
        Args:
            student_model: å­¦ç”Ÿæ¨¡å‹
            teacher_model: æ•™å¸ˆæ¨¡å‹
            batch: è®­ç»ƒæ‰¹æ¬¡æ•°æ®
            optimizer: ä¼˜åŒ–å™¨
            
        Returns:
            æŸå¤±å­—å…¸
        """
        student_model.train()
        teacher_model.eval()
        
        input_ids = batch['input_ids']
        labels = batch.get('labels', input_ids)
        
        # æ•™å¸ˆæ¨¡å‹å‰å‘ä¼ æ’­ (ä¸è®¡ç®—æ¢¯åº¦)
        with torch.no_grad():
            teacher_outputs = teacher_model(input_ids, output_hidden_states=True)
            teacher_logits = teacher_outputs.logits if hasattr(teacher_outputs, 'logits') else teacher_outputs[0]
            teacher_features = teacher_outputs.hidden_states if hasattr(teacher_outputs, 'hidden_states') else None
        
        # å­¦ç”Ÿæ¨¡å‹å‰å‘ä¼ æ’­
        student_outputs = student_model(input_ids, output_hidden_states=True)
        student_logits = student_outputs.logits if hasattr(student_outputs, 'logits') else student_outputs[0]
        student_features = student_outputs.hidden_states if hasattr(student_outputs, 'hidden_states') else None
        
        # è®¡ç®—è’¸é¦æŸå¤±
        losses = {}
        
        if self.distill_type == 'response':
            # å“åº”è’¸é¦
            loss = self.response_distillation_loss(
                student_logits, teacher_logits, labels
            )
            losses['response_loss'] = loss.item()
        
        elif self.distill_type == 'feature':
            # ç‰¹å¾è’¸é¦
            if student_features and teacher_features:
                loss = self.multi_layer_feature_distillation(
                    student_features, teacher_features
                )
                losses['feature_loss'] = loss.item()
            else:
                raise ValueError("æ¨¡å‹éœ€è¦æ”¯æŒè¾“å‡ºhidden_states")
        
        elif self.distill_type == 'combined':
            # ç»„åˆè’¸é¦
            response_loss = self.response_distillation_loss(
                student_logits, teacher_logits, labels
            )
            
            feature_loss = torch.tensor(0.0, device=input_ids.device)
            if student_features and teacher_features:
                feature_loss = self.multi_layer_feature_distillation(
                    student_features, teacher_features
                )
            
            loss = response_loss + 0.1 * feature_loss
            losses['response_loss'] = response_loss.item()
            losses['feature_loss'] = feature_loss.item()
        
        else:
            raise ValueError(f"æœªçŸ¥çš„è’¸é¦ç±»å‹: {self.distill_type}")
        
        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()
        
        losses['total_loss'] = loss.item()
        return losses
    
    def distill_model(
        self,
        student_model: nn.Module,
        teacher_model: nn.Module,
        train_dataloader: DataLoader,
        optimizer: torch.optim.Optimizer,
        num_epochs: int = 3,
        device: str = 'cuda'
    ):
        """
        å®Œæ•´çš„æ¨¡å‹è’¸é¦æµç¨‹
        
        Args:
            student_model: å­¦ç”Ÿæ¨¡å‹
            teacher_model: æ•™å¸ˆæ¨¡å‹
            train_dataloader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            optimizer: ä¼˜åŒ–å™¨
            num_epochs: è®­ç»ƒè½®æ•°
            device: è®¾å¤‡
        """
        print("ğŸ“ å¼€å§‹çŸ¥è¯†è’¸é¦è®­ç»ƒ...")
        
        student_model.to(device)
        teacher_model.to(device)
        
        for epoch in range(num_epochs):
            epoch_losses = []
            
            for batch_idx, batch in enumerate(train_dataloader):
                # å°†æ•°æ®ç§»åˆ°è®¾å¤‡
                batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v 
                        for k, v in batch.items()}
                
                # è®­ç»ƒæ­¥éª¤
                losses = self.distill_training_step(
                    student_model, teacher_model, batch, optimizer
                )
                epoch_losses.append(losses['total_loss'])
                
                # æ—¥å¿—
                if batch_idx % 100 == 0:
                    print(f"Epoch {epoch+1}/{num_epochs} | "
                          f"Batch {batch_idx}/{len(train_dataloader)} | "
                          f"Loss: {losses['total_loss']:.4f}")
            
            avg_loss = sum(epoch_losses) / len(epoch_losses)
            print(f"ğŸ“Š Epoch {epoch+1} å¹³å‡æŸå¤±: {avg_loss:.4f}")
        
        print("âœ… çŸ¥è¯†è’¸é¦å®Œæˆ!")
    
    # ==================== å·¥å…·æ–¹æ³• ====================
    
    def compress_model(
        self,
        teacher_model: nn.Module,
        compression_ratio: float = 0.5
    ) -> nn.Module:
        """
        æ ¹æ®å‹ç¼©æ¯”åˆ›å»ºå­¦ç”Ÿæ¨¡å‹
        
        Args:
            teacher_model: æ•™å¸ˆæ¨¡å‹
            compression_ratio: å‹ç¼©æ¯” (0-1)
            
        Returns:
            å­¦ç”Ÿæ¨¡å‹
        """
        # TODO: æ ¹æ®æ•™å¸ˆæ¨¡å‹æ¶æ„è‡ªåŠ¨åˆ›å»ºå‹ç¼©åçš„å­¦ç”Ÿæ¨¡å‹
        # è¿™éœ€è¦äº†è§£å…·ä½“çš„æ¨¡å‹ç»“æ„
        pass
    
    # ==================== æ’ä»¶é’©å­ ====================
    
    def on_training_start(self, context: Dict[str, Any]):
        """è®­ç»ƒå¼€å§‹æ—¶åˆå§‹åŒ–è’¸é¦"""
        if context.get('enable_distillation'):
            print("ğŸ“ å¯ç”¨çŸ¥è¯†è’¸é¦æ¨¡å¼")
            self.teacher_model = context.get('teacher_model')
            if self.teacher_model:
                self.teacher_model.eval()


# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================

if __name__ == "__main__":
    # é…ç½®
    config = {
        'temperature': 4.0,
        'alpha': 0.7,      # è’¸é¦æŸå¤±æƒé‡
        'beta': 0.3,       # çœŸå®æ ‡ç­¾æƒé‡
        'distill_type': 'response',  # response/feature/combined
    }
    
    plugin = ModelDistillationPlugin(config)
    
    # ç¤ºä¾‹:å“åº”è’¸é¦
    batch_size, seq_len, vocab_size = 8, 128, 50000
    student_logits = torch.randn(batch_size, seq_len, vocab_size)
    teacher_logits = torch.randn(batch_size, seq_len, vocab_size)
    labels = torch.randint(0, vocab_size, (batch_size, seq_len))
    
    loss = plugin.response_distillation_loss(
        student_logits, teacher_logits, labels
    )
    print(f"å“åº”è’¸é¦æŸå¤±: {loss.item():.4f}")
    
    print("\nâœ… çŸ¥è¯†è’¸é¦æ’ä»¶ç¤ºä¾‹å®Œæˆ!")
    print("\nğŸ’¡ ä½¿ç”¨å»ºè®®:")
    print("1. å“åº”è’¸é¦é€‚åˆå¤§å¤šæ•°åœºæ™¯,æ•ˆæœå¥½ä¸”ç®€å•")
    print("2. ç‰¹å¾è’¸é¦é€‚åˆæ¨¡å‹ç»“æ„ç›¸ä¼¼çš„æƒ…å†µ")
    print("3. ç»„åˆè’¸é¦æ•ˆæœæœ€å¥½,ä½†è®­ç»ƒè¾ƒæ…¢")
    print("4. æ¸©åº¦å‚æ•°Tå»ºè®®è®¾ç½®ä¸º2-8ä¹‹é—´")
