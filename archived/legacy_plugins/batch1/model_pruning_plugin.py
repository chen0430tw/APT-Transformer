"""
Model Pruning Plugin for APT
æ¨¡å‹å‰ªææ’ä»¶ - å‡å°‘æ¨¡å‹å‚æ•°é‡,æå‡æ¨ç†é€Ÿåº¦
"""

import torch
import torch.nn as nn
import torch.nn.utils.prune as prune
from typing import Dict, Any, List, Tuple
import numpy as np


class ModelPruningPlugin:
    """
    æ¨¡å‹å‰ªææ’ä»¶
    
    æ”¯æŒçš„å‰ªæç­–ç•¥:
    1. ç»“æ„åŒ–å‰ªæ (Structured Pruning) - å‰ªé™¤æ•´ä¸ªé€šé“/å±‚
    2. éç»“æ„åŒ–å‰ªæ (Unstructured Pruning) - å‰ªé™¤å•ä¸ªæƒé‡
    3. æƒé‡å¤§å°å‰ªæ (Magnitude-based) - åŸºäºæƒé‡ç»å¯¹å€¼
    4. Taylorå‰ªæ (Taylor Pruning) - åŸºäºä¸€é˜¶æ³°å‹’å±•å¼€
    5. å½©ç¥¨å‡è¯´å‰ªæ (Lottery Ticket Hypothesis)
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.name = "model-pruning"
        self.version = "1.0.0"
        self.config = config
        
        self.prune_ratio = config.get('prune_ratio', 0.3)  # å‰ªææ¯”ä¾‹
        self.prune_type = config.get('prune_type', 'magnitude')  # å‰ªæç±»å‹
        self.structured = config.get('structured', False)  # æ˜¯å¦ç»“æ„åŒ–å‰ªæ
    
    # ==================== æƒé‡å¤§å°å‰ªæ ====================
    
    def magnitude_pruning(
        self,
        model: nn.Module,
        prune_ratio: float,
        structured: bool = False
    ) -> nn.Module:
        """
        åŸºäºæƒé‡å¤§å°çš„å‰ªæ
        
        ç§»é™¤ç»å¯¹å€¼æœ€å°çš„æƒé‡
        
        Args:
            model: å¾…å‰ªææ¨¡å‹
            prune_ratio: å‰ªææ¯”ä¾‹ (0-1)
            structured: æ˜¯å¦ç»“æ„åŒ–å‰ªæ
            
        Returns:
            å‰ªæåçš„æ¨¡å‹
        """
        print(f"âœ‚ï¸ å¼€å§‹æƒé‡å¤§å°å‰ªæ (å‰ªææ¯”ä¾‹: {prune_ratio*100:.1f}%)")
        
        parameters_to_prune = []
        
        # æ”¶é›†æ‰€æœ‰éœ€è¦å‰ªæçš„å‚æ•°
        for name, module in model.named_modules():
            if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d)):
                parameters_to_prune.append((module, 'weight'))
        
        if structured:
            # ç»“æ„åŒ–å‰ªæ (å‰ªé™¤æ•´ä¸ªè¾“å‡ºé€šé“)
            for module, param_name in parameters_to_prune:
                prune.ln_structured(
                    module, name=param_name,
                    amount=prune_ratio, n=2, dim=0
                )
        else:
            # éç»“æ„åŒ–å‰ªæ (L1èŒƒæ•°)
            prune.global_unstructured(
                parameters_to_prune,
                pruning_method=prune.L1Unstructured,
                amount=prune_ratio,
            )
        
        # è®¡ç®—å®é™…å‰ªææ¯”ä¾‹
        pruned_params, total_params = self._count_pruned_params(model)
        actual_ratio = pruned_params / total_params
        
        print(f"âœ… å‰ªæå®Œæˆ! å‰ªé™¤å‚æ•°: {pruned_params}/{total_params} ({actual_ratio*100:.2f}%)")
        
        return model
    
    # ==================== Taylorå‰ªæ ====================
    
    def taylor_pruning(
        self,
        model: nn.Module,
        dataloader: torch.utils.data.DataLoader,
        prune_ratio: float,
        device: str = 'cuda'
    ) -> nn.Module:
        """
        åŸºäºä¸€é˜¶æ³°å‹’å±•å¼€çš„å‰ªæ
        
        ä½¿ç”¨æ¢¯åº¦ä¿¡æ¯è¯„ä¼°å‚æ•°é‡è¦æ€§:
        importance = |weight * gradient|
        
        Args:
            model: å¾…å‰ªææ¨¡å‹
            dataloader: æ•°æ®åŠ è½½å™¨(ç”¨äºè®¡ç®—æ¢¯åº¦)
            prune_ratio: å‰ªææ¯”ä¾‹
            device: è®¾å¤‡
            
        Returns:
            å‰ªæåçš„æ¨¡å‹
        """
        print(f"âœ‚ï¸ å¼€å§‹Taylorå‰ªæ (å‰ªææ¯”ä¾‹: {prune_ratio*100:.1f}%)")
        
        model.to(device)
        model.train()
        
        # 1. è®¡ç®—å‚æ•°çš„Tayloré‡è¦æ€§åˆ†æ•°
        importance_scores = {}
        
        # å‰å‘ä¼ æ’­å¹¶è®¡ç®—æ¢¯åº¦
        for batch_idx, batch in enumerate(dataloader):
            if batch_idx >= 10:  # ä½¿ç”¨10ä¸ªæ‰¹æ¬¡ä¼°ç®—é‡è¦æ€§
                break
            
            # å‡†å¤‡è¾“å…¥
            if isinstance(batch, dict):
                inputs = {k: v.to(device) if isinstance(v, torch.Tensor) else v 
                         for k, v in batch.items()}
                outputs = model(**inputs)
            else:
                inputs = batch[0].to(device) if isinstance(batch, (list, tuple)) else batch.to(device)
                outputs = model(inputs)
            
            # è®¡ç®—æŸå¤±
            loss = outputs.loss if hasattr(outputs, 'loss') else outputs[0]
            loss.backward()
        
        # 2. è®¡ç®—æ¯ä¸ªå‚æ•°çš„é‡è¦æ€§: |weight * gradient|
        for name, param in model.named_parameters():
            if param.grad is not None and param.requires_grad:
                importance = torch.abs(param * param.grad)
                importance_scores[name] = importance.detach().cpu()
        
        # æ¸…é™¤æ¢¯åº¦
        model.zero_grad()
        
        # 3. å…¨å±€æ’åºå¹¶å‰ªæ
        all_scores = torch.cat([scores.flatten() for scores in importance_scores.values()])
        threshold = torch.quantile(all_scores, prune_ratio)
        
        # 4. åº”ç”¨å‰ªæ
        for name, param in model.named_parameters():
            if name in importance_scores:
                mask = (importance_scores[name] > threshold).float().to(device)
                param.data *= mask
        
        print("âœ… Taylorå‰ªæå®Œæˆ!")
        
        return model
    
    # ==================== ç»“æ„åŒ–å‰ªæ ====================
    
    def structured_channel_pruning(
        self,
        model: nn.Module,
        prune_ratio: float,
        criterion: str = 'l1'
    ) -> nn.Module:
        """
        ç»“æ„åŒ–é€šé“å‰ªæ
        
        å‰ªé™¤æ•´ä¸ªå·ç§¯/çº¿æ€§å±‚çš„è¾“å‡ºé€šé“
        
        Args:
            model: å¾…å‰ªææ¨¡å‹
            prune_ratio: å‰ªææ¯”ä¾‹
            criterion: å‰ªææ ‡å‡† ('l1', 'l2', 'random')
            
        Returns:
            å‰ªæåçš„æ¨¡å‹
        """
        print(f"âœ‚ï¸ å¼€å§‹ç»“æ„åŒ–é€šé“å‰ªæ (æ ‡å‡†: {criterion})")
        
        for name, module in model.named_modules():
            if isinstance(module, (nn.Linear, nn.Conv1d)):
                # è®¡ç®—æ¯ä¸ªè¾“å‡ºé€šé“çš„èŒƒæ•°
                weight = module.weight.data
                out_channels = weight.shape[0]
                
                if criterion == 'l1':
                    # L1èŒƒæ•°
                    channel_norms = torch.norm(weight.view(out_channels, -1), p=1, dim=1)
                elif criterion == 'l2':
                    # L2èŒƒæ•°
                    channel_norms = torch.norm(weight.view(out_channels, -1), p=2, dim=1)
                elif criterion == 'random':
                    # éšæœº
                    channel_norms = torch.rand(out_channels)
                
                # é€‰æ‹©è¦ä¿ç•™çš„é€šé“
                num_keep = int(out_channels * (1 - prune_ratio))
                _, keep_indices = torch.topk(channel_norms, num_keep)
                keep_indices = sorted(keep_indices.tolist())
                
                # å‰ªæ (å®é™…å®ç°ä¸­éœ€è¦è°ƒæ•´åç»­å±‚çš„è¾“å…¥ç»´åº¦)
                # è¿™é‡Œåªæ˜¯ç¤ºæ„,çœŸå®æƒ…å†µéœ€è¦æ›´å¤æ‚çš„å¤„ç†
                prune.ln_structured(
                    module, name='weight',
                    amount=prune_ratio, n=2, dim=0
                )
        
        print("âœ… ç»“æ„åŒ–å‰ªæå®Œæˆ!")
        
        return model
    
    # ==================== å½©ç¥¨å‡è¯´å‰ªæ ====================
    
    def lottery_ticket_pruning(
        self,
        model: nn.Module,
        train_function: callable,
        prune_iterations: int = 5,
        prune_ratio_per_iter: float = 0.2
    ) -> Tuple[nn.Module, Dict]:
        """
        å½©ç¥¨å‡è¯´å‰ªæ (Lottery Ticket Hypothesis)
        
        è¿­ä»£å¼å‰ªæ:
        1. è®­ç»ƒæ¨¡å‹åˆ°æ”¶æ•›
        2. å‰ªæä¸€éƒ¨åˆ†æƒé‡
        3. å°†å‰©ä½™æƒé‡é‡ç½®åˆ°åˆå§‹å€¼
        4. é‡å¤æ­¥éª¤1-3
        
        Args:
            model: å¾…å‰ªææ¨¡å‹
            train_function: è®­ç»ƒå‡½æ•°
            prune_iterations: è¿­ä»£æ¬¡æ•°
            prune_ratio_per_iter: æ¯æ¬¡è¿­ä»£çš„å‰ªææ¯”ä¾‹
            
        Returns:
            (å‰ªæåçš„æ¨¡å‹, å†å²è®°å½•)
        """
        print("ğŸ° å¼€å§‹å½©ç¥¨å‡è¯´å‰ªæ...")
        
        # ä¿å­˜åˆå§‹æƒé‡
        initial_weights = {name: param.data.clone() 
                          for name, param in model.named_parameters()}
        
        history = {
            'iteration': [],
            'total_params': [],
            'remaining_params': [],
            'accuracy': []
        }
        
        for iteration in range(prune_iterations):
            print(f"\nğŸ¯ è¿­ä»£ {iteration+1}/{prune_iterations}")
            
            # 1. è®­ç»ƒæ¨¡å‹
            print("  è®­ç»ƒä¸­...")
            accuracy = train_function(model)
            
            # 2. å‰ªæ
            print(f"  å‰ªæ {prune_ratio_per_iter*100:.1f}%...")
            model = self.magnitude_pruning(model, prune_ratio_per_iter)
            
            # 3. é‡ç½®æœªå‰ªæçš„æƒé‡åˆ°åˆå§‹å€¼
            print("  é‡ç½®æƒé‡åˆ°åˆå§‹å€¼...")
            for name, param in model.named_parameters():
                if name in initial_weights:
                    # è·å–å‰ªææ©ç 
                    if hasattr(param, '_forward_pre_hooks'):
                        # åªé‡ç½®æœªè¢«å‰ªæçš„æƒé‡
                        mask = torch.ones_like(param.data)
                        for hook in param._forward_pre_hooks.values():
                            if hasattr(hook, 'mask'):
                                mask = hook.mask
                        param.data = initial_weights[name] * mask
            
            # è®°å½•ç»Ÿè®¡
            total_params, remaining_params = self._count_total_params(model)
            history['iteration'].append(iteration + 1)
            history['total_params'].append(total_params)
            history['remaining_params'].append(remaining_params)
            history['accuracy'].append(accuracy)
            
            print(f"  å‰©ä½™å‚æ•°: {remaining_params}/{total_params} "
                  f"({remaining_params/total_params*100:.2f}%)")
        
        print("\nâœ… å½©ç¥¨å‡è¯´å‰ªæå®Œæˆ!")
        
        return model, history
    
    # ==================== å‰ªæåå¤„ç† ====================
    
    def make_pruning_permanent(self, model: nn.Module) -> nn.Module:
        """
        æ°¸ä¹…åº”ç”¨å‰ªæ (ç§»é™¤æ©ç ,çœŸæ­£åˆ é™¤å‚æ•°)
        
        Args:
            model: å‰ªæåçš„æ¨¡å‹
            
        Returns:
            æ°¸ä¹…å‰ªæçš„æ¨¡å‹
        """
        print("ğŸ”§ æ°¸ä¹…åº”ç”¨å‰ªæ...")
        
        for module in model.modules():
            if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d)):
                try:
                    # ç§»é™¤å‰ªæçš„é‡å‚æ•°åŒ–
                    prune.remove(module, 'weight')
                except:
                    pass
        
        print("âœ… å‰ªæå·²æ°¸ä¹…åº”ç”¨!")
        
        return model
    
    def fine_tune_after_pruning(
        self,
        model: nn.Module,
        train_dataloader: torch.utils.data.DataLoader,
        optimizer: torch.optim.Optimizer,
        num_epochs: int = 3,
        device: str = 'cuda'
    ) -> nn.Module:
        """
        å‰ªæåå¾®è°ƒ
        
        å‰ªæä¼šæŸå¤±ä¸€äº›ç²¾åº¦,éœ€è¦å¾®è°ƒæ¢å¤
        
        Args:
            model: å‰ªæåçš„æ¨¡å‹
            train_dataloader: è®­ç»ƒæ•°æ®
            optimizer: ä¼˜åŒ–å™¨
            num_epochs: å¾®è°ƒè½®æ•°
            device: è®¾å¤‡
            
        Returns:
            å¾®è°ƒåçš„æ¨¡å‹
        """
        print(f"ğŸ¯ å¼€å§‹å‰ªæåå¾®è°ƒ ({num_epochs} epochs)...")
        
        model.to(device)
        model.train()
        
        for epoch in range(num_epochs):
            epoch_loss = 0
            
            for batch_idx, batch in enumerate(train_dataloader):
                # å‡†å¤‡æ•°æ®
                if isinstance(batch, dict):
                    batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v 
                            for k, v in batch.items()}
                    outputs = model(**batch)
                else:
                    inputs = batch[0].to(device)
                    outputs = model(inputs)
                
                # è®¡ç®—æŸå¤±
                loss = outputs.loss if hasattr(outputs, 'loss') else outputs[0]
                
                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                
                epoch_loss += loss.item()
                
                if batch_idx % 100 == 0:
                    print(f"  Epoch {epoch+1}/{num_epochs} | "
                          f"Batch {batch_idx}/{len(train_dataloader)} | "
                          f"Loss: {loss.item():.4f}")
            
            avg_loss = epoch_loss / len(train_dataloader)
            print(f"ğŸ“Š Epoch {epoch+1} å¹³å‡æŸå¤±: {avg_loss:.4f}")
        
        print("âœ… å¾®è°ƒå®Œæˆ!")
        
        return model
    
    # ==================== å·¥å…·æ–¹æ³• ====================
    
    def _count_pruned_params(self, model: nn.Module) -> Tuple[int, int]:
        """è®¡ç®—è¢«å‰ªæçš„å‚æ•°æ•°é‡"""
        pruned = 0
        total = 0
        
        for module in model.modules():
            if hasattr(module, 'weight'):
                weight = module.weight
                if hasattr(weight, '_forward_pre_hooks'):
                    # æœ‰å‰ªææ©ç 
                    for hook in weight._forward_pre_hooks.values():
                        if hasattr(hook, 'mask'):
                            mask = hook.mask
                            pruned += (mask == 0).sum().item()
                            total += mask.numel()
                else:
                    # æ£€æŸ¥æ˜¯å¦ä¸º0
                    pruned += (weight == 0).sum().item()
                    total += weight.numel()
        
        return pruned, total
    
    def _count_total_params(self, model: nn.Module) -> Tuple[int, int]:
        """è®¡ç®—æ€»å‚æ•°å’Œå‰©ä½™å‚æ•°"""
        total = sum(p.numel() for p in model.parameters())
        pruned, _ = self._count_pruned_params(model)
        remaining = total - pruned
        return total, remaining
    
    def get_pruning_statistics(self, model: nn.Module) -> Dict:
        """è·å–å‰ªæç»Ÿè®¡ä¿¡æ¯"""
        stats = {
            'layer_stats': [],
            'total_params': 0,
            'pruned_params': 0,
            'remaining_params': 0,
            'sparsity': 0.0
        }
        
        for name, module in model.named_modules():
            if hasattr(module, 'weight'):
                weight = module.weight
                layer_pruned = (weight == 0).sum().item()
                layer_total = weight.numel()
                
                stats['layer_stats'].append({
                    'name': name,
                    'total': layer_total,
                    'pruned': layer_pruned,
                    'sparsity': layer_pruned / layer_total if layer_total > 0 else 0
                })
                
                stats['total_params'] += layer_total
                stats['pruned_params'] += layer_pruned
        
        stats['remaining_params'] = stats['total_params'] - stats['pruned_params']
        stats['sparsity'] = stats['pruned_params'] / stats['total_params'] if stats['total_params'] > 0 else 0
        
        return stats
    
    # ==================== æ’ä»¶é’©å­ ====================
    
    def on_training_end(self, context: Dict[str, Any]):
        """è®­ç»ƒç»“æŸåè‡ªåŠ¨å‰ªæ"""
        if self.config.get('auto_prune', False):
            model = context.get('model')
            print("\nğŸš€ è‡ªåŠ¨è§¦å‘æ¨¡å‹å‰ªæ...")
            
            model = self.magnitude_pruning(
                model,
                prune_ratio=self.prune_ratio,
                structured=self.structured
            )
            
            # ä¿å­˜å‰ªæåçš„æ¨¡å‹
            save_path = context.get('output_dir', './pruned_model')
            torch.save(model.state_dict(), f"{save_path}/pruned_model.pt")


# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================

if __name__ == "__main__":
    # é…ç½®
    config = {
        'prune_ratio': 0.3,  # å‰ªæ30%çš„å‚æ•°
        'prune_type': 'magnitude',  # magnitude/taylor/structured/lottery
        'structured': False,  # éç»“æ„åŒ–å‰ªæ
        'auto_prune': True,  # è®­ç»ƒåè‡ªåŠ¨å‰ªæ
    }
    
    plugin = ModelPruningPlugin(config)
    
    # åˆ›å»ºæµ‹è¯•æ¨¡å‹
    class SimpleModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.fc1 = nn.Linear(100, 50)
            self.fc2 = nn.Linear(50, 10)
        
        def forward(self, x):
            x = torch.relu(self.fc1(x))
            return self.fc2(x)
    
    model = SimpleModel()
    
    print("åŸå§‹æ¨¡å‹å‚æ•°é‡:", sum(p.numel() for p in model.parameters()))
    
    # åº”ç”¨å‰ªæ
    model = plugin.magnitude_pruning(model, prune_ratio=0.3, structured=False)
    
    # è·å–ç»Ÿè®¡
    stats = plugin.get_pruning_statistics(model)
    print(f"\nå‰ªæç»Ÿè®¡:")
    print(f"  æ€»å‚æ•°: {stats['total_params']}")
    print(f"  å‰ªæå‚æ•°: {stats['pruned_params']}")
    print(f"  å‰©ä½™å‚æ•°: {stats['remaining_params']}")
    print(f"  ç¨€ç–åº¦: {stats['sparsity']*100:.2f}%")
    
    print("\nâœ… æ¨¡å‹å‰ªææ’ä»¶ç¤ºä¾‹å®Œæˆ!")
