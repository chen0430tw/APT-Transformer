#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
é”™è¯¯æŒä¹…åŒ–ç³»ç»Ÿ

æä¾›é”™è¯¯æ—¥å¿—æŒä¹…åŒ–ã€æ¨¡å¼åˆ†æå’ŒæŸ¥è¯¢åŠŸèƒ½ã€‚
æ”¯æŒSQLiteæ•°æ®åº“å­˜å‚¨ï¼Œé”™è¯¯æ¨¡å¼è¯†åˆ«ï¼Œä»¥åŠæœªæ¥WebUI/APIé›†æˆã€‚
"""

import os
import re
import sqlite3
import traceback
import hashlib
import json
from datetime import datetime
from pathlib import Path
from typing import Optional, List, Dict, Any, Tuple
from collections import defaultdict, Counter


class ErrorPersistence:
    """
    é”™è¯¯æŒä¹…åŒ–ç®¡ç†å™¨

    åŠŸèƒ½:
    - SQLiteæ•°æ®åº“å­˜å‚¨é”™è¯¯æ—¥å¿—
    - é”™è¯¯åˆ†ç±»å’Œä¸¥é‡æ€§è¯„ä¼°
    - é”™è¯¯æ¨¡å¼è¯†åˆ«å’Œèšåˆ
    - é”™è¯¯ç»Ÿè®¡å’Œè¶‹åŠ¿åˆ†æ
    - æœªæ¥WebUI/APIæ•°æ®å¯¼å‡º
    """

    # é”™è¯¯ä¸¥é‡æ€§çº§åˆ«
    SEVERITY_DEBUG = 0
    SEVERITY_INFO = 1
    SEVERITY_WARNING = 2
    SEVERITY_ERROR = 3
    SEVERITY_CRITICAL = 4

    SEVERITY_NAMES = {
        0: 'DEBUG',
        1: 'INFO',
        2: 'WARNING',
        3: 'ERROR',
        4: 'CRITICAL'
    }

    def __init__(self, db_path: str = ".cache/errors/errors.db", auto_init: bool = True):
        """
        åˆå§‹åŒ–é”™è¯¯æŒä¹…åŒ–ç³»ç»Ÿ

        å‚æ•°:
            db_path: SQLiteæ•°æ®åº“è·¯å¾„ï¼ˆç›¸å¯¹è·¯å¾„ï¼Œå¯è¿ç§»ï¼‰
            auto_init: æ˜¯å¦è‡ªåŠ¨åˆå§‹åŒ–æ•°æ®åº“
        """
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(parents=True, exist_ok=True)

        self.conn = None
        self.cursor = None

        if auto_init:
            self._init_database()

    def _init_database(self):
        """åˆå§‹åŒ–SQLiteæ•°æ®åº“å’Œè¡¨ç»“æ„"""
        self.conn = sqlite3.connect(str(self.db_path), check_same_thread=False)
        self.cursor = self.conn.cursor()

        # åˆ›å»ºé”™è¯¯æ—¥å¿—è¡¨
        self.cursor.execute("""
            CREATE TABLE IF NOT EXISTS error_logs (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                severity INTEGER NOT NULL,
                module TEXT,
                function TEXT,
                error_type TEXT,
                error_message TEXT,
                error_hash TEXT,
                stacktrace TEXT,
                context TEXT,
                epoch INTEGER,
                global_step INTEGER,
                resolved BOOLEAN DEFAULT 0,
                created_at DATETIME DEFAULT CURRENT_TIMESTAMP
            )
        """)

        # åˆ›å»ºé”™è¯¯æ¨¡å¼è¡¨ï¼ˆç”¨äºèšåˆç›¸åŒé”™è¯¯ï¼‰
        self.cursor.execute("""
            CREATE TABLE IF NOT EXISTS error_patterns (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                error_hash TEXT UNIQUE NOT NULL,
                error_type TEXT,
                error_pattern TEXT,
                occurrence_count INTEGER DEFAULT 1,
                first_seen DATETIME,
                last_seen DATETIME,
                severity_max INTEGER,
                resolved BOOLEAN DEFAULT 0
            )
        """)

        # åˆ›å»ºç´¢å¼•ä»¥åŠ é€ŸæŸ¥è¯¢
        self.cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_error_hash
            ON error_logs(error_hash)
        """)

        self.cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_timestamp
            ON error_logs(timestamp)
        """)

        self.cursor.execute("""
            CREATE INDEX IF NOT EXISTS idx_severity
            ON error_logs(severity)
        """)

        self.conn.commit()

    def log_error(
        self,
        error: Exception,
        severity: int = SEVERITY_ERROR,
        module: str = None,
        function: str = None,
        context: Dict[str, Any] = None,
        epoch: int = None,
        global_step: int = None
    ) -> int:
        """
        è®°å½•é”™è¯¯åˆ°æ•°æ®åº“

        å‚æ•°:
            error: å¼‚å¸¸å¯¹è±¡
            severity: é”™è¯¯ä¸¥é‡æ€§çº§åˆ«
            module: æ¨¡å—åç§°
            function: å‡½æ•°åç§°
            context: ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼ˆå­—å…¸ï¼‰
            epoch: è®­ç»ƒè½®æ¬¡
            global_step: å…¨å±€è®­ç»ƒæ­¥æ•°

        è¿”å›:
            error_id: é”™è¯¯è®°å½•ID
        """
        timestamp = datetime.now().isoformat()
        error_type = type(error).__name__
        error_message = str(error)
        stacktrace = traceback.format_exc()

        # ç”Ÿæˆé”™è¯¯å“ˆå¸Œï¼ˆç”¨äºæ¨¡å¼è¯†åˆ«ï¼‰
        error_hash = self._generate_error_hash(error_type, error_message, stacktrace)

        # åºåˆ—åŒ–ä¸Šä¸‹æ–‡
        context_json = json.dumps(context) if context else None

        # æ’å…¥é”™è¯¯æ—¥å¿—
        self.cursor.execute("""
            INSERT INTO error_logs
            (timestamp, severity, module, function, error_type, error_message,
             error_hash, stacktrace, context, epoch, global_step)
            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
        """, (
            timestamp, severity, module, function, error_type, error_message,
            error_hash, stacktrace, context_json, epoch, global_step
        ))

        error_id = self.cursor.lastrowid
        self.conn.commit()

        # æ›´æ–°é”™è¯¯æ¨¡å¼
        self._update_error_pattern(error_hash, error_type, error_message, severity, timestamp)

        return error_id

    def _generate_error_hash(self, error_type: str, error_message: str, stacktrace: str) -> str:
        """
        ç”Ÿæˆé”™è¯¯å“ˆå¸Œï¼ˆç”¨äºè¯†åˆ«ç›¸åŒé”™è¯¯ï¼‰

        é€šè¿‡æå–é”™è¯¯ç±»å‹ã€æ¶ˆæ¯æ¨¡å¼å’Œè°ƒç”¨æ ˆå…³é”®éƒ¨åˆ†ç”Ÿæˆå“ˆå¸Œ
        """
        # æå–é”™è¯¯æ¶ˆæ¯çš„æ¨¡å¼ï¼ˆå»é™¤æ•°å­—ã€è·¯å¾„ç­‰åŠ¨æ€éƒ¨åˆ†ï¼‰
        message_pattern = re.sub(r'\d+', 'N', error_message)
        message_pattern = re.sub(r'/[^\s]+', '/PATH', message_pattern)
        message_pattern = re.sub(r'0x[0-9a-fA-F]+', '0xADDR', message_pattern)

        # æå–è°ƒç”¨æ ˆå…³é”®éƒ¨åˆ†ï¼ˆåªä¿ç•™æ–‡ä»¶åå’Œå‡½æ•°åï¼‰
        stack_lines = stacktrace.split('\n')
        stack_pattern = []
        for line in stack_lines:
            if 'File' in line and 'line' in line:
                # æå–æ–‡ä»¶åå’Œå‡½æ•°
                match = re.search(r'File "([^"]+)", line \d+, in (\w+)', line)
                if match:
                    filename = Path(match.group(1)).name
                    function = match.group(2)
                    stack_pattern.append(f"{filename}:{function}")

        # ç»„åˆå“ˆå¸Œå†…å®¹
        hash_content = f"{error_type}|{message_pattern}|{'->'.join(stack_pattern[-3:])}"

        # ç”ŸæˆMD5å“ˆå¸Œ
        return hashlib.md5(hash_content.encode()).hexdigest()[:16]

    def _update_error_pattern(
        self,
        error_hash: str,
        error_type: str,
        error_message: str,
        severity: int,
        timestamp: str
    ):
        """æ›´æ–°é”™è¯¯æ¨¡å¼è¡¨"""
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        self.cursor.execute("""
            SELECT id, occurrence_count, severity_max, first_seen
            FROM error_patterns
            WHERE error_hash = ?
        """, (error_hash,))

        result = self.cursor.fetchone()

        # æå–é”™è¯¯æ¨¡å¼ï¼ˆæ³›åŒ–çš„é”™è¯¯æ¶ˆæ¯ï¼‰
        error_pattern = re.sub(r'\d+', 'N', error_message)
        error_pattern = re.sub(r'/[^\s]+', '/PATH', error_pattern)

        if result:
            # æ›´æ–°ç°æœ‰æ¨¡å¼
            pattern_id, count, max_severity, first_seen = result
            new_count = count + 1
            new_max_severity = max(max_severity, severity)

            self.cursor.execute("""
                UPDATE error_patterns
                SET occurrence_count = ?,
                    severity_max = ?,
                    last_seen = ?,
                    error_pattern = ?
                WHERE id = ?
            """, (new_count, new_max_severity, timestamp, error_pattern, pattern_id))
        else:
            # åˆ›å»ºæ–°æ¨¡å¼
            self.cursor.execute("""
                INSERT INTO error_patterns
                (error_hash, error_type, error_pattern, occurrence_count,
                 first_seen, last_seen, severity_max)
                VALUES (?, ?, ?, 1, ?, ?, ?)
            """, (error_hash, error_type, error_pattern, timestamp, timestamp, severity))

        self.conn.commit()

    def get_error_patterns(
        self,
        min_occurrences: int = 2,
        unresolved_only: bool = True,
        limit: int = 50
    ) -> List[Dict[str, Any]]:
        """
        è·å–é”™è¯¯æ¨¡å¼åˆ—è¡¨

        å‚æ•°:
            min_occurrences: æœ€å°å‡ºç°æ¬¡æ•°
            unresolved_only: åªæ˜¾ç¤ºæœªè§£å†³çš„é”™è¯¯
            limit: è¿”å›æ•°é‡é™åˆ¶

        è¿”å›:
            é”™è¯¯æ¨¡å¼åˆ—è¡¨
        """
        query = """
            SELECT error_hash, error_type, error_pattern, occurrence_count,
                   first_seen, last_seen, severity_max, resolved
            FROM error_patterns
            WHERE occurrence_count >= ?
        """
        params = [min_occurrences]

        if unresolved_only:
            query += " AND resolved = 0"

        query += " ORDER BY occurrence_count DESC, last_seen DESC LIMIT ?"
        params.append(limit)

        self.cursor.execute(query, params)

        patterns = []
        for row in self.cursor.fetchall():
            patterns.append({
                'error_hash': row[0],
                'error_type': row[1],
                'error_pattern': row[2],
                'occurrence_count': row[3],
                'first_seen': row[4],
                'last_seen': row[5],
                'severity': self.SEVERITY_NAMES.get(row[6], 'UNKNOWN'),
                'resolved': bool(row[7])
            })

        return patterns

    def get_error_statistics(self, hours: int = 24) -> Dict[str, Any]:
        """
        è·å–é”™è¯¯ç»Ÿè®¡ä¿¡æ¯

        å‚æ•°:
            hours: ç»Ÿè®¡æœ€è¿‘Nå°æ—¶çš„é”™è¯¯

        è¿”å›:
            ç»Ÿè®¡ä¿¡æ¯å­—å…¸
        """
        from datetime import timedelta
        cutoff_time = (datetime.now() - timedelta(hours=hours)).isoformat()

        # æ€»é”™è¯¯æ•°
        self.cursor.execute("""
            SELECT COUNT(*) FROM error_logs WHERE timestamp >= ?
        """, (cutoff_time,))
        total_errors = self.cursor.fetchone()[0]

        # æŒ‰ä¸¥é‡æ€§åˆ†ç»„
        self.cursor.execute("""
            SELECT severity, COUNT(*)
            FROM error_logs
            WHERE timestamp >= ?
            GROUP BY severity
        """, (cutoff_time,))

        severity_counts = {}
        for row in self.cursor.fetchall():
            severity_counts[self.SEVERITY_NAMES.get(row[0], 'UNKNOWN')] = row[1]

        # æŒ‰é”™è¯¯ç±»å‹åˆ†ç»„
        self.cursor.execute("""
            SELECT error_type, COUNT(*)
            FROM error_logs
            WHERE timestamp >= ?
            GROUP BY error_type
            ORDER BY COUNT(*) DESC
            LIMIT 10
        """, (cutoff_time,))

        error_type_counts = {row[0]: row[1] for row in self.cursor.fetchall()}

        # æœ€å¸¸è§çš„é”™è¯¯æ¨¡å¼
        self.cursor.execute("""
            SELECT error_pattern, occurrence_count, severity_max
            FROM error_patterns
            WHERE last_seen >= ?
            ORDER BY occurrence_count DESC
            LIMIT 5
        """, (cutoff_time,))

        top_patterns = []
        for row in self.cursor.fetchall():
            top_patterns.append({
                'pattern': row[0],
                'count': row[1],
                'severity': self.SEVERITY_NAMES.get(row[2], 'UNKNOWN')
            })

        return {
            'total_errors': total_errors,
            'severity_breakdown': severity_counts,
            'error_types': error_type_counts,
            'top_patterns': top_patterns,
            'time_window_hours': hours
        }

    def mark_pattern_resolved(self, error_hash: str):
        """æ ‡è®°é”™è¯¯æ¨¡å¼ä¸ºå·²è§£å†³"""
        self.cursor.execute("""
            UPDATE error_patterns SET resolved = 1 WHERE error_hash = ?
        """, (error_hash,))

        self.cursor.execute("""
            UPDATE error_logs SET resolved = 1 WHERE error_hash = ?
        """, (error_hash,))

        self.conn.commit()

    def search_errors(
        self,
        keyword: str = None,
        severity: int = None,
        error_type: str = None,
        limit: int = 100
    ) -> List[Dict[str, Any]]:
        """
        æœç´¢é”™è¯¯æ—¥å¿—

        å‚æ•°:
            keyword: å…³é”®è¯ï¼ˆæœç´¢é”™è¯¯æ¶ˆæ¯ï¼‰
            severity: ä¸¥é‡æ€§çº§åˆ«
            error_type: é”™è¯¯ç±»å‹
            limit: è¿”å›æ•°é‡é™åˆ¶

        è¿”å›:
            é”™è¯¯æ—¥å¿—åˆ—è¡¨
        """
        query = "SELECT * FROM error_logs WHERE 1=1"
        params = []

        if keyword:
            query += " AND error_message LIKE ?"
            params.append(f"%{keyword}%")

        if severity is not None:
            query += " AND severity = ?"
            params.append(severity)

        if error_type:
            query += " AND error_type = ?"
            params.append(error_type)

        query += " ORDER BY timestamp DESC LIMIT ?"
        params.append(limit)

        self.cursor.execute(query, params)

        errors = []
        for row in self.cursor.fetchall():
            errors.append({
                'id': row[0],
                'timestamp': row[1],
                'severity': self.SEVERITY_NAMES.get(row[2], 'UNKNOWN'),
                'module': row[3],
                'function': row[4],
                'error_type': row[5],
                'error_message': row[6],
                'error_hash': row[7],
                'stacktrace': row[8],
                'context': json.loads(row[9]) if row[9] else None,
                'epoch': row[10],
                'global_step': row[11],
                'resolved': bool(row[12])
            })

        return errors

    # ========================================================================
    # ğŸ”® WebUI/API Export Interface
    # ========================================================================

    def export_for_webui(self, export_path: str = None) -> Dict[str, Any]:
        """
        å¯¼å‡ºé”™è¯¯æ•°æ®ä¾›WebUI/APIä½¿ç”¨

        æœªæ¥APIç«¯ç‚¹:
        - GET /api/errors/statistics
        - GET /api/errors/patterns
        - GET /api/errors/timeline
        - POST /api/errors/resolve/{hash}

        å‚æ•°:
            export_path: JSONæ–‡ä»¶å¯¼å‡ºè·¯å¾„ï¼ˆå¯é€‰ï¼‰

        è¿”å›:
            å®Œæ•´çš„é”™è¯¯åˆ†ææ•°æ®
        """
        # è·å–ç»Ÿè®¡ä¿¡æ¯
        stats_24h = self.get_error_statistics(hours=24)
        stats_7d = self.get_error_statistics(hours=24 * 7)

        # è·å–é”™è¯¯æ¨¡å¼
        patterns = self.get_error_patterns(min_occurrences=1, unresolved_only=False, limit=100)

        # è·å–æœ€è¿‘é”™è¯¯
        recent_errors = self.search_errors(limit=50)

        # é”™è¯¯æ—¶é—´çº¿ï¼ˆæŒ‰å°æ—¶èšåˆï¼‰
        self.cursor.execute("""
            SELECT
                strftime('%Y-%m-%d %H:00:00', timestamp) as hour,
                severity,
                COUNT(*) as count
            FROM error_logs
            WHERE timestamp >= datetime('now', '-7 days')
            GROUP BY hour, severity
            ORDER BY hour DESC
        """)

        timeline = defaultdict(lambda: {'total': 0, 'by_severity': {}})
        for row in self.cursor.fetchall():
            hour, severity, count = row
            severity_name = self.SEVERITY_NAMES.get(severity, 'UNKNOWN')
            timeline[hour]['total'] += count
            timeline[hour]['by_severity'][severity_name] = count

        timeline_list = [
            {'hour': hour, **data}
            for hour, data in sorted(timeline.items(), reverse=True)
        ]

        data = {
            'statistics': {
                'last_24h': stats_24h,
                'last_7d': stats_7d
            },
            'patterns': patterns,
            'recent_errors': recent_errors,
            'timeline': timeline_list,
            'generated_at': datetime.now().isoformat()
        }

        # å¯¼å‡ºåˆ°JSONæ–‡ä»¶
        if export_path:
            export_file = Path(export_path)
            export_file.parent.mkdir(parents=True, exist_ok=True)
            with open(export_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)

        return data

    def generate_error_report(self, output_path: str = None) -> str:
        """
        ç”Ÿæˆé”™è¯¯åˆ†ææŠ¥å‘Šï¼ˆMarkdownæ ¼å¼ï¼‰

        å‚æ•°:
            output_path: æŠ¥å‘Šä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰

        è¿”å›:
            Markdownæ ¼å¼çš„æŠ¥å‘Šå†…å®¹
        """
        stats = self.get_error_statistics(hours=24)
        patterns = self.get_error_patterns(min_occurrences=2, limit=20)

        report = []
        report.append("# é”™è¯¯åˆ†ææŠ¥å‘Š")
        report.append(f"\nç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"\nç»Ÿè®¡æ—¶é—´çª—å£: æœ€è¿‘ {stats['time_window_hours']} å°æ—¶\n")

        # ç»Ÿè®¡æ¦‚è§ˆ
        report.append("## é”™è¯¯ç»Ÿè®¡æ¦‚è§ˆ\n")
        report.append(f"- **æ€»é”™è¯¯æ•°**: {stats['total_errors']}\n")

        report.append("### æŒ‰ä¸¥é‡æ€§åˆ†ç±»\n")
        for severity, count in stats['severity_breakdown'].items():
            report.append(f"- **{severity}**: {count}")

        report.append("\n### é”™è¯¯ç±»å‹åˆ†å¸ƒ\n")
        for error_type, count in stats['error_types'].items():
            report.append(f"- `{error_type}`: {count}")

        # é«˜é¢‘é”™è¯¯æ¨¡å¼
        report.append("\n## é«˜é¢‘é”™è¯¯æ¨¡å¼\n")
        if patterns:
            report.append("| é”™è¯¯ç±»å‹ | é”™è¯¯æ¨¡å¼ | å‡ºç°æ¬¡æ•° | ä¸¥é‡æ€§ | çŠ¶æ€ |")
            report.append("|----------|----------|----------|--------|------|")
            for p in patterns:
                status = "âœ… å·²è§£å†³" if p['resolved'] else "âš ï¸ æœªè§£å†³"
                report.append(
                    f"| {p['error_type']} | {p['error_pattern'][:50]}... | "
                    f"{p['occurrence_count']} | {p['severity']} | {status} |"
                )
        else:
            report.append("*æ— é‡å¤é”™è¯¯æ¨¡å¼*")

        # æœ€å¸¸è§é”™è¯¯
        report.append("\n## æœ€å¸¸è§é”™è¯¯ (Top 5)\n")
        if stats['top_patterns']:
            for i, pattern in enumerate(stats['top_patterns'], 1):
                report.append(f"{i}. **[{pattern['severity']}]** {pattern['pattern']} "
                            f"(å‡ºç° {pattern['count']} æ¬¡)")
        else:
            report.append("*æš‚æ— æ•°æ®*")

        # å»ºè®®
        report.append("\n## æ”¹è¿›å»ºè®®\n")
        if stats['total_errors'] > 100:
            report.append("- âš ï¸ é”™è¯¯æ•°é‡è¾ƒé«˜ï¼Œå»ºè®®æ£€æŸ¥è®­ç»ƒé…ç½®å’Œæ•°æ®è´¨é‡")

        critical_count = stats['severity_breakdown'].get('CRITICAL', 0)
        if critical_count > 0:
            report.append(f"- ğŸš¨ å‘ç° {critical_count} ä¸ªä¸¥é‡é”™è¯¯ï¼Œéœ€è¦ç«‹å³å¤„ç†")

        if len(patterns) > 10:
            report.append(f"- ğŸ” å‘ç° {len(patterns)} ä¸ªé‡å¤é”™è¯¯æ¨¡å¼ï¼Œå»ºè®®è¿›è¡Œæ¨¡å¼åˆ†æå’Œæ‰¹é‡ä¿®å¤")

        report_text = '\n'.join(report)

        # ä¿å­˜æŠ¥å‘Š
        if output_path:
            report_file = Path(output_path)
            report_file.parent.mkdir(parents=True, exist_ok=True)
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report_text)

        return report_text

    def close(self):
        """å…³é—­æ•°æ®åº“è¿æ¥"""
        if self.conn:
            self.conn.close()

    def __enter__(self):
        """æ”¯æŒä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        return self

    def __exit__(self, exc_type, exc_val, exc_tb):
        """é€€å‡ºä¸Šä¸‹æ–‡æ—¶è‡ªåŠ¨å…³é—­"""
        self.close()


# ============================================================================
# ä¾¿æ·å‡½æ•°
# ============================================================================

def get_global_error_persistence() -> ErrorPersistence:
    """
    è·å–å…¨å±€é”™è¯¯æŒä¹…åŒ–å®ä¾‹ï¼ˆå•ä¾‹æ¨¡å¼ï¼‰

    è¿”å›:
        ErrorPersistenceå®ä¾‹
    """
    global _global_error_persistence

    if '_global_error_persistence' not in globals():
        _global_error_persistence = ErrorPersistence()

    return _global_error_persistence


def log_training_error(
    error: Exception,
    severity: int = ErrorPersistence.SEVERITY_ERROR,
    epoch: int = None,
    global_step: int = None,
    context: Dict[str, Any] = None
):
    """
    ä¾¿æ·å‡½æ•°: è®°å½•è®­ç»ƒè¿‡ç¨‹ä¸­çš„é”™è¯¯

    å‚æ•°:
        error: å¼‚å¸¸å¯¹è±¡
        severity: ä¸¥é‡æ€§çº§åˆ«
        epoch: è®­ç»ƒè½®æ¬¡
        global_step: å…¨å±€æ­¥æ•°
        context: é¢å¤–ä¸Šä¸‹æ–‡ä¿¡æ¯
    """
    ep = get_global_error_persistence()

    # è‡ªåŠ¨æå–è°ƒç”¨æ ˆä¿¡æ¯
    import inspect
    frame = inspect.currentframe().f_back
    module = frame.f_globals.get('__name__', 'unknown')
    function = frame.f_code.co_name

    return ep.log_error(
        error=error,
        severity=severity,
        module=module,
        function=function,
        context=context,
        epoch=epoch,
        global_step=global_step
    )
