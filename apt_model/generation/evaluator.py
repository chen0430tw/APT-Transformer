#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Text generation evaluation module for APT Model (自生成变换器)

This module provides tools to evaluate the quality of text generated by APT models.
It includes different evaluation metrics and scoring functions tailored to
various text types and applications.
"""

import re
import math
import logging
from typing import Tuple, Dict, List, Optional, Union, Any

import numpy as np

try:
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
except ImportError:  # pragma: no cover - optional dependency path
    TfidfVectorizer = None
    cosine_similarity = None

logger = logging.getLogger('apt_model.generation.evaluator')

class TextQualityEvaluator:
    """
    Evaluates the quality of generated text.
    
    This class provides methods to assess various aspects of text quality,
    including diversity, coherence, fluency, and relevance to prompts or references.
    """
    
    def __init__(self, use_external_metrics: bool = True, metrics_weights: Optional[Dict[str, float]] = None):
        """
        Initialize the text quality evaluator.
        
        Args:
            use_external_metrics: Whether to use external NLP metrics (requires additional dependencies)
            metrics_weights: Custom weights for different evaluation metrics
        """
        self.use_external_metrics = use_external_metrics
        
        # Default metric weights
        self.metrics_weights = metrics_weights or {
            "length": 0.10,
            "diversity": 0.25,
            "structure": 0.20,
            "fluency": 0.20,
            "relevance": 0.25
        }
        
        # Load external metrics if available
        self._load_external_metrics()
        
    def _load_external_metrics(self):
        """Load external NLP metrics if available"""
        self.has_nlp_metrics = False
        
        if not self.use_external_metrics:
            return
            
        if TfidfVectorizer is not None and cosine_similarity is not None:
            self.has_nlp_metrics = True
        else:
            logger.warning("sklearn not available, some evaluation metrics will be disabled")
    
    def evaluate_text_quality(self, text: str, reference: Optional[str] = None, context: Optional[str] = None) -> Tuple[float, str]:
        """
        Evaluate the quality of generated text.
        
        Args:
            text: The generated text to evaluate
            reference: Optional reference text to compare against
            context: Optional context or prompt that generated the text
            
        Returns:
            Tuple containing (quality_score, feedback_message)
        """
        # Basic validity check
        if not text or len(text.strip()) < 5:
            return 10.0, "Text is too short or empty"
        
        # Calculate individual metrics
        metrics = {}
        metrics["length"] = self._evaluate_length(text)
        metrics["diversity"] = self._evaluate_diversity(text)
        metrics["structure"] = self._evaluate_structure(text)
        metrics["fluency"] = self._evaluate_fluency(text)
        
        # Calculate relevance if reference or context is provided
        if reference or context:
            metrics["relevance"] = self._evaluate_relevance(text, reference, context)
        
        # Calculate weighted score
        total_score = 0.0
        total_weight = 0.0
        
        for metric, score in metrics.items():
            weight = self.metrics_weights.get(metric, 0.0)
            total_score += score * weight
            total_weight += weight
        
        if total_weight > 0:
            final_score = total_score / total_weight
        else:
            final_score = 50.0  # Default middle score if no metrics were applied
            
        # Ensure the score is within bounds
        final_score = min(100.0, max(0.0, final_score))
        
        # Generate feedback based on the score and individual metrics
        feedback = self._generate_feedback(final_score, metrics)
        
        return final_score, feedback
    
    def _evaluate_length(self, text: str) -> float:
        """
        Evaluate text based on its length.
        
        Returns a score between 0-100.
        """
        words = text.split()
        word_count = len(words)
        
        # Too short texts receive lower scores
        if word_count < 5:
            return 20.0
        elif word_count < 20:
            return 50.0
        elif word_count < 50:
            return 70.0
        elif word_count < 100:
            return 90.0
        else:
            return 100.0
    
    def _evaluate_diversity(self, text: str) -> float:
        """
        Evaluate lexical diversity of the text.
        
        Returns a score between 0-100.
        """
        words = [word.lower() for word in re.findall(r'\b\w+\b', text)]
        
        if not words:
            return 0.0
            
        # Calculate type-token ratio (unique words / total words)
        unique_words = set(words)
        ttr = len(unique_words) / len(words)
        
        # Adjust TTR to account for diminishing returns with longer texts
        adjusted_ttr = ttr * (1 + math.log10(min(len(words), 1000) / 10) / 2)
        
        # Scale to 0-100
        diversity_score = min(100.0, adjusted_ttr * 100.0)
        
        return diversity_score
    
    def _evaluate_structure(self, text: str) -> float:
        """
        Evaluate the structural quality of the text (paragraphs, sentences).
        
        Returns a score between 0-100.
        """
        # Count paragraphs
        paragraphs = [p for p in text.split('\n\n') if p.strip()]
        
        # Count sentences
        sentence_enders = ['.', '!', '?', '。', '！', '？']
        estimated_sentences = sum(text.count(ender) for ender in sentence_enders)
        
        # Basic structure assessment
        if len(paragraphs) < 1 or estimated_sentences < 1:
            return 30.0  # Text lacks basic structure
        
        # Calculate average sentences per paragraph (for complexity assessment)
        sentences_per_paragraph = estimated_sentences / max(1, len(paragraphs))
        
        # Score based on paragraphs and sentence distribution
        if len(paragraphs) >= 4 and 2 <= sentences_per_paragraph <= 6:
            structure_score = 90.0  # Well-structured text
        elif len(paragraphs) >= 2 and 1 <= sentences_per_paragraph <= 8:
            structure_score = 70.0  # Decent structure
        else:
            structure_score = 50.0  # Basic structure
            
        return structure_score
    
    def _evaluate_fluency(self, text: str) -> float:
        """
        Evaluate linguistic fluency of the text.
        
        Returns a score between 0-100.
        """
        # This is a simplified fluency measure using basic heuristics
        
        # Check for sentence fragments (sentences without verbs)
        sentences = re.split(r'[.!?。！？]\s*', text)
        sentences = [s.strip() for s in sentences if s.strip()]
        
        # Placeholder for more sophisticated fluency analysis
        # A proper implementation would use linguistic tools
        
        # For now, use a basic approach based on sentence length variation
        if not sentences:
            return 30.0
            
        sentence_lengths = [len(s.split()) for s in sentences]
        
        # Very uniform or very chaotic sentence lengths suggest lower fluency
        if len(sentence_lengths) >= 3:
            mean_length = sum(sentence_lengths) / len(sentence_lengths)
            variance = sum((length - mean_length) ** 2 for length in sentence_lengths) / len(sentence_lengths)
            std_dev = math.sqrt(variance)
            
            # Ideal std deviation is between 2-6 for fluent text
            if 2 <= std_dev <= 6:
                return 80.0
            elif 1 <= std_dev <= 8:
                return 65.0
            else:
                return 50.0
        
        return 60.0  # Default fluency score for short texts
    
    def _evaluate_relevance(self, text: str, reference: Optional[str] = None, context: Optional[str] = None) -> float:
        """
        Evaluate relevance of the text to a reference or context.
        
        Returns a score between 0-100.
        """
        # If we have NLP metrics and both text and reference
        if self.has_nlp_metrics and reference and TfidfVectorizer is not None and cosine_similarity is not None:
            try:
                # Use TF-IDF and cosine similarity
                vectorizer = TfidfVectorizer(stop_words='english')
                tfidf_matrix = vectorizer.fit_transform([reference, text])
                similarity = cosine_similarity(tfidf_matrix[0:1], tfidf_matrix[1:2])[0][0]
                
                # Scale similarity to 0-100
                relevance_score = min(100.0, similarity * 100.0)
                
                # Apply length penalty/bonus
                ref_len = len(reference.split())
                text_len = len(text.split())
                
                length_ratio = min(text_len / max(1, ref_len), max(1, ref_len) / max(1, text_len))
                length_factor = (length_ratio - 0.5) * 20  # Bonus for length similarity
                
                relevance_score = min(100.0, max(0.0, relevance_score + length_factor))
                
                return relevance_score
            except Exception as e:
                logger.warning(f"Error computing relevance score: {e}")
                pass
        
        # Context-based relevance (simplified)
        if context:
            # Check for key words from context in the generated text
            context_words = set([w.lower() for w in re.findall(r'\b\w+\b', context)])
            text_words = set([w.lower() for w in re.findall(r'\b\w+\b', text)])
            
            # Calculate word overlap
            if context_words:
                overlap = len(context_words.intersection(text_words)) / len(context_words)
                return min(100.0, overlap * 100.0)
        
        # Default score if no proper comparison could be done
        return 60.0
    
    def _generate_feedback(self, score: float, metrics: Dict[str, float]) -> str:
        """
        Generate feedback based on the quality score and metrics.
        
        Args:
            score: Overall quality score (0-100)
            metrics: Individual metric scores
            
        Returns:
            Feedback string with assessment of the text quality
        """
        if score >= 85:
            feedback = "High quality text with excellent structure and vocabulary"
        elif score >= 70:
            feedback = "Good quality text, well-structured and clear"
        elif score >= 55:
            feedback = "Acceptable quality, text is coherent but could be improved"
        elif score >= 40:
            feedback = "Basic quality, text has some issues that need attention"
        else:
            feedback = "Low quality text with significant issues"
        
        # Add specific feedback for metrics with particularly low or high scores
        specific_feedback = []
        
        for metric, metric_score in metrics.items():
            if metric_score < 40:
                if metric == "length":
                    specific_feedback.append("text is too short")
                elif metric == "diversity":
                    specific_feedback.append("vocabulary is limited")
                elif metric == "structure":
                    specific_feedback.append("structure needs improvement")
                elif metric == "fluency":
                    specific_feedback.append("fluency could be enhanced")
                elif metric == "relevance":
                    specific_feedback.append("relevance to the prompt is low")
            elif metric_score > 85:
                if metric == "diversity":
                    specific_feedback.append("rich vocabulary")
                elif metric == "structure":
                    specific_feedback.append("excellent structure")
        
        if specific_feedback:
            feedback += " (" + ", ".join(specific_feedback) + ")"
            
        return feedback


class CodeQualityEvaluator:
    """
    Evaluates the quality of generated code.
    
    This class provides methods to assess code quality, structure,
    and potential functionality.
    """
    
    def __init__(self):
        """Initialize the code quality evaluator."""
        self.supported_languages = {
            "python": {
                "extensions": [".py"],
                "elements": [
                    ("function_definition", r"def\s+\w+\s*\("),
                    ("class_definition", r"class\s+\w+"),
                    ("return_statement", r"return\s+"),
                    ("conditional", r"if\s+.*:"),
                    ("loop", r"for\s+.*:|while\s+.*:"),
                    ("list_comprehension", r"\[\s*.*\s+for\s+.*\s+in\s+.*\s*\]"),
                    ("error_handling", r"try\s*:|except\s+.*:"),
                    ("import_statement", r"import\s+|from\s+.*\s+import"),
                    ("docstring", r'""".*"""|\'\'\'.*\'\'\''),
                    ("comment", r"#.*")
                ]
            },
            "javascript": {
                "extensions": [".js", ".jsx", ".ts", ".tsx"],
                "elements": [
                    ("function_definition", r"function\s+\w+\s*\(|const\s+\w+\s*=\s*\(.*\)\s*=>"),
                    ("class_definition", r"class\s+\w+"),
                    ("return_statement", r"return\s+"),
                    ("conditional", r"if\s*\(.*\)"),
                    ("loop", r"for\s*\(.*\)|while\s*\(.*\)"),
                    ("arrow_function", r"=>\s*{"),
                    ("error_handling", r"try\s*{|catch\s*\(.*\)\s*{"),
                    ("import_export", r"import\s+|export\s+"),
                    ("comment", r"\/\/.*|\/\*[\s\S]*?\*\/")
                ]
            }
        }
        
    def evaluate_code_quality(self, code: str, language: str = None, reference: str = None) -> Tuple[float, str]:
        """
        Evaluate the quality of generated code.
        
        Args:
            code: The generated code to evaluate
            language: Programming language of the code
            reference: Optional reference code to compare against
            
        Returns:
            Tuple containing (quality_score, feedback_message)
        """
        if not code or len(code.strip()) < 5:
            return 10.0, "Code is too short or empty"
            
        # Detect language if not provided
        if language is None:
            language = self._detect_language(code)
        
        language = language.lower()
        
        # Check if language is supported
        if language not in self.supported_languages:
            # Use generic evaluation for unsupported languages
            return self._evaluate_generic_code(code, reference)
        
        # Get language-specific patterns
        language_data = self.supported_languages[language]
        
        # Score code structure and elements
        structure_score, elements_found = self._score_code_structure(code, language_data["elements"])
        
        # Score syntax correctness (simple heuristics)
        syntax_score = self._score_syntax(code, language)
        
        # Score complexity
        complexity_score = self._score_complexity(code)
        
        # Calculate final score
        final_score = (structure_score * 0.4) + (syntax_score * 0.4) + (complexity_score * 0.2)
        final_score = min(100.0, max(0.0, final_score))
        
        # Generate feedback
        feedback = self._generate_code_feedback(final_score, elements_found, language)
        
        return final_score, feedback
    
    def _detect_language(self, code: str) -> str:
        """
        Attempt to detect the programming language from code.
        
        Args:
            code: Code to analyze
            
        Returns:
            Detected language name or "generic" if detection fails
        """
        # Simple language detection based on keywords and syntax
        if re.search(r"def\s+\w+\s*\(|import\s+|from\s+.*\s+import", code):
            return "python"
        elif re.search(r"function\s+\w+\s*\(|var\s+|const\s+|let\s+", code):
            return "javascript"
        else:
            return "generic"
    
    def _score_code_structure(self, code: str, elements: List[Tuple[str, str]]) -> Tuple[float, List[str]]:
        """
        Score the structure of code based on expected elements.
        
        Args:
            code: Code to analyze
            elements: List of (element_name, regex_pattern) tuples to check
            
        Returns:
            Tuple of (structure_score, list_of_found_elements)
        """
        found_elements = []
        
        for element_name, pattern in elements:
            if re.search(pattern, code):
                found_elements.append(element_name)
        
        # Calculate score based on proportion of elements found
        if elements:
            structure_score = (len(found_elements) / len(elements)) * 100.0
        else:
            structure_score = 50.0
            
        return structure_score, found_elements
    
    def _score_syntax(self, code: str, language: str) -> float:
        """
        Score syntax correctness with simple heuristics.
        
        Args:
            code: Code to analyze
            language: Programming language
            
        Returns:
            Syntax score (0-100)
        """
        syntax_score = 80.0  # Start with assumption of mostly correct syntax
        
        if language == "python":
            # Check indentation consistency
            lines = code.split('\n')
            indent_levels = []
            
            for line in lines:
                if line.strip() and not line.strip().startswith('#'):
                    indent = len(line) - len(line.lstrip())
                    indent_levels.append(indent)
            
            if indent_levels:
                # Check if indentation is consistent
                unique_indents = set(indent_levels)
                if len(unique_indents) > 5:  # Too many different indentation levels
                    syntax_score -= 20.0
                
                # Check for potential syntax errors
                if ":" in code and not re.search(r"if.*:|for.*:|def.*:|class.*:|while.*:|try.*:|except.*:", code):
                    syntax_score -= 10.0
                
                # Check for matching parentheses and brackets
                if code.count('(') != code.count(')'):
                    syntax_score -= 15.0
                
                if code.count('[') != code.count(']'):
                    syntax_score -= 15.0
                
                if code.count('{') != code.count('}'):
                    syntax_score -= 15.0
        
        elif language == "javascript":
            # Check for potential syntax errors
            if code.count('{') != code.count('}'):
                syntax_score -= 20.0
            
            if code.count('(') != code.count(')'):
                syntax_score -= 15.0
            
            if code.count('[') != code.count(']'):
                syntax_score -= 15.0
            
            # Check for missing semicolons in appropriate places
            if re.search(r"var\s+\w+\s*=.*;|const\s+\w+\s*=.*;|let\s+\w+\s*=.*;", code) and not re.search(r";", code):
                syntax_score -= 10.0
        
        return max(0.0, syntax_score)
    
    def _score_complexity(self, code: str) -> float:
        """
        Score code complexity.
        
        Args:
            code: Code to analyze
            
        Returns:
            Complexity score (0-100)
        """
        # Simplified complexity scoring
        lines = code.split('\n')
        non_empty_lines = [line for line in lines if line.strip()]
        
        if not non_empty_lines:
            return 0.0
            
        # Code length factor
        if len(non_empty_lines) < 5:
            length_score = 40.0
        elif len(non_empty_lines) < 15:
            length_score = 70.0
        elif len(non_empty_lines) < 50:
            length_score = 90.0
        else:
            length_score = 100.0
            
        # Commenting factor
        comment_lines = [line for line in lines if re.search(r"^\s*#|^\s*\/\/|^\s*\/\*|\*\/\s*$", line.strip())]
        if non_empty_lines:
            comment_ratio = len(comment_lines) / len(non_empty_lines)
            if comment_ratio < 0.05:
                comment_score = 50.0  # Too few comments
            elif comment_ratio < 0.2:
                comment_score = 90.0  # Good comment ratio
            else:
                comment_score = 70.0  # Possibly too many comments
        else:
            comment_score = 0.0
            
        # Average line length (too long lines are bad)
        avg_line_length = sum(len(line) for line in non_empty_lines) / len(non_empty_lines)
        if avg_line_length > 100:
            line_length_score = 50.0  # Lines too long
        elif avg_line_length > 80:
            line_length_score = 70.0  # Lines a bit long
        else:
            line_length_score = 90.0  # Good line length
            
        # Combine scores
        complexity_score = (length_score * 0.4) + (comment_score * 0.3) + (line_length_score * 0.3)
        return complexity_score
    
    def _evaluate_generic_code(self, code: str, reference: str = None) -> Tuple[float, str]:
        """
        Evaluate code when language-specific evaluation is not available.
        
        Args:
            code: Code to evaluate
            reference: Reference code for comparison
            
        Returns:
            Tuple of (score, feedback)
        """
        # Basic code quality checks
        lines = code.split('\n')
        non_empty_lines = [line for line in lines if line.strip()]
        
        if not non_empty_lines:
            return 10.0, "Code is empty or contains only whitespace"
            
        # Length assessment
        if len(non_empty_lines) < 3:
            length_score = 30.0
        elif len(non_empty_lines) < 10:
            length_score = 60.0
        else:
            length_score = 80.0
            
        # Structure check (brackets, parentheses matching)
        if code.count('(') == code.count(')') and code.count('{') == code.count('}') and code.count('[') == code.count(']'):
            structure_score = 80.0
        else:
            structure_score = 40.0
            
        # Final score
        final_score = (length_score * 0.5) + (structure_score * 0.5)
        
        # Feedback
        if final_score >= 70:
            feedback = "Code appears well-structured"
        elif final_score >= 50:
            feedback = "Code has basic structure but may contain issues"
        else:
            feedback = "Code has structural problems"
            
        return final_score, feedback
    
    def _generate_code_feedback(self, score: float, elements_found: List[str], language: str) -> str:
        """
        Generate feedback for code quality evaluation.
        
        Args:
            score: Quality score
            elements_found: Code elements found
            language: Programming language
            
        Returns:
            Feedback string
        """
        # Base feedback on score
        if score >= 85:
            feedback = f"高质量{language}代码，结构完善"
        elif score >= 70:
            feedback = f"质量良好的{language}代码，基本实现了功能"
        elif score >= 50:
            feedback = f"{language}代码质量一般，可能需要改进"
        else:
            feedback = f"{language}代码质量较差，存在问题"
            
        # Add details about elements
        if elements_found:
            elements_str = ", ".join(elements_found)
            feedback += f"。包含以下元素: {elements_str}"
            
        return feedback


class ChineseTextEvaluator:
    """
    Specialized evaluator for Chinese text quality.
    
    This class provides methods specific to evaluating Chinese text,
    considering its unique linguistic features.
    """
    
    def __init__(self):
        """Initialize the Chinese text evaluator."""
        pass
        
    def evaluate_chinese_text(self, text: str, reference: Optional[str] = None) -> Tuple[float, str]:
        """
        Evaluate Chinese text quality.
        
        Args:
            text: Chinese text to evaluate
            reference: Optional reference text for comparison
            
        Returns:
            Tuple of (score, feedback)
        """
        if not text or len(text.strip()) < 5:
            return 10.0, "文本过短"
            
        # Check for Chinese characters
        if not re.search('[\u4e00-\u9fff]', text):
            return 30.0, "文本不包含中文字符"
            
        # Analyze sentence completeness based on punctuation
        punctuations = ['。', '！', '？', '；']
        sentence_ends = sum(text.count(p) for p in punctuations)
        
        if sentence_ends < 1:
            structure_score = 40.0
        elif sentence_ends < 3:
            structure_score = 70.0
        else:
            structure_score = 90.0
            
        # Check diversity of characters (simplified)
        chars = [c for c in text if '\u4e00' <= c <= '\u9fff']
        if not chars:
            diversity_score = 0.0
        else:
            unique_chars = len(set(chars))
            total_chars = len(chars)
            
            # For Chinese, the character diversity ratio is typically lower than in English
            char_diversity = unique_chars / total_chars
            
            if char_diversity < 0.3:
                diversity_score = 50.0
            elif char_diversity < 0.4:
                diversity_score = 70.0
            else:
                diversity_score = 90.0
                
        # Compare with reference if provided
        if reference:
            ref_chars = set(c for c in reference if '\u4e00' <= c <= '\u9fff')
            text_chars = set(c for c in text if '\u4e00' <= c <= '\u9fff')
            
            if ref_chars:
                common_chars = ref_chars.intersection(text_chars)
                similarity_score = min(100.0, (len(common_chars) / len(ref_chars)) * 100.0)
            else:
                similarity_score = 50.0
        else:
            similarity_score = 70.0  # Default if no reference
            
        # Calculate final score
        final_score = (structure_score * 0.4) + (diversity_score * 0.3) + (similarity_score * 0.3)
        final_score = min(100.0, max(0.0, final_score))
        
        # Generate feedback
        if final_score >= 80:
            feedback = "中文表达很好，内容相关性高"
        elif final_score >= 60:
            feedback = "中文表达良好，内容基本相关"
        elif final_score >= 40:
            feedback = "中文表达一般，内容相关性有限"
        else:
            feedback = "中文表达较差，内容需要改进"
            
        return final_score, feedback


def evaluate_text_quality(text: str, reference: Optional[str] = None, context: Optional[str] = None) -> Tuple[float, str]:
    """
    Simple wrapper function for text quality evaluation.
    
    Args:
        text: Text to evaluate
        reference: Optional reference text
        context: Optional context or prompt
        
    Returns:
        Tuple of (score, feedback)
    """
    evaluator = TextQualityEvaluator()
    return evaluator.evaluate_text_quality(text, reference, context)


def evaluate_code_quality(code: str, language: Optional[str] = None) -> Tuple[float, str]:
    """
    Simple wrapper function for code quality evaluation.
    
    Args:
        code: Code to evaluate
        language: Programming language
        
    Returns:
        Tuple of (score, feedback)
    """
    evaluator = CodeQualityEvaluator()
    return evaluator.evaluate_code_quality(code, language)


def evaluate_chinese_text(text: str, reference: Optional[str] = None) -> Tuple[float, str]:
    """
    Simple wrapper function for Chinese text evaluation.
    
    Args:
        text: Chinese text to evaluate
        reference: Optional reference text
        
    Returns:
        Tuple of (score, feedback)
    """
    evaluator = ChineseTextEvaluator()
    return evaluator.evaluate_chinese_text(text, reference)