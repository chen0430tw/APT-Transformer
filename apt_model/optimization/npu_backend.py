"""
NPUåç«¯é€‚é…å™¨ - ç»Ÿä¸€GPU/NPU/XPU/CPUæ¥å£

æ”¯æŒçš„åŠ é€Ÿå™¨ï¼š
- ğŸŸ¢ NVIDIA CUDA GPU (cuda) - torch.cuda
- ğŸŸ¡ åä¸ºæ˜‡è…¾NPU (npu) - torch_npu
- ğŸ”µ Intel XPU/Ultra NPU (xpu) - intel_extension_for_pytorch
- ğŸŸ£ Intel Habana Gaudi (hpu) - habana_frameworks.torch
- ğŸŸ  AMD ROCm GPU (cuda) - torch.cuda with ROCm
- âšª CPU Fallback (cpu)

ä½œè€…: claude + chen0430tw
ç‰ˆæœ¬: 2.0 (Multi-Vendor Accelerator Support)
"""

import torch
from typing import Optional, Dict, List, Any, Tuple
import warnings


class DeviceBackend:
    """ç»Ÿä¸€è®¾å¤‡åç«¯æ¥å£"""

    def __init__(self, device: torch.device):
        """
        åˆå§‹åŒ–è®¾å¤‡åç«¯

        Args:
            device: PyTorchè®¾å¤‡å¯¹è±¡
        """
        self.device = device
        self.device_type = device.type
        self.device_index = device.index if device.index is not None else 0

        # æ£€æµ‹å„ç±»åŠ é€Ÿå™¨æ”¯æŒ
        self._detect_accelerators()

        # éªŒè¯è®¾å¤‡ç±»å‹
        self._validate_device()

    def _detect_accelerators(self):
        """æ£€æµ‹æ‰€æœ‰å¯ç”¨çš„åŠ é€Ÿå™¨"""
        # åä¸ºæ˜‡è…¾ NPU (Ascend)
        self.npu_available = False
        self.torch_npu = None
        try:
            import torch_npu
            self.torch_npu = torch_npu
            self.npu_available = torch_npu.npu.is_available()
        except ImportError:
            pass

        # Intel Habana Gaudi HPU
        self.hpu_available = False
        self.habana_torch = None
        try:
            import habana_frameworks.torch as habana_torch
            self.habana_torch = habana_torch
            self.hpu_available = hasattr(habana_torch, 'hpu') and habana_torch.hpu.is_available()
        except ImportError:
            pass

        # Intel XPU (åŒ…æ‹¬Ultra NPU)
        self.xpu_available = False
        self.ipex = None
        try:
            import intel_extension_for_pytorch as ipex
            self.ipex = ipex
            self.xpu_available = hasattr(ipex, 'xpu') and ipex.xpu.is_available()
        except ImportError:
            pass

        # Qualcomm (é€šè¿‡QNNï¼Œä½†PyTorchä¸ç›´æ¥æ”¯æŒï¼Œéœ€è¦ExecuTorch)
        self.qnn_available = False
        # QNNé€šå¸¸ç”¨äºè¾¹ç¼˜è®¾å¤‡ï¼Œä¸é€šè¿‡æ ‡å‡†PyTorchæ¥å£

    def _validate_device(self):
        """éªŒè¯è¯·æ±‚çš„è®¾å¤‡ç±»å‹æ˜¯å¦å¯ç”¨"""
        if self.device_type == 'npu' and not self.npu_available:
            warnings.warn("NPU device requested but torch_npu not available, falling back to CPU")
            self.device = torch.device('cpu')
            self.device_type = 'cpu'
        elif self.device_type == 'hpu' and not self.hpu_available:
            warnings.warn("HPU device requested but habana_frameworks.torch not available, falling back to CPU")
            self.device = torch.device('cpu')
            self.device_type = 'cpu'
        elif self.device_type == 'xpu' and not self.xpu_available:
            warnings.warn("XPU device requested but intel_extension_for_pytorch not available, falling back to CPU")
            self.device = torch.device('cpu')
            self.device_type = 'cpu'

    def is_available(self) -> bool:
        """æ£€æŸ¥è®¾å¤‡æ˜¯å¦å¯ç”¨"""
        if self.device_type == 'cuda':
            return torch.cuda.is_available()
        elif self.device_type == 'npu':
            return self.npu_available
        elif self.device_type == 'hpu':
            return self.hpu_available
        elif self.device_type == 'xpu':
            return self.xpu_available
        else:  # cpu
            return True

    def device_count(self) -> int:
        """è·å–è®¾å¤‡æ•°é‡"""
        if self.device_type == 'cuda':
            return torch.cuda.device_count()
        elif self.device_type == 'npu' and self.npu_available:
            return self.torch_npu.npu.device_count()
        elif self.device_type == 'hpu' and self.hpu_available:
            return self.habana_torch.hpu.device_count()
        elif self.device_type == 'xpu' and self.xpu_available:
            return self.ipex.xpu.device_count()
        else:
            return 1  # CPU

    def get_device_name(self, index: int = 0) -> str:
        """è·å–è®¾å¤‡åç§°"""
        if self.device_type == 'cuda':
            return torch.cuda.get_device_name(index)
        elif self.device_type == 'npu' and self.npu_available:
            return f"Huawei Ascend NPU {self.torch_npu.npu.get_device_name(index)}"
        elif self.device_type == 'hpu' and self.hpu_available:
            return f"Intel Habana Gaudi HPU {index}"
        elif self.device_type == 'xpu' and self.xpu_available:
            return f"Intel XPU {self.ipex.xpu.get_device_name(index)}"
        else:
            return "CPU"

    def get_device_properties(self, index: int = 0) -> Dict[str, Any]:
        """è·å–è®¾å¤‡å±æ€§"""
        props = {
            'name': self.get_device_name(index),
            'type': self.device_type,
            'total_memory': 0,
            'multi_processor_count': 0,
        }

        if self.device_type == 'cuda':
            cuda_props = torch.cuda.get_device_properties(index)
            props['total_memory'] = cuda_props.total_memory
            props['multi_processor_count'] = cuda_props.multi_processor_count
            props['major'] = cuda_props.major
            props['minor'] = cuda_props.minor
        elif self.device_type == 'npu' and self.npu_available:
            # Huawei Ascend NPU
            try:
                props['total_memory'] = self.torch_npu.npu.get_device_properties(index).total_memory
            except AttributeError:
                props['total_memory'] = 32 * 1024**3  # Ascend 910B: 32GB HBM
        elif self.device_type == 'hpu' and self.hpu_available:
            # Intel Habana Gaudi
            try:
                props['total_memory'] = self.habana_torch.hpu.get_device_properties(index).total_memory
            except AttributeError:
                props['total_memory'] = 96 * 1024**3  # Gaudi2: 96GB HBM2E
        elif self.device_type == 'xpu' and self.xpu_available:
            # Intel XPU
            try:
                xpu_props = self.ipex.xpu.get_device_properties(index)
                props['total_memory'] = xpu_props.total_memory if hasattr(xpu_props, 'total_memory') else 0
            except AttributeError:
                props['total_memory'] = 8 * 1024**3  # é»˜è®¤8GB

        return props

    def memory_allocated(self, index: Optional[int] = None) -> int:
        """è·å–å·²åˆ†é…å†…å­˜ï¼ˆå­—èŠ‚ï¼‰"""
        if index is None:
            index = self.device_index

        if self.device_type == 'cuda':
            return torch.cuda.memory_allocated(index)
        elif self.device_type == 'npu' and self.npu_available:
            return self.torch_npu.npu.memory_allocated(index)
        elif self.device_type == 'hpu' and self.hpu_available:
            try:
                return self.habana_torch.hpu.memory_allocated(index)
            except AttributeError:
                return 0
        elif self.device_type == 'xpu' and self.xpu_available:
            try:
                return self.ipex.xpu.memory_allocated(index)
            except AttributeError:
                return 0
        else:
            return 0

    def memory_reserved(self, index: Optional[int] = None) -> int:
        """è·å–å·²ä¿ç•™å†…å­˜ï¼ˆå­—èŠ‚ï¼‰"""
        if index is None:
            index = self.device_index

        if self.device_type == 'cuda':
            return torch.cuda.memory_reserved(index)
        elif self.device_type == 'npu' and self.npu_available:
            return self.torch_npu.npu.memory_reserved(index)
        elif self.device_type == 'hpu' and self.hpu_available:
            try:
                return self.habana_torch.hpu.memory_reserved(index)
            except AttributeError:
                return 0
        elif self.device_type == 'xpu' and self.xpu_available:
            try:
                return self.ipex.xpu.memory_reserved(index)
            except AttributeError:
                return 0
        else:
            return 0

    def max_memory_allocated(self, index: Optional[int] = None) -> int:
        """è·å–å³°å€¼å·²åˆ†é…å†…å­˜ï¼ˆå­—èŠ‚ï¼‰"""
        if index is None:
            index = self.device_index

        if self.device_type == 'cuda':
            return torch.cuda.max_memory_allocated(index)
        elif self.device_type == 'npu' and self.npu_available:
            return self.torch_npu.npu.max_memory_allocated(index)
        elif self.device_type == 'hpu' and self.hpu_available:
            try:
                return self.habana_torch.hpu.max_memory_allocated(index)
            except AttributeError:
                return 0
        elif self.device_type == 'xpu' and self.xpu_available:
            try:
                return self.ipex.xpu.max_memory_allocated(index)
            except AttributeError:
                return 0
        else:
            return 0

    def empty_cache(self) -> None:
        """æ¸…ç©ºç¼“å­˜"""
        if self.device_type == 'cuda':
            torch.cuda.empty_cache()
        elif self.device_type == 'npu' and self.npu_available:
            self.torch_npu.npu.empty_cache()
        elif self.device_type == 'hpu' and self.hpu_available:
            try:
                self.habana_torch.hpu.empty_cache()
            except AttributeError:
                pass
        elif self.device_type == 'xpu' and self.xpu_available:
            try:
                self.ipex.xpu.empty_cache()
            except AttributeError:
                pass

    def synchronize(self, index: Optional[int] = None) -> None:
        """åŒæ­¥è®¾å¤‡"""
        if index is None:
            index = self.device_index

        if self.device_type == 'cuda':
            torch.cuda.synchronize(index)
        elif self.device_type == 'npu' and self.npu_available:
            self.torch_npu.npu.synchronize(index)
        elif self.device_type == 'hpu' and self.hpu_available:
            try:
                self.habana_torch.hpu.synchronize(index)
            except AttributeError:
                pass
        elif self.device_type == 'xpu' and self.xpu_available:
            try:
                self.ipex.xpu.synchronize(index)
            except AttributeError:
                pass

    def set_device(self, index: int) -> None:
        """è®¾ç½®å½“å‰è®¾å¤‡"""
        if self.device_type == 'cuda':
            torch.cuda.set_device(index)
        elif self.device_type == 'npu' and self.npu_available:
            self.torch_npu.npu.set_device(index)
        elif self.device_type == 'hpu' and self.hpu_available:
            try:
                self.habana_torch.hpu.set_device(index)
            except AttributeError:
                pass
        elif self.device_type == 'xpu' and self.xpu_available:
            try:
                self.ipex.xpu.set_device(index)
            except AttributeError:
                pass

        self.device_index = index

    def manual_seed_all(self, seed: int) -> None:
        """è®¾ç½®æ‰€æœ‰è®¾å¤‡éšæœºç§å­"""
        if self.device_type == 'cuda':
            torch.cuda.manual_seed_all(seed)
        elif self.device_type == 'npu' and self.npu_available:
            self.torch_npu.npu.manual_seed_all(seed)
        elif self.device_type == 'hpu' and self.hpu_available:
            try:
                self.habana_torch.hpu.manual_seed_all(seed)
            except AttributeError:
                pass
        elif self.device_type == 'xpu' and self.xpu_available:
            try:
                self.ipex.xpu.manual_seed_all(seed)
            except AttributeError:
                pass

    def to_device(self, tensor: torch.Tensor) -> torch.Tensor:
        """å°†tensorç§»åŠ¨åˆ°è®¾å¤‡"""
        return tensor.to(self.device)

    def stream_context(self):
        """è·å–è®¾å¤‡æµä¸Šä¸‹æ–‡"""
        if self.device_type == 'cuda':
            return torch.cuda.stream(torch.cuda.Stream())
        elif self.device_type == 'npu' and self.npu_available:
            return self.torch_npu.npu.stream(self.torch_npu.npu.Stream())
        elif self.device_type == 'hpu' and self.hpu_available:
            try:
                return self.habana_torch.hpu.stream(self.habana_torch.hpu.Stream())
            except AttributeError:
                from contextlib import nullcontext
                return nullcontext()
        elif self.device_type == 'xpu' and self.xpu_available:
            try:
                return self.ipex.xpu.stream(self.ipex.xpu.Stream())
            except AttributeError:
                from contextlib import nullcontext
                return nullcontext()
        else:
            # CPUç­‰å…¶ä»–è®¾å¤‡æ²¡æœ‰streamæ¦‚å¿µï¼Œè¿”å›ç©ºä¸Šä¸‹æ–‡
            from contextlib import nullcontext
            return nullcontext()

    def get_memory_summary(self) -> Dict[str, Any]:
        """è·å–å†…å­˜ä½¿ç”¨æ‘˜è¦"""
        summary = {
            'device_type': self.device_type,
            'device_name': self.get_device_name(self.device_index),
            'allocated_mb': self.memory_allocated() / (1024**2),
            'reserved_mb': self.memory_reserved() / (1024**2),
            'max_allocated_mb': self.max_memory_allocated() / (1024**2),
        }

        props = self.get_device_properties(self.device_index)
        if props['total_memory'] > 0:
            summary['total_mb'] = props['total_memory'] / (1024**2)
            summary['utilization_pct'] = (self.memory_allocated() / props['total_memory']) * 100

        return summary

    def __repr__(self) -> str:
        return f"DeviceBackend(device={self.device}, type={self.device_type})"


class UnifiedDeviceManager:
    """ç»Ÿä¸€è®¾å¤‡ç®¡ç†å™¨ - è·¨GPU/NPU/CPU"""

    def __init__(self):
        self.backends: Dict[str, DeviceBackend] = {}
        self._detect_devices()

    def _detect_devices(self):
        """æ£€æµ‹æ‰€æœ‰å¯ç”¨è®¾å¤‡"""
        # æ£€æµ‹CUDA (NVIDIA GPU / AMD ROCm)
        if torch.cuda.is_available():
            for i in range(torch.cuda.device_count()):
                device = torch.device(f'cuda:{i}')
                self.backends[f'cuda:{i}'] = DeviceBackend(device)

        # æ£€æµ‹åä¸ºæ˜‡è…¾NPU
        try:
            import torch_npu
            if torch_npu.npu.is_available():
                for i in range(torch_npu.npu.device_count()):
                    device = torch.device(f'npu:{i}')
                    self.backends[f'npu:{i}'] = DeviceBackend(device)
        except ImportError:
            pass

        # æ£€æµ‹Intel Habana Gaudi HPU
        try:
            import habana_frameworks.torch as habana_torch
            if hasattr(habana_torch, 'hpu') and habana_torch.hpu.is_available():
                for i in range(habana_torch.hpu.device_count()):
                    device = torch.device(f'hpu:{i}')
                    self.backends[f'hpu:{i}'] = DeviceBackend(device)
        except ImportError:
            pass

        # æ£€æµ‹Intel XPU (åŒ…æ‹¬Ultra NPU)
        try:
            import intel_extension_for_pytorch as ipex
            if hasattr(ipex, 'xpu') and ipex.xpu.is_available():
                for i in range(ipex.xpu.device_count()):
                    device = torch.device(f'xpu:{i}')
                    self.backends[f'xpu:{i}'] = DeviceBackend(device)
        except ImportError:
            pass

        # CPUæ€»æ˜¯å¯ç”¨
        self.backends['cpu'] = DeviceBackend(torch.device('cpu'))

    def get_backend(self, device: torch.device) -> DeviceBackend:
        """è·å–è®¾å¤‡åç«¯"""
        key = str(device)
        if key not in self.backends:
            # åˆ›å»ºæ–°åç«¯
            self.backends[key] = DeviceBackend(device)
        return self.backends[key]

    def get_best_device(self, prefer_npu: bool = False, prefer_hpu: bool = False, prefer_xpu: bool = False) -> torch.device:
        """
        è·å–æœ€ä½³è®¾å¤‡

        Args:
            prefer_npu: æ˜¯å¦ä¼˜å…ˆä½¿ç”¨åä¸ºæ˜‡è…¾NPU
            prefer_hpu: æ˜¯å¦ä¼˜å…ˆä½¿ç”¨Intel Habana Gaudi HPU
            prefer_xpu: æ˜¯å¦ä¼˜å…ˆä½¿ç”¨Intel XPU

        Returns:
            torch.device: æœ€ä½³å¯ç”¨è®¾å¤‡
        """
        # ç”¨æˆ·æŒ‡å®šä¼˜å…ˆçº§
        if prefer_hpu:
            hpu_devices = [k for k in self.backends.keys() if k.startswith('hpu')]
            if hpu_devices:
                return torch.device(hpu_devices[0])

        if prefer_npu:
            npu_devices = [k for k in self.backends.keys() if k.startswith('npu')]
            if npu_devices:
                return torch.device(npu_devices[0])

        if prefer_xpu:
            xpu_devices = [k for k in self.backends.keys() if k.startswith('xpu')]
            if xpu_devices:
                return torch.device(xpu_devices[0])

        # é»˜è®¤ä¼˜å…ˆçº§: CUDA > HPU > NPU > XPU > CPU
        cuda_devices = [k for k in self.backends.keys() if k.startswith('cuda')]
        if cuda_devices:
            return torch.device(cuda_devices[0])

        hpu_devices = [k for k in self.backends.keys() if k.startswith('hpu')]
        if hpu_devices:
            return torch.device(hpu_devices[0])

        npu_devices = [k for k in self.backends.keys() if k.startswith('npu')]
        if npu_devices:
            return torch.device(npu_devices[0])

        xpu_devices = [k for k in self.backends.keys() if k.startswith('xpu')]
        if xpu_devices:
            return torch.device(xpu_devices[0])

        # æœ€åCPU
        return torch.device('cpu')

    def get_all_devices(self) -> List[torch.device]:
        """è·å–æ‰€æœ‰å¯ç”¨è®¾å¤‡"""
        return [torch.device(k) for k in self.backends.keys()]

    def get_device_summary(self) -> Dict[str, Any]:
        """è·å–æ‰€æœ‰è®¾å¤‡æ‘˜è¦"""
        summary = {
            'total_devices': len(self.backends),
            'cuda_devices': sum(1 for k in self.backends if k.startswith('cuda')),
            'npu_devices': sum(1 for k in self.backends if k.startswith('npu')),
            'hpu_devices': sum(1 for k in self.backends if k.startswith('hpu')),
            'xpu_devices': sum(1 for k in self.backends if k.startswith('xpu')),
            'devices': {}
        }

        for name, backend in self.backends.items():
            if backend.device_type != 'cpu':
                summary['devices'][name] = backend.get_memory_summary()

        return summary

    def cleanup_all(self):
        """æ¸…ç†æ‰€æœ‰è®¾å¤‡ç¼“å­˜"""
        for backend in self.backends.values():
            backend.empty_cache()

    def __repr__(self) -> str:
        return f"UnifiedDeviceManager(devices={list(self.backends.keys())})"


# å…¨å±€è®¾å¤‡ç®¡ç†å™¨å®ä¾‹
_global_device_manager = None


def get_device_manager() -> UnifiedDeviceManager:
    """è·å–å…¨å±€è®¾å¤‡ç®¡ç†å™¨"""
    global _global_device_manager
    if _global_device_manager is None:
        _global_device_manager = UnifiedDeviceManager()
    return _global_device_manager


def get_unified_backend(device: Optional[torch.device] = None) -> DeviceBackend:
    """
    è·å–ç»Ÿä¸€è®¾å¤‡åç«¯

    Args:
        device: è®¾å¤‡å¯¹è±¡ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨æœ€ä½³è®¾å¤‡

    Returns:
        DeviceBackend: è®¾å¤‡åç«¯
    """
    manager = get_device_manager()
    if device is None:
        device = manager.get_best_device()
    return manager.get_backend(device)


# ä¾¿æ·å‡½æ•°
def is_cuda_available() -> bool:
    """æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨ï¼ˆNVIDIA GPU / AMD ROCmï¼‰"""
    return torch.cuda.is_available()


def is_npu_available() -> bool:
    """æ£€æŸ¥åä¸ºæ˜‡è…¾NPUæ˜¯å¦å¯ç”¨"""
    try:
        import torch_npu
        return torch_npu.npu.is_available()
    except ImportError:
        return False


def is_hpu_available() -> bool:
    """æ£€æŸ¥Intel Habana Gaudi HPUæ˜¯å¦å¯ç”¨"""
    try:
        import habana_frameworks.torch as habana_torch
        return hasattr(habana_torch, 'hpu') and habana_torch.hpu.is_available()
    except ImportError:
        return False


def is_xpu_available() -> bool:
    """æ£€æŸ¥Intel XPU (åŒ…æ‹¬Ultra NPU)æ˜¯å¦å¯ç”¨"""
    try:
        import intel_extension_for_pytorch as ipex
        return hasattr(ipex, 'xpu') and ipex.xpu.is_available()
    except ImportError:
        return False


def get_accelerator_type() -> str:
    """
    è·å–å½“å‰æœ€ä½³åŠ é€Ÿå™¨ç±»å‹

    ä¼˜å…ˆçº§: cuda > hpu > npu > xpu > cpu

    Returns:
        str: 'cuda', 'hpu', 'npu', 'xpu', æˆ– 'cpu'
    """
    if is_cuda_available():
        return 'cuda'
    elif is_hpu_available():
        return 'hpu'
    elif is_npu_available():
        return 'npu'
    elif is_xpu_available():
        return 'xpu'
    else:
        return 'cpu'


def get_all_accelerator_types() -> List[str]:
    """
    è·å–æ‰€æœ‰å¯ç”¨çš„åŠ é€Ÿå™¨ç±»å‹åˆ—è¡¨

    Returns:
        List[str]: å¯ç”¨åŠ é€Ÿå™¨ç±»å‹åˆ—è¡¨
    """
    available = []
    if is_cuda_available():
        available.append('cuda')
    if is_hpu_available():
        available.append('hpu')
    if is_npu_available():
        available.append('npu')
    if is_xpu_available():
        available.append('xpu')
    available.append('cpu')  # CPUæ€»æ˜¯å¯ç”¨
    return available


__all__ = [
    'DeviceBackend',
    'UnifiedDeviceManager',
    'get_device_manager',
    'get_unified_backend',
    # è®¾å¤‡æ£€æµ‹
    'is_cuda_available',
    'is_npu_available',
    'is_hpu_available',
    'is_xpu_available',
    # å·¥å…·å‡½æ•°
    'get_accelerator_type',
    'get_all_accelerator_types',
]
