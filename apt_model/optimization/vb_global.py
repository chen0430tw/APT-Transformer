"""
è™šæ‹ŸBlackwellå…¨å±€å¯ç”¨å™¨

ä¸€è¡Œä»£ç å¯ç”¨è™šæ‹ŸBlackwellä¼˜åŒ–ï¼š
    import apt_model.optimization.vb_global as vb
    vb.enable()

æ‰€æœ‰åç»­åˆ›å»ºçš„APTæ¨¡å‹éƒ½ä¼šè‡ªåŠ¨åº”ç”¨VGPUä¼˜åŒ–ã€‚
"""

import torch
import torch.nn as nn
from typing import Optional, Dict
import os

# è™šæ‹ŸBlackwellç»„ä»¶
from apt_model.optimization.vgpu_stack import VGPUStack, create_vgpu_stack
from apt_model.optimization.vgpu_estimator import VGPUResourceEstimator, ModelConfig

# å…¨å±€çŠ¶æ€
_vb_enabled = False
_vb_stack = None
_vb_config = {
    'use_fp4': False,
    'use_flash_attn': False,
    'mixed_precision': False,
    'gradient_checkpointing': False,
    'auto_estimate': True,
    'verbose': True,
}


def enable(use_fp4: bool = False,
          use_flash_attn: bool = False,
          mixed_precision: bool = False,
          gradient_checkpointing: bool = False,
          auto_estimate: bool = True,
          vgpu_config: Optional[Dict] = None,
          verbose: bool = True):
    """
    å…¨å±€å¯ç”¨è™šæ‹ŸBlackwellä¼˜åŒ–

    Args:
        use_fp4: å¯ç”¨FP4é‡åŒ–
        use_flash_attn: å¯ç”¨Flash Attention
        mixed_precision: å¯ç”¨æ··åˆç²¾åº¦
        gradient_checkpointing: å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
        auto_estimate: è‡ªåŠ¨ä¼°ç®—èµ„æºéœ€æ±‚
        vgpu_config: è‡ªå®šä¹‰VGPUé…ç½®ï¼ˆNone=ä½¿ç”¨é»˜è®¤ï¼‰
        verbose: æ‰“å°è¯¦ç»†ä¿¡æ¯

    Example:
        >>> import apt_model.optimization.vb_global as vb
        >>> vb.enable(use_fp4=True, use_flash_attn=True)
        >>>
        >>> # ä¹‹åæ‰€æœ‰APTæ¨¡å‹éƒ½ä¼šè‡ªåŠ¨ä¼˜åŒ–
        >>> from apt_model.modeling.apt_model import APTLargeModel
        >>> model = APTLargeModel(config)  # è‡ªåŠ¨åº”ç”¨VGPUä¼˜åŒ–
    """
    global _vb_enabled, _vb_stack, _vb_config

    _vb_enabled = True
    _vb_config.update({
        'use_fp4': use_fp4,
        'use_flash_attn': use_flash_attn,
        'mixed_precision': mixed_precision,
        'gradient_checkpointing': gradient_checkpointing,
        'auto_estimate': auto_estimate,
        'verbose': verbose,
    })

    # åˆ›å»ºVGPU Stack
    if vgpu_config:
        from apt_model.optimization.vgpu_stack import VGPUStack
        _vb_stack = VGPUStack(vgpu_config)
    else:
        _vb_stack = create_vgpu_stack()

    if verbose:
        print("\n" + "="*70)
        print("ğŸš€ è™šæ‹ŸBlackwellå·²å…¨å±€å¯ç”¨")
        print("="*70)
        print(f"FP4é‡åŒ–:         {'âœ… å¯ç”¨' if use_fp4 else 'âŒ ç¦ç”¨'}")
        print(f"Flash Attention: {'âœ… å¯ç”¨' if use_flash_attn else 'âŒ ç¦ç”¨'}")
        print(f"æ··åˆç²¾åº¦:        {'âœ… å¯ç”¨' if mixed_precision else 'âŒ ç¦ç”¨'}")
        print(f"æ¢¯åº¦æ£€æŸ¥ç‚¹:      {'âœ… å¯ç”¨' if gradient_checkpointing else 'âŒ ç¦ç”¨'}")
        print(f"è‡ªåŠ¨ä¼°ç®—:        {'âœ… å¯ç”¨' if auto_estimate else 'âŒ ç¦ç”¨'}")
        print("="*70 + "\n")


def disable():
    """ç¦ç”¨è™šæ‹ŸBlackwellä¼˜åŒ–"""
    global _vb_enabled, _vb_stack
    _vb_enabled = False
    _vb_stack = None
    print("è™šæ‹ŸBlackwellå·²ç¦ç”¨")


def is_enabled() -> bool:
    """æ£€æŸ¥è™šæ‹ŸBlackwellæ˜¯å¦å·²å¯ç”¨"""
    return _vb_enabled


def get_stack() -> Optional[VGPUStack]:
    """è·å–å…¨å±€VGPU Stack"""
    return _vb_stack


def get_config() -> Dict:
    """è·å–å½“å‰é…ç½®"""
    return _vb_config.copy()


def optimize_model(model: nn.Module, model_name: str = "model") -> nn.Module:
    """
    ä¼˜åŒ–å•ä¸ªæ¨¡å‹

    Args:
        model: PyTorchæ¨¡å‹
        model_name: æ¨¡å‹åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰

    Returns:
        ä¼˜åŒ–åçš„æ¨¡å‹
    """
    if not _vb_enabled:
        return model

    if _vb_stack is None:
        raise RuntimeError("è¯·å…ˆè°ƒç”¨vb.enable()å¯ç”¨è™šæ‹ŸBlackwell")

    verbose = _vb_config['verbose']

    if verbose:
        print(f"\nä¼˜åŒ–æ¨¡å‹: {model_name}")

    # å¯¼å…¥ä¼˜åŒ–åŒ…è£…å™¨
    from training.test_vb_apt_integration import VBOptimizedAPTModel

    # è·å–æ¨¡å‹é…ç½®
    if hasattr(model, 'config'):
        apt_config = model.config
    else:
        raise ValueError("æ¨¡å‹å¿…é¡»æœ‰configå±æ€§")

    # åˆ›å»ºä¼˜åŒ–æ¨¡å‹
    optimized_model = VBOptimizedAPTModel(
        apt_config,
        _vb_stack,
        use_fp4=_vb_config['use_fp4'],
        use_flash_attn=_vb_config['use_flash_attn']
    )

    # å¤åˆ¶åŸå§‹æƒé‡
    optimized_model.base_model.load_state_dict(model.state_dict())

    if verbose:
        print(f"âœ“ å·²ä¼˜åŒ– {len(optimized_model.optimized_layers)} ä¸ªçº¿æ€§å±‚")

    return optimized_model


def estimate_model_resources(model_config, batch_size: int = 8):
    """
    ä¼°ç®—æ¨¡å‹èµ„æºéœ€æ±‚

    Args:
        model_config: APTæ¨¡å‹é…ç½®
        batch_size: æ‰¹æ¬¡å¤§å°
    """
    if not _vb_config['auto_estimate']:
        return

    # è½¬æ¢ä¸ºè¯„ä¼°å™¨é…ç½®
    estimator_config = ModelConfig(
        vocab_size=getattr(model_config, 'vocab_size', 50000),
        hidden_size=getattr(model_config, 'hidden_size', 768),
        num_layers=getattr(model_config, 'num_layers', 12),
        num_heads=getattr(model_config, 'num_heads', 12),
        seq_length=getattr(model_config, 'max_position_embeddings', 2048),
        batch_size=batch_size,
        mixed_precision=_vb_config['mixed_precision'],
        gradient_checkpointing=_vb_config['gradient_checkpointing']
    )

    # è¯„ä¼°
    estimator = VGPUResourceEstimator()
    estimator.estimate_transformer(estimator_config)

    # ç”ŸæˆVGPUé…ç½®
    available_gpus = []
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            available_gpus.append({
                'device': f'cuda:{i}',
                'vram_gb': props.total_memory / (1024**3),
                'speed_gbps': 900
            })

    if available_gpus:
        estimator.generate_vgpu_config(available_gpus)
        estimator.print_report()


def get_stats() -> Dict:
    """è·å–VGPUç»Ÿè®¡ä¿¡æ¯"""
    if _vb_stack is None:
        return {}
    return _vb_stack.get_stats()


def print_stats():
    """æ‰“å°VGPUç»Ÿè®¡ä¿¡æ¯"""
    if _vb_stack is None:
        print("è™šæ‹ŸBlackwellæœªå¯ç”¨")
        return
    _vb_stack.print_stats()


# ä¾¿æ·é¢„è®¾
def enable_full_optimization():
    """å¯ç”¨æ‰€æœ‰ä¼˜åŒ–ï¼ˆæœ€å¤§æ˜¾å­˜èŠ‚çœï¼‰"""
    enable(
        use_fp4=True,
        use_flash_attn=True,
        mixed_precision=True,
        gradient_checkpointing=True,
        auto_estimate=True
    )


def enable_speed_mode():
    """å¯ç”¨é€Ÿåº¦æ¨¡å¼ï¼ˆFP4é‡åŒ–ï¼‰"""
    enable(
        use_fp4=True,
        use_flash_attn=False,
        mixed_precision=False,
        gradient_checkpointing=False,
        auto_estimate=True
    )


def enable_memory_mode():
    """å¯ç”¨æ˜¾å­˜æ¨¡å¼ï¼ˆFlash Attention + æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼‰"""
    enable(
        use_fp4=False,
        use_flash_attn=True,
        mixed_precision=True,
        gradient_checkpointing=True,
        auto_estimate=True
    )


def enable_balanced_mode():
    """å¯ç”¨å¹³è¡¡æ¨¡å¼ï¼ˆæ¨èï¼‰"""
    enable(
        use_fp4=False,
        use_flash_attn=True,
        mixed_precision=True,
        gradient_checkpointing=False,
        auto_estimate=True
    )


# ç¯å¢ƒå˜é‡æ§åˆ¶ï¼ˆå¯é€šè¿‡ç¯å¢ƒå˜é‡è‡ªåŠ¨å¯ç”¨ï¼‰
if os.getenv('ENABLE_VIRTUAL_BLACKWELL', '').lower() in ('1', 'true', 'yes'):
    mode = os.getenv('VB_MODE', 'balanced').lower()

    if mode == 'full':
        enable_full_optimization()
    elif mode == 'speed':
        enable_speed_mode()
    elif mode == 'memory':
        enable_memory_mode()
    else:
        enable_balanced_mode()

    print(f"âœ… é€šè¿‡ç¯å¢ƒå˜é‡è‡ªåŠ¨å¯ç”¨è™šæ‹ŸBlackwell ({mode}æ¨¡å¼)")


if __name__ == "__main__":
    # æµ‹è¯•
    print("è™šæ‹ŸBlackwellå…¨å±€å¯ç”¨å™¨")
    print("\nä½¿ç”¨ç¤ºä¾‹:")
    print("```python")
    print("import apt_model.optimization.vb_global as vb")
    print("")
    print("# å¯ç”¨è™šæ‹ŸBlackwell")
    print("vb.enable(use_fp4=True, use_flash_attn=True)")
    print("")
    print("# ä¹‹åæ‰€æœ‰APTæ¨¡å‹éƒ½ä¼šè‡ªåŠ¨ä¼˜åŒ–")
    print("from apt_model.modeling.apt_model import APTLargeModel")
    print("model = APTLargeModel(config)  # è‡ªåŠ¨åº”ç”¨VGPUä¼˜åŒ–")
    print("")
    print("# æŸ¥çœ‹ç»Ÿè®¡")
    print("vb.print_stats()")
    print("```")
