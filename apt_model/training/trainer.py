#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ä¿®æ”¹APTæ¨¡å‹è®­ç»ƒå™¨ä»¥æ”¯æŒä¸­æ–‡åˆ†è¯
"""

import os
import glob
import logging
import torch
import torch.nn.functional as F
import traceback
from contextlib import nullcontext
from tqdm import tqdm
from datetime import datetime
from torch.utils.data import Dataset, DataLoader

# å°è¯•å¯¼å…¥richï¼Œå¦‚æœä¸å¯ç”¨åˆ™å›é€€åˆ°tqdm
try:
    from rich.progress import (
        Progress,
        SpinnerColumn,
        BarColumn,
        TextColumn,
        TimeElapsedColumn,
        TimeRemainingColumn,
        MofNCompleteColumn,
    )
    RICH_AVAILABLE = True
except ImportError:
    from tqdm import tqdm
    RICH_AVAILABLE = False

from apt_model.utils import set_seed
from apt_model.utils import get_device, device
from apt_model.config.apt_config import APTConfig
from apt_model.modeling.apt_model import APTModel, APTLargeModel
from apt_model.generation.generator import generate_natural_text
from apt_model.generation.evaluator import evaluate_text_quality
from apt_model.config.settings_manager import settings

# å¯¼å…¥ä¸­æ–‡åˆ†è¯å™¨ç›¸å…³å‡½æ•°
from apt_model.modeling.chinese_tokenizer_integration import (
    get_appropriate_tokenizer,
    save_tokenizer,
    is_chinese_text
)

# å¯¼å…¥æ–°çš„codecç³»ç»Ÿ
from apt_model.codecs import get_codec_for_language, list_available_codecs
from apt_model.codecs.compat import CodecTokenizerWrapper

# å¯¼å…¥callbackç³»ç»Ÿ
from apt_model.training.callbacks import (
    CallbackManager,
    create_default_callbacks,
)


# ============================================================================
# Codecç³»ç»Ÿé›†æˆ
# ============================================================================

def get_tokenizer_from_codec(texts, tokenizer_type=None, language=None):
    """
    ä½¿ç”¨æ–°çš„codecç³»ç»Ÿè·å–tokenizer

    è¿™ä¸ªå‡½æ•°ä½¿ç”¨æ–°çš„æ’ä»¶åŒ–codecæ¶æ„ï¼Œä½†è¿”å›ä¸€ä¸ªå…¼å®¹transformersæ¥å£çš„tokenizeråŒ…è£…å™¨ã€‚

    å‚æ•°:
        texts: æ–‡æœ¬åˆ—è¡¨ï¼ˆç”¨äºè¯­è¨€æ£€æµ‹ï¼‰
        tokenizer_type: æŒ‡å®šçš„åˆ†è¯å™¨ç±»å‹ï¼ˆå¯é€‰ï¼‰
        language: æŒ‡å®šçš„è¯­è¨€ï¼ˆå¯é€‰ï¼Œ'en', 'zh', 'ja'ç­‰ï¼‰

    è¿”å›:
        (tokenizer_wrapper, detected_language): tokenizeråŒ…è£…å™¨å’Œæ£€æµ‹åˆ°çš„è¯­è¨€
    """
    import logging
    logger = logging.getLogger('apt_model.codec')

    # å¦‚æœæœªæŒ‡å®šè¯­è¨€ï¼Œä½¿ç”¨æ—§çš„è¯­è¨€æ£€æµ‹é€»è¾‘
    if language is None:
        from apt_model.modeling.chinese_tokenizer_integration import detect_language
        language = detect_language(texts)
        logger.info(f"è‡ªåŠ¨æ£€æµ‹è¯­è¨€: {language}")

    # æ˜ å°„tokenizer_typeåˆ°codecåç§°
    codec_name = None
    if tokenizer_type:
        type_to_codec = {
            'gpt2': 'en_gpt2',
            'chinese-char': 'zh_char',
            'chinese-word': 'zh_char',
            'ja-mecab': 'ja_mecab',
        }
        codec_name = type_to_codec.get(tokenizer_type)

    # å°è¯•è·å–codec
    try:
        codec = get_codec_for_language(language, prefer=codec_name)

        if codec is None:
            logger.warning(f"æœªæ‰¾åˆ°è¯­è¨€ '{language}' çš„codecï¼Œå›é€€åˆ°æ—§ç³»ç»Ÿ")
            return get_appropriate_tokenizer(texts, tokenizer_type, language)

        logger.info(f"ä½¿ç”¨codec: {codec.name} (è¯­è¨€: {language})")

        # åŒ…è£…ä¸ºtokenizeræ¥å£
        tokenizer_wrapper = CodecTokenizerWrapper(codec)

        return tokenizer_wrapper, language

    except Exception as e:
        logger.warning(f"Codecç³»ç»Ÿå¤±è´¥ ({e})ï¼Œå›é€€åˆ°æ—§åˆ†è¯å™¨ç³»ç»Ÿ")
        return get_appropriate_tokenizer(texts, tokenizer_type, language)


# ============================================================================
# Loggerè®¾ç½®
# ============================================================================

def setup_training_logger(name="APT-Trainer", log_file=None):
    """
    è®¾ç½®è®­ç»ƒæ—¥å¿—è®°å½•å™¨

    å‚æ•°:
        name: Loggeråç§°
        log_file: å¯é€‰çš„æ—¥å¿—æ–‡ä»¶è·¯å¾„

    è¿”å›:
        é…ç½®å¥½çš„logger
    """
    logger = logging.getLogger(name)

    # é¿å…é‡å¤æ·»åŠ handler
    if logger.handlers:
        return logger

    logger.setLevel(logging.DEBUG)

    # æ ¼å¼åŒ–å™¨ - å¸¦æ—¶é—´æˆ³å’Œæ¨¡å—å
    formatter = logging.Formatter(
        '%(asctime)s [%(name)s] %(message)s',
        datefmt='%m-%d %H:%M:%S'
    )

    # æ§åˆ¶å°å¤„ç†å™¨
    console_handler = logging.StreamHandler()
    console_handler.setFormatter(formatter)
    console_handler.setLevel(logging.INFO)
    logger.addHandler(console_handler)

    # æ–‡ä»¶å¤„ç†å™¨ï¼ˆå¯é€‰ï¼‰
    if log_file:
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setFormatter(formatter)
        file_handler.setLevel(logging.DEBUG)
        logger.addHandler(file_handler)

    return logger

# å…¨å±€loggerå®ä¾‹
_training_logger = None

def get_training_logger():
    """è·å–è®­ç»ƒloggerå®ä¾‹"""
    global _training_logger
    if _training_logger is None:
        _training_logger = setup_training_logger()
    return _training_logger


# ============================================================================
# Debugè¾“å‡ºè¾…åŠ©å‡½æ•°ï¼ˆä¿æŒå‘åå…¼å®¹ï¼‰
# ============================================================================

def debug_print(*args, **kwargs):
    """ä»…åœ¨Debugæ¨¡å¼ä¸‹æ‰“å°ä¿¡æ¯"""
    if settings.get_debug_enabled():
        logger = get_training_logger()
        message = ' '.join(str(arg) for arg in args)
        logger.debug(message)

def info_print(*args, **kwargs):
    """å§‹ç»ˆæ‰“å°çš„å…³é”®ä¿¡æ¯ï¼ˆéDebugæ¨¡å¼ä¹Ÿæ˜¾ç¤ºï¼‰"""
    logger = get_training_logger()
    message = ' '.join(str(arg) for arg in args)
    logger.info(message)


# ============================================================================
# è¿›åº¦æ¡åˆ›å»ºå‡½æ•°
# ============================================================================

def create_training_progress_bar(dataloader, epoch, total_epochs):
    """
    åˆ›å»ºè®­ç»ƒè¿›åº¦æ¡ï¼ˆä¼˜å…ˆä½¿ç”¨richï¼Œå›é€€åˆ°tqdmï¼‰

    å‚æ•°:
        dataloader: æ•°æ®åŠ è½½å™¨
        epoch: å½“å‰epoch (ä»0å¼€å§‹)
        total_epochs: æ€»epochæ•°

    è¿”å›:
        è¿›åº¦æ¡å¯¹è±¡å’Œä¸Šä¸‹æ–‡ç®¡ç†å™¨
    """
    if RICH_AVAILABLE:
        # ä½¿ç”¨richåˆ›å»ºæ¼‚äº®çš„è¿›åº¦æ¡
        progress = Progress(
            SpinnerColumn(),  # æ—‹è½¬åŠ¨ç”»
            TextColumn("[bold blue]{task.description}"),
            BarColumn(),
            TextColumn("[progress.percentage]{task.percentage:>3.0f}%"),
            MofNCompleteColumn(),  # å½“å‰/æ€»æ•°
            TimeElapsedColumn(),
            TextColumn("â€¢"),
            TimeRemainingColumn(),
            TextColumn("[cyan]{task.fields[loss]}", justify="right"),
            TextColumn("[green]{task.fields[lr]}", justify="right"),
        )
        task_id = progress.add_task(
            f"Epoch {epoch+1}/{total_epochs}",
            total=len(dataloader),
            loss="Loss: -.----",
            lr="LR: -.------"
        )
        return progress, task_id
    else:
        # å›é€€åˆ°tqdm
        from tqdm import tqdm
        progress_bar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{total_epochs}")
        return progress_bar, None


# ============================================================================
# æ•°æ®é›†ç±»å®šä¹‰
# ============================================================================

class TextDataset(Dataset):
    """
    æ–‡æœ¬æ•°æ®é›†ç±»

    å°†æ–‡æœ¬åˆ—è¡¨è½¬æ¢ä¸ºæ¨¡å‹å¯ç”¨çš„tokenåºåˆ—
    """
    def __init__(self, texts, tokenizer, max_length=128):
        """
        åˆå§‹åŒ–æ•°æ®é›†

        å‚æ•°:
            texts: æ–‡æœ¬åˆ—è¡¨
            tokenizer: åˆ†è¯å™¨
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
        """
        self.texts = texts
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        encoding = self.tokenizer.encode(
            text,
            return_tensors="pt",
            max_length=self.max_length,
            truncation=True
        ).squeeze(0)
        return encoding, encoding


def create_collate_fn(tokenizer):
    """
    åˆ›å»ºæ‰¹æ¬¡æ•´ç†å‡½æ•°

    å‚æ•°:
        tokenizer: åˆ†è¯å™¨ï¼Œç”¨äºè·å–pad_token_id

    è¿”å›:
        collate_fn: æ‰¹æ¬¡æ•´ç†å‡½æ•°
    """
    def collate_fn(batch):
        """æ•´ç†æ‰¹æ¬¡æ•°æ®ï¼Œè¿›è¡Œå¡«å……"""
        src_ids_list, tgt_ids_list = zip(*batch)
        src_ids = torch.nn.utils.rnn.pad_sequence(
            src_ids_list,
            batch_first=True,
            padding_value=tokenizer.pad_token_id
        )
        tgt_ids = torch.nn.utils.rnn.pad_sequence(
            tgt_ids_list,
            batch_first=True,
            padding_value=tokenizer.pad_token_id
        )
        return src_ids, tgt_ids

    return collate_fn


class DummyGradScaler:
    """
    å…¼å®¹æ€§GradScalerç±»

    å½“CUDAä¸å¯ç”¨æˆ–ä¸æ”¯æŒæ··åˆç²¾åº¦æ—¶ä½¿ç”¨
    æä¾›ä¸torch.cuda.amp.GradScalerç›¸åŒçš„æ¥å£
    """
    def scale(self, loss):
        """ä¸è¿›è¡Œç¼©æ”¾ï¼Œç›´æ¥è¿”å›æŸå¤±"""
        return loss

    def step(self, optimizer):
        """ç›´æ¥æ‰§è¡Œä¼˜åŒ–å™¨æ­¥éª¤"""
        optimizer.step()

    def update(self):
        """ç©ºæ“ä½œ"""
        pass


# ============================================================================
# è¾…åŠ©å‡½æ•°
# ============================================================================

def _log_message(logger, message, level="info"):
    """
    ç»Ÿä¸€çš„æ—¥å¿—è®°å½•å‡½æ•°

    å‚æ•°:
        logger: æ—¥å¿—è®°å½•å™¨ï¼ˆå¯ä¸ºNoneï¼‰
        message: æ—¥å¿—æ¶ˆæ¯
        level: æ—¥å¿—çº§åˆ« ("info", "warning", "error")
    """
    if logger:
        if level == "info":
            logger.info(message)
        elif level == "warning":
            logger.warning(message)
        elif level == "error":
            logger.error(message)
    else:
        print(message)

def get_training_texts():
    """
    è·å–è®­ç»ƒæ–‡æœ¬æ•°æ®ã€‚å¦‚æœåœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹å­˜åœ¨ "train.txt" æ–‡ä»¶ï¼Œåˆ™è¯»å–æ–‡ä»¶ï¼Œå¦åˆ™è¿”å›å†…ç½®é¢„è®¾æ•°æ®ã€‚
    """
    import os
    # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆå‡è®¾ä»£ç æ–‡ä»¶åœ¨é¡¹ç›®å­ç›®å½•ä¸­ï¼‰
    script_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    train_file = os.path.join(script_dir, "train.txt")
    
    print("æ£€æŸ¥è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„ï¼š", train_file)
    
    if os.path.exists(train_file):
        with open(train_file, "r", encoding="utf-8") as f:
            texts = [line.strip() for line in f if line.strip()]
        if texts:
            return texts
        else:
            print("è®­ç»ƒæ•°æ®æ–‡ä»¶ 'train.txt' ä¸ºç©ºï¼Œä½¿ç”¨é¢„è®¾è®­ç»ƒæ•°æ®ã€‚")
    else:
        print("æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶ 'train.txt'ï¼Œä½¿ç”¨é¢„è®¾è®­ç»ƒæ•°æ®ã€‚")
    
    # é¢„è®¾è®­ç»ƒæ•°æ®é›†
    return [
        # åŸºæœ¬å¯¹è¯
        "Hello, how are you?",
        "I'm doing well, thank you for asking. How about you?",
        "Good morning! How did you sleep last night?",
        "Good morning! I slept very well, thank you for asking.",
        "What's your name?",
        "My name is Claude. It's nice to meet you.",
        "How's the weather today?",
        "The weather is lovely today. It's sunny and warm.",
        "Can you help me with a question?",
        "Of course, I'd be happy to help you with any questions you have.",
        "What time is it?",
        "I'm sorry, I don't have access to real-time information like the current time.",
        
        # åŸç¥ç›¸å…³å†…å®¹
        "å®‰æŸï¼šä¸€èµ·æ¥è®­ç»ƒå§ï¼",
        "å®‰æŸæ˜¯è’™å¾·åŸçš„ä¾¦å¯Ÿéª‘å£«ï¼Œæ“…é•¿å¼“ç®­å’Œä¾¦å¯Ÿã€‚",
        "æ—…è¡Œè€…ï¼Œæ¬¢è¿æ¥åˆ°æç“¦ç‰¹å¤§é™†ã€‚",
        "åŸç¥æ˜¯ä¸€æ¬¾å¼€æ”¾ä¸–ç•Œå†’é™©æ¸¸æˆã€‚",
        "é£èµ·ä¸‡å±±æ‘‡ï¼Œæš—æ½®å¯„ä½™ç”Ÿã€‚",
        "ç’ƒæœˆæ¸¯æ˜¯ä¸ƒå›½ä¹‹ä¸€ç’ƒæœˆçš„ä¸»è¦æ¸¯å£åŸå¸‚ã€‚",
        "å…ƒç´ åŠ›é‡æ˜¯æç“¦ç‰¹å¤§é™†ä¸Šçš„åŸºç¡€èƒ½åŠ›ä½“ç³»ã€‚",
        "å®‰æŸï¼šè®­ç»ƒ...è¿˜ä¸å¤Ÿ...",
        "æ´¾è’™æ˜¯æ—…è¡Œè€…çš„åŒä¼´ï¼Œè¢«ç§°ä¸ºåº”æ€¥é£Ÿå“ã€‚",
        "ä¸ƒç¥ç»Ÿæ²»ç€æç“¦ç‰¹å¤§é™†çš„ä¸ƒä¸ªå›½å®¶ã€‚",
        "éª‘å£«å›¢è´Ÿè´£å®ˆæŠ¤è’™å¾·åŸçš„å’Œå¹³ä¸å®‰å…¨ã€‚",
        "å†’é™©å®¶åä¼šä¸ºæ—…è¡Œè€…æä¾›å„ç§ä»»åŠ¡å’Œæƒ…æŠ¥ã€‚",
        "æç“¦ç‰¹å¤§é™†æœ‰é£ã€å²©ã€é›·ã€æ°´ã€ç«ã€è‰ã€å†°ä¸ƒç§å…ƒç´ ã€‚",
        
        # åŸºæœ¬è§£é‡Š
        "What is artificial intelligence?",
        "Artificial intelligence refers to computer systems designed to perform tasks that typically require human intelligence.",
        "Can you explain what a neural network is?",
        "A neural network is a computational model inspired by the human brain that consists of layers of interconnected nodes.",
        "What is machine learning?",
        "Machine learning is a subset of artificial intelligence that focuses on developing algorithms that allow computers to learn from data.",
        "What is deep learning?",
        "Deep learning is a subset of machine learning that uses artificial neural networks with multiple layers to extract features from data.",
        
        # å®Œæ•´å¥å­
        "This is a test sentence for APT model training.",
        "Deep learning models require a lot of data.",
        "Transformers are widely used in NLP tasks.",
        "The quick brown fox jumps over the lazy dog.",
        "I enjoy reading books in my free time.",
        "Music has the power to change our mood.",
        "The Internet has revolutionized how we access information.",
        "Learning a new language can be challenging but rewarding.",
        "Yesterday I went to the store to buy some groceries.",
        "The mountains looked beautiful against the sunset sky.",
        "She opened the window to let in some fresh air.",
        "They decided to take a vacation to the beach this summer.",
        "The professor explained the complex theory to the students.",
        "My favorite season is autumn when the leaves change color.",
        "The company announced their new product at the conference.",
        "We should meet for coffee sometime next week.",
        "The children played happily in the park all afternoon.",
        "He finished writing his novel after five years of work.",
        
        # é—®ç­”å¯¹
        "What is the capital of France?",
        "The capital of France is Paris.",
        "Who wrote Romeo and Juliet?",
        "William Shakespeare wrote Romeo and Juliet.",
        "What is photosynthesis?",
        "Photosynthesis is the process by which green plants use sunlight to synthesize foods with carbon dioxide and water.",
        "How far is the Moon from Earth?",
        "The average distance between the Earth and the Moon is about 384,400 kilometers.",
        "What is the largest ocean on Earth?",
        "The Pacific Ocean is the largest and deepest ocean on Earth.",
        
        # å¸¸è§çŸ­è¯­
        "Thank you very much.",
        "You're welcome.",
        "How can I help you today?",
        "That's a great question.",
        "I'm not sure about that.",
        "Let me think about it.",
        "Could you please explain that again?",
        "I understand what you're saying.",
        "That's an interesting perspective.",
        "I agree with your point of view.",
        
        # æ›´å¤æ‚çš„å†…å®¹
        "In recent years, large language models have demonstrated impressive capabilities in understanding and generating human language.",
        "The development of self-driving cars represents a significant advancement in artificial intelligence.",
        "Climate change is one of the most pressing challenges facing our planet today.",
        "The human brain contains approximately 86 billion neurons.",
        "Quantum computing has the potential to solve certain problems exponentially faster than classical computers.",
        "The history of art spans thousands of years, reflecting human civilization.",
        "Telescopes allow us to observe distant galaxies.",
        "Blockchain technology provides a secure way to record transactions.",
        "Photosynthesis is essential for most life on Earth.",
        "Biodiversity is crucial for maintaining healthy ecosystems.",
        
        # è¿è´¯æ®µè½
        """
        The sun was setting behind the mountains, casting long shadows across the valley.
        Birds were returning to their nests, filling the air with their evening songs.
        A gentle breeze rustled the leaves, bringing the sweet scent of wildflowers.
        In the distance, the small town's lights twinkled like fallen stars.
        """,
        
        """
        Learning to code can be challenging but rewarding.
        It requires patience, logical thinking, and attention to detail.
        Many beginners start with simple languages like Python.
        The key to success is consistent practice and learning from mistakes.
        """,
        
        """
        The history of aviation began with the Wright brothers.
        Since then, aviation has transformed global travel.
        Modern aircraft can fly faster than the speed of sound.
        Air travel connects people across continents in just hours.
        """,
        
        # ä¸­æ–‡å†…å®¹
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œå®ƒä½¿ç”¨æ•°æ®å’Œç®—æ³•æ¥æ¨¡ä»¿äººç±»å­¦ä¹ çš„æ–¹å¼ã€‚",
        "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ç§ç‰¹æ®Šå½¢å¼ï¼Œå®ƒä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œå¤„ç†å¤æ‚çš„æ¨¡å¼ã€‚",
        "è‡ªç„¶è¯­è¨€å¤„ç†è®©è®¡ç®—æœºèƒ½å¤Ÿç†è§£ã€åˆ†æå’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚",
        "è®¡ç®—æœºè§†è§‰ä½¿æœºå™¨èƒ½å¤Ÿä»å›¾åƒå’Œè§†é¢‘ä¸­è·å–æœ‰æ„ä¹‰çš„ä¿¡æ¯ã€‚",
        "å¼ºåŒ–å­¦ä¹ æ˜¯ä¸€ç§è®©æœºå™¨é€šè¿‡è¯•é”™æ¥å­¦ä¹ çš„æ–¹æ³•ï¼Œä»¥è·å¾—æœ€å¤§çš„å¥–åŠ±ã€‚",
        "å¤§å‹è¯­è¨€æ¨¡å‹å¦‚GPTèƒ½å¤Ÿç”Ÿæˆæµç•…çš„æ–‡æœ¬ï¼Œå¹¶å›ç­”å„ç§é—®é¢˜ã€‚",
        "äººå·¥æ™ºèƒ½ä¼¦ç†å…³æ³¨AIå‘å±•ä¸­çš„é“å¾·é—®é¢˜å’Œç¤¾ä¼šå½±å“ã€‚",
        "æ•°æ®ç§‘å­¦ç»“åˆäº†ç»Ÿè®¡å­¦ã€ç¼–ç¨‹å’Œé¢†åŸŸçŸ¥è¯†æ¥æå–æ•°æ®ä¸­çš„ä»·å€¼ã€‚",
        "æœºå™¨å­¦ä¹ ç®—æ³•å¯ä»¥åˆ†ä¸ºç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚",
        "ç¥ç»ç½‘ç»œæ˜¯å—äººè„‘ç»“æ„å¯å‘è€Œè®¾è®¡çš„ç®—æ³•ã€‚"
    ]

# =============================================================================
# è®­ç»ƒè¾…åŠ©å‡½æ•°
# =============================================================================

def _setup_training_data(train_texts, tokenizer, batch_size):
    """
    è®¾ç½®è®­ç»ƒæ•°æ®å’ŒDataLoader

    å‚æ•°:
        train_texts: è®­ç»ƒæ–‡æœ¬åˆ—è¡¨
        tokenizer: åˆ†è¯å™¨
        batch_size: æ‰¹æ¬¡å¤§å°

    è¿”å›:
        dataloader: DataLoaderå®ä¾‹
    """
    debug_print("æ­£åœ¨å‡†å¤‡æ•°æ®é›†...")
    dataset = TextDataset(train_texts, tokenizer)
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        collate_fn=create_collate_fn(tokenizer),
        pin_memory=True
    )
    return dataloader


def _setup_model_and_optimizer(tokenizer, learning_rate, dataloader, epochs):
    """
    è®¾ç½®æ¨¡å‹ã€ä¼˜åŒ–å™¨å’Œè°ƒåº¦å™¨

    å‚æ•°:
        tokenizer: åˆ†è¯å™¨
        learning_rate: å­¦ä¹ ç‡
        dataloader: DataLoaderå®ä¾‹
        epochs: è®­ç»ƒè½®æ•°

    è¿”å›:
        model: APTæ¨¡å‹
        config: æ¨¡å‹é…ç½®
        optimizer: ä¼˜åŒ–å™¨
        scheduler: å­¦ä¹ ç‡è°ƒåº¦å™¨
    """
    debug_print("åˆ›å»ºæ¨¡å‹é…ç½®...")
    config = APTConfig(
        vocab_size=tokenizer.vocab_size,
        d_model=768,
        d_ff=2048,
        num_heads=12,
        num_encoder_layers=4,
        num_decoder_layers=4,
        max_seq_len=128,
        dropout=0.2,
        epsilon=2.0,
        alpha=0.001,
        beta=0.001,
        base_lr=learning_rate
    )

    debug_print("åˆå§‹åŒ–æ¨¡å‹...")
    model = APTLargeModel(config).to(device)
    model.train()

    # ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨è®¾ç½®
    from apt_model.training.optimizer import create_optimizer_and_scheduler
    optimizer, scheduler = create_optimizer_and_scheduler(
        model, learning_rate, len(dataloader), epochs
    )

    return model, config, optimizer, scheduler


def _setup_grad_scaler():
    """
    è®¾ç½®æ¢¯åº¦ç¼©æ”¾å™¨ï¼ˆæ··åˆç²¾åº¦è®­ç»ƒï¼‰

    è¿”å›:
        scaler: æ¢¯åº¦ç¼©æ”¾å™¨å®ä¾‹
    """
    try:
        from torch.cuda.amp import GradScaler
        scaler = GradScaler()
        debug_print("æ··åˆç²¾åº¦è®­ç»ƒå·²å¯ç”¨")
    except (ImportError, AttributeError):
        scaler = DummyGradScaler()
        debug_print("è­¦å‘Š: æ··åˆç²¾åº¦è®­ç»ƒä¸å¯ç”¨ï¼Œä½¿ç”¨æ ‡å‡†ç²¾åº¦è®­ç»ƒ")

    return scaler


def _setup_tensorboard(save_path):
    """
    è®¾ç½®tensorboardè®°å½•å™¨

    å‚æ•°:
        save_path: ä¿å­˜è·¯å¾„

    è¿”å›:
        writer: SummaryWriterå®ä¾‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        use_tensorboard: æ˜¯å¦ä½¿ç”¨tensorboard
    """
    try:
        from torch.utils.tensorboard import SummaryWriter
        writer = SummaryWriter(log_dir=f"{save_path}_logs")
        debug_print("Tensorboardè®°å½•å·²å¯ç”¨")
        return writer, True
    except:
        debug_print("æœªå®‰è£…tensorboardï¼Œå°†ä¸ä½¿ç”¨tensorboardè®°å½•è®­ç»ƒè¿‡ç¨‹")
        return None, False


def _process_batch(model, batch, optimizer, scaler, tokenizer, accumulation_steps,
                   batch_idx, logger, resource_monitor, gradient_monitor=None, global_step=0):
    """
    å¤„ç†å•ä¸ªæ‰¹æ¬¡çš„è®­ç»ƒ

    å‚æ•°:
        model: æ¨¡å‹
        batch: æ‰¹æ¬¡æ•°æ®
        optimizer: ä¼˜åŒ–å™¨
        scaler: æ¢¯åº¦ç¼©æ”¾å™¨
        tokenizer: åˆ†è¯å™¨
        accumulation_steps: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
        batch_idx: æ‰¹æ¬¡ç´¢å¼•
        logger: æ—¥å¿—è®°å½•å™¨
        resource_monitor: èµ„æºç›‘è§†å™¨
        gradient_monitor: æ¢¯åº¦ç›‘æ§å™¨ï¼ˆå¯é€‰ï¼‰
        global_step: å…¨å±€è®­ç»ƒæ­¥æ•°ï¼ˆç”¨äºæ¢¯åº¦ç›‘æ§ï¼‰

    è¿”å›:
        loss_value: æŸå¤±å€¼ï¼ˆå¤±è´¥è¿”å›Noneï¼‰
        should_update: æ˜¯å¦åº”è¯¥æ›´æ–°å‚æ•°
    """
    try:
        if resource_monitor:
            resource_monitor.check_resources()

        src_ids, tgt_ids = batch
        src_ids = src_ids.to(device)
        tgt_ids = tgt_ids.to(device)

        src_padding_mask = (src_ids == tokenizer.pad_token_id)

        # åªåœ¨ç´¯ç§¯å‘¨æœŸå¼€å§‹æ—¶æ¸…é›¶æ¢¯åº¦
        if batch_idx % accumulation_steps == 0:
            optimizer.zero_grad()

        # æ··åˆç²¾åº¦å‰å‘è®¡ç®—
        with torch.amp.autocast('cuda'):
            try:
                logits = model(
                    src_tokens=src_ids,
                    tgt_tokens=src_ids,
                    src_key_padding_mask=src_padding_mask,
                    src_mask=None
                )
            except Exception as e:
                _log_message(logger, f"å‰å‘ä¼ æ’­å‡ºé”™: {e}", "error")
                debug_print(f"è­¦å‘Š: å‰å‘ä¼ æ’­å¤±è´¥: {e}ï¼Œè·³è¿‡å½“å‰æ‰¹æ¬¡")
                return None, False

            if torch.isnan(logits).any():
                debug_print(f"è­¦å‘Š: æ‰¹æ¬¡{batch_idx+1}çš„logitsåŒ…å«NaNï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                return None, False

            # è®¡ç®—æŸå¤±
            shift_logits = logits[:, :-1, :].contiguous()
            shift_labels = tgt_ids[:, 1:].contiguous()

            try:
                loss = F.cross_entropy(
                    shift_logits.view(-1, shift_logits.size(-1)),
                    shift_labels.view(-1),
                    ignore_index=tokenizer.pad_token_id,
                    label_smoothing=0.1
                )
                loss = loss / accumulation_steps
            except Exception as e:
                _log_message(logger, f"æŸå¤±è®¡ç®—å‡ºé”™: {e}", "error")
                debug_print(f"è­¦å‘Š: æŸå¤±è®¡ç®—å¤±è´¥: {e}ï¼Œè·³è¿‡å½“å‰æ‰¹æ¬¡")
                return None, False

            if torch.isnan(loss).any():
                debug_print(f"è­¦å‘Š: æ‰¹æ¬¡{batch_idx+1}å‘ç°NaNæŸå¤±ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                return None, False

        # åå‘ä¼ æ’­
        try:
            scaler.scale(loss).backward()
        except Exception as e:
            _log_message(logger, f"åå‘ä¼ æ’­å‡ºé”™: {e}", "error")
            debug_print(f"è­¦å‘Š: åå‘ä¼ æ’­å¤±è´¥: {e}ï¼Œè·³è¿‡å½“å‰æ‰¹æ¬¡")
            optimizer.zero_grad()
            return None, False

        # æ¢¯åº¦ç›‘æ§ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if gradient_monitor is not None:
            try:
                # æ£€æŸ¥æ¢¯åº¦æµï¼ˆæ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸æ£€æµ‹ï¼‰
                gradients, issues = gradient_monitor.check_gradient_flow()
                if issues:
                    for issue in issues[:3]:  # åªæ˜¾ç¤ºå‰3ä¸ªé—®é¢˜
                        debug_print(f"  {issue}")
                    if logger:
                        logger.warning(f"Step {global_step}: å‘ç° {len(issues)} ä¸ªæ¢¯åº¦é—®é¢˜")

                # è®°å½•æ¢¯åº¦èŒƒæ•°
                total_norm = gradient_monitor.log_gradient_norms(global_step)

                # æ£€æµ‹æ¢¯åº¦å¼‚å¸¸
                anomalies = gradient_monitor.detect_gradient_anomalies()
                if anomalies:
                    _log_message(logger, f"Step {global_step}: æ£€æµ‹åˆ°æ¢¯åº¦å¼‚å¸¸ - {anomalies}", "warning")
                    debug_print(f"âš ï¸  æ¢¯åº¦å¼‚å¸¸: {anomalies}")
            except Exception as e:
                debug_print(f"æ¢¯åº¦ç›‘æ§å¤±è´¥: {e}")

        # æ¢¯åº¦è£å‰ª
        try:
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
        except Exception as e:
            _log_message(logger, f"æ¢¯åº¦è£å‰ªå‡ºé”™: {e}", "warning")
            debug_print(f"è­¦å‘Š: æ¢¯åº¦è£å‰ªå¤±è´¥: {e}")

        loss_value = loss.item() * accumulation_steps
        return loss_value, True

    except Exception as e:
        _log_message(logger, f"å¤„ç†æ‰¹æ¬¡ {batch_idx} æ—¶å‡ºé”™: {e}", "error")
        debug_print(f"æ‰¹æ¬¡å¤„ç†é”™è¯¯: {e}ï¼Œè·³è¿‡å½“å‰æ‰¹æ¬¡")
        return None, False


# =============================================================================
# ä¸»è®­ç»ƒå‡½æ•°
# =============================================================================

def train_model(epochs=20, batch_size=8, learning_rate=3e-5, save_path="apt_model",
                logger=None, resource_monitor=None, multimodal_config=None,
                tokenizer_type=None, language=None, texts=None, tokenizer=None,
                checkpoint_dir="./outputs", resume_from=None, temp_checkpoint_freq=100,
                enable_gradient_monitoring=False):
    """
    è®­ç»ƒæ¨¡å‹çš„ä¸»å‡½æ•°

    å‚æ•°:
        epochs: è®­ç»ƒè½®æ•°
        batch_size: æ‰¹æ¬¡å¤§å°
        learning_rate: å­¦ä¹ ç‡
        save_path: æ¨¡å‹ä¿å­˜è·¯å¾„ï¼ˆå·²å¼ƒç”¨ï¼Œå»ºè®®ä½¿ç”¨checkpoint_dirï¼‰
        logger: æ—¥å¿—è®°å½•å™¨
        resource_monitor: èµ„æºç›‘è§†å™¨
        multimodal_config: å¤šæ¨¡æ€é…ç½®ï¼ˆæœªä½¿ç”¨ï¼‰
        tokenizer_type: åˆ†è¯å™¨ç±»å‹
        language: è¯­è¨€ç±»å‹
        texts: è®­ç»ƒæ–‡æœ¬ï¼ˆNoneåˆ™ä½¿ç”¨é»˜è®¤æ•°æ®ï¼‰
        tokenizer: åˆ†è¯å™¨ï¼ˆNoneåˆ™è‡ªåŠ¨é€‰æ‹©ï¼‰
        checkpoint_dir: checkpointä¿å­˜ç›®å½•ï¼ˆç›¸å¯¹è·¯å¾„ï¼Œå¯è¿ç§»ï¼‰ï¼Œé»˜è®¤"./outputs"
        resume_from: æ¢å¤è®­ç»ƒçš„checkpointè·¯å¾„ï¼Œå¯é€‰
        temp_checkpoint_freq: ä¸´æ—¶checkpointä¿å­˜é¢‘ç‡ï¼ˆæ¯Næ­¥ï¼‰ï¼Œé»˜è®¤100
        enable_gradient_monitoring: å¯ç”¨æ¢¯åº¦ç›‘æ§ï¼ˆè°ƒè¯•/åˆ†æç”¨ï¼‰ï¼Œé»˜è®¤False

    è¿”å›:
        model: è®­ç»ƒåçš„æ¨¡å‹
        tokenizer: ä½¿ç”¨çš„åˆ†è¯å™¨
        config: æ¨¡å‹é…ç½®
    """
    # è®¾ç½®éšæœºç§å­
    set_seed(42)

    _log_message(logger, "å¼€å§‹è®­ç»ƒæ¨¡å‹...")

    # è·å–è®­ç»ƒæ•°æ®
    if texts is None:
        train_texts = get_training_texts()
    else:
        train_texts = texts

    info_print(f"è®­ç»ƒæ•°æ®é›†å¤§å°: {len(train_texts)} æ¡æ–‡æœ¬")

    if len(train_texts) == 0:
        raise ValueError("è®­ç»ƒæ•°æ®ä¸ºç©ºï¼Œè¯·ç¡®ä¿æ•°æ®æ–‡ä»¶å­˜åœ¨æˆ–å†…ç½®æ•°æ®æ­£ç¡®åŠ è½½ã€‚")

    # è‡ªåŠ¨æ£€æµ‹è¯­è¨€å¹¶é€‰æ‹©åˆé€‚çš„åˆ†è¯å™¨
    if tokenizer is None:
        # ä¼˜å…ˆä½¿ç”¨æ–°çš„codecç³»ç»Ÿ
        tokenizer, detected_language = get_tokenizer_from_codec(
            train_texts,
            tokenizer_type=tokenizer_type,
            language=language
        )
        info_print(f"ä½¿ç”¨{detected_language}è¯­è¨€åˆ†è¯å™¨: {type(tokenizer).__name__}")
    else:
        detected_language = language or "en"
        debug_print(f"ä½¿ç”¨æä¾›çš„åˆ†è¯å™¨: {type(tokenizer).__name__}")

    # è®¾ç½®æ•°æ®å’Œæ¨¡å‹
    dataloader = _setup_training_data(train_texts, tokenizer, batch_size)
    model, config, optimizer, scheduler = _setup_model_and_optimizer(
        tokenizer, learning_rate, dataloader, epochs
    )

    # è®¾ç½®è®­ç»ƒå·¥å…·
    scaler = _setup_grad_scaler()
    writer, use_tensorboard = _setup_tensorboard(save_path)

    # ä¿å­˜è®­ç»ƒå‰çš„æ¨¡å‹ç”¨äºæ¯”è¾ƒ
    untrained_model = APTLargeModel(config).to(device)
    untrained_model.load_state_dict(model.state_dict())
    untrained_model.eval()

    # åˆå§‹åŒ–æ¢¯åº¦ç›‘æ§å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰
    gradient_monitor = None
    if enable_gradient_monitoring:
        from apt_model.training.gradient_monitor import GradientMonitor
        from pathlib import Path
        gradient_export_dir = Path(checkpoint_dir) / "gradient_monitor"
        gradient_export_dir.mkdir(parents=True, exist_ok=True)
        gradient_monitor = GradientMonitor(
            model,
            logger=logger,
            export_dir=gradient_export_dir
        )
        info_print(f"âœ… æ¢¯åº¦ç›‘æ§å·²å¯ç”¨ï¼ŒæŠ¥å‘Šå°†ä¿å­˜åˆ°: {gradient_export_dir}")

    # æ—©åœè®¾ç½®
    best_loss = float('inf')
    patience = 5
    patience_counter = 0

    # è®­ç»ƒçŠ¶æ€
    global_step = 0
    train_losses = []
    best_quality_score = 0.0
    
    print(f"å¼€å§‹è®­ç»ƒï¼Œæ€»å…± {epochs} è½®...")
    
    # åˆ›å»ºä¸€ä¸ªå‡çš„ GradScaler ä»¥ä¿æŒä»£ç å…¼å®¹æ€§
    class DummyScaler:
        def scale(self, loss):
            return loss

        def step(self, optimizer):
            optimizer.step()

        def update(self):
            pass

    # åœ¨è®­ç»ƒå¼€å§‹å‰åˆå§‹åŒ– GradScaler
    if torch.cuda.is_available():
        try:
            from torch.cuda.amp import GradScaler
            scaler = GradScaler()
            autocast_context = torch.cuda.amp.autocast
        except (ImportError, AttributeError):
            scaler = DummyScaler()
            autocast_context = nullcontext
            print("è­¦å‘Š: CUDA AMP ä¸å¯ç”¨ï¼Œä½¿ç”¨æ ‡å‡†ç²¾åº¦è®­ç»ƒ")
    else:
        scaler = DummyScaler()
        autocast_context = nullcontext
        print("è­¦å‘Š: æœªæ£€æµ‹åˆ°GPUï¼Œä½¿ç”¨æ ‡å‡†ç²¾åº¦è®­ç»ƒ")

    # æ£€æŸ¥æ˜¯å¦å­˜åœ¨checkpointï¼ˆç”¨äºæ¢å¤è®­ç»ƒï¼‰
    checkpoint_dir = os.path.join(save_path, "checkpoints")
    has_checkpoints = False
    if os.path.exists(checkpoint_dir):
        checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.endswith('.pt')]
        if checkpoint_files:
            has_checkpoints = True
            logger = get_training_logger()
            logger.info(f"å‘ç° {len(checkpoint_files)} ä¸ªå·²æœ‰checkpoint")
            logger.info(f"æœ€æ–°checkpoint: {sorted(checkpoint_files)[-1]}")
            logger.info("æç¤º: å¯ä»¥ä½¿ç”¨CheckpointManager.load_checkpoint()æ¢å¤è®­ç»ƒ")
            # TODO: å®ç°è‡ªåŠ¨æ¢å¤åŠŸèƒ½ï¼ˆå°†åœ¨checkpointæ”¹è¿›ä¸­å®Œæˆï¼‰

    info_print(f"å¼€å§‹è®­ç»ƒï¼Œæ€»å…± {epochs} è½®...")

    # æ¢¯åº¦ç´¯ç§¯å‚æ•°
    accumulation_steps = 4

    # å¯¼å…¥å¿…è¦çš„å‡½æ•°
    from apt_model.training.checkpoint import save_model, CheckpointManager

    # åˆå§‹åŒ–CheckpointManagerï¼ˆä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼Œå¯è¿ç§»ï¼‰
    checkpoint_mgr = CheckpointManager(
        save_dir=checkpoint_dir,
        model_name="apt_model",
        save_freq=1,  # æ¯ä¸ªepochä¿å­˜
        logger=logger
    )
    info_print(f"Checkpointå°†ä¿å­˜åˆ°: {checkpoint_dir}/checkpoints/")

    # åˆ›å»ºtempç›®å½•ç”¨äºä¸´æ—¶checkpoint
    temp_dir = os.path.join(".cache", "temp")
    os.makedirs(temp_dir, exist_ok=True)
    info_print(f"ä¸´æ—¶checkpointå°†ä¿å­˜åˆ°: {temp_dir}/")

    # åˆå§‹åŒ–è®­ç»ƒçŠ¶æ€
    start_epoch = 0
    resume_global_step = 0
    resume_loss_history = []

    # å¦‚æœéœ€è¦æ¢å¤è®­ç»ƒ
    if resume_from:
        try:
            info_print(f"ä»checkpointæ¢å¤è®­ç»ƒ: {resume_from}")
            start_epoch, resume_global_step, resume_loss_history, resume_metrics = checkpoint_mgr.load_checkpoint(
                model=model,
                optimizer=optimizer,
                scheduler=scheduler,
                checkpoint_path=resume_from
            )
            # æ¢å¤åä»ä¸‹ä¸€ä¸ªepochå¼€å§‹
            start_epoch += 1
            global_step = resume_global_step
            train_losses = resume_loss_history.copy()
            info_print(f"æˆåŠŸæ¢å¤è®­ç»ƒ: ä»Epoch {start_epoch}ç»§ç»­, global_step={global_step}")
            if resume_metrics:
                info_print(f"æ¢å¤çš„æŒ‡æ ‡: {resume_metrics}")
        except Exception as e:
            _log_message(logger, f"æ¢å¤checkpointå¤±è´¥: {e}", "error")
            info_print(f"è­¦å‘Š: æ— æ³•æ¢å¤checkpointï¼Œä»å¤´å¼€å§‹è®­ç»ƒ")
            import traceback
            debug_print(f"æ¢å¤å¤±è´¥è¯¦æƒ…: {traceback.format_exc()}")

    # åˆå§‹åŒ–callbackç³»ç»Ÿ
    modules = {}
    # å°è¯•æå–æ¨¡å‹ä¸­çš„å¯è°ƒåº¦æ¨¡å—
    if hasattr(model, 'moe_layer'):
        modules['moe'] = model.moe_layer
    if hasattr(model, 'align_layer'):
        modules['align'] = model.align_layer
    if hasattr(model, 'voter'):
        modules['voter'] = model.voter
    if hasattr(model, 'router'):
        modules['router'] = model.router

    total_steps = epochs * len(dataloader)
    callbacks = create_default_callbacks(config, modules, epochs, total_steps,
                                        use_rich_progress=False)  # Set to True for rich display
    callback_manager = CallbackManager(callbacks)

    # è§¦å‘è®­ç»ƒå¼€å§‹å›è°ƒ
    callback_manager.trigger('on_train_begin', model=model, config=config, total_epochs=epochs)

    # ä¸»è®­ç»ƒå¾ªç¯ï¼ˆä»start_epochå¼€å§‹ï¼Œæ”¯æŒæ¢å¤è®­ç»ƒï¼‰
    for epoch in range(start_epoch, epochs):
        # è§¦å‘epochå¼€å§‹å›è°ƒï¼ˆä¼ é€’dataloaderç»™ProgressCallbackï¼‰
        callback_manager.trigger('on_epoch_begin', epoch=epoch, dataloader=dataloader)

        total_loss = 0
        # æ³¨æ„ï¼šè¿›åº¦æ¡ç°åœ¨ç”±ProgressCallbackç®¡ç†ï¼Œè¿™é‡Œä¿ç•™tqdmä½œä¸ºåå¤‡
        # å¦‚æœæƒ³å®Œå…¨ä½¿ç”¨ProgressCallbackï¼Œå¯ä»¥æ³¨é‡Šæ‰ä¸‹é¢è¿™è¡Œ
        progress_bar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{epochs}", disable=True)

        for i, batch in enumerate(progress_bar):
            try:
                if resource_monitor:
                    resource_monitor.check_resources()
                
                src_ids, tgt_ids = batch
                src_ids = src_ids.to(device)
                tgt_ids = tgt_ids.to(device)
                
                src_padding_mask = (src_ids == tokenizer.pad_token_id)  # å¡«å……æ©ç  [batch, src_len]
                tgt_mask = torch.triu(torch.ones(tgt_ids.size(1), tgt_ids.size(1), device=tgt_ids.device) * float('-inf'), diagonal=1)
                
                # åªåœ¨ç´¯ç§¯å‘¨æœŸå¼€å§‹æ—¶æ¸…é›¶æ¢¯åº¦
                if i % accumulation_steps == 0:
                    optimizer.zero_grad()
                
                # ä½¿ç”¨æ›´æ–°åçš„ autocast è¿›è¡Œæ··åˆç²¾åº¦å‰å‘è®¡ç®—å’ŒæŸå¤±è®¡ç®—
                with autocast_context() if autocast_context is not nullcontext else nullcontext():
                    try:
                        # åœ¨è¿™é‡Œæ·»åŠ æ‰“å°è¯­å¥
                        import inspect
                        #print(f"Model type: {type(model)}")
                        #print(f"Model forward signature: {inspect.signature(model.forward)}")
                        
                        logits = model(src_tokens=src_ids, tgt_tokens=src_ids, src_key_padding_mask=src_padding_mask, src_mask=None)
                    except Exception as e:
                        if logger:
                            logger.error(f"å‰å‘ä¼ æ’­å‡ºé”™: {e}")
                        print(f"è­¦å‘Š: å‰å‘ä¼ æ’­å¤±è´¥: {e}ï¼Œè·³è¿‡å½“å‰æ‰¹æ¬¡")
                        continue
                    
                    if torch.isnan(logits).any():
                        print(f"è­¦å‘Š: ç¬¬{epoch+1}è½®ç¬¬{i+1}æ‰¹æ¬¡çš„logitsåŒ…å«NaNï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                        continue
                    
                    # è®¡ç®—æŸå¤±
                    shift_logits = logits[:, :-1, :].contiguous()
                    shift_labels = tgt_ids[:, 1:].contiguous()
                    
                    try:
                        loss = F.cross_entropy(
                            shift_logits.view(-1, shift_logits.size(-1)),
                            shift_labels.view(-1), 
                            ignore_index=tokenizer.pad_token_id,
                            label_smoothing=0.1
                        )
                        # æ ¹æ®ç´¯ç§¯æ­¥éª¤ç¼©æ”¾æŸå¤±
                        loss = loss / accumulation_steps
                    except Exception as e:
                        if logger:
                            logger.error(f"æŸå¤±è®¡ç®—å‡ºé”™: {e}")
                        print(f"è­¦å‘Š: æŸå¤±è®¡ç®—å¤±è´¥: {e}ï¼Œè·³è¿‡å½“å‰æ‰¹æ¬¡")
                        continue
                    
                    if torch.isnan(loss).any():
                        print(f"è­¦å‘Š: ç¬¬{epoch+1}è½®ç¬¬{i+1}æ‰¹æ¬¡å‘ç°NaNæŸå¤±ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                        continue
                
                # ä½¿ç”¨ GradScaler è¿›è¡Œåå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°
                try:
                    scaler.scale(loss).backward()
                except Exception as e:
                    if logger:
                        logger.error(f"åå‘ä¼ æ’­å‡ºé”™: {e}")
                    print(f"è­¦å‘Š: åå‘ä¼ æ’­å¤±è´¥: {e}ï¼Œè·³è¿‡å½“å‰æ‰¹æ¬¡")
                    optimizer.zero_grad()
                    continue
                
                # æ¢¯åº¦è£å‰ªï¼ˆå¦‚æœéœ€è¦ï¼Œå¯ä»¥æ”¾åœ¨ scaler.step() å‰åï¼‰
                try:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                except Exception as e:
                    if logger:
                        logger.warning(f"æ¢¯åº¦è£å‰ªå‡ºé”™ï¼Œè·³è¿‡: {e}")
                    print(f"è­¦å‘Š: æ¢¯åº¦è£å‰ªå¤±è´¥: {e}")
                
                # åªåœ¨ç´¯ç§¯å®Œæˆåæ›´æ–°å‚æ•°
                if (i + 1) % accumulation_steps == 0 or (i + 1) == len(dataloader):
                    # è¿›è¡Œå‚æ•°æ›´æ–°
                    scaler.step(optimizer)
                    scaler.update()
                    scheduler.step()
                
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()  # æ¸…ç†GPUç¼“å­˜
                    
                    try:
                        current_lr = scheduler.get_last_lr()[0]
                        model.update_dynamic_taylor_parameters(current_lr)
                    except Exception as e:
                        if logger:
                            logger.warning(f"åŠ¨æ€å‚æ•°æ›´æ–°å‡ºé”™: {e}")
                        print(f"è­¦å‘Š: åŠ¨æ€å‚æ•°æ›´æ–°å¤±è´¥: {e}")
                
                total_loss += loss.item() * accumulation_steps  # æ¢å¤å®é™…æŸå¤±
                train_losses.append(loss.item() * accumulation_steps)
                progress_bar.set_postfix({"loss": f"{loss.item() * accumulation_steps:.4f}", "lr": f"{scheduler.get_last_lr()[0]:.6f}"})
                
                if use_tensorboard:
                    writer.add_scalar('Loss/train', loss.item() * accumulation_steps, global_step)
                    writer.add_scalar('Learning_rate', scheduler.get_last_lr()[0], global_step)
                
                global_step += 1
                
                if global_step % 50 == 0 or i == len(dataloader) - 1:
                    # æµ‹è¯•ç”Ÿæˆå’Œè¯„ä¼°ä»£ç ä¿æŒä¸å˜...
                    pass
                    
            except Exception as e:
                if logger:
                    logger.error(f"å¤„ç†æ‰¹æ¬¡ {i} æ—¶å‡ºé”™: {e}")
                    logger.error(traceback.format_exc())
                print(f"æ‰¹æ¬¡å¤„ç†é”™è¯¯: {e}ï¼Œè·³è¿‡å½“å‰æ‰¹æ¬¡")
                continue

            # ç´¯ç§¯æŸå¤±
            total_loss += loss_value
            train_losses.append(loss_value)
            progress_bar.set_postfix({
                "loss": f"{loss_value:.4f}",
                "lr": f"{scheduler.get_last_lr()[0]:.6f}"
            })

            # è·å–GPUä½¿ç”¨ç‡ï¼ˆå¦‚æœå¯ç”¨ï¼‰
            gpu_usage = None
            if torch.cuda.is_available():
                try:
                    allocated = torch.cuda.memory_allocated() / 1024**3
                    total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                    gpu_usage = (allocated / total) * 100
                except:
                    pass

            # è§¦å‘batchç»“æŸå›è°ƒï¼ˆä¼ é€’ç»™ProgressCallbackï¼‰
            callback_manager.trigger('on_batch_end', batch_idx=i, loss=loss_value,
                                    lr=scheduler.get_last_lr()[0], gpu_usage=gpu_usage,
                                    epoch=epoch)

            # åªåœ¨ç´¯ç§¯å®Œæˆåæ›´æ–°å‚æ•°
            if (i + 1) % accumulation_steps == 0 or (i + 1) == len(dataloader):
                scaler.step(optimizer)
                scaler.update()
                scheduler.step()

                # è§¦å‘optimization stepå›è°ƒ
                callback_manager.trigger('on_step', step=global_step)

                torch.cuda.empty_cache()

                # æ›´æ–°åŠ¨æ€Taylorå‚æ•°
                try:
                    current_lr = scheduler.get_last_lr()[0]
                    model.update_dynamic_taylor_parameters(current_lr)
                except Exception as e:
                    _log_message(logger, f"åŠ¨æ€å‚æ•°æ›´æ–°å‡ºé”™: {e}", "warning")
                    debug_print(f"è­¦å‘Š: åŠ¨æ€å‚æ•°æ›´æ–°å¤±è´¥: {e}")

            # è®°å½•åˆ°tensorboard
            if use_tensorboard:
                writer.add_scalar('Loss/train', loss_value, global_step)
                writer.add_scalar('Learning_rate', scheduler.get_last_lr()[0], global_step)

            global_step += 1

            # æ¯Næ­¥ä¿å­˜ä¸´æ—¶checkpointï¼ˆç”¨äºå´©æºƒæ¢å¤ï¼‰
            if temp_checkpoint_freq > 0 and global_step % temp_checkpoint_freq == 0:
                try:
                    temp_checkpoint_path = os.path.join(temp_dir, f"temp_epoch{epoch}_step{global_step}.pt")
                    torch.save({
                        'epoch': epoch,
                        'global_step': global_step,
                        'batch_idx': i,
                        'model_state_dict': model.state_dict(),
                        'optimizer_state_dict': optimizer.state_dict(),
                        'scheduler_state_dict': scheduler.state_dict(),
                        'train_losses': train_losses,
                    }, temp_checkpoint_path)
                    debug_print(f"ä¸´æ—¶checkpointå·²ä¿å­˜: {temp_checkpoint_path}")
                except Exception as e:
                    _log_message(logger, f"ä¿å­˜ä¸´æ—¶checkpointå¤±è´¥: {e}", "warning")
                    debug_print(f"è­¦å‘Š: ä¸´æ—¶checkpointä¿å­˜å¤±è´¥: {e}")

        # Epochç»“æŸå¤„ç†
        avg_loss = total_loss / max(1, len(dataloader))
        info_print(f"Epoch {epoch+1}/{epochs} å®Œæˆ, å¹³å‡æŸå¤±: {avg_loss:.4f}")

        if use_tensorboard:
            writer.add_scalar('Loss/epoch', avg_loss, epoch)

        # è§¦å‘epochç»“æŸå›è°ƒ
        callback_manager.trigger('on_epoch_end', epoch=epoch, metrics={'avg_loss': avg_loss})

        # ä¿å­˜checkpointï¼ˆæ¯ä¸ªepochï¼‰
        try:
            is_best = avg_loss < best_loss
            if is_best:
                best_loss = avg_loss
                patience_counter = 0
            else:
                patience_counter += 1

            # ä½¿ç”¨CheckpointManagerä¿å­˜å®Œæ•´è®­ç»ƒçŠ¶æ€
            checkpoint_path = checkpoint_mgr.save_checkpoint(
                model=model,
                optimizer=optimizer,
                scheduler=scheduler,
                epoch=epoch,
                global_step=global_step,
                loss_history=train_losses,
                metrics={'avg_loss': avg_loss, 'best_loss': best_loss},
                tokenizer=tokenizer,
                config=config,
                is_best=is_best
            )
            info_print(f"Checkpointå·²ä¿å­˜: {checkpoint_path}")
            if is_best:
                info_print(f"âœ¨ å‘ç°æ–°çš„æœ€ä½³æ¨¡å‹! æŸå¤±: {best_loss:.4f}")

            # æ¸…ç†tempæ–‡ä»¶å¤¹ï¼ˆepochç»“æŸåï¼‰
            try:
                temp_files = glob.glob(os.path.join(temp_dir, "temp_*.pt"))
                for temp_file in temp_files:
                    os.remove(temp_file)
                debug_print(f"å·²æ¸…ç† {len(temp_files)} ä¸ªä¸´æ—¶checkpointæ–‡ä»¶")
            except Exception as e:
                debug_print(f"æ¸…ç†ä¸´æ—¶æ–‡ä»¶å¤±è´¥: {e}")

            # æ—©åœæ£€æŸ¥
            if patience_counter >= patience:
                info_print(f"æ—©åœ: {patience} è½®æ²¡æœ‰æ”¹å–„ï¼Œåœæ­¢è®­ç»ƒ")
                break

            # æµ‹è¯•ç”Ÿæˆæ•ˆæœï¼ˆä»…åœ¨Debugæ¨¡å¼ä¸‹æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ï¼‰
            if settings.get_debug_enabled():
                _test_generation_after_epoch(model, tokenizer, logger, detected_language)
        except Exception as e:
            _log_message(logger, f"è½®æ¬¡ç»“æŸå¤„ç†å‡ºé”™: {e}", "error")
            debug_print(f"è­¦å‘Š: è½®æ¬¡ç»“æŸå¤„ç†å¤±è´¥: {e}")

    # è®­ç»ƒç»“æŸ
    if use_tensorboard:
        writer.close()

    info_print("è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜ã€‚")

    # æ¨¡å‹å¯¹æ¯”ï¼ˆä»…åœ¨Debugæ¨¡å¼ä¸‹ï¼‰
    if settings.get_debug_enabled():
        try:
            _compare_model_outputs(untrained_model, model, tokenizer, detected_language)
        except Exception as e:
            _log_message(logger, f"æ¨¡å‹æ¯”è¾ƒå‡ºé”™: {e}", "error")
            debug_print(f"è­¦å‘Š: æ¨¡å‹æ¯”è¾ƒå¤±è´¥: {e}")

    # ç”Ÿæˆæ¢¯åº¦ç›‘æ§æŠ¥å‘Šï¼ˆå¦‚æœå¯ç”¨ï¼‰
    if gradient_monitor is not None:
        try:
            info_print("\nç”Ÿæˆæ¢¯åº¦ç›‘æ§æŠ¥å‘Š...")
            reports = gradient_monitor.generate_all_reports()
            info_print(f"âœ… æ¢¯åº¦ç›‘æ§æŠ¥å‘Šå·²ç”Ÿæˆ:")
            for report_type, report_path in reports.items():
                info_print(f"  - {report_type}: {report_path}")

            # ğŸ”® WebUI/APIä¼ç¬”: å¯¼å‡ºJSONæ•°æ®ä¾›æœªæ¥ä½¿ç”¨
            webui_data = gradient_monitor.export_for_webui()
            info_print(f"  - WebUIæ•°æ®å·²å¯¼å‡º (æœªæ¥APIå¯ç”¨): {len(webui_data['gradient_timeline'])} ä¸ªæ¢¯åº¦è®°å½•")
        except Exception as e:
            _log_message(logger, f"ç”Ÿæˆæ¢¯åº¦ç›‘æ§æŠ¥å‘Šå¤±è´¥: {e}", "warning")
            debug_print(f"è­¦å‘Š: æ¢¯åº¦ç›‘æ§æŠ¥å‘Šç”Ÿæˆå¤±è´¥: {e}")

    # è§¦å‘è®­ç»ƒç»“æŸå›è°ƒ
    callback_manager.trigger('on_train_end', model=model, config=config)

    return model, tokenizer, config

def _test_generation_after_epoch(model, tokenizer, logger=None, language="en"):
    """
    æµ‹è¯•æ¯ä¸ªè½®æ¬¡åçš„ç”Ÿæˆæ•ˆæœ

    å‚æ•°:
        model: æ¨¡å‹
        tokenizer: åˆ†è¯å™¨
        logger: æ—¥å¿—è®°å½•å™¨
        language: è¯­è¨€ç±»å‹

    è¿”å›:
        avg_quality: å¹³å‡è´¨é‡åˆ†æ•°
    """
    # æ ¹æ®è¯­è¨€é€‰æ‹©æµ‹è¯•æç¤º
    if language == "zh":
        test_prompts = ["äººå·¥æ™ºèƒ½", "æ·±åº¦å­¦ä¹ ", "è‡ªç„¶è¯­è¨€", "å®‰æŸæ˜¯"]
    else:
        test_prompts = ["Hello", "What is", "The quick", "Artificial"]

    model.eval()
    debug_print("\næœ¬è½®è®­ç»ƒåçš„æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹:")

    gen_texts = []
    for prompt in test_prompts:
        with torch.no_grad():
            gen_text, _, _, _ = generate_natural_text(model, tokenizer, prompt, max_steps=15)
            debug_print(f"æç¤º: '{prompt}'")
            debug_print(f"ç”Ÿæˆ: '{gen_text}'")
            debug_print("-" * 30)
            gen_texts.append(gen_text)

    avg_quality = sum(evaluate_text_quality(text)[0] for text in gen_texts) / len(gen_texts)
    debug_print(f"æœ¬è½®ç”Ÿæˆæ–‡æœ¬å¹³å‡è´¨é‡: {avg_quality:.2f}/100")

    if avg_quality < 40:
        debug_print("\nå®‰æŸï¼šè®­ç»ƒ...è¿˜ä¸å¤Ÿ...")

    model.train()
    return avg_quality

def _compare_model_outputs(untrained_model, trained_model, tokenizer, language="en"):
    """
    æ¯”è¾ƒè®­ç»ƒå‰åçš„æ¨¡å‹è¾“å‡º

    å‚æ•°:
        untrained_model: æœªè®­ç»ƒçš„æ¨¡å‹
        trained_model: è®­ç»ƒåçš„æ¨¡å‹
        tokenizer: åˆ†è¯å™¨
        language: è¯­è¨€ç±»å‹
    """
    info_print("\n====================")
    info_print("è®­ç»ƒå‰åæ•ˆæœå¯¹æ¯”")
    info_print("====================")

    # æ ¹æ®è¯­è¨€é€‰æ‹©æµ‹è¯•æç¤º
    if language == "zh":
        test_prompts = [
            "äººå·¥æ™ºèƒ½æ˜¯",
            "æ·±åº¦å­¦ä¹ å¯ä»¥",
            "è‡ªç„¶è¯­è¨€å¤„ç†",
            "å®‰æŸæ˜¯",
            "æˆ‘è®¤ä¸º"
        ]
    else:
        test_prompts = [
            "Hello, how are you",
            "What is artificial intelligence",
            "The future of technology",
            "I think that",
            "The best way to"
        ]

    trained_model.eval()
    untrained_model.eval()
    untrained_scores = []
    trained_scores = []

    for prompt in test_prompts:
        info_print(f"\næç¤º: '{prompt}'")

        with torch.no_grad():
            untrained_text, _, _, _ = generate_natural_text(
                untrained_model, tokenizer, prompt, max_steps=20
            )
            untrained_score, untrained_feedback = evaluate_text_quality(untrained_text)
            untrained_scores.append(untrained_score)

        info_print(f"æœªè®­ç»ƒæ¨¡å‹: '{untrained_text}'")
        info_print(f"è´¨é‡è¯„åˆ†: {untrained_score}/100 - {untrained_feedback}")

        with torch.no_grad():
            trained_text, _, _, _ = generate_natural_text(
                trained_model, tokenizer, prompt, max_steps=20
            )
            trained_score, trained_feedback = evaluate_text_quality(trained_text)
            trained_scores.append(trained_score)

        info_print(f"è®­ç»ƒåæ¨¡å‹: '{trained_text}'")
        info_print(f"è´¨é‡è¯„åˆ†: {trained_score}/100 - {trained_feedback}")
        info_print("-" * 50)

    avg_untrained = sum(untrained_scores) / len(untrained_scores)
    avg_trained = sum(trained_scores) / len(trained_scores)
    improvement = avg_trained - avg_untrained

    info_print(f"\næ•´ä½“è¯„ä¼°:")
    info_print(f"æœªè®­ç»ƒæ¨¡å‹å¹³å‡è´¨é‡: {avg_untrained:.2f}/100")
    info_print(f"è®­ç»ƒåæ¨¡å‹å¹³å‡è´¨é‡: {avg_trained:.2f}/100")
    info_print(f"è´¨é‡æå‡: {improvement:.2f} åˆ†")

    if avg_trained < 50:
        info_print("\nå®‰æŸï¼šè®­ç»ƒ...è¿˜ä¸å¤Ÿ...")
    else:
        info_print("\nå®‰æŸï¼šè®­ç»ƒå®Œæˆå¾—ä¸é”™ï¼")