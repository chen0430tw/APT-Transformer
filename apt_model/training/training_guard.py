#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Training Guard - è®­ç»ƒä¿æŠ¤æœºåˆ¶

é˜²æ­¢æ— é™è®­ç»ƒã€èµ„æºè€—å°½ã€MCP è¶…æ—¶ç­‰é—®é¢˜

Features:
- Early stopping (validation loss based)
- Max steps/time limits
- Memory monitoring
- Gradient anomaly detection
- MCP-aware checkpointing
- Auto-cleanup
"""

from apt_model.utils.fake_torch import get_torch
torch = get_torch()
import time
import psutil
import os
from typing import Optional, Dict, List, Callable, Any
from datetime import datetime, timedelta
import warnings


class EarlyStopping:
    """
    Early Stopping æœºåˆ¶

    ç›‘æ§éªŒè¯æŸå¤±ï¼Œå¦‚æœè¿ç»­ patience ä¸ª epoch æ²¡æœ‰æ”¹å–„åˆ™åœæ­¢è®­ç»ƒ
    """

    def __init__(
        self,
        patience: int = 5,
        min_delta: float = 0.0,
        mode: str = 'min',
        verbose: bool = True
    ):
        """
        Args:
            patience: å®¹å¿çš„ epoch æ•°
            min_delta: æœ€å°æ”¹å–„é‡
            mode: 'min' (è¶Šå°è¶Šå¥½) æˆ– 'max' (è¶Šå¤§è¶Šå¥½)
            verbose: æ˜¯å¦æ‰“å°æ¶ˆæ¯
        """
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        self.verbose = verbose

        self.counter = 0
        self.best_score = None
        self.early_stop = False

    def __call__(self, score: float) -> bool:
        """
        Returns:
            æ˜¯å¦åº”è¯¥åœæ­¢è®­ç»ƒ
        """
        if self.best_score is None:
            self.best_score = score
            return False

        # æ£€æŸ¥æ˜¯å¦æ”¹å–„
        if self.mode == 'min':
            improved = score < (self.best_score - self.min_delta)
        else:
            improved = score > (self.best_score + self.min_delta)

        if improved:
            self.best_score = score
            self.counter = 0
            if self.verbose:
                print(f"âœ“ éªŒè¯æŒ‡æ ‡æ”¹å–„: {score:.4f}")
        else:
            self.counter += 1
            if self.verbose:
                print(f"âš  éªŒè¯æŒ‡æ ‡æœªæ”¹å–„ ({self.counter}/{self.patience})")

            if self.counter >= self.patience:
                self.early_stop = True
                if self.verbose:
                    print(f"ğŸ›‘ Early stopping triggered (patience={self.patience})")
                return True

        return False


class TrainingGuard:
    """
    è®­ç»ƒä¿æŠ¤å™¨ - é˜²æ­¢æ— é™è®­ç»ƒå’Œèµ„æºè€—å°½

    ç›‘æ§:
    - æœ€å¤§è®­ç»ƒæ­¥æ•°
    - æœ€å¤§è®­ç»ƒæ—¶é—´
    - å†…å­˜ä½¿ç”¨
    - æ¢¯åº¦å¼‚å¸¸
    - æŸå¤±å¼‚å¸¸ (NaN/Inf)
    """

    def __init__(
        self,
        max_steps: Optional[int] = None,
        max_time_hours: Optional[float] = None,
        max_memory_percent: float = 90.0,
        check_gradients: bool = True,
        auto_cleanup_every: int = 100,
        early_stopping: Optional[EarlyStopping] = None,
        mcp_checkpoint_interval: int = 1000,
        verbose: bool = True
    ):
        """
        Args:
            max_steps: æœ€å¤§è®­ç»ƒæ­¥æ•° (None = æ— é™åˆ¶)
            max_time_hours: æœ€å¤§è®­ç»ƒæ—¶é—´ï¼ˆå°æ—¶ï¼‰
            max_memory_percent: æœ€å¤§å†…å­˜ä½¿ç”¨ç™¾åˆ†æ¯”
            check_gradients: æ˜¯å¦æ£€æŸ¥æ¢¯åº¦å¼‚å¸¸
            auto_cleanup_every: æ¯ N æ­¥è‡ªåŠ¨æ¸…ç†ç¼“å­˜
            early_stopping: EarlyStopping å®ä¾‹
            mcp_checkpoint_interval: MCP æ£€æŸ¥ç‚¹é—´éš”
            verbose: è¯¦ç»†è¾“å‡º
        """
        self.max_steps = max_steps
        self.max_time_hours = max_time_hours
        self.max_memory_percent = max_memory_percent
        self.check_gradients = check_gradients
        self.auto_cleanup_every = auto_cleanup_every
        self.early_stopping = early_stopping
        self.mcp_checkpoint_interval = mcp_checkpoint_interval
        self.verbose = verbose

        # å†…éƒ¨çŠ¶æ€
        self.start_time = None
        self.step_count = 0
        self.should_stop = False
        self.stop_reason = None

        # ç»Ÿè®¡
        self.stats = {
            'nan_losses': 0,
            'inf_losses': 0,
            'gradient_explosions': 0,
            'memory_warnings': 0,
            'cleanups': 0
        }

    def start(self):
        """å¼€å§‹è®­ç»ƒç›‘æ§"""
        self.start_time = time.time()
        self.step_count = 0
        self.should_stop = False
        self.stop_reason = None

        if self.verbose:
            print("ğŸ›¡ï¸ Training Guard å·²å¯åŠ¨")
            if self.max_steps:
                print(f"  æœ€å¤§æ­¥æ•°: {self.max_steps}")
            if self.max_time_hours:
                print(f"  æœ€å¤§æ—¶é—´: {self.max_time_hours:.1f} å°æ—¶")
            print(f"  å†…å­˜é™åˆ¶: {self.max_memory_percent}%")
            if self.early_stopping:
                print(f"  Early Stopping: patience={self.early_stopping.patience}")

    def step(
        self,
        loss: Optional[float] = None,
        model: Optional[torch.nn.Module] = None
    ) -> bool:
        """
        æ¯ä¸ªè®­ç»ƒæ­¥è°ƒç”¨

        Args:
            loss: å½“å‰æŸå¤±å€¼
            model: æ¨¡å‹ï¼ˆç”¨äºæ¢¯åº¦æ£€æŸ¥ï¼‰

        Returns:
            æ˜¯å¦åº”è¯¥ç»§ç»­è®­ç»ƒ (True=ç»§ç»­, False=åœæ­¢)
        """
        self.step_count += 1

        # 1. æ£€æŸ¥æœ€å¤§æ­¥æ•°
        if self.max_steps and self.step_count >= self.max_steps:
            self._stop(f"è¾¾åˆ°æœ€å¤§æ­¥æ•° {self.max_steps}")
            return False

        # 2. æ£€æŸ¥æœ€å¤§æ—¶é—´
        if self.max_time_hours:
            elapsed_hours = (time.time() - self.start_time) / 3600
            if elapsed_hours >= self.max_time_hours:
                self._stop(f"è¾¾åˆ°æœ€å¤§æ—¶é—´ {self.max_time_hours:.1f} å°æ—¶")
                return False

        # 3. æ£€æŸ¥æŸå¤±å¼‚å¸¸
        if loss is not None:
            if not self._check_loss(loss):
                return False

        # 4. æ£€æŸ¥æ¢¯åº¦
        if self.check_gradients and model is not None:
            if not self._check_gradients(model):
                return False

        # 5. æ£€æŸ¥å†…å­˜
        if not self._check_memory():
            return False

        # 6. è‡ªåŠ¨æ¸…ç†
        if self.step_count % self.auto_cleanup_every == 0:
            self._cleanup()

        # 7. MCP æ£€æŸ¥ç‚¹æé†’
        if self.step_count % self.mcp_checkpoint_interval == 0:
            if self.verbose:
                print(f"ğŸ’¾ å»ºè®®ä¿å­˜æ£€æŸ¥ç‚¹ (æ­¥æ•°: {self.step_count})")

        return True

    def validate(self, val_score: float) -> bool:
        """
        éªŒè¯æ­¥éª¤ï¼ˆç”¨äº early stoppingï¼‰

        Args:
            val_score: éªŒè¯åˆ†æ•°

        Returns:
            æ˜¯å¦åº”è¯¥ç»§ç»­è®­ç»ƒ
        """
        if self.early_stopping is not None:
            if self.early_stopping(val_score):
                self._stop("Early stopping triggered")
                return False
        return True

    def _check_loss(self, loss: float) -> bool:
        """æ£€æŸ¥æŸå¤±å€¼"""
        import math

        if math.isnan(loss):
            self.stats['nan_losses'] += 1
            self._stop(f"æŸå¤±ä¸º NaN (ç¬¬ {self.stats['nan_losses']} æ¬¡)")
            return False

        if math.isinf(loss):
            self.stats['inf_losses'] += 1
            self._stop(f"æŸå¤±ä¸º Inf (ç¬¬ {self.stats['inf_losses']} æ¬¡)")
            return False

        return True

    def _check_gradients(self, model: torch.nn.Module) -> bool:
        """æ£€æŸ¥æ¢¯åº¦å¼‚å¸¸"""
        import math

        total_norm = 0.0
        for p in model.parameters():
            if p.grad is not None:
                param_norm = p.grad.data.norm(2)
                total_norm += param_norm.item() ** 2

                # æ£€æŸ¥ NaN/Inf
                if torch.isnan(p.grad).any() or torch.isinf(p.grad).any():
                    self._stop("æ¢¯åº¦åŒ…å« NaN æˆ– Inf")
                    return False

        total_norm = total_norm ** 0.5

        # æ£€æŸ¥æ¢¯åº¦çˆ†ç‚¸ (é˜ˆå€¼: 100)
        if total_norm > 100.0:
            self.stats['gradient_explosions'] += 1
            if self.verbose:
                warnings.warn(f"âš ï¸ æ¢¯åº¦èŒƒæ•°è¿‡å¤§: {total_norm:.2f}")

            if self.stats['gradient_explosions'] >= 10:
                self._stop("æ¢¯åº¦æŒç»­çˆ†ç‚¸ (10 æ¬¡)")
                return False

        return True

    def _check_memory(self) -> bool:
        """æ£€æŸ¥å†…å­˜ä½¿ç”¨"""
        memory = psutil.virtual_memory()
        percent = memory.percent

        if percent > self.max_memory_percent:
            self.stats['memory_warnings'] += 1

            if self.verbose:
                warnings.warn(
                    f"âš ï¸ å†…å­˜ä½¿ç”¨è¿‡é«˜: {percent:.1f}% "
                    f"(é™åˆ¶: {self.max_memory_percent}%)"
                )

            # å¼ºåˆ¶æ¸…ç†
            self._cleanup()

            # å†æ¬¡æ£€æŸ¥
            memory = psutil.virtual_memory()
            if memory.percent > self.max_memory_percent:
                self._stop(f"å†…å­˜è€—å°½: {memory.percent:.1f}%")
                return False

        return True

    def _cleanup(self):
        """æ¸…ç†ç¼“å­˜"""
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

        self.stats['cleanups'] += 1

        if self.verbose and self.step_count % (self.auto_cleanup_every * 10) == 0:
            print(f"ğŸ§¹ è‡ªåŠ¨æ¸…ç† (ç¬¬ {self.stats['cleanups']} æ¬¡)")

    def _stop(self, reason: str):
        """åœæ­¢è®­ç»ƒ"""
        self.should_stop = True
        self.stop_reason = reason

        if self.verbose:
            print(f"\nğŸ›‘ è®­ç»ƒåœæ­¢: {reason}")
            print(f"   æ€»æ­¥æ•°: {self.step_count}")
            elapsed = time.time() - self.start_time
            print(f"   è®­ç»ƒæ—¶é—´: {elapsed/3600:.2f} å°æ—¶")

    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        elapsed = time.time() - self.start_time if self.start_time else 0

        return {
            'total_steps': self.step_count,
            'elapsed_hours': elapsed / 3600,
            'stopped': self.should_stop,
            'stop_reason': self.stop_reason,
            **self.stats
        }


class SafeTrainingContext:
    """
    å®‰å…¨è®­ç»ƒä¸Šä¸‹æ–‡ç®¡ç†å™¨

    Usage:
        with SafeTrainingContext(max_steps=10000, max_time_hours=2) as guard:
            for epoch in range(epochs):
                for batch in dataloader:
                    loss = train_step(batch)

                    # æ£€æŸ¥æ˜¯å¦ç»§ç»­
                    if not guard.step(loss=loss, model=model):
                        break

                # éªŒè¯
                val_loss = validate()
                if not guard.validate(val_loss):
                    break
    """

    def __init__(self, **guard_kwargs):
        self.guard = TrainingGuard(**guard_kwargs)

    def __enter__(self):
        self.guard.start()
        return self.guard

    def __exit__(self, exc_type, exc_val, exc_tb):
        if self.guard.verbose:
            stats = self.guard.get_stats()
            print("\n" + "="*80)
            print("è®­ç»ƒä¿æŠ¤ç»Ÿè®¡:")
            print(f"  æ€»æ­¥æ•°: {stats['total_steps']}")
            print(f"  è®­ç»ƒæ—¶é—´: {stats['elapsed_hours']:.2f} å°æ—¶")
            print(f"  NaN æŸå¤±: {stats['nan_losses']}")
            print(f"  Inf æŸå¤±: {stats['inf_losses']}")
            print(f"  æ¢¯åº¦çˆ†ç‚¸: {stats['gradient_explosions']}")
            print(f"  å†…å­˜è­¦å‘Š: {stats['memory_warnings']}")
            print(f"  è‡ªåŠ¨æ¸…ç†: {stats['cleanups']}")
            if stats['stopped']:
                print(f"  åœæ­¢åŸå› : {stats['stop_reason']}")
            print("="*80)


# ==================== MCP-Aware Training ====================

class MCPSafeTrainer:
    """
    MCP-Safe è®­ç»ƒå™¨åŒ…è£…å™¨

    è‡ªåŠ¨å¤„ç† MCP ç›¸å…³çš„è®­ç»ƒé—®é¢˜ï¼š
    - å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
    - ä¼˜é›…å¤„ç†ä¸­æ–­
    - èµ„æºç›‘æ§
    """

    def __init__(
        self,
        trainer: Any,
        checkpoint_dir: str = "./checkpoints",
        checkpoint_interval: int = 1000,
        max_checkpoints: int = 5,
        resume_from_checkpoint: bool = True
    ):
        """
        Args:
            trainer: åŸå§‹è®­ç»ƒå™¨å®ä¾‹
            checkpoint_dir: æ£€æŸ¥ç‚¹ç›®å½•
            checkpoint_interval: æ£€æŸ¥ç‚¹é—´éš”ï¼ˆæ­¥æ•°ï¼‰
            max_checkpoints: æœ€å¤šä¿ç•™æ£€æŸ¥ç‚¹æ•°
            resume_from_checkpoint: æ˜¯å¦è‡ªåŠ¨æ¢å¤
        """
        self.trainer = trainer
        self.checkpoint_dir = checkpoint_dir
        self.checkpoint_interval = checkpoint_interval
        self.max_checkpoints = max_checkpoints
        self.resume_from_checkpoint = resume_from_checkpoint

        os.makedirs(checkpoint_dir, exist_ok=True)

        # å°è¯•æ¢å¤
        if resume_from_checkpoint:
            self._try_resume()

    def train_step(self, *args, **kwargs):
        """åŒ…è£…çš„è®­ç»ƒæ­¥éª¤"""
        result = self.trainer.train_step(*args, **kwargs)

        # å®šæœŸä¿å­˜
        if hasattr(self.trainer, 'step_count'):
            if self.trainer.step_count % self.checkpoint_interval == 0:
                self.save_checkpoint()

        return result

    def save_checkpoint(self, name: Optional[str] = None):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        if name is None:
            name = f"checkpoint_step_{self.trainer.step_count}.pt"

        checkpoint_path = os.path.join(self.checkpoint_dir, name)

        # ä¿å­˜
        if hasattr(self.trainer, 'save_model'):
            self.trainer.save_model(checkpoint_path)

        # æ¸…ç†æ—§æ£€æŸ¥ç‚¹
        self._cleanup_old_checkpoints()

        print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {checkpoint_path}")

    def _cleanup_old_checkpoints(self):
        """æ¸…ç†æ—§æ£€æŸ¥ç‚¹"""
        checkpoints = sorted(
            [f for f in os.listdir(self.checkpoint_dir) if f.startswith('checkpoint_')],
            key=lambda x: os.path.getmtime(os.path.join(self.checkpoint_dir, x))
        )

        # ä¿ç•™æœ€æ–°çš„ N ä¸ª
        if len(checkpoints) > self.max_checkpoints:
            for old_ckpt in checkpoints[:-self.max_checkpoints]:
                os.remove(os.path.join(self.checkpoint_dir, old_ckpt))

    def _try_resume(self):
        """å°è¯•æ¢å¤è®­ç»ƒ"""
        checkpoints = sorted(
            [f for f in os.listdir(self.checkpoint_dir) if f.startswith('checkpoint_')],
            key=lambda x: os.path.getmtime(os.path.join(self.checkpoint_dir, x))
        )

        if checkpoints:
            latest = os.path.join(self.checkpoint_dir, checkpoints[-1])
            if hasattr(self.trainer, 'load_model'):
                try:
                    self.trainer.load_model(latest)
                    print(f"â™»ï¸ ä»æ£€æŸ¥ç‚¹æ¢å¤: {latest}")
                except Exception as e:
                    print(f"âš ï¸ æ¢å¤å¤±è´¥: {e}")


# ==================== Example Usage ====================

if __name__ == "__main__":
    # Example 1: Basic Early Stopping
    early_stop = EarlyStopping(patience=5, mode='min')

    for epoch in range(100):
        train_loss = 0.5 / (epoch + 1)  # Simulated decreasing loss
        val_loss = 0.5 / (epoch + 1) + (0.01 if epoch > 10 else 0)

        if early_stop(val_loss):
            print(f"Training stopped at epoch {epoch}")
            break

    # Example 2: Training Guard
    print("\n" + "="*80)

    guard = TrainingGuard(
        max_steps=100,
        max_time_hours=0.01,  # 36 seconds for demo
        early_stopping=EarlyStopping(patience=3)
    )

    guard.start()

    for step in range(1000):  # Will stop before 1000
        loss = 1.0 / (step + 1)

        if not guard.step(loss=loss):
            break

        # Simulate validation every 10 steps
        if step % 10 == 0:
            val_loss = loss + 0.01
            if not guard.validate(val_loss):
                break

    print(guard.get_stats())
