#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
APT Model (è‡ªç”Ÿæˆå˜æ¢å™¨) CLI Commands
Implementation of command-line commands for APT model tool

é‡æ„åçš„å‘½ä»¤ç³»ç»Ÿï¼š
- æå–å…¬å…±ä»£ç åˆ°è¾…åŠ©å‡½æ•°
- æ”¯æŒæ’ä»¶å‘½ä»¤æ³¨å†Œ
- æ¸…æ™°çš„å‘½ä»¤åˆ†ç±»
- ç»Ÿä¸€çš„é”™è¯¯å¤„ç†
"""

import os
import sys
# å¼ºåˆ¶ä½¿ç”¨UTF-8ç¼–ç 
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')
import traceback
from datetime import datetime

from apt_model.utils.logging_utils import setup_logging
from apt_model.utils.resource_monitor import ResourceMonitor
from apt_model.utils.language_manager import LanguageManager
from apt_model.utils.hardware_check import check_hardware_compatibility
from apt_model.utils.cache_manager import CacheManager
from apt_model.config.apt_config import APTConfig
from apt_model.training.trainer import train_model
from apt_model.data.external_data import train_with_external_data, load_external_data
from apt_model.interactive.chat import chat_with_model
from apt_model.evaluation.model_evaluator import evaluate_model
from apt_model.utils.visualization import ModelVisualizer
from apt_model.utils.time_estimator import TrainingTimeEstimator
from apt_model.utils import get_device, set_seed
from apt_model.utils.common import _initialize_common
from apt_model.cli.command_registry import register_command


# ============================================================================
# è¾…åŠ©å‡½æ•° - å…¬å…±ä»£ç æå–
# ============================================================================

def _setup_resource_monitor(args, logger):
    """è®¾ç½®èµ„æºç›‘æ§å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰"""
    if args.monitor_resources:
        return ResourceMonitor(logger=logger, log_interval=args.log_interval)
    return None


def _start_monitor(resource_monitor):
    """å¯åŠ¨èµ„æºç›‘æ§å™¨"""
    if resource_monitor:
        resource_monitor.start()


def _stop_monitor(resource_monitor):
    """åœæ­¢èµ„æºç›‘æ§å™¨"""
    if resource_monitor:
        resource_monitor.stop()


def _handle_command_error(command_name, error, logger):
    """ç»Ÿä¸€çš„å‘½ä»¤é”™è¯¯å¤„ç†"""
    logger.error(f"{command_name}è¿‡ç¨‹ä¸­å‡ºé”™: {error}")
    logger.error(traceback.format_exc())
    print(f"é”™è¯¯: {error}")
    return 1


def _get_tokenizer_with_detection(texts, args):
    """
    è·å–tokenizerå¹¶æ£€æµ‹è¯­è¨€

    ä½¿ç”¨æ–°çš„codecç³»ç»Ÿï¼ˆä¼˜å…ˆï¼‰æˆ–å›é€€åˆ°æ—§ç³»ç»Ÿ
    """
    from apt_model.codecs import get_codec_for_language
    from apt_model.codecs.compat import CodecTokenizerWrapper
    from apt_model.modeling.chinese_tokenizer_integration import detect_language

    # è‡ªåŠ¨æ£€æµ‹è¯­è¨€
    detected_language = args.model_language or detect_language(texts)

    # å°è¯•ä½¿ç”¨æ–°çš„codecç³»ç»Ÿ
    try:
        codec = get_codec_for_language(detected_language)
        if codec:
            tokenizer = CodecTokenizerWrapper(codec)
            return tokenizer, detected_language
    except Exception as e:
        print(f"Codecç³»ç»Ÿå¤±è´¥ï¼Œå›é€€åˆ°æ—§åˆ†è¯å™¨: {e}")

    # å›é€€åˆ°æ—§ç³»ç»Ÿ
    from apt_model.modeling.chinese_tokenizer_integration import get_appropriate_tokenizer
    return get_appropriate_tokenizer(
        texts,
        tokenizer_type=args.tokenizer_type,
        language=args.model_language
    )


def _create_visualizations(args, model, logger):
    """åˆ›å»ºæ¨¡å‹å¯è§†åŒ–ï¼ˆå¦‚æœè¯·æ±‚ï¼‰"""
    if args.create_plots and model:
        visualizer = ModelVisualizer(logger=logger)
        history = {'loss': []}  # å®é™…åº”ä»è®­ç»ƒè¿‡ç¨‹è·å–

        if args.output_dir:
            cache_manager = CacheManager(cache_dir=args.output_dir, logger=logger)
            visualizer.cache_manager = cache_manager

        visualizer.create_training_history_plot(
            history,
            title=f"{os.path.basename(args.save_path)} Training History"
        )


# ============================================================================
# è®­ç»ƒç›¸å…³å‘½ä»¤
# ============================================================================

def run_train_command(args):
    """
    æ‰§è¡Œè®­ç»ƒå‘½ä»¤

    å‚æ•°:
        args: å‘½ä»¤è¡Œå‚æ•°

    è¿”å›:
        int: é€€å‡ºç 
    """
    logger, lang_manager, device = _initialize_common(args)
    _ = lambda key, *params: lang_manager.get(key).format(*params) if params else lang_manager.get(key)

    logger.info(_("training.start"))

    # æ£€æŸ¥ç¡¬ä»¶å…¼å®¹æ€§
    model_config = APTConfig()
    check_hardware_compatibility(model_config, logger)

    # è®¾ç½®èµ„æºç›‘æ§
    resource_monitor = _setup_resource_monitor(args, logger)
    _start_monitor(resource_monitor)

    try:
        # è°ƒç”¨è®­ç»ƒå‡½æ•°
        model, tokenizer, config = train_model(
            epochs=args.epochs,
            batch_size=args.batch_size,
            learning_rate=args.learning_rate,
            save_path=args.save_path,
            logger=logger,
            resource_monitor=resource_monitor,
            tokenizer_type=args.tokenizer_type,
            language=args.model_language
        )

        # åˆ›å»ºå¯è§†åŒ–
        _create_visualizations(args, model, logger)

        return 0  # æˆåŠŸ
    except Exception as e:
        return _handle_command_error("è®­ç»ƒ", e, logger)
    finally:
        _stop_monitor(resource_monitor)


def run_train_custom_command(args):
    """
    æ‰§è¡Œè‡ªå®šä¹‰æ•°æ®è®­ç»ƒå‘½ä»¤

    å‚æ•°:
        args: å‘½ä»¤è¡Œå‚æ•°

    è¿”å›:
        int: é€€å‡ºç 
    """
    logger, lang_manager, device = _initialize_common(args)

    logger.info("å¼€å§‹ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®è®­ç»ƒæ¨¡å‹...")

    # è®¾ç½®èµ„æºç›‘æ§
    resource_monitor = _setup_resource_monitor(args, logger)
    _start_monitor(resource_monitor)

    try:
        # åŠ è½½è‡ªå®šä¹‰æ•°æ®
        custom_texts = None
        if args.data_path:
            try:
                custom_texts = load_external_data(args.data_path, max_samples=args.max_samples)
                print(f"æˆåŠŸåŠ è½½è‡ªå®šä¹‰æ•°æ®: {args.data_path}ï¼Œå…± {len(custom_texts)} æ¡æ–‡æœ¬")
            except FileNotFoundError:
                logger.warning(f"æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶: {args.data_path}ï¼Œå°†ä½¿ç”¨é¢„è®¾è®­ç»ƒæ•°æ®")
                print(f"æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶: {args.data_path}ï¼Œå°†ä½¿ç”¨é¢„è®¾è®­ç»ƒæ•°æ®")
                from apt_model.training.trainer import get_training_texts
                custom_texts = get_training_texts()
                print(f"ä½¿ç”¨é¢„è®¾æ•°æ®ï¼Œå…± {len(custom_texts)} æ¡æ–‡æœ¬")

        # éªŒè¯æ•°æ®åŠ è½½
        if not custom_texts or len(custom_texts) == 0:
            logger.error("æ— æ³•åŠ è½½æ•°æ®æˆ–æ•°æ®ä¸ºç©º")
            print("é”™è¯¯: æ— æ³•åŠ è½½è®­ç»ƒæ•°æ®æˆ–æ•°æ®ä¸ºç©º")
            print("è¯·æ£€æŸ¥:")
            print(f"  1. æ•°æ®æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®: {args.data_path if args.data_path else '(æœªæŒ‡å®š)'}")
            print("  2. æ•°æ®æ–‡ä»¶æ˜¯å¦åŒ…å«æœ‰æ•ˆå†…å®¹")
            print("  3. æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡® (txt/json)")
            return 1

        # è·å–tokenizerå¹¶æ£€æµ‹è¯­è¨€
        tokenizer, detected_language = _get_tokenizer_with_detection(custom_texts, args)
        logger.info(f"ä½¿ç”¨{detected_language}è¯­è¨€åˆ†è¯å™¨: {type(tokenizer).__name__}")
        print(f"ä½¿ç”¨{detected_language}è¯­è¨€åˆ†è¯å™¨: {type(tokenizer).__name__}")

        # ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®è®­ç»ƒ
        model, tokenizer, config = train_with_external_data(
            data_path=None,
            epochs=args.epochs,
            batch_size=args.batch_size,
            learning_rate=args.learning_rate,
            save_path=args.save_path,
            max_samples=args.max_samples,
            tokenizer=tokenizer,
            language=detected_language,
            custom_texts=custom_texts
        )
        return 0 if model else 1
    except Exception as e:
        return _handle_command_error("è‡ªå®šä¹‰è®­ç»ƒ", e, logger)
    finally:
        _stop_monitor(resource_monitor)


# ============================================================================
# äº¤äº’ç›¸å…³å‘½ä»¤
# ============================================================================

def run_chat_command(args):
    """
    æ‰§è¡ŒèŠå¤©å‘½ä»¤

    å‚æ•°:
        args: å‘½ä»¤è¡Œå‚æ•°

    è¿”å›:
        int: é€€å‡ºç 
    """
    # è®¾ç½®æ—¥å¿—
    log_file = f"apt_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    logger = setup_logging(log_file=log_file)
    logger.info("å¼€å§‹ä¸æ¨¡å‹äº¤äº’å¯¹è¯...")

    try:
        # è·å–æ¨¡å‹è·¯å¾„
        model_dir = args.model_path[0] if isinstance(args.model_path, list) else args.model_path

        # è°ƒç”¨èŠå¤©å‡½æ•°
        chat_with_model(
            model_path=model_dir,
            temperature=args.temperature,
            top_p=args.top_p,
            max_length=args.max_length,
            logger=logger
        )
        return 0
    except Exception as e:
        logger.error(f"èŠå¤©è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        logger.error(traceback.format_exc())
        print(f"é”™è¯¯: {e}")
        print("å¦‚æœæ‚¨è¿˜æ²¡æœ‰è®­ç»ƒæ¨¡å‹ï¼Œè¯·å…ˆè¿è¡Œ 'python -m apt_model train' å‘½ä»¤è®­ç»ƒæ¨¡å‹ã€‚")
        return 1


# ============================================================================
# è¯„ä¼°ç›¸å…³å‘½ä»¤
# ============================================================================

def run_evaluate_command(args):
    """
    Run the evaluate command (evaluate model performance) for one or more models.

    Parameters:
        args: Command line arguments

    Returns:
        int: Exit code (0 for success, non-zero for error)
    """
    logger, lang_manager, device = _initialize_common(args)
    logger.info(f"Starting evaluation of models: {args.model_path}")

    overall_success = True

    for model_path in args.model_path:
        logger.info(f"Evaluating model: {model_path}")
        try:
            evaluation_results = evaluate_model(
                model_path=model_path,
                output_dir=args.output_dir,
                eval_sets=args.eval_sets,
                num_samples=args.num_eval_samples,
                logger=logger
            )

            if evaluation_results and args.output_dir:
                print(f"\nEvaluation report for model '{model_path}' saved to: {args.output_dir}")
            elif not evaluation_results:
                overall_success = False

        except Exception as e:
            logger.error(f"Error during evaluation of model '{model_path}': {e}")
            logger.error(traceback.format_exc())
            overall_success = False

    return 0 if overall_success else 1


def run_visualize_command(args):
    """
    æ‰§è¡Œ visualize å‘½ä»¤ï¼Œç”Ÿæˆæ¨¡å‹è¯„ä¼°å¯è§†åŒ–å›¾è¡¨
    """
    import matplotlib.pyplot as plt
    import numpy as np

    print("=" * 60)
    print("Starting visualization command")
    print("=" * 60)

    try:
        # è·å–å‘½ä»¤è¡Œå‚æ•°
        model_path = args.model_path
        if args.output_dir:
            output_dir = args.output_dir
        else:
            project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
            output_dir = os.path.join(project_root, "report")

        print(f"æ¨¡å‹è·¯å¾„: {model_path}")
        print(f"è¾“å‡ºç›®å½•: {output_dir}")

        # åˆå§‹åŒ– logger
        try:
            logger, _, _ = _initialize_common(args)
            logger.info(f"Creating visualizations for model: {model_path}")
        except Exception as e:
            print(f"Warning: Could not initialize logger: {e}")
            logger = None

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        abs_output_dir = os.path.abspath(output_dir)

        # é…ç½®å¯è§†åŒ–åº“ï¼ˆæ”¯æŒä¸­æ–‡ï¼‰
        plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False

        charts_created = 0

        # å°è¯•åŠ è½½æ¨¡å‹
        model = None
        try:
            print(f"å°è¯•åŠ è½½æ¨¡å‹: {model_path}")
            from apt_model.training.checkpoint import load_model
            if not os.path.exists(model_path):
                raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            model, tokenizer, config = load_model(model_path)
            print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            print("ç»§ç»­æ‰§è¡Œä¸ä¾èµ–æ¨¡å‹çš„å¯è§†åŒ–...")

        # åˆ›å»ºå„ç§å›¾è¡¨
        charts_created += _create_category_chart(abs_output_dir, logger)
        charts_created += _create_training_history_chart(abs_output_dir, logger)
        charts_created += _create_capability_radar_chart(abs_output_dir, logger)
        charts_created += _create_quality_trend_chart(abs_output_dir, logger)

        if args.visualize_attention:
            charts_created += _create_attention_heatmap(abs_output_dir, logger)

        # ç”Ÿæˆæ•´ä½“æŠ¥å‘Š
        _create_visualization_report(abs_output_dir, args, logger)

        print("\n" + "=" * 60)
        print(f"å¯è§†åŒ–å®Œæˆï¼æ‰€æœ‰å›¾è¡¨å·²ä¿å­˜åˆ°: {abs_output_dir}")
        print("=" * 60)

        # å°è¯•æ‰“å¼€è¾“å‡ºç›®å½•
        _try_open_directory(abs_output_dir)

        return 0

    except Exception as e:
        if 'logger' in locals() and logger:
            logger.error(f"å¯è§†åŒ–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            logger.error(traceback.format_exc())
        print(f"Error during visualization: {e}")
        print(traceback.format_exc())
        return 1


# å¯è§†åŒ–è¾…åŠ©å‡½æ•°
def _create_category_chart(output_dir, logger):
    """åˆ›å»ºç±»åˆ«å¯¹æ¯”å›¾"""
    import matplotlib.pyplot as plt

    print("\nåˆ›å»ºç±»åˆ«å¯¹æ¯”å›¾...")
    category_scores = {
        "äº‹å®æ€§": 60, "é€»è¾‘æ€§": 55, "åˆ›é€ æ€§": 70,
        "ç¼–ç¨‹": 50, "ä¸­æ–‡": 45
    }
    plt.figure(figsize=(10, 6))
    categories = list(category_scores.keys())
    scores = list(category_scores.values())
    bars = plt.bar(categories, scores, color='skyblue')
    plt.title("æ¨¡å‹å„ç±»åˆ«æ€§èƒ½è¯„ä¼°")
    plt.xlabel("ç±»åˆ«")
    plt.ylabel("å¾—åˆ†")
    plt.ylim(0, 100)
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height, f'{height:.1f}',
                 ha='center', va='bottom')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    chart_path = os.path.join(output_dir, "category_performance.png")
    plt.savefig(chart_path)
    plt.close()
    print(f"âœ“ ç±»åˆ«å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: {chart_path}")
    if logger:
        logger.info(f"Category chart saved to: {chart_path}")
    return 1


def _create_training_history_chart(output_dir, logger):
    """åˆ›å»ºè®­ç»ƒå†å²å›¾"""
    import matplotlib.pyplot as plt

    print("\nåˆ›å»ºè®­ç»ƒå†å²å›¾...")
    epochs = range(1, 21)
    train_loss = [4.5 - i * 0.16 for i in range(20)]
    val_loss = [4.6 - i * 0.15 for i in range(20)]
    plt.figure(figsize=(12, 6))
    plt.plot(epochs, train_loss, 'b-', label='è®­ç»ƒæŸå¤±')
    plt.plot(epochs, val_loss, 'r-', label='éªŒè¯æŸå¤±')
    plt.title("æ¨¡å‹è®­ç»ƒå†å²")
    plt.xlabel("è½®æ¬¡")
    plt.ylabel("æŸå¤±")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    history_path = os.path.join(output_dir, "training_history.png")
    plt.savefig(history_path)
    plt.close()
    print(f"âœ“ è®­ç»ƒå†å²å›¾å·²ä¿å­˜åˆ°: {history_path}")
    if logger:
        logger.info(f"Training history chart saved to: {history_path}")
    return 1


def _create_capability_radar_chart(output_dir, logger):
    """åˆ›å»ºèƒ½åŠ›é›·è¾¾å›¾"""
    import matplotlib.pyplot as plt
    import numpy as np

    print("\nåˆ›å»ºèƒ½åŠ›é›·è¾¾å›¾...")
    categories_list = ['ç”Ÿæˆè´¨é‡', 'æ¨ç†èƒ½åŠ›', 'å¤šè¯­ç§', 'åˆ›é€ æ€§', 'ç¼–ç¨‹']
    values = [85, 72, 68, 80, 75]
    N = len(categories_list)
    angles = [n / float(N) * 2 * np.pi for n in range(N)]
    angles += angles[:1]
    values_for_chart = values + [values[0]]
    fig = plt.figure(figsize=(8, 8))
    ax = fig.add_subplot(111, polar=True)
    ax.plot(angles, values_for_chart, 'o-', linewidth=2)
    ax.fill(angles, values_for_chart, alpha=0.25)
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(categories_list)
    ax.set_ylim(0, 100)
    plt.title('æ¨¡å‹èƒ½åŠ›è¯„ä¼°', size=15)
    radar_chart_path = os.path.join(output_dir, "capability_radar.png")
    plt.savefig(radar_chart_path)
    plt.close()
    print(f"âœ“ é›·è¾¾å›¾å·²ä¿å­˜åˆ°: {radar_chart_path}")
    if logger:
        logger.info(f"Radar chart saved to: {radar_chart_path}")
    return 1


def _create_quality_trend_chart(output_dir, logger):
    """åˆ›å»ºè´¨é‡è¯„ä¼°è¶‹åŠ¿å›¾"""
    import matplotlib.pyplot as plt
    import numpy as np

    print("\nåˆ›å»ºè´¨é‡è¯„ä¼°è¶‹åŠ¿å›¾...")
    epochs_list = list(range(1, 21, 2))
    quality_scores = [45, 52, 60, 65, 70, 78, 82, 85, 87, 90]
    plt.figure(figsize=(10, 6))
    plt.plot(epochs_list, quality_scores, 'g-o', label='ç”Ÿæˆè´¨é‡è¯„åˆ†')
    z = np.polyfit(epochs_list, quality_scores, 1)
    p = np.poly1d(z)
    plt.plot(epochs_list, p(epochs_list), "r--", label='è¶‹åŠ¿çº¿')
    plt.title("è´¨é‡è¯„ä¼°è¶‹åŠ¿å›¾")
    plt.xlabel("è®­ç»ƒè½®æ¬¡")
    plt.ylabel("è´¨é‡è¯„åˆ†")
    plt.ylim(0, 100)
    plt.grid(True)
    plt.legend()
    quality_path = os.path.join(output_dir, "quality_trend.png")
    plt.savefig(quality_path)
    plt.close()
    print(f"âœ“ è´¨é‡è¯„ä¼°è¶‹åŠ¿å›¾å·²ä¿å­˜åˆ°: {quality_path}")
    if logger:
        logger.info(f"Quality trend chart saved to: {quality_path}")
    return 1


def _create_attention_heatmap(output_dir, logger):
    """åˆ›å»ºæ³¨æ„åŠ›çƒ­å›¾"""
    import matplotlib.pyplot as plt
    import numpy as np

    print("\nåˆ›å»ºæ³¨æ„åŠ›çƒ­å›¾...")
    try:
        import seaborn as sns
        tokens = ["å¼€å§‹", "APT", "æ¨¡å‹", "æœ‰", "æƒŠäºº", "çš„", "ç”Ÿæˆ", "èƒ½åŠ›"]
        attention = np.random.rand(len(tokens), len(tokens))
        np.fill_diagonal(attention, np.random.uniform(0.7, 1.0, size=len(tokens)))
        plt.figure(figsize=(10, 8))
        sns.heatmap(attention, xticklabels=tokens, yticklabels=tokens,
                    cmap="YlGnBu", vmin=0, vmax=1, annot=True, fmt=".2f")
        plt.title("æ³¨æ„åŠ›çƒ­å›¾")
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        attention_path = os.path.join(output_dir, "attention_heatmap.png")
        plt.savefig(attention_path)
        plt.close()
        print(f"âœ“ æ³¨æ„åŠ›çƒ­å›¾å·²ä¿å­˜åˆ°: {attention_path}")
        if logger:
            logger.info(f"Attention heatmap saved to: {attention_path}")
        return 1
    except Exception as e:
        print(f"Error creating attention heatmap: {e}")
        if logger:
            logger.error(f"Error creating attention heatmap: {e}")
        return 0


def _create_visualization_report(output_dir, args, logger):
    """ç”Ÿæˆæ•´ä½“å¯è§†åŒ–æŠ¥å‘Š"""
    print("\nç”Ÿæˆæ•´ä½“å¯è§†åŒ–æŠ¥å‘Š...")
    report_path = os.path.join(output_dir, "visualization_report.md")
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# APTæ¨¡å‹å¯è§†åŒ–æŠ¥å‘Š\n\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write("## æ¨¡å‹æ€§èƒ½è¯„ä¼°\n\n")
        f.write("![ç±»åˆ«æ€§èƒ½](./category_performance.png)\n\n")
        f.write("æ¨¡å‹åœ¨å„ä¸ªèƒ½åŠ›ç±»åˆ«ä¸Šçš„è¡¨ç°è¯„ä¼°ã€‚\n\n")
        f.write("## è®­ç»ƒå†å²\n\n")
        f.write("![è®­ç»ƒå†å²](./training_history.png)\n\n")
        f.write("æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­æŸå¤±å‡½æ•°çš„å˜åŒ–ã€‚\n\n")
        if args.visualize_attention:
            f.write("## æ³¨æ„åŠ›æœºåˆ¶å¯è§†åŒ–\n\n")
            f.write("![æ³¨æ„åŠ›çƒ­å›¾](./attention_heatmap.png)\n\n")
            f.write("æ¨¡å‹å¤„ç†è¾“å…¥æ—¶çš„æ³¨æ„åŠ›åˆ†é…æƒé‡ã€‚\n\n")
        f.write("## ç”Ÿæˆè´¨é‡è¶‹åŠ¿\n\n")
        f.write("![è´¨é‡è¶‹åŠ¿](./quality_trend.png)\n\n")
        f.write("æ¨¡å‹ç”Ÿæˆè´¨é‡éšè®­ç»ƒè½®æ¬¡çš„å˜åŒ–è¶‹åŠ¿ã€‚\n\n")
    print(f"âœ“ å¯è§†åŒ–æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
    if logger:
        logger.info(f"Visualization report saved to: {report_path}")


def _try_open_directory(directory):
    """å°è¯•æ‰“å¼€ç›®å½•"""
    try:
        import platform
        system = platform.system()
        if system == 'Windows':
            os.startfile(directory)
        elif system == 'Darwin':  # macOS
            import subprocess
            subprocess.run(['open', directory])
        elif system == 'Linux':
            import subprocess
            subprocess.run(['xdg-open', directory])
        print("å·²å°è¯•æ‰“å¼€è¾“å‡ºç›®å½•ã€‚")
    except Exception as e:
        print(f"æ— æ³•è‡ªåŠ¨æ‰“å¼€è¾“å‡ºç›®å½•: {e}")
        print(f"è¯·æ‰‹åŠ¨æ‰“å¼€ç›®å½•: {directory}")


# ============================================================================
# å·¥å…·ç›¸å…³å‘½ä»¤
# ============================================================================

def run_clean_cache_command(args):
    """
    Run the clean-cache command (clean cache files)
    """
    logger, lang_manager, device = _initialize_common(args)
    logger.info("Starting cache cleanup...")

    try:
        cache_manager = CacheManager(cache_dir=args.cache_dir, logger=logger)
        result = cache_manager.clean_cache(days=args.clean_days)
        logger.info(f"Cache cleanup completed. Cleaned {result.get('cleaned_files', 0)} files, "
                   f"{result.get('cleaned_dirs', 0)} directories")
        if result.get('errors', []):
            logger.warning(f"Encountered {len(result['errors'])} errors during cleanup")
        return 0
    except Exception as e:
        return _handle_command_error("ç¼“å­˜æ¸…ç†", e, logger)


def run_estimate_command(args):
    """
    Run the estimate command (estimate training time)
    """
    logger, lang_manager, device = _initialize_common(args)
    logger.info("Estimating training time...")

    try:
        from apt_model.data.data_processor import get_training_texts
        dataset_size = len(get_training_texts())
        if args.data_path:
            try:
                custom_data = load_external_data(args.data_path, max_samples=args.max_samples)
                dataset_size = len(custom_data)
            except Exception:
                pass
        model_config = APTConfig()
        estimator = TrainingTimeEstimator(
            model_config=model_config,
            dataset_size=dataset_size,
            batch_size=args.batch_size,
            epochs=args.epochs,
            logger=logger
        )
        estimation = estimator.print_estimation()
        return 0
    except Exception as e:
        return _handle_command_error("æ—¶é—´ä¼°ç®—", e, logger)


# ============================================================================
# å ä½ç¬¦å‘½ä»¤ - å¾…å®ç°
# ============================================================================

def run_info_command(args):
    """
    æ˜¾ç¤ºæ¨¡å‹æˆ–æ•°æ®é›†çš„è¯¦ç»†ä¿¡æ¯

    ç”¨æ³•:
        python -m apt_model info --model ./apt_model
        python -m apt_model info --data train.txt
        python -m apt_model info --model ./model --verbose

    å‚æ•°:
        args: å‘½ä»¤è¡Œå‚æ•°

    è¿”å›:
        int: é€€å‡ºç 
    """
    logger, lang_manager, device = _initialize_common(args)

    try:
        import torch
        import json

        model_path = getattr(args, 'model', None)
        data_path = getattr(args, 'data', None)
        verbose = getattr(args, 'verbose', False)

        if not model_path and not data_path:
            print("âŒ é”™è¯¯: è¯·æŒ‡å®š --model æˆ– --data å‚æ•°")
            return 1

        # æ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯
        if model_path:
            print("\n" + "="*70)
            print("ğŸ“¦ æ¨¡å‹ä¿¡æ¯")
            print("="*70)

            if not os.path.exists(model_path):
                print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
                return 1

            # æ£€æŸ¥é…ç½®æ–‡ä»¶
            config_path = os.path.join(model_path, 'config.json')
            if os.path.exists(config_path):
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)

                print(f"\næ¨¡å‹è·¯å¾„: {model_path}")
                print(f"\né…ç½®ä¿¡æ¯:")
                for key, value in config.items():
                    if verbose or key in ['d_model', 'num_encoder_layers', 'num_decoder_layers', 'vocab_size', 'n_heads']:
                        print(f"  {key}: {value}")

            # æ£€æŸ¥æƒé‡æ–‡ä»¶
            weight_files = []
            if os.path.isdir(model_path):
                for ext in ['.pt', '.pth', '.bin', '.safetensors']:
                    weight_files.extend([f for f in os.listdir(model_path) if f.endswith(ext)])
            elif os.path.isfile(model_path):
                # å•ä¸ªæ¨¡å‹æ–‡ä»¶
                for ext in ['.pt', '.pth', '.bin', '.safetensors']:
                    if model_path.endswith(ext):
                        weight_files.append(os.path.basename(model_path))

            if weight_files:
                print(f"\næƒé‡æ–‡ä»¶:")
                total_size = 0
                for wf in weight_files:
                    file_path = os.path.join(model_path, wf)
                    size = os.path.getsize(file_path)
                    total_size += size
                    print(f"  {wf}: {size / 1024 / 1024:.2f} MB")

                print(f"\næ€»å¤§å°: {total_size / 1024 / 1024:.2f} MB ({total_size / 1024 / 1024 / 1024:.2f} GB)")

            # æ£€æŸ¥åˆ†è¯å™¨
            tokenizer_files = ['vocab.json', 'merges.txt', 'tokenizer_config.json']
            found_tokenizer = any(os.path.exists(os.path.join(model_path, f)) for f in tokenizer_files)

            if found_tokenizer:
                print(f"\nâœ“ åŒ…å«åˆ†è¯å™¨æ–‡ä»¶")

        # æ˜¾ç¤ºæ•°æ®ä¿¡æ¯
        if data_path:
            print("\n" + "="*70)
            print("ğŸ“Š æ•°æ®é›†ä¿¡æ¯")
            print("="*70)

            if not os.path.exists(data_path):
                print(f"âŒ æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {data_path}")
                return 1

            print(f"\næ•°æ®è·¯å¾„: {data_path}")

            # è¯»å–æ•°æ®ç»Ÿè®¡
            file_size = os.path.getsize(data_path)
            print(f"æ–‡ä»¶å¤§å°: {file_size / 1024:.2f} KB ({file_size / 1024 / 1024:.2f} MB)")

            # è¯»å–æ ·æœ¬ç»Ÿè®¡
            try:
                with open(data_path, 'r', encoding='utf-8') as f:
                    lines = [line.strip() for line in f if line.strip()]
            except UnicodeDecodeError:
                print("âš ï¸  è­¦å‘Š: æ–‡ä»¶ç¼–ç ä¸æ˜¯UTF-8ï¼Œå°è¯•ä½¿ç”¨å…¶ä»–ç¼–ç ...")
                try:
                    with open(data_path, 'r', encoding='gbk') as f:
                        lines = [line.strip() for line in f if line.strip()]
                except:
                    print("âš ï¸  æ— æ³•è¯»å–æ–‡ä»¶å†…å®¹ï¼ˆå¯èƒ½æ˜¯äºŒè¿›åˆ¶æ–‡ä»¶ï¼‰")
                    lines = []

            print(f"æ ·æœ¬æ•°é‡: {len(lines)}")

            if lines:
                avg_len = sum(len(line) for line in lines) / len(lines)
                max_len = max(len(line) for line in lines)
                min_len = min(len(line) for line in lines)

                print(f"å¹³å‡é•¿åº¦: {avg_len:.1f} å­—ç¬¦")
                print(f"æœ€å¤§é•¿åº¦: {max_len} å­—ç¬¦")
                print(f"æœ€å°é•¿åº¦: {min_len} å­—ç¬¦")

                # æ˜¾ç¤ºç¤ºä¾‹
                if verbose:
                    print(f"\nå‰3ä¸ªæ ·æœ¬:")
                    for i, line in enumerate(lines[:3]):
                        print(f"  [{i+1}] {line[:100]}...")

        print("\n" + "="*70 + "\n")
        return 0

    except Exception as e:
        return _handle_command_error("ä¿¡æ¯æŸ¥çœ‹", e, logger)


def run_list_command(args):
    """
    åˆ—å‡ºå¯ç”¨çš„æ¨¡å‹ã€æ•°æ®é›†å’Œæ£€æŸ¥ç‚¹

    ç”¨æ³•:
        python -m apt_model list
        python -m apt_model list --type models
        python -m apt_model list --type data
        python -m apt_model list --dir ./custom_path

    å‚æ•°:
        args: å‘½ä»¤è¡Œå‚æ•°

    è¿”å›:
        int: é€€å‡ºç 
    """
    logger, lang_manager, device = _initialize_common(args)

    try:
        from datetime import datetime

        resource_type = getattr(args, 'type', 'all')  # models, data, checkpoints, all
        base_dir = getattr(args, 'dir', '.')

        print("\n" + "="*70)
        print("ğŸ“‹ å¯ç”¨èµ„æºåˆ—è¡¨")
        print("="*70)

        # åˆ—å‡ºæ¨¡å‹
        if resource_type in ['models', 'all']:
            print("\nğŸ“¦ æ¨¡å‹:")
            model_dirs = []

            # æœç´¢å¯èƒ½çš„æ¨¡å‹ç›®å½•
            for root, dirs, files in os.walk(base_dir):
                # è·³è¿‡éšè—ç›®å½•å’Œç¼“å­˜
                dirs[:] = [d for d in dirs if not d.startswith('.') and d != '__pycache__']

                # æ£€æŸ¥æ˜¯å¦åŒ…å«æ¨¡å‹æ–‡ä»¶
                has_model = any(f.endswith(('.pt', '.pth', '.bin', '.safetensors')) for f in files)
                has_config = 'config.json' in files

                if has_model or has_config:
                    # è®¡ç®—ç›®å½•å¤§å°
                    dir_size = sum(os.path.getsize(os.path.join(root, f)) for f in files)
                    mtime = os.path.getmtime(root)
                    model_dirs.append((root, dir_size, mtime))

            if model_dirs:
                # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
                model_dirs.sort(key=lambda x: x[2], reverse=True)

                for model_path, size, mtime in model_dirs:
                    rel_path = os.path.relpath(model_path, base_dir)
                    date_str = datetime.fromtimestamp(mtime).strftime('%Y-%m-%d %H:%M')
                    size_mb = size / 1024 / 1024
                    print(f"  â€¢ {rel_path:40s} {size_mb:>8.1f} MB  {date_str}")
            else:
                print("  (æœªæ‰¾åˆ°æ¨¡å‹)")

        # åˆ—å‡ºæ•°æ®é›†
        if resource_type in ['data', 'all']:
            print("\nğŸ“Š æ•°æ®é›†:")
            data_files = []

            for root, dirs, files in os.walk(base_dir):
                dirs[:] = [d for d in dirs if not d.startswith('.') and d != '__pycache__']

                for f in files:
                    if f.endswith(('.txt', '.json', '.jsonl', '.csv')):
                        file_path = os.path.join(root, f)
                        size = os.path.getsize(file_path)
                        mtime = os.path.getmtime(file_path)
                        data_files.append((file_path, size, mtime))

            if data_files:
                # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
                data_files.sort(key=lambda x: x[2], reverse=True)

                for file_path, size, mtime in data_files[:20]:  # åªæ˜¾ç¤ºå‰20ä¸ª
                    rel_path = os.path.relpath(file_path, base_dir)
                    date_str = datetime.fromtimestamp(mtime).strftime('%Y-%m-%d %H:%M')
                    size_kb = size / 1024
                    print(f"  â€¢ {rel_path:40s} {size_kb:>8.1f} KB  {date_str}")

                if len(data_files) > 20:
                    print(f"  ... è¿˜æœ‰ {len(data_files) - 20} ä¸ªæ–‡ä»¶")
            else:
                print("  (æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶)")

        # åˆ—å‡ºæ£€æŸ¥ç‚¹
        if resource_type in ['checkpoints', 'all']:
            print("\nğŸ’¾ æ£€æŸ¥ç‚¹:")
            checkpoint_files = []

            for root, dirs, files in os.walk(base_dir):
                dirs[:] = [d for d in dirs if not d.startswith('.') and d != '__pycache__']

                for f in files:
                    if 'checkpoint' in f.lower() and f.endswith(('.pt', '.pth')):
                        file_path = os.path.join(root, f)
                        size = os.path.getsize(file_path)
                        mtime = os.path.getmtime(file_path)
                        checkpoint_files.append((file_path, size, mtime))

            if checkpoint_files:
                # æŒ‰ä¿®æ”¹æ—¶é—´æ’åº
                checkpoint_files.sort(key=lambda x: x[2], reverse=True)

                for file_path, size, mtime in checkpoint_files[:10]:  # åªæ˜¾ç¤ºå‰10ä¸ª
                    rel_path = os.path.relpath(file_path, base_dir)
                    date_str = datetime.fromtimestamp(mtime).strftime('%Y-%m-%d %H:%M')
                    size_mb = size / 1024 / 1024
                    print(f"  â€¢ {rel_path:40s} {size_mb:>8.1f} MB  {date_str}")

                if len(checkpoint_files) > 10:
                    print(f"  ... è¿˜æœ‰ {len(checkpoint_files) - 10} ä¸ªæ£€æŸ¥ç‚¹")
            else:
                print("  (æœªæ‰¾åˆ°æ£€æŸ¥ç‚¹)")

        print("\n" + "="*70 + "\n")
        return 0

    except Exception as e:
        return _handle_command_error("èµ„æºåˆ—è¡¨", e, logger)


def run_prune_command(args):
    """
    åˆ é™¤æ—§çš„æ¨¡å‹ã€æ£€æŸ¥ç‚¹æˆ–ç¼“å­˜æ–‡ä»¶

    ç”¨æ³•:
        python -m apt_model prune --type checkpoints --keep 3
        python -m apt_model prune --type cache
        python -m apt_model prune --type old --days 30

    å‚æ•°:
        args: å‘½ä»¤è¡Œå‚æ•°

    è¿”å›:
        int: é€€å‡ºç 
    """
    logger, lang_manager, device = _initialize_common(args)

    try:
        import shutil
        from datetime import datetime, timedelta

        prune_type = getattr(args, 'type', 'checkpoints')  # checkpoints, cache, old, all
        keep_count = getattr(args, 'keep', 3)  # ä¿ç•™æœ€è¿‘çš„Nä¸ª
        days_old = getattr(args, 'days', 30)  # åˆ é™¤Nå¤©å‰çš„æ–‡ä»¶
        dry_run = getattr(args, 'dry_run', False)  # ä»…é¢„è§ˆä¸åˆ é™¤
        base_dir = getattr(args, 'dir', '.')

        print("\n" + "="*70)
        print(f"ğŸ—‘ï¸  æ¸…ç†{'ï¼ˆé¢„è§ˆæ¨¡å¼ï¼‰' if dry_run else ''}")
        print("="*70)

        deleted_count = 0
        freed_space = 0

        # æ¸…ç†æ£€æŸ¥ç‚¹
        if prune_type in ['checkpoints', 'all']:
            print(f"\næ¸…ç†æ£€æŸ¥ç‚¹ (ä¿ç•™æœ€è¿‘ {keep_count} ä¸ª):")

            checkpoint_files = []
            for root, dirs, files in os.walk(base_dir):
                for f in files:
                    if 'checkpoint' in f.lower() and f.endswith(('.pt', '.pth')):
                        file_path = os.path.join(root, f)
                        mtime = os.path.getmtime(file_path)
                        size = os.path.getsize(file_path)
                        checkpoint_files.append((file_path, mtime, size))

            # æŒ‰æ—¶é—´æ’åºï¼Œä¿ç•™æœ€æ–°çš„
            checkpoint_files.sort(key=lambda x: x[1], reverse=True)

            if len(checkpoint_files) > keep_count:
                to_delete = checkpoint_files[keep_count:]

                for file_path, mtime, size in to_delete:
                    date_str = datetime.fromtimestamp(mtime).strftime('%Y-%m-%d %H:%M')
                    size_mb = size / 1024 / 1024
                    print(f"  - {file_path} ({size_mb:.1f} MB, {date_str})")

                    if not dry_run:
                        os.remove(file_path)
                        deleted_count += 1
                        freed_space += size

                print(f"  åˆ é™¤äº† {len(to_delete)} ä¸ªæ—§æ£€æŸ¥ç‚¹")
            else:
                print(f"  åªæœ‰ {len(checkpoint_files)} ä¸ªæ£€æŸ¥ç‚¹ï¼Œæ— éœ€æ¸…ç†")

        # æ¸…ç†ç¼“å­˜ - ä½¿ç”¨ CacheManager
        if prune_type in ['cache', 'all']:
            print(f"\næ¸…ç†ç¼“å­˜æ–‡ä»¶:")

            try:
                from apt_model.utils.cache_manager import CacheManager

                # ä½¿ç”¨ CacheManager æ¸…ç†ç¼“å­˜
                cache_manager = CacheManager(cache_dir=base_dir, logger=logger)

                if not dry_run:
                    result = cache_manager.clean_cache(days=days_old)
                    cleaned_files = result.get('cleaned_files', 0)
                    cleaned_dirs = result.get('cleaned_dirs', 0)
                    cache_size = result.get('freed_space', 0)

                    print(f"  æ¸…ç†äº† {cleaned_files} ä¸ªæ–‡ä»¶å’Œ {cleaned_dirs} ä¸ªç›®å½•")
                    print(f"  é‡Šæ”¾ç©ºé—´: {cache_size / 1024 / 1024:.2f} MB")

                    deleted_count += cleaned_files + cleaned_dirs
                    freed_space += cache_size
                else:
                    # é¢„è§ˆæ¨¡å¼ï¼šæ‰«æç¼“å­˜ç›®å½•
                    cache_dirs = ['__pycache__', '.pytest_cache', '.apt_cache', 'apt_cache']

                    for root, dirs, files in os.walk(base_dir):
                        for cache_dir in cache_dirs:
                            if cache_dir in dirs:
                                cache_path = os.path.join(root, cache_dir)
                                # è®¡ç®—å¤§å°
                                dir_size = sum(
                                    os.path.getsize(os.path.join(dirpath, f))
                                    for dirpath, dirnames, filenames in os.walk(cache_path)
                                    for f in filenames
                                )

                                size_mb = dir_size / 1024 / 1024
                                print(f"  - {cache_path} ({size_mb:.1f} MB)")
                                deleted_count += 1
                                freed_space += dir_size

            except Exception as e:
                logger.warning(f"ä½¿ç”¨ CacheManager æ¸…ç†ç¼“å­˜å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•: {e}")

                # å¤‡ç”¨æ–¹æ³•ï¼šæ‰‹åŠ¨æ¸…ç†
                cache_dirs = ['__pycache__', '.pytest_cache', '.apt_cache', 'apt_cache']

                for root, dirs, files in os.walk(base_dir):
                    for cache_dir in cache_dirs:
                        if cache_dir in dirs:
                            cache_path = os.path.join(root, cache_dir)
                            # è®¡ç®—å¤§å°
                            dir_size = sum(
                                os.path.getsize(os.path.join(dirpath, f))
                                for dirpath, dirnames, filenames in os.walk(cache_path)
                                for f in filenames
                            )

                            size_mb = dir_size / 1024 / 1024
                            print(f"  - {cache_path} ({size_mb:.1f} MB)")

                            if not dry_run:
                                shutil.rmtree(cache_path)
                                deleted_count += 1
                                freed_space += dir_size

        # æ¸…ç†æ—§æ–‡ä»¶
        if prune_type in ['old', 'all']:
            print(f"\næ¸…ç† {days_old} å¤©å‰çš„æ–‡ä»¶:")

            cutoff_date = datetime.now() - timedelta(days=days_old)
            old_files = []

            for root, dirs, files in os.walk(base_dir):
                # è·³è¿‡éšè—ç›®å½•
                dirs[:] = [d for d in dirs if not d.startswith('.')]

                for f in files:
                    if f.endswith(('.log', '.tmp', '.temp')):
                        file_path = os.path.join(root, f)
                        mtime = datetime.fromtimestamp(os.path.getmtime(file_path))

                        if mtime < cutoff_date:
                            size = os.path.getsize(file_path)
                            old_files.append((file_path, mtime, size))

            if old_files:
                for file_path, mtime, size in old_files:
                    date_str = mtime.strftime('%Y-%m-%d')
                    size_kb = size / 1024
                    print(f"  - {file_path} ({size_kb:.1f} KB, {date_str})")

                    if not dry_run:
                        os.remove(file_path)
                        deleted_count += 1
                        freed_space += size

                print(f"  åˆ é™¤äº† {len(old_files)} ä¸ªæ—§æ–‡ä»¶")
            else:
                print(f"  æœªæ‰¾åˆ°è¶…è¿‡ {days_old} å¤©çš„æ–‡ä»¶")

        # æ€»ç»“
        print("\n" + "="*70)
        if dry_run:
            print(f"é¢„è§ˆ: å°†åˆ é™¤ {deleted_count} ä¸ªæ–‡ä»¶/ç›®å½•")
            print(f"å°†é‡Šæ”¾ç©ºé—´: {freed_space / 1024 / 1024:.2f} MB")
            print("\næç¤º: æ·»åŠ  --no-dry-run æ‰§è¡Œå®é™…åˆ é™¤")
        else:
            print(f"âœ… å·²åˆ é™¤ {deleted_count} ä¸ªæ–‡ä»¶/ç›®å½•")
            print(f"âœ… é‡Šæ”¾ç©ºé—´: {freed_space / 1024 / 1024:.2f} MB")
        print("="*70 + "\n")

        return 0

    except Exception as e:
        return _handle_command_error("æ¸…ç†", e, logger)


def run_size_command(args):
    """
    è®¡ç®—æ¨¡å‹ã€æ•°æ®é›†æˆ–ç›®å½•çš„å¤§å°

    ç”¨æ³•:
        python -m apt_model size --model ./apt_model
        python -m apt_model size --data train.txt
        python -m apt_model size --dir ./checkpoints

    å‚æ•°:
        args: å‘½ä»¤è¡Œå‚æ•°

    è¿”å›:
        int: é€€å‡ºç 
    """
    logger, lang_manager, device = _initialize_common(args)

    def format_param_count(count):
        """æ ¼å¼åŒ–å‚æ•°æ•°é‡ï¼Œè‡ªåŠ¨é€‰æ‹© M/B å•ä½"""
        if count >= 1e9:
            return f"{count / 1e9:.2f}B"
        elif count >= 1e6:
            return f"{count / 1e6:.2f}M"
        elif count >= 1e3:
            return f"{count / 1e3:.2f}K"
        else:
            return str(count)

    try:
        model_path = getattr(args, 'model', None)
        data_path = getattr(args, 'data', None)
        dir_path = getattr(args, 'dir', None)
        detailed = getattr(args, 'detailed', False)

        if not any([model_path, data_path, dir_path]):
            print("âŒ é”™è¯¯: è¯·æŒ‡å®š --model, --data æˆ– --dir å‚æ•°")
            return 1

        print("\n" + "="*70)
        print("ğŸ“ å¤§å°è®¡ç®—")
        print("="*70)

        # è®¡ç®—æ¨¡å‹å¤§å°
        if model_path:
            if not os.path.exists(model_path):
                print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
                return 1

            print(f"\næ¨¡å‹: {model_path}")

            # å°è¯•åŠ è½½æ¨¡å‹å¹¶è®¡ç®—å‚æ•°é‡
            try:
                import torch
                from apt_model.modeling.apt_model import APTModel
                from apt_model.config.apt_config import APTConfig

                # æ£€æŸ¥æ˜¯å¦æ˜¯APTæ¨¡å‹ç›®å½•
                config_path = os.path.join(model_path, 'config.json') if os.path.isdir(model_path) else None

                if config_path and os.path.exists(config_path):
                    print("\nğŸ“Š æ¨¡å‹å‚æ•°ç»Ÿè®¡:")

                    # åŠ è½½é…ç½®
                    config = APTConfig.from_pretrained(model_path)

                    # åŠ è½½æ¨¡å‹ï¼ˆåªä¸ºäº†è®¡ç®—å‚æ•°ï¼‰
                    model = APTModel.from_pretrained(model_path, config=config)

                    # è®¡ç®—å‚æ•°é‡
                    total_params = sum(p.numel() for p in model.parameters())
                    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
                    frozen_params = total_params - trainable_params

                    print(f"  æ€»å‚æ•°: {total_params:,} ({format_param_count(total_params)})")
                    print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,} ({format_param_count(trainable_params)})")

                    if frozen_params > 0:
                        print(f"  å†»ç»“å‚æ•°: {frozen_params:,} ({format_param_count(frozen_params)})")

                    # ä¼°ç®—å†…å­˜å ç”¨
                    # FP32: 4 bytes per parameter
                    # FP16: 2 bytes per parameter
                    fp32_memory = total_params * 4 / 1024 / 1024  # MB
                    fp16_memory = total_params * 2 / 1024 / 1024  # MB

                    print(f"\n  å†…å­˜å ç”¨ä¼°ç®—:")
                    print(f"    FP32: {fp32_memory:.2f} MB ({fp32_memory / 1024:.2f} GB)")
                    print(f"    FP16: {fp16_memory:.2f} MB ({fp16_memory / 1024:.2f} GB)")

                    # åˆ†å±‚å‚æ•°ç»Ÿè®¡
                    if detailed:
                        print(f"\n  åˆ†å±‚å‚æ•°ç»Ÿè®¡:")
                        layer_params = {}
                        for name, param in model.named_parameters():
                            # æå–å±‚ç±»å‹
                            layer_type = name.split('.')[0] if '.' in name else name
                            if layer_type not in layer_params:
                                layer_params[layer_type] = 0
                            layer_params[layer_type] += param.numel()

                        # æŒ‰å‚æ•°é‡æ’åº
                        sorted_layers = sorted(layer_params.items(), key=lambda x: x[1], reverse=True)
                        for layer_name, param_count in sorted_layers[:10]:
                            print(f"    {layer_name:30s} {param_count:>12,} ({format_param_count(param_count):>8s})")

                    # æ¸…ç†æ¨¡å‹é‡Šæ”¾å†…å­˜
                    del model
                    import gc
                    gc.collect()
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()

            except Exception as e:
                print(f"\nâš ï¸  æ— æ³•åŠ è½½æ¨¡å‹è®¡ç®—å‚æ•°é‡: {e}")

            # è®¡ç®—æ–‡ä»¶å¤§å°
            print("\nğŸ’¾ æ–‡ä»¶å¤§å°ç»Ÿè®¡:")

            if os.path.isfile(model_path):
                # å•ä¸ªæ–‡ä»¶
                size = os.path.getsize(model_path)
                print(f"  æ–‡ä»¶å¤§å°: {size / 1024 / 1024:.2f} MB ({size / 1024 / 1024 / 1024:.2f} GB)")
            else:
                # ç›®å½•
                total_size = 0
                file_count = 0
                file_sizes = []

                for root, dirs, files in os.walk(model_path):
                    for f in files:
                        file_path = os.path.join(root, f)
                        size = os.path.getsize(file_path)
                        total_size += size
                        file_count += 1

                        if detailed:
                            file_sizes.append((f, size))

                print(f"  æ€»å¤§å°: {total_size / 1024 / 1024:.2f} MB ({total_size / 1024 / 1024 / 1024:.2f} GB)")
                print(f"  æ–‡ä»¶æ•°: {file_count}")

                if detailed and file_sizes:
                    print("\n  æ–‡ä»¶æ˜ç»†:")
                    file_sizes.sort(key=lambda x: x[1], reverse=True)
                    for fname, fsize in file_sizes[:10]:
                        print(f"    {fname:40s} {fsize / 1024 / 1024:>8.2f} MB")
                    if len(file_sizes) > 10:
                        print(f"    ... è¿˜æœ‰ {len(file_sizes) - 10} ä¸ªæ–‡ä»¶")

        # è®¡ç®—æ•°æ®å¤§å°
        if data_path:
            if not os.path.exists(data_path):
                print(f"âŒ æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {data_path}")
                return 1

            print(f"\næ•°æ®é›†: {data_path}")

            if os.path.isfile(data_path):
                size = os.path.getsize(data_path)
                print(f"æ–‡ä»¶å¤§å°: {size / 1024:.2f} KB ({size / 1024 / 1024:.2f} MB)")

                # ç»Ÿè®¡è¡Œæ•°
                try:
                    with open(data_path, 'r', encoding='utf-8') as f:
                        line_count = sum(1 for line in f if line.strip())
                except UnicodeDecodeError:
                    try:
                        with open(data_path, 'r', encoding='gbk') as f:
                            line_count = sum(1 for line in f if line.strip())
                    except:
                        print("âš ï¸  æ— æ³•è¯»å–æ–‡ä»¶å†…å®¹ï¼ˆå¯èƒ½æ˜¯äºŒè¿›åˆ¶æ–‡ä»¶ï¼‰")
                        line_count = 0

                if line_count > 0:
                    print(f"æ ·æœ¬æ•°é‡: {line_count}")
                    print(f"å¹³å‡æ¯æ¡: {size / line_count / 1024:.2f} KB")
            else:
                # ç›®å½•ä¸­çš„å¤šä¸ªæ•°æ®æ–‡ä»¶
                total_size = 0
                file_count = 0

                for root, dirs, files in os.walk(data_path):
                    for f in files:
                        if f.endswith(('.txt', '.json', '.jsonl', '.csv')):
                            file_path = os.path.join(root, f)
                            size = os.path.getsize(file_path)
                            total_size += size
                            file_count += 1

                print(f"æ€»å¤§å°: {total_size / 1024 / 1024:.2f} MB")
                print(f"æ–‡ä»¶æ•°: {file_count}")

        # è®¡ç®—ç›®å½•å¤§å°
        if dir_path:
            if not os.path.exists(dir_path):
                print(f"âŒ ç›®å½•ä¸å­˜åœ¨: {dir_path}")
                return 1

            print(f"\nç›®å½•: {dir_path}")

            total_size = 0
            file_count = 0
            dir_count = 0
            type_stats = {}

            for root, dirs, files in os.walk(dir_path):
                dir_count += len(dirs)
                for f in files:
                    file_path = os.path.join(root, f)
                    size = os.path.getsize(file_path)
                    total_size += size
                    file_count += 1

                    # ç»Ÿè®¡æ–‡ä»¶ç±»å‹
                    ext = os.path.splitext(f)[1] or '(æ— æ‰©å±•å)'
                    type_stats[ext] = type_stats.get(ext, 0) + size

            print(f"æ€»å¤§å°: {total_size / 1024 / 1024:.2f} MB ({total_size / 1024 / 1024 / 1024:.2f} GB)")
            print(f"æ–‡ä»¶æ•°: {file_count}")
            print(f"ç›®å½•æ•°: {dir_count}")

            if detailed and type_stats:
                print("\næŒ‰æ–‡ä»¶ç±»å‹:")
                sorted_types = sorted(type_stats.items(), key=lambda x: x[1], reverse=True)
                for ext, size in sorted_types[:10]:
                    print(f"  {ext:20s} {size / 1024 / 1024:>8.2f} MB")

        print("\n" + "="*70 + "\n")
        return 0

    except Exception as e:
        return _handle_command_error("å¤§å°è®¡ç®—", e, logger)


def run_test_command(args):
    """
    æµ‹è¯•æ¨¡å‹çš„ç”Ÿæˆèƒ½åŠ›å’Œæ€§èƒ½

    ç”¨æ³•:
        python -m apt_model test --model ./apt_model
        python -m apt_model test --model ./model --prompt "æµ‹è¯•æ–‡æœ¬"
        python -m apt_model test --model ./model --test-file test_prompts.txt

    å‚æ•°:
        args: å‘½ä»¤è¡Œå‚æ•°

    è¿”å›:
        int: é€€å‡ºç 
    """
    logger, lang_manager, device = _initialize_common(args)

    try:
        import torch
        import time

        model_path = getattr(args, 'model', 'apt_model')

        if not os.path.exists(model_path):
            print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            return 1

        print("\n" + "="*70)
        print("ğŸ§ª æ¨¡å‹æµ‹è¯•")
        print("="*70)
        print(f"\næ¨¡å‹è·¯å¾„: {model_path}")

        # åŠ è½½æ¨¡å‹
        print("\næ­£åœ¨åŠ è½½æ¨¡å‹...")
        from apt_model.modeling.apt_model import APTModel
        from apt_model.config.apt_config import APTConfig
        from apt_model.modeling.chinese_tokenizer_integration import get_appropriate_tokenizer

        # åŠ è½½é…ç½®
        config = APTConfig.from_pretrained(model_path)
        print(f"âœ“ é…ç½®åŠ è½½å®Œæˆ")

        # åŠ è½½æ¨¡å‹
        model = APTModel.from_pretrained(model_path, config=config)
        model = model.to(device)
        model.eval()
        print(f"âœ“ æ¨¡å‹åŠ è½½å®Œæˆ (è®¾å¤‡: {device})")

        # å‡†å¤‡æµ‹è¯•æç¤ºè¯
        test_prompts = []
        prompt_arg = getattr(args, 'prompt', None)
        test_file = getattr(args, 'test_file', None)

        if prompt_arg:
            test_prompts.append(prompt_arg)
        elif test_file and os.path.exists(test_file):
            with open(test_file, 'r', encoding='utf-8') as f:
                test_prompts = [line.strip() for line in f if line.strip()]
        else:
            # é»˜è®¤æµ‹è¯•æç¤ºè¯
            test_prompts = [
                "äººå·¥æ™ºèƒ½æ˜¯",
                "æ·±åº¦å­¦ä¹ çš„åº”ç”¨åŒ…æ‹¬",
                "è‡ªç„¶è¯­è¨€å¤„ç†æŠ€æœ¯"
            ]

        # åŠ è½½åˆ†è¯å™¨
        tokenizer, _ = get_appropriate_tokenizer(texts=test_prompts)

        print(f"\næµ‹è¯•æç¤ºè¯æ•°é‡: {len(test_prompts)}")
        print("="*70)

        # æ‰§è¡Œæµ‹è¯•
        total_time = 0
        total_tokens = 0
        max_length = getattr(args, 'max_length', 50)

        for i, prompt in enumerate(test_prompts, 1):
            print(f"\n[æµ‹è¯• {i}/{len(test_prompts)}]")
            print(f"è¾“å…¥: {prompt}")

            # ç¼–ç 
            input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)

            # ç”Ÿæˆ
            start_time = time.time()
            with torch.no_grad():
                if hasattr(model, 'generate'):
                    output_ids = model.generate(
                        input_ids=input_ids,
                        max_length=max_length,
                        temperature=getattr(args, 'temperature', 0.7),
                        top_p=getattr(args, 'top_p', 0.9),
                        do_sample=True
                    )
                else:
                    # å¦‚æœæ¨¡å‹æ²¡æœ‰ generate æ–¹æ³•ï¼Œä½¿ç”¨å‰å‘ä¼ æ’­
                    outputs = model(input_ids, input_ids)
                    output_ids = input_ids  # ç®€åŒ–å¤„ç†

            end_time = time.time()
            elapsed = end_time - start_time

            # è§£ç 
            generated_text = tokenizer.decode(output_ids[0], skip_special_tokens=True)

            print(f"è¾“å‡º: {generated_text}")
            print(f"ç”Ÿæˆæ—¶é—´: {elapsed:.3f}ç§’")
            print(f"è¾“å‡ºé•¿åº¦: {len(output_ids[0])} tokens")

            if elapsed > 0:
                tokens_per_sec = len(output_ids[0]) / elapsed
                print(f"ç”Ÿæˆé€Ÿåº¦: {tokens_per_sec:.1f} tokens/ç§’")

            total_time += elapsed
            total_tokens += len(output_ids[0])

        # æ€»ç»“
        print("\n" + "="*70)
        print("ğŸ“Š æµ‹è¯•æ€»ç»“")
        print("="*70)
        print(f"æµ‹è¯•æ ·æœ¬: {len(test_prompts)}")
        print(f"æ€»è€—æ—¶: {total_time:.3f}ç§’")
        print(f"æ€»ç”Ÿæˆ: {total_tokens} tokens")

        if total_time > 0:
            avg_time = total_time / len(test_prompts)
            avg_speed = total_tokens / total_time
            print(f"å¹³å‡æ—¶é—´: {avg_time:.3f}ç§’/æ ·æœ¬")
            print(f"å¹³å‡é€Ÿåº¦: {avg_speed:.1f} tokens/ç§’")

        print("="*70 + "\n")
        print("âœ… æµ‹è¯•å®Œæˆï¼")

        return 0

    except Exception as e:
        return _handle_command_error("æ¨¡å‹æµ‹è¯•", e, logger)


def run_compare_command(args):
    """
    æ¯”è¾ƒå¤šä¸ªæ¨¡å‹çš„æ€§èƒ½

    ç”¨æ³•:
        python -m apt_model compare --models model1:path1 model2:path2 --prompts "test prompt"
        python -m apt_model compare --models base:./apt_model fine:./apt_model_finetuned

    å‚æ•°:
        args: å‘½ä»¤è¡Œå‚æ•°

    è¿”å›:
        int: é€€å‡ºç 
    """
    logger, lang_manager, device = _initialize_common(args)

    try:
        from apt_model.evaluation.comparison import ModelComparison

        # åˆ›å»ºæ¯”è¾ƒå™¨
        output_dir = getattr(args, 'output_dir', './comparison_results')
        comparator = ModelComparison(logger=logger, output_dir=output_dir)

        # æ·»åŠ æ¨¡å‹ï¼ˆæ ¼å¼ï¼šname:pathï¼‰
        models = getattr(args, 'models', [])
        if not models:
            print("âŒ é”™è¯¯: è¯·ä½¿ç”¨ --models å‚æ•°æŒ‡å®šè¦æ¯”è¾ƒçš„æ¨¡å‹")
            print("   ç¤ºä¾‹: --models base:./model1 fine:./model2")
            return 1

        for model_spec in models:
            if ':' not in model_spec:
                print(f"âŒ é”™è¯¯: æ¨¡å‹è§„æ ¼æ ¼å¼é”™è¯¯: {model_spec}")
                print("   åº”ä¸º: name:path")
                return 1

            name, path = model_spec.split(':', 1)
            if not comparator.add_model(name, path):
                print(f"âŒ æ— æ³•æ·»åŠ æ¨¡å‹: {name}")
                return 1

        # æ‰§è¡Œæ¯”è¾ƒ
        prompts = getattr(args, 'prompts', None)
        num_samples = getattr(args, 'num_samples', 10)

        print(f"\nğŸ” å¼€å§‹æ¯”è¾ƒ {len(models)} ä¸ªæ¨¡å‹...")
        results = comparator.compare(
            prompts=prompts.split(',') if prompts else None,
            num_samples=num_samples
        )

        # æ˜¾ç¤ºç»“æœ
        print("\n" + "="*70)
        print("ğŸ“Š æ¯”è¾ƒç»“æœ")
        print("="*70)

        if 'summary' in results:
            for model_name, metrics in results['summary'].items():
                print(f"\næ¨¡å‹: {model_name}")
                for metric, value in metrics.items():
                    print(f"  {metric}: {value:.4f}" if isinstance(value, float) else f"  {metric}: {value}")

        print(f"\nâœ… è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: {output_dir}")
        return 0

    except Exception as e:
        return _handle_command_error("æ¨¡å‹æ¯”è¾ƒ", e, logger)


def run_train_hf_command(args):
    """
    è®­ç»ƒ HuggingFace å…¼å®¹çš„æ¨¡å‹

    ç”¨æ³•:
        python -m apt_model train-hf --model gpt2 --data train.txt
        python -m apt_model train-hf --model bert-base-chinese --data corpus.txt --task mlm
        python -m apt_model train-hf --model t5-small --data seq2seq_data.json --task seq2seq

    å‚æ•°:
        args: å‘½ä»¤è¡Œå‚æ•°

    è¿”å›:
        int: é€€å‡ºç 
    """
    logger, lang_manager, device = _initialize_common(args)
    resource_monitor = _setup_resource_monitor(args, logger)
    _start_monitor(resource_monitor)

    try:
        from apt_model.data.huggingface_loader import load_hf_model_and_tokenizer, load_hf_dataset
        from transformers import TrainingArguments, Trainer
        import torch

        # è·å–æ¨¡å‹åç§°
        model_name = getattr(args, 'model', 'gpt2')
        task_type = getattr(args, 'task', 'clm')  # clm, mlm, seq2seq

        print("\n" + "="*70)
        print("ğŸ¤— HuggingFace æ¨¡å‹è®­ç»ƒ")
        print("="*70)
        print(f"\næ¨¡å‹: {model_name}")
        print(f"ä»»åŠ¡: {task_type}")

        # åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
        print("\næ­£åœ¨åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨...")
        model, tokenizer = load_hf_model_and_tokenizer(
            model_name=model_name,
            task=task_type,
            device=device
        )
        print("âœ“ æ¨¡å‹å’Œåˆ†è¯å™¨åŠ è½½å®Œæˆ")

        # åŠ è½½æ•°æ®
        data_path = getattr(args, 'data_path', None)
        if not data_path:
            print("âŒ é”™è¯¯: è¯·æŒ‡å®šè®­ç»ƒæ•°æ®è·¯å¾„ --data-path")
            return 1

        print(f"\næ­£åœ¨åŠ è½½è®­ç»ƒæ•°æ®: {data_path}")
        train_dataset, eval_dataset = load_hf_dataset(
            data_path=data_path,
            tokenizer=tokenizer,
            task=task_type,
            max_length=getattr(args, 'max_length', 512),
            test_size=getattr(args, 'test_split', 0.1)
        )
        print(f"âœ“ è®­ç»ƒæ ·æœ¬: {len(train_dataset)}")
        if eval_dataset:
            print(f"âœ“ éªŒè¯æ ·æœ¬: {len(eval_dataset)}")

        # é…ç½®è®­ç»ƒå‚æ•°
        output_dir = getattr(args, 'save_path', './hf_model_output')
        training_args = TrainingArguments(
            output_dir=output_dir,
            num_train_epochs=getattr(args, 'epochs', 3),
            per_device_train_batch_size=getattr(args, 'batch_size', 8),
            per_device_eval_batch_size=getattr(args, 'eval_batch_size', 8),
            learning_rate=getattr(args, 'learning_rate', 5e-5),
            weight_decay=getattr(args, 'weight_decay', 0.01),
            warmup_steps=getattr(args, 'warmup_steps', 500),
            logging_steps=getattr(args, 'logging_steps', 100),
            save_steps=getattr(args, 'save_steps', 1000),
            eval_steps=getattr(args, 'eval_steps', 500),
            evaluation_strategy="steps" if eval_dataset else "no",
            save_total_limit=getattr(args, 'save_total_limit', 3),
            load_best_model_at_end=True if eval_dataset else False,
            push_to_hub=False,
            fp16=torch.cuda.is_available(),
        )

        print(f"\nè®­ç»ƒé…ç½®:")
        print(f"  Epochs: {training_args.num_train_epochs}")
        print(f"  Batch Size: {training_args.per_device_train_batch_size}")
        print(f"  Learning Rate: {training_args.learning_rate}")
        print(f"  è¾“å‡ºç›®å½•: {output_dir}")

        # åˆ›å»º Trainer
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=train_dataset,
            eval_dataset=eval_dataset,
            tokenizer=tokenizer,
        )

        print("\n" + "="*70)
        print("å¼€å§‹è®­ç»ƒ...")
        print("="*70 + "\n")

        # å¼€å§‹è®­ç»ƒ
        trainer.train()

        # ä¿å­˜æ¨¡å‹
        print(f"\nä¿å­˜æ¨¡å‹åˆ°: {output_dir}")
        trainer.save_model(output_dir)
        tokenizer.save_pretrained(output_dir)

        print("\n" + "="*70)
        print("âœ… HuggingFace æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        print("="*70)
        print(f"\næ¨¡å‹å·²ä¿å­˜åˆ°: {output_dir}")
        print("\nä½¿ç”¨ä»¥ä¸‹å‘½ä»¤åŠ è½½æ¨¡å‹:")
        print(f"  from transformers import AutoModel, AutoTokenizer")
        print(f"  model = AutoModel.from_pretrained('{output_dir}')")
        print(f"  tokenizer = AutoTokenizer.from_pretrained('{output_dir}')")

        return 0

    except Exception as e:
        return _handle_command_error("HuggingFaceè®­ç»ƒ", e, logger)
    finally:
        _stop_monitor(resource_monitor)


def run_distill_command(args):
    """
    æ‰§è¡ŒçŸ¥è¯†è’¸é¦è®­ç»ƒ

    ç”¨æ³•:
        python -m apt_model distill --teacher-model gpt2 --student-model ./student --data train.txt
        python -m apt_model distill --teacher-api openai --student-model ./student --temperature 4.0

    å‚æ•°:
        args: å‘½ä»¤è¡Œå‚æ•°

    è¿”å›:
        int: é€€å‡ºç 
    """
    logger, lang_manager, device = _initialize_common(args)
    resource_monitor = _setup_resource_monitor(args, logger)
    _start_monitor(resource_monitor)

    try:
        from apt_model.plugins.visual_distillation_plugin import VisualDistillationPlugin
        from apt_model.plugins.teacher_api import TeacherAPIPlugin
        from apt_model.training.trainer import train_model
        from apt_model.data.external_data import load_external_data

        # é…ç½®è’¸é¦å‚æ•°
        distill_config = {
            'temperature': getattr(args, 'temperature', 4.0),
            'alpha': getattr(args, 'alpha', 0.7),  # KD lossæƒé‡
            'beta': getattr(args, 'beta', 0.3),     # CE lossæƒé‡
            'show_samples': True,
            'sample_frequency': 50
        }

        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨APIä½œä¸ºæ•™å¸ˆ
        teacher_api = getattr(args, 'teacher_api', None)
        if teacher_api:
            print(f"ğŸ“¡ ä½¿ç”¨ {teacher_api} API ä½œä¸ºæ•™å¸ˆæ¨¡å‹")
            teacher_plugin = TeacherAPIPlugin({
                'provider': teacher_api,
                'model': getattr(args, 'teacher_model_name', 'gpt-4'),
                'temperature': distill_config['temperature']
            })
        else:
            print("ğŸ“š ä½¿ç”¨æœ¬åœ°æ¨¡å‹ä½œä¸ºæ•™å¸ˆ")

        # åˆ›å»ºè’¸é¦æ’ä»¶
        distill_plugin = VisualDistillationPlugin(distill_config)

        # åŠ è½½å­¦ç”Ÿæ¨¡å‹
        student_path = getattr(args, 'student_model', None)
        if not student_path:
            print("âŒ é”™è¯¯: è¯·æŒ‡å®šå­¦ç”Ÿæ¨¡å‹è·¯å¾„ --student-model")
            return 1

        # åŠ è½½è®­ç»ƒæ•°æ®
        data_path = getattr(args, 'data_path', 'train.txt')
        train_texts = load_external_data(data_path)

        print(f"\nğŸ“ å¼€å§‹çŸ¥è¯†è’¸é¦è®­ç»ƒ...")
        print(f"   æ¸©åº¦: {distill_config['temperature']}")
        print(f"   Alpha (KD): {distill_config['alpha']}")
        print(f"   Beta (CE): {distill_config['beta']}")
        print(f"   è®­ç»ƒæ ·æœ¬: {len(train_texts)} æ¡\n")

        # TODO: é›†æˆè’¸é¦åˆ°å®é™…è®­ç»ƒæµç¨‹
        # è¿™é‡Œéœ€è¦ä¿®æ”¹ trainer.py æ¥æ”¯æŒè’¸é¦æŸå¤±

        print("âœ… çŸ¥è¯†è’¸é¦è®­ç»ƒå®Œæˆï¼")
        return 0

    except Exception as e:
        return _handle_command_error("çŸ¥è¯†è’¸é¦", e, logger)
    finally:
        _stop_monitor(resource_monitor)


def run_train_reasoning_command(args):
    """
    æ‰§è¡Œæ¨ç†æ¨¡å‹è®­ç»ƒå‘½ä»¤

    å‚æ•°:
        args: å‘½ä»¤è¡Œå‚æ•°

    è¿”å›:
        int: é€€å‡ºç 
    """
    logger, lang_manager, device = _initialize_common(args)
    logger.info("å¼€å§‹è®­ç»ƒæ¨ç†å¢å¼ºæ¨¡å‹...")

    # è®¾ç½®èµ„æºç›‘æ§
    resource_monitor = _setup_resource_monitor(args, logger)
    _start_monitor(resource_monitor)

    try:
        # Import reasoning training
        from apt_model.training.train_reasoning import train_reasoning_model, load_reasoning_dataset
        from apt_model.modeling.gpt4o_model import VeinSubspaceShared

        # Load base model (placeholder - should load actual model)
        logger.info("åŠ è½½åŸºç¡€æ¨¡å‹...")
        # TODO: Load actual pre-trained base model
        from transformers import AutoModelForCausalLM, AutoTokenizer

        model_name = getattr(args, 'base_model', 'gpt2')
        base_model = AutoModelForCausalLM.from_pretrained(model_name)
        tokenizer = AutoTokenizer.from_pretrained(model_name)

        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        # Get model dimension
        d_model = base_model.config.hidden_size

        # Create vein projector
        rank = getattr(args, 'vein_rank', 4)
        vein = VeinSubspaceShared(d_model=d_model, rank=rank)

        logger.info(f"ä½¿ç”¨ Vein å­ç©ºé—´: d_model={d_model}, rank={rank}")

        # Train reasoning model
        reasoning_controller, training_info = train_reasoning_model(
            base_model=base_model,
            vein_projector=vein,
            tokenizer=tokenizer,
            data_path=getattr(args, 'data_path', None),
            epochs=args.epochs,
            batch_size=args.batch_size,
            learning_rate=args.learning_rate,
            max_reasoning_steps=getattr(args, 'max_reasoning_steps', 6),
            use_budgeted=getattr(args, 'use_budgeted', True),
            global_budget=getattr(args, 'global_budget', 0.15),
            save_path=args.save_path,
            logger=logger,
            resource_monitor=resource_monitor,
        )

        logger.info("æ¨ç†æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        logger.info(f"è®­ç»ƒä¿¡æ¯: {training_info}")

        return 0  # æˆåŠŸ
    except Exception as e:
        return _handle_command_error("æ¨ç†è®­ç»ƒ", e, logger)
    finally:
        _stop_monitor(resource_monitor)


def run_process_data_command(args):
    """
    å¤„ç†å’Œæ¸…æ´—æ•°æ®é›†

    ç”¨æ³•:
        python -m apt_model process-data --input raw_data.txt --output clean_data.txt
        python -m apt_model process-data --input data.json --output processed.json --language zh --clean

    å‚æ•°:
        args: å‘½ä»¤è¡Œå‚æ•°

    è¿”å›:
        int: é€€å‡ºç 
    """
    logger, lang_manager, device = _initialize_common(args)

    try:
        from apt_model.data.data_processor import DataProcessor

        # è·å–è¾“å…¥è¾“å‡ºè·¯å¾„
        input_path = getattr(args, 'input', None)
        output_path = getattr(args, 'output', None)

        if not input_path:
            print("âŒ é”™è¯¯: è¯·æŒ‡å®šè¾“å…¥æ–‡ä»¶ --input")
            return 1

        if not output_path:
            output_path = input_path.replace('.txt', '_processed.txt').replace('.json', '_processed.json')
            print(f"â„¹ï¸  æœªæŒ‡å®šè¾“å‡ºè·¯å¾„ï¼Œä½¿ç”¨: {output_path}")

        # åˆ›å»ºæ•°æ®å¤„ç†å™¨
        language = getattr(args, 'language', 'en')
        processor = DataProcessor(
            max_seq_length=getattr(args, 'max_length', 512),
            lower_case=getattr(args, 'lowercase', False),
            remove_accents=getattr(args, 'remove_accents', False),
            clean_text=getattr(args, 'clean', True),
            language=language
        )

        print(f"\nğŸ“Š å¼€å§‹å¤„ç†æ•°æ®...")
        print(f"   è¾“å…¥: {input_path}")
        print(f"   è¾“å‡º: {output_path}")
        print(f"   è¯­è¨€: {language}")

        # è¯»å–è¾“å…¥æ•°æ®
        try:
            with open(input_path, 'r', encoding='utf-8') as f:
                raw_texts = [line.strip() for line in f if line.strip()]
        except UnicodeDecodeError:
            print("âš ï¸  è­¦å‘Š: æ–‡ä»¶ç¼–ç ä¸æ˜¯UTF-8ï¼Œå°è¯•ä½¿ç”¨GBKç¼–ç ...")
            try:
                with open(input_path, 'r', encoding='gbk') as f:
                    raw_texts = [line.strip() for line in f if line.strip()]
            except Exception as e:
                print(f"âŒ é”™è¯¯: æ— æ³•è¯»å–æ–‡ä»¶ - {e}")
                return 1

        if len(raw_texts) == 0:
            print("âŒ é”™è¯¯: è¾“å…¥æ–‡ä»¶ä¸ºç©ºæˆ–æ— æœ‰æ•ˆæ•°æ®")
            return 1

        print(f"   åŸå§‹æ ·æœ¬æ•°: {len(raw_texts)}")

        # å¤„ç†æ•°æ®
        processed_texts = []
        for text in raw_texts:
            processed = processor.process_text(text)
            if processed:  # åªä¿ç•™éç©ºæ–‡æœ¬
                processed_texts.append(processed)

        print(f"   å¤„ç†åæ ·æœ¬æ•°: {len(processed_texts)}")

        # ä¿å­˜ç»“æœ
        with open(output_path, 'w', encoding='utf-8') as f:
            for text in processed_texts:
                f.write(text + '\n')

        print(f"\nâœ… æ•°æ®å¤„ç†å®Œæˆï¼")
        # æ³¨æ„ï¼šlen(raw_texts) > 0 å·²åœ¨å‰é¢æ£€æŸ¥ï¼Œè¿™é‡Œæ˜¯å®‰å…¨çš„
        clean_rate = (1 - len(processed_texts)/len(raw_texts))*100 if len(raw_texts) > 0 else 0
        print(f"   æ¸…æ´—ç‡: {clean_rate:.1f}%")
        print(f"   ä¿å­˜åˆ°: {output_path}")

        return 0

    except Exception as e:
        return _handle_command_error("æ•°æ®å¤„ç†", e, logger)


def run_backup_command(args):
    """
    å¤‡ä»½æ¨¡å‹ã€æ£€æŸ¥ç‚¹æˆ–æ•°æ®

    ç”¨æ³•:
        python -m apt_model backup --model ./apt_model --output ./backups
        python -m apt_model backup --dir ./checkpoints --output ./backups/checkpoints.tar.gz
        python -m apt_model backup --model ./model --compress

    å‚æ•°:
        args: å‘½ä»¤è¡Œå‚æ•°

    è¿”å›:
        int: é€€å‡ºç 
    """
    logger, lang_manager, device = _initialize_common(args)

    try:
        import shutil
        import tarfile
        from datetime import datetime

        source_model = getattr(args, 'model', None)
        source_dir = getattr(args, 'dir', None)
        output_path = getattr(args, 'output', './backups')
        compress = getattr(args, 'compress', True)  # é»˜è®¤å‹ç¼©
        exclude_checkpoints = getattr(args, 'exclude_checkpoints', False)

        if not source_model and not source_dir:
            print("âŒ é”™è¯¯: è¯·æŒ‡å®š --model æˆ– --dir å‚æ•°")
            return 1

        source = source_model or source_dir
        if not os.path.exists(source):
            print(f"âŒ æºè·¯å¾„ä¸å­˜åœ¨: {source}")
            return 1

        print("\n" + "="*70)
        print("ğŸ’¾ å¤‡ä»½æ“ä½œ")
        print("="*70)

        # åˆ›å»ºå¤‡ä»½ç›®å½•
        os.makedirs(output_path, exist_ok=True)

        # ç”Ÿæˆå¤‡ä»½æ–‡ä»¶å
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        source_name = os.path.basename(source.rstrip('/'))
        backup_name = f"{source_name}_backup_{timestamp}"

        if compress:
            backup_file = os.path.join(output_path, f"{backup_name}.tar.gz")
        else:
            backup_file = os.path.join(output_path, backup_name)

        print(f"\næºè·¯å¾„: {source}")
        print(f"å¤‡ä»½åˆ°: {backup_file}")

        # æ‰§è¡Œå¤‡ä»½
        if compress:
            print(f"\næ­£åœ¨åˆ›å»ºå‹ç¼©å¤‡ä»½...")

            with tarfile.open(backup_file, 'w:gz') as tar:
                # æ·»åŠ è¿‡æ»¤å™¨æ’é™¤æŸäº›æ–‡ä»¶
                def filter_func(tarinfo):
                    # æ’é™¤ç¼“å­˜å’Œä¸´æ—¶æ–‡ä»¶
                    if '__pycache__' in tarinfo.name or tarinfo.name.endswith('.pyc'):
                        return None
                    # å¯é€‰ï¼šæ’é™¤æ£€æŸ¥ç‚¹æ–‡ä»¶
                    if exclude_checkpoints and 'checkpoint' in tarinfo.name.lower():
                        return None
                    return tarinfo

                # æ·»åŠ åˆ°å½’æ¡£
                if os.path.isfile(source):
                    tar.add(source, arcname=os.path.basename(source), filter=filter_func)
                else:
                    tar.add(source, arcname=source_name, filter=filter_func)

            backup_size = os.path.getsize(backup_file)
            print(f"âœ“ å‹ç¼©å¤‡ä»½å®Œæˆ")
            print(f"  å¤‡ä»½æ–‡ä»¶: {backup_file}")
            print(f"  æ–‡ä»¶å¤§å°: {backup_size / 1024 / 1024:.2f} MB")

        else:
            print(f"\næ­£åœ¨åˆ›å»ºå¤‡ä»½...")

            if os.path.isfile(source):
                # å¤åˆ¶å•ä¸ªæ–‡ä»¶
                shutil.copy2(source, backup_file)
            else:
                # å¤åˆ¶æ•´ä¸ªç›®å½•
                def ignore_func(directory, files):
                    ignored = []
                    for f in files:
                        if f == '__pycache__' or f.endswith('.pyc'):
                            ignored.append(f)
                        if exclude_checkpoints and 'checkpoint' in f.lower():
                            ignored.append(f)
                    return ignored

                # å¦‚æœå¤‡ä»½ç›®å½•å·²å­˜åœ¨ï¼Œå…ˆåˆ é™¤
                if os.path.exists(backup_file):
                    print(f"âš ï¸  å¤‡ä»½ç›®æ ‡å·²å­˜åœ¨ï¼Œå°†è¢«è¦†ç›–: {backup_file}")
                    shutil.rmtree(backup_file)

                shutil.copytree(source, backup_file, ignore=ignore_func)

            # è®¡ç®—æ€»å¤§å°
            if os.path.isfile(backup_file):
                backup_size = os.path.getsize(backup_file)
            else:
                backup_size = sum(
                    os.path.getsize(os.path.join(dirpath, f))
                    for dirpath, dirnames, filenames in os.walk(backup_file)
                    for f in filenames
                )

            print(f"âœ“ å¤‡ä»½å®Œæˆ")
            print(f"  å¤‡ä»½è·¯å¾„: {backup_file}")
            print(f"  æ€»å¤§å°: {backup_size / 1024 / 1024:.2f} MB")

        # ç”Ÿæˆå¤‡ä»½å…ƒæ•°æ®
        metadata_file = os.path.join(output_path, f"{backup_name}_metadata.txt")
        with open(metadata_file, 'w', encoding='utf-8') as f:
            f.write(f"å¤‡ä»½æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"æºè·¯å¾„: {source}\n")
            f.write(f"å¤‡ä»½æ–‡ä»¶: {backup_file}\n")
            f.write(f"å‹ç¼©: {'æ˜¯' if compress else 'å¦'}\n")
            f.write(f"å¤§å°: {backup_size / 1024 / 1024:.2f} MB\n")

        print(f"\nå…ƒæ•°æ®å·²ä¿å­˜: {metadata_file}")

        print("\n" + "="*70)
        print("âœ… å¤‡ä»½å®Œæˆï¼")
        print("="*70)
        print(f"\næ¢å¤å‘½ä»¤:")
        if compress:
            print(f"  tar -xzf {backup_file} -C /path/to/restore/")
        else:
            print(f"  cp -r {backup_file} /path/to/restore/")
        print()

        return 0

    except Exception as e:
        return _handle_command_error("å¤‡ä»½", e, logger)


def run_upload_command(args):
    """
    ä¸Šä¼ æ¨¡å‹åˆ° HuggingFace Hub æˆ–å…¶ä»–å¹³å°

    ç”¨æ³•:
        python -m apt_model upload --model ./apt_model --repo username/model-name
        python -m apt_model upload --model ./model --repo user/repo --platform huggingface
        python -m apt_model upload --model ./model --repo user/repo --private

    å‚æ•°:
        args: å‘½ä»¤è¡Œå‚æ•°

    è¿”å›:
        int: é€€å‡ºç 
    """
    logger, lang_manager, device = _initialize_common(args)

    try:
        model_path = getattr(args, 'model', None)
        repo_name = getattr(args, 'repo', None)
        platform = getattr(args, 'platform', 'huggingface')  # huggingface, modelscope
        private = getattr(args, 'private', False)
        commit_message = getattr(args, 'message', 'Upload model via APT CLI')

        if not model_path:
            print("âŒ é”™è¯¯: è¯·æŒ‡å®šæ¨¡å‹è·¯å¾„ --model")
            return 1

        if not repo_name:
            print("âŒ é”™è¯¯: è¯·æŒ‡å®šä»“åº“åç§° --repo (æ ¼å¼: username/repo-name)")
            return 1

        if not os.path.exists(model_path):
            print(f"âŒ æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            return 1

        print("\n" + "="*70)
        print("ğŸ“¤ æ¨¡å‹ä¸Šä¼ ")
        print("="*70)
        print(f"\næ¨¡å‹è·¯å¾„: {model_path}")
        print(f"ç›®æ ‡ä»“åº“: {repo_name}")
        print(f"å¹³å°: {platform}")
        print(f"å¯è§æ€§: {'ç§æœ‰' if private else 'å…¬å¼€'}")

        if platform == 'huggingface':
            print("\næ­£åœ¨ä¸Šä¼ åˆ° HuggingFace Hub...")

            try:
                from huggingface_hub import HfApi, create_repo, upload_folder
            except ImportError:
                print("âŒ é”™è¯¯: éœ€è¦å®‰è£… huggingface_hub")
                print("   è¿è¡Œ: pip install huggingface_hub")
                return 1

            # æ£€æŸ¥è®¤è¯
            try:
                api = HfApi()
                user_info = api.whoami()
                user_name = user_info.get('name') or user_info.get('username', 'Unknown')
                print(f"âœ“ å·²ç™»å½•ç”¨æˆ·: {user_name}")
            except Exception as e:
                print("âŒ é”™è¯¯: æœªç™»å½• HuggingFace")
                print("   è¯·å…ˆè¿è¡Œ: huggingface-cli login")
                return 1

            # åˆ›å»ºä»“åº“ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
            print(f"\næ£€æŸ¥ä»“åº“...")
            try:
                create_repo(
                    repo_id=repo_name,
                    private=private,
                    exist_ok=True
                )
                print(f"âœ“ ä»“åº“å‡†å¤‡å°±ç»ª: https://huggingface.co/{repo_name}")
            except Exception as e:
                print(f"âš ï¸  ä»“åº“åˆ›å»ºè­¦å‘Š: {e}")

            # ä¸Šä¼ æ¨¡å‹
            print(f"\næ­£åœ¨ä¸Šä¼ æ–‡ä»¶...")
            try:
                if os.path.isfile(model_path):
                    # ä¸Šä¼ å•ä¸ªæ–‡ä»¶
                    from huggingface_hub import upload_file
                    upload_file(
                        path_or_fileobj=model_path,
                        path_in_repo=os.path.basename(model_path),
                        repo_id=repo_name,
                        commit_message=commit_message
                    )
                else:
                    # ä¸Šä¼ æ•´ä¸ªç›®å½•
                    upload_folder(
                        folder_path=model_path,
                        repo_id=repo_name,
                        commit_message=commit_message,
                        ignore_patterns=["*.pyc", "__pycache__", ".git"]
                    )

                print(f"âœ… ä¸Šä¼ å®Œæˆï¼")
                print(f"\næ¨¡å‹é“¾æ¥: https://huggingface.co/{repo_name}")
                print(f"\nä½¿ç”¨ä»¥ä¸‹ä»£ç åŠ è½½æ¨¡å‹:")
                print(f"  from transformers import AutoModel, AutoTokenizer")
                print(f"  model = AutoModel.from_pretrained('{repo_name}')")
                print(f"  tokenizer = AutoTokenizer.from_pretrained('{repo_name}')")

            except Exception as e:
                print(f"âŒ ä¸Šä¼ å¤±è´¥: {e}")
                return 1

        elif platform == 'modelscope':
            print("\næ­£åœ¨ä¸Šä¼ åˆ° ModelScope...")

            try:
                from modelscope.hub.api import HubApi
            except ImportError:
                print("âŒ é”™è¯¯: éœ€è¦å®‰è£… modelscope")
                print("   è¿è¡Œ: pip install modelscope")
                return 1

            # ModelScopeä¸Šä¼ é€»è¾‘
            print("âš ï¸  ModelScope ä¸Šä¼ åŠŸèƒ½å¼€å‘ä¸­...")
            print("   è¯·æ‰‹åŠ¨è®¿é—® https://modelscope.cn ä¸Šä¼ æ¨¡å‹")
            return 1

        else:
            print(f"âŒ ä¸æ”¯æŒçš„å¹³å°: {platform}")
            print("   æ”¯æŒçš„å¹³å°: huggingface, modelscope")
            return 1

        print("\n" + "="*70 + "\n")
        return 0

    except Exception as e:
        return _handle_command_error("ä¸Šä¼ ", e, logger)


def run_export_ollama_command(args):
    """
    å¯¼å‡ºæ¨¡å‹ä¸º Ollama æ ¼å¼

    ç”¨æ³•:
        python -m apt_model export-ollama --model ./apt_model --output ./ollama_model
        python -m apt_model export-ollama --model ./model --output ./ollama --quantization Q4_K_M

    å‚æ•°:
        args: å‘½ä»¤è¡Œå‚æ•°

    è¿”å›:
        int: é€€å‡ºç 
    """
    logger, lang_manager, device = _initialize_common(args)

    try:
        from apt_model.plugins.ollama_export_plugin import OllamaExportPlugin

        # è·å–æ¨¡å‹è·¯å¾„
        model_path = getattr(args, 'model', None)
        if not model_path:
            print("âŒ é”™è¯¯: è¯·æŒ‡å®šæ¨¡å‹è·¯å¾„ --model")
            return 1

        # è·å–è¾“å‡ºè·¯å¾„
        output_path = getattr(args, 'output', './ollama_export')

        # é…ç½®å¯¼å‡ºå‚æ•°
        export_config = {
            'quantization': getattr(args, 'quantization', 'Q4_K_M'),  # Q4_0, Q4_K_M, Q5_K_M, Q8_0
            'context_length': getattr(args, 'context_length', 2048),
            'temperature': getattr(args, 'temperature', 0.7)
        }

        print(f"\nğŸ“¦ å¼€å§‹å¯¼å‡ºä¸º Ollama æ ¼å¼...")
        print(f"   æ¨¡å‹è·¯å¾„: {model_path}")
        print(f"   è¾“å‡ºè·¯å¾„: {output_path}")
        print(f"   é‡åŒ–æ–¹å¼: {export_config['quantization']}")

        # åˆ›å»ºå¯¼å‡ºæ’ä»¶
        exporter = OllamaExportPlugin(export_config)

        # å¯¼å‡ºä¸º GGUF æ ¼å¼
        gguf_path = exporter.export_to_gguf(
            model_path=model_path,
            output_path=output_path,
            quantization=export_config['quantization']
        )

        # åˆ›å»º Modelfile
        modelfile_path = exporter.create_modelfile(
            gguf_path=gguf_path,
            model_name=getattr(args, 'model_name', 'apt-model'),
            output_dir=output_path
        )

        # å¯é€‰ï¼šè‡ªåŠ¨æ³¨å†Œåˆ° Ollama
        if getattr(args, 'register', False):
            print("\nğŸš€ æ³¨å†Œåˆ° Ollama...")
            success = exporter.register_to_ollama(
                modelfile_path=modelfile_path,
                model_name=getattr(args, 'model_name', 'apt-model')
            )
            if success:
                print("âœ… å·²æ³¨å†Œåˆ° Ollamaï¼")
                print(f"   ä½¿ç”¨: ollama run {getattr(args, 'model_name', 'apt-model')}")
            else:
                print("âš ï¸  æ³¨å†Œå¤±è´¥ï¼Œè¯·æ‰‹åŠ¨è¿è¡Œ: ollama create -f " + modelfile_path)
        else:
            print("\nğŸ’¡ æç¤º: ä½¿ç”¨ --register è‡ªåŠ¨æ³¨å†Œåˆ° Ollama")
            print(f"   æˆ–æ‰‹åŠ¨è¿è¡Œ: ollama create -f {modelfile_path}")

        print(f"\nâœ… å¯¼å‡ºå®Œæˆï¼")
        return 0

    except Exception as e:
        return _handle_command_error("Ollamaå¯¼å‡º", e, logger)


def run_fine_tune_command(args):
    """
    æ‰§è¡Œæ¨¡å‹å¾®è°ƒå‘½ä»¤

    ç”¨æ³•:
        python -m apt_model fine-tune --model-path apt_model --data-path finetune_data.txt
        python -m apt_model fine-tune --model-path apt_model --data-path train.txt --val-data val.txt --freeze-embeddings
        python -m apt_model fine-tune --model-path apt_model --data-path train.txt --freeze-encoder-layers 2

    å‚æ•°:
        args: å‘½ä»¤è¡Œå‚æ•°

    è¿”å›:
        int: é€€å‡ºç 
    """
    logger, lang_manager, device = _initialize_common(args)
    logger.info("å¼€å§‹å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹...")

    # è®¾ç½®èµ„æºç›‘æ§
    resource_monitor = _setup_resource_monitor(args, logger)
    _start_monitor(resource_monitor)

    try:
        from apt_model.training.finetuner import fine_tune_model

        # æ£€æŸ¥æ¨¡å‹è·¯å¾„
        if not os.path.exists(args.model_path):
            logger.error(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {args.model_path}")
            print(f"é”™è¯¯: æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {args.model_path}")
            return 1

        # æ£€æŸ¥æ•°æ®è·¯å¾„
        if not os.path.exists(args.data_path):
            logger.error(f"è®­ç»ƒæ•°æ®è·¯å¾„ä¸å­˜åœ¨: {args.data_path}")
            print(f"é”™è¯¯: è®­ç»ƒæ•°æ®è·¯å¾„ä¸å­˜åœ¨: {args.data_path}")
            return 1

        # éªŒè¯æ•°æ®è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        val_data_path = None
        if hasattr(args, 'val_data_path') and args.val_data_path:
            if os.path.exists(args.val_data_path):
                val_data_path = args.val_data_path
            else:
                logger.warning(f"éªŒè¯æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {args.val_data_path}ï¼Œå°†ä¸ä½¿ç”¨éªŒè¯é›†")
                print(f"è­¦å‘Š: éªŒè¯æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {args.val_data_path}")

        print("\n" + "="*60)
        print("ğŸ¯ APTæ¨¡å‹å¾®è°ƒ")
        print("="*60)
        print(f"\né¢„è®­ç»ƒæ¨¡å‹: {args.model_path}")
        print(f"è®­ç»ƒæ•°æ®: {args.data_path}")
        if val_data_path:
            print(f"éªŒè¯æ•°æ®: {val_data_path}")
        print(f"\né…ç½®:")
        print(f"  Epochs: {args.epochs}")
        print(f"  Batch Size: {args.batch_size}")
        print(f"  Learning Rate: {args.learning_rate}")

        # å†»ç»“å±‚è®¾ç½®
        freeze_embeddings = getattr(args, 'freeze_embeddings', False)
        freeze_encoder_layers = getattr(args, 'freeze_encoder_layers', None)
        freeze_decoder_layers = getattr(args, 'freeze_decoder_layers', None)

        if freeze_embeddings or freeze_encoder_layers or freeze_decoder_layers:
            print(f"\nå†»ç»“å±‚è®¾ç½®:")
            if freeze_embeddings:
                print(f"  Embeddings: å†»ç»“")
            if freeze_encoder_layers:
                print(f"  Encoderå‰{freeze_encoder_layers}å±‚: å†»ç»“")
            if freeze_decoder_layers:
                print(f"  Decoderå‰{freeze_decoder_layers}å±‚: å†»ç»“")

        print("="*60 + "\n")

        # æ‰§è¡Œå¾®è°ƒ
        model, tokenizer, config = fine_tune_model(
            pretrained_model_path=args.model_path,
            train_data_path=args.data_path,
            val_data_path=val_data_path,
            epochs=args.epochs,
            batch_size=args.batch_size,
            learning_rate=args.learning_rate,
            freeze_embeddings=freeze_embeddings,
            freeze_encoder_layers=freeze_encoder_layers,
            freeze_decoder_layers=freeze_decoder_layers,
            save_path=args.save_path,
            early_stopping_patience=getattr(args, 'early_stopping_patience', 3),
            eval_steps=getattr(args, 'eval_steps', 100),
            save_steps=getattr(args, 'save_steps', 500),
            max_samples=getattr(args, 'max_samples', None),
            logger=logger
        )

        logger.info(f"å¾®è°ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ°: {args.save_path}")
        print(f"\nâœ… å¾®è°ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ°: {args.save_path}")

        return 0  # æˆåŠŸ

    except Exception as e:
        return _handle_command_error("å¾®è°ƒ", e, logger)
    finally:
        _stop_monitor(resource_monitor)


def run_config_command(args):
    """
    é…ç½®ç®¡ç†å‘½ä»¤ - ç®¡ç†å…¨å±€é…ç½®

    ç”¨æ³•:
        python -m apt_model config --show                    # æ˜¾ç¤ºæ‰€æœ‰é…ç½®
        python -m apt_model config --set-debug on            # å¯ç”¨Debugæ¨¡å¼
        python -m apt_model config --set-debug off           # ç¦ç”¨Debugæ¨¡å¼
        python -m apt_model config --get debug.enabled       # è·å–ç‰¹å®šé…ç½®
        python -m apt_model config --set training.default_epochs 30  # è®¾ç½®é»˜è®¤epochs
        python -m apt_model config --reset                   # é‡ç½®ä¸ºé»˜è®¤é…ç½®
    """
    from apt_model.config.settings_manager import settings, enable_debug, disable_debug
    import yaml

    print("=" * 60)
    print("APT Model é…ç½®ç®¡ç†")
    print("=" * 60)
    print()

    # æ˜¾ç¤ºæ‰€æœ‰é…ç½®
    if hasattr(args, 'show_config') and args.show_config:
        print("å½“å‰é…ç½®:")
        print("-" * 60)
        config = settings.get_all_config()
        print(yaml.dump(config, allow_unicode=True, default_flow_style=False))
        print(f"é…ç½®æ–‡ä»¶ä½ç½®: {settings.CONFIG_FILE}")
        return 0

    # è®¾ç½®Debugæ¨¡å¼
    if hasattr(args, 'set_debug'):
        debug_value = args.set_debug
        if debug_value in ['on', 'true', '1', 'yes']:
            enable_debug()
        elif debug_value in ['off', 'false', '0', 'no']:
            disable_debug()
        else:
            print(f"âŒ æ— æ•ˆçš„debugå€¼: {debug_value}")
            print("   æœ‰æ•ˆå€¼: on, off, true, false, 1, 0, yes, no")
            return 1
        return 0

    # è·å–ç‰¹å®šé…ç½®
    if hasattr(args, 'get_config') and args.get_config:
        key = args.get_config
        value = settings.get(key)
        if value is not None:
            print(f"{key} = {value}")
        else:
            print(f"âŒ é…ç½®é”® '{key}' ä¸å­˜åœ¨")
            return 1
        return 0

    # è®¾ç½®ç‰¹å®šé…ç½®
    if hasattr(args, 'set_config_key') and hasattr(args, 'set_config_value'):
        key = args.set_config_key
        value = args.set_config_value
        try:
            settings.set(key, value)
            print(f"âœ“ å·²è®¾ç½®: {key} = {value}")
            print(f"  é…ç½®æ–‡ä»¶: {settings.CONFIG_FILE}")
        except Exception as e:
            print(f"âŒ è®¾ç½®å¤±è´¥: {e}")
            return 1
        return 0

    # é‡ç½®é…ç½®
    if hasattr(args, 'reset_config') and args.reset_config:
        confirm = input("ç¡®è®¤è¦é‡ç½®æ‰€æœ‰é…ç½®ä¸ºé»˜è®¤å€¼å—ï¼Ÿ(y/N): ")
        if confirm.lower() in ['y', 'yes']:
            settings.reset_to_default()
            print("âœ“ é…ç½®å·²é‡ç½®ä¸ºé»˜è®¤å€¼")
        else:
            print("å·²å–æ¶ˆé‡ç½®")
        return 0

    # å¦‚æœæ²¡æœ‰æŒ‡å®šä»»ä½•æ“ä½œï¼Œæ˜¾ç¤ºå¸®åŠ©
    print("é…ç½®ç®¡ç†å‘½ä»¤ç”¨æ³•:")
    print()
    print("  --show                     æ˜¾ç¤ºæ‰€æœ‰é…ç½®")
    print("  --set-debug on|off         å¯ç”¨/ç¦ç”¨Debugæ¨¡å¼")
    print("  --get KEY                  è·å–æŒ‡å®šé…ç½®é¡¹")
    print("  --set KEY VALUE            è®¾ç½®æŒ‡å®šé…ç½®é¡¹")
    print("  --reset                    é‡ç½®ä¸ºé»˜è®¤é…ç½®")
    print()
    print("ç¤ºä¾‹:")
    print("  python -m apt_model config --show")
    print("  python -m apt_model config --set-debug on")
    print("  python -m apt_model config --set-debug off")
    print("  python -m apt_model config --get debug.enabled")
    print("  python -m apt_model config --set training.default_epochs 30")
    print()
    print(f"é…ç½®æ–‡ä»¶ä½ç½®: {settings.CONFIG_FILE}")

    return 0


def run_debug_command(args):
    """
    æ‰§è¡ŒDebugå‘½ä»¤ - è¯Šæ–­å’Œè°ƒè¯•å·¥å…·

    ç”¨æ³•:
        python -m apt_model debug --type io          # æ£€æŸ¥IOå’Œç¯å¢ƒ
        python -m apt_model debug --type model       # æ£€æŸ¥æ¨¡å‹æ¶æ„
        python -m apt_model debug --type data        # æ£€æŸ¥æ•°æ®åŠ è½½
        python -m apt_model debug --type tokenizer   # æ£€æŸ¥åˆ†è¯å™¨
        python -m apt_model debug                    # æ‰§è¡Œæ‰€æœ‰æ£€æŸ¥
    """
    print("=" * 60)
    print("APT Debug Mode - ç³»ç»Ÿè¯Šæ–­å·¥å…·")
    print("=" * 60)
    print()

    debug_type = args.debug_type if hasattr(args, 'debug_type') and args.debug_type else 'all'

    results = {}

    # 1. è°ƒè¯•IOæµç¨‹
    if debug_type in ['io', 'all']:
        print("[1/4] æ£€æŸ¥IOå’ŒPythonç¯å¢ƒ...")
        print("-" * 60)
        results['io'] = debug_io_pipeline(args)
        print()

    # 2. è°ƒè¯•æ¨¡å‹æ¶æ„
    if debug_type in ['model', 'all']:
        print("[2/4] æ£€æŸ¥æ¨¡å‹æ¶æ„...")
        print("-" * 60)
        results['model'] = debug_model_architecture(args)
        print()

    # 3. è°ƒè¯•æ•°æ®åŠ è½½
    if debug_type in ['data', 'all']:
        print("[3/4] æ£€æŸ¥æ•°æ®åŠ è½½...")
        print("-" * 60)
        results['data'] = debug_data_loading(args)
        print()

    # 4. è°ƒè¯•åˆ†è¯å™¨
    if debug_type in ['tokenizer', 'all']:
        print("[4/4] æ£€æŸ¥åˆ†è¯å™¨...")
        print("-" * 60)
        results['tokenizer'] = debug_tokenizer(args)
        print()

    # ç”Ÿæˆè°ƒè¯•æŠ¥å‘Š
    print("=" * 60)
    print("è¯Šæ–­æŠ¥å‘Š")
    print("=" * 60)

    all_success = True
    for key, result in results.items():
        status = "âœ“" if result['success'] else "âœ—"
        color = "\033[32m" if result['success'] else "\033[31m"
        reset = "\033[0m"
        print(f"{color}{status}{reset} {key:12s}: {result['message']}")
        if not result['success']:
            all_success = False

    print()
    if all_success:
        print("âœ“ æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼ç³»ç»Ÿè¿è¡Œæ­£å¸¸ã€‚")
        return 0
    else:
        print("âœ— éƒ¨åˆ†æ£€æŸ¥å¤±è´¥ï¼Œè¯·æŸ¥çœ‹ä¸Šé¢çš„è¯¦ç»†ä¿¡æ¯ã€‚")
        return 1


def debug_io_pipeline(args):
    """è°ƒè¯•IOæµç¨‹"""
    try:
        import sys

        print(f"  Pythonç‰ˆæœ¬: {sys.version}")
        print(f"  å·¥ä½œç›®å½•: {os.getcwd()}")

        print(f"  æ£€æŸ¥PyTorch...")
        import torch
        print(f"    âœ“ PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"    âœ“ CUDAå¯ç”¨: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"    âœ“ CUDAç‰ˆæœ¬: {torch.version.cuda}")
            print(f"    âœ“ GPUæ•°é‡: {torch.cuda.device_count()}")

        print(f"  æ£€æŸ¥å¿…è¦çš„åŒ…...")
        packages = ['transformers', 'numpy', 'tqdm']
        for pkg in packages:
            try:
                __import__(pkg)
                print(f"    âœ“ {pkg}")
            except ImportError:
                print(f"    âœ— {pkg} - æœªå®‰è£…")

        return {'success': True, 'message': 'IOæµç¨‹æ­£å¸¸'}

    except Exception as e:
        return {'success': False, 'message': f'IOæµç¨‹é”™è¯¯: {e}'}


def debug_model_architecture(args):
    """è°ƒè¯•æ¨¡å‹æ¶æ„"""
    try:
        print(f"  åŠ è½½æ¨¡å‹é…ç½®...")

        from apt_model.config.apt_config import APTConfig
        from apt_model.modeling.apt_model import APTModel

        # åˆ›å»ºæµ‹è¯•é…ç½®
        config = APTConfig(
            vocab_size=1000,
            d_model=256,
            num_encoder_layers=2,
            num_decoder_layers=2
        )

        print(f"    âœ“ é…ç½®åˆ›å»ºæˆåŠŸ")

        print(f"  åˆ›å»ºæ¨¡å‹å®ä¾‹...")
        model = APTModel(config)

        param_count = sum(p.numel() for p in model.parameters())
        print(f"    âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        print(f"    - å‚æ•°æ•°é‡: {param_count:,}")

        print(f"  æµ‹è¯•å‰å‘ä¼ æ’­...")
        import torch
        batch_size = 2
        seq_len = 10

        src_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))
        tgt_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))

        with torch.no_grad():
            outputs = model(src_ids, tgt_ids)
            print(f"    âœ“ å‰å‘ä¼ æ’­æˆåŠŸ: {outputs.shape}")

        print(f"  æµ‹è¯•ç”Ÿæˆæ–¹æ³•...")
        if hasattr(model, 'generate'):
            with torch.no_grad():
                generated = model.generate(input_ids=src_ids, max_length=15)
                print(f"    âœ“ ç”Ÿæˆæ–¹æ³•æˆåŠŸ: {generated.shape}")
        else:
            print(f"    âš ï¸  æ²¡æœ‰generateæ–¹æ³•")

        return {'success': True, 'message': 'æ¨¡å‹æ¶æ„æ­£å¸¸'}

    except Exception as e:
        import traceback
        traceback.print_exc()
        return {'success': False, 'message': f'æ¨¡å‹æ¶æ„é”™è¯¯: {str(e)[:50]}'}


def debug_data_loading(args):
    """è°ƒè¯•æ•°æ®åŠ è½½"""
    try:
        data_path = args.data_path if hasattr(args, 'data_path') and args.data_path else None

        if not data_path:
            print(f"    âš ï¸  æœªæŒ‡å®šæ•°æ®è·¯å¾„ï¼Œä½¿ç”¨æµ‹è¯•æ•°æ®")
            test_texts = ["æµ‹è¯•æ–‡æœ¬1", "æµ‹è¯•æ–‡æœ¬2", "æµ‹è¯•æ–‡æœ¬3"]
        else:
            print(f"  åŠ è½½æ•°æ®: {data_path}")
            with open(data_path, 'r', encoding='utf-8') as f:
                test_texts = [line.strip() for line in f if line.strip()]
            print(f"    âœ“ åŠ è½½äº† {len(test_texts)} æ¡æ•°æ®")

        if len(test_texts) > 0:
            print(f"    - ç¬¬ä¸€æ¡: {test_texts[0][:50]}...")
            print(f"    - å¹³å‡é•¿åº¦: {sum(len(t) for t in test_texts) / len(test_texts):.1f}")

        print(f"  æµ‹è¯•DataLoader...")
        from torch.utils.data import Dataset, DataLoader

        class SimpleDataset(Dataset):
            def __init__(self, texts):
                self.texts = texts

            def __len__(self):
                return len(self.texts)

            def __getitem__(self, idx):
                return self.texts[idx]

        dataset = SimpleDataset(test_texts[:10])
        dataloader = DataLoader(dataset, batch_size=2)

        batch = next(iter(dataloader))
        print(f"    âœ“ DataLoaderæ­£å¸¸: æ‰¹æ¬¡å¤§å°={len(batch)}")

        return {'success': True, 'message': 'æ•°æ®åŠ è½½æ­£å¸¸'}

    except Exception as e:
        import traceback
        traceback.print_exc()
        return {'success': False, 'message': f'æ•°æ®åŠ è½½é”™è¯¯: {str(e)[:50]}'}


def debug_tokenizer(args):
    """è°ƒè¯•åˆ†è¯å™¨"""
    try:
        print(f"  æµ‹è¯•åˆ†è¯å™¨...")

        from apt_model.modeling.chinese_tokenizer_integration import get_appropriate_tokenizer

        test_texts = ["äººå·¥æ™ºèƒ½", "æ·±åº¦å­¦ä¹ ", "è‡ªç„¶è¯­è¨€å¤„ç†"]

        print(f"  åˆå§‹åŒ–åˆ†è¯å™¨...")
        tokenizer, lang = get_appropriate_tokenizer(texts=test_texts)

        print(f"    âœ“ åˆ†è¯å™¨åˆ›å»ºæˆåŠŸ")
        print(f"    - æ£€æµ‹è¯­è¨€: {lang}")
        print(f"    - è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")

        print(f"  æµ‹è¯•ç¼–ç è§£ç ...")
        test_text = test_texts[0]

        # ç¼–ç 
        ids = tokenizer.encode(test_text)
        print(f"    - åŸæ–‡: {test_text}")
        print(f"    - ç¼–ç : {ids[:10]}...")

        # è§£ç 
        decoded = tokenizer.decode(ids)
        print(f"    - è§£ç : {decoded}")

        # æ£€æŸ¥å¾€è¿”ä¸€è‡´æ€§
        if test_text == decoded or decoded.replace(" ", "") == test_text:
            print(f"    âœ“ ç¼–ç è§£ç å¾€è¿”ä¸€è‡´")
        else:
            print(f"    âš ï¸  ç¼–ç è§£ç å­˜åœ¨å·®å¼‚")

        return {'success': True, 'message': 'åˆ†è¯å™¨æ­£å¸¸'}

    except Exception as e:
        import traceback
        traceback.print_exc()
        return {'success': False, 'message': f'åˆ†è¯å™¨é”™è¯¯: {str(e)[:50]}'}


def show_help(args=None):
    """
    Show help information
    """
    from apt_model.cli.command_registry import command_registry

    print("Welcome to APT Model!")
    print("\nUsage:")
    print("  python -m apt_model [action] [options]")
    print("\nå¯ç”¨å‘½ä»¤:")

    # æŒ‰ç±»åˆ«æ˜¾ç¤ºå‘½ä»¤
    commands_by_category = command_registry.get_commands_by_category(include_placeholders=True)

    for category in sorted(commands_by_category.keys()):
        print(f"\n{category.upper()}:")
        for metadata in commands_by_category[category]:
            status = " (å°šæœªå®ç°)" if metadata.is_placeholder else ""
            help_text = metadata.help_text or "æ— è¯´æ˜"
            print(f"  {metadata.name:<20} - {help_text}{status}")
            if metadata.aliases:
                print(f"    {'':18}åˆ«å: {', '.join(metadata.aliases)}")

    print("\nå¸¸ç”¨é€‰é¡¹:")
    print("  --epochs N          - è®­ç»ƒè½®æ•° (é»˜è®¤: 20)")
    print("  --batch-size N      - æ‰¹æ¬¡å¤§å° (é»˜è®¤: 8)")
    print("  --learning-rate N   - å­¦ä¹ ç‡ (é»˜è®¤: 3e-5)")
    print("  --save-path PATH    - æ¨¡å‹ä¿å­˜è·¯å¾„ (é»˜è®¤: 'apt_model')")
    print("  --model-path PATH   - æ¨¡å‹åŠ è½½è·¯å¾„ (é»˜è®¤: 'apt_model')")
    print("  --temperature N     - ç”Ÿæˆæ¸©åº¦ (é»˜è®¤: 0.7)")
    print("  --language LANG     - ç•Œé¢è¯­è¨€ (é»˜è®¤: zh_CN)")
    print("  --force-cpu         - å¼ºåˆ¶ä½¿ç”¨CPU")
    print("\nç¤ºä¾‹:")
    print("  python -m apt_model train")
    print("  python -m apt_model train --epochs 10")
    print("  python -m apt_model chat")
    print("  python -m apt_model evaluate")

    return 0


# ============================================================================
# å‘½ä»¤æ³¨å†Œ
# ============================================================================

def register_core_commands():
    """
    æ³¨å†Œæ ¸å¿ƒå‘½ä»¤åˆ°å‘½ä»¤æ³¨å†Œä¸­å¿ƒ

    è¿™ä¸ªå‡½æ•°åœ¨æ¨¡å—å¯¼å…¥æ—¶è‡ªåŠ¨è°ƒç”¨ï¼Œæ³¨å†Œæ‰€æœ‰æ ¸å¿ƒå‘½ä»¤ã€‚
    æ’ä»¶å¯ä»¥é€šè¿‡è°ƒç”¨ register_command() æ·»åŠ è‡ªå·±çš„å‘½ä»¤ã€‚
    """
    # è®­ç»ƒç›¸å…³å‘½ä»¤
    register_command("train", run_train_command, category="training",
                    help_text="è®­ç»ƒæ¨¡å‹")
    register_command("train-custom", run_train_custom_command, category="training",
                    help_text="ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®è®­ç»ƒæ¨¡å‹")
    register_command("fine-tune", run_fine_tune_command, category="training",
                    help_text="å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹")

    # äº¤äº’ç›¸å…³å‘½ä»¤
    register_command("chat", run_chat_command, category="interactive",
                    help_text="ä¸æ¨¡å‹äº¤äº’å¯¹è¯")

    # è¯„ä¼°ç›¸å…³å‘½ä»¤
    register_command("evaluate", run_evaluate_command, category="evaluation",
                    help_text="è¯„ä¼°æ¨¡å‹æ€§èƒ½", aliases=["eval"])
    register_command("visualize", run_visualize_command, category="evaluation",
                    help_text="ç”Ÿæˆæ¨¡å‹è¯„ä¼°å¯è§†åŒ–å›¾è¡¨")

    # å·¥å…·ç›¸å…³å‘½ä»¤
    register_command("clean-cache", run_clean_cache_command, category="tools",
                    help_text="æ¸…ç†ç¼“å­˜æ–‡ä»¶")
    register_command("estimate", run_estimate_command, category="tools",
                    help_text="ä¼°ç®—è®­ç»ƒæ—¶é—´")

    # å·¥å…·å‘½ä»¤
    register_command("info", run_info_command, category="info",
                    help_text="æ˜¾ç¤ºæ¨¡å‹/æ•°æ®è¯¦ç»†ä¿¡æ¯")
    register_command("list", run_list_command, category="info",
                    help_text="åˆ—å‡ºå¯ç”¨èµ„æº")
    register_command("prune", run_prune_command, category="maintenance",
                    help_text="åˆ é™¤æ—§æ¨¡å‹æˆ–æ•°æ®")
    register_command("size", run_size_command, category="info",
                    help_text="è®¡ç®—æ•°æ®æˆ–æ¨¡å‹å¤§å°")
    register_command("test", run_test_command, category="testing",
                    help_text="æµ‹è¯•æ¨¡å‹")
    register_command("compare", run_compare_command, category="evaluation",
                    help_text="æ¯”è¾ƒæ¨¡å‹æ€§èƒ½")
    register_command("train-hf", run_train_hf_command, category="training",
                    help_text="è®­ç»ƒ Hugging Face å…¼å®¹æ¨¡å‹")
    register_command("distill", run_distill_command, category="training",
                    help_text="è’¸é¦æ¨¡å‹")
    register_command("train-reasoning", run_train_reasoning_command, category="training",
                    help_text="è®­ç»ƒé€»è¾‘æ¨ç†èƒ½åŠ›æ¨¡å‹")
    register_command("process-data", run_process_data_command, category="data",
                    help_text="å¤„ç†æ•°æ®é›†")
    register_command("backup", run_backup_command, category="maintenance",
                    help_text="å¤‡ä»½æ¨¡å‹æˆ–æ•°æ®")
    register_command("upload", run_upload_command, category="distribution",
                    help_text="ä¸Šä¼ æ¨¡å‹æˆ–æ•°æ®")
    register_command("export-ollama", run_export_ollama_command, category="distribution",
                    help_text="å¯¼å‡ºæ¨¡å‹åˆ° Ollama æ ¼å¼")

    # å¸®åŠ©å‘½ä»¤
    register_command("help", show_help, category="general",
                    help_text="æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯")


# è‡ªåŠ¨æ³¨å†Œæ ¸å¿ƒå‘½ä»¤
register_core_commands()
