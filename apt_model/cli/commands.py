#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
APT Model (è‡ªç”Ÿæˆå˜æ¢å™¨) CLI Commands
Implementation of command-line commands for APT model tool

é‡æ„åçš„å‘½ä»¤ç³»ç»Ÿï¼š
- æå–å…¬å…±ä»£ç åˆ°è¾…åŠ©å‡½æ•°
- æ”¯æŒæ’ä»¶å‘½ä»¤æ³¨å†Œ
- æ¸…æ™°çš„å‘½ä»¤åˆ†ç±»
- ç»Ÿä¸€çš„é”™è¯¯å¤„ç†
"""

import os
import sys
# å¼ºåˆ¶ä½¿ç”¨UTF-8ç¼–ç 
if sys.platform == 'win32':
    import codecs
    sys.stdout = codecs.getwriter('utf-8')(sys.stdout.buffer, 'strict')
    sys.stderr = codecs.getwriter('utf-8')(sys.stderr.buffer, 'strict')
import traceback
from datetime import datetime

from apt_model.utils.logging_utils import setup_logging
from apt_model.utils.resource_monitor import ResourceMonitor
from apt_model.utils.language_manager import LanguageManager
from apt_model.utils.hardware_check import check_hardware_compatibility
from apt_model.utils.cache_manager import CacheManager
from apt_model.config.apt_config import APTConfig
from apt_model.training.trainer import train_model
from apt_model.data.external_data import train_with_external_data, load_external_data
from apt_model.interactive.chat import chat_with_model
from apt_model.evaluation.model_evaluator import evaluate_model
from apt_model.utils.visualization import ModelVisualizer
from apt_model.utils.time_estimator import TrainingTimeEstimator
from apt_model.utils import get_device, set_seed
from apt_model.utils.common import _initialize_common
from apt_model.cli.command_registry import register_command


# ============================================================================
# è¾…åŠ©å‡½æ•° - å…¬å…±ä»£ç æå–
# ============================================================================

def _setup_resource_monitor(args, logger):
    """è®¾ç½®èµ„æºç›‘æ§å™¨ï¼ˆå¦‚æœå¯ç”¨ï¼‰"""
    if args.monitor_resources:
        return ResourceMonitor(logger=logger, log_interval=args.log_interval)
    return None


def _start_monitor(resource_monitor):
    """å¯åŠ¨èµ„æºç›‘æ§å™¨"""
    if resource_monitor:
        resource_monitor.start()


def _stop_monitor(resource_monitor):
    """åœæ­¢èµ„æºç›‘æ§å™¨"""
    if resource_monitor:
        resource_monitor.stop()


def _handle_command_error(command_name, error, logger):
    """ç»Ÿä¸€çš„å‘½ä»¤é”™è¯¯å¤„ç†"""
    logger.error(f"{command_name}è¿‡ç¨‹ä¸­å‡ºé”™: {error}")
    logger.error(traceback.format_exc())
    print(f"é”™è¯¯: {error}")
    return 1


def _get_tokenizer_with_detection(texts, args):
    """
    è·å–tokenizerå¹¶æ£€æµ‹è¯­è¨€

    ä½¿ç”¨æ–°çš„codecç³»ç»Ÿï¼ˆä¼˜å…ˆï¼‰æˆ–å›é€€åˆ°æ—§ç³»ç»Ÿ
    """
    from apt_model.codecs import get_codec_for_language
    from apt_model.codecs.compat import CodecTokenizerWrapper
    from apt_model.modeling.chinese_tokenizer_integration import detect_language

    # è‡ªåŠ¨æ£€æµ‹è¯­è¨€
    detected_language = args.model_language or detect_language(texts)

    # å°è¯•ä½¿ç”¨æ–°çš„codecç³»ç»Ÿ
    try:
        codec = get_codec_for_language(detected_language)
        if codec:
            tokenizer = CodecTokenizerWrapper(codec)
            return tokenizer, detected_language
    except Exception as e:
        print(f"Codecç³»ç»Ÿå¤±è´¥ï¼Œå›é€€åˆ°æ—§åˆ†è¯å™¨: {e}")

    # å›é€€åˆ°æ—§ç³»ç»Ÿ
    from apt_model.modeling.chinese_tokenizer_integration import get_appropriate_tokenizer
    return get_appropriate_tokenizer(
        texts,
        tokenizer_type=args.tokenizer_type,
        language=args.model_language
    )


def _create_visualizations(args, model, logger):
    """åˆ›å»ºæ¨¡å‹å¯è§†åŒ–ï¼ˆå¦‚æœè¯·æ±‚ï¼‰"""
    if args.create_plots and model:
        visualizer = ModelVisualizer(logger=logger)
        history = {'loss': []}  # å®é™…åº”ä»è®­ç»ƒè¿‡ç¨‹è·å–

        if args.output_dir:
            cache_manager = CacheManager(cache_dir=args.output_dir, logger=logger)
            visualizer.cache_manager = cache_manager

        visualizer.create_training_history_plot(
            history,
            title=f"{os.path.basename(args.save_path)} Training History"
        )


# ============================================================================
# è®­ç»ƒç›¸å…³å‘½ä»¤
# ============================================================================

def run_train_command(args):
    """
    æ‰§è¡Œè®­ç»ƒå‘½ä»¤

    å‚æ•°:
        args: å‘½ä»¤è¡Œå‚æ•°

    è¿”å›:
        int: é€€å‡ºç 
    """
    logger, lang_manager, device = _initialize_common(args)
    _ = lambda key, *params: lang_manager.get(key).format(*params) if params else lang_manager.get(key)

    # æ˜¾ç¤º APT å…”å­å‰ç¥¥ç‰©ï¼ˆç±»ä¼¼ Linux Tuxï¼‰
    try:
        from apt_model.utils.mascot_render import print_apt_mascot
        print_apt_mascot(cols=60, show_banner=True)
    except Exception as e:
        # å¦‚æœæ¸²æŸ“å¤±è´¥ï¼Œç»§ç»­è®­ç»ƒæµç¨‹
        logger.debug(f"å‰ç¥¥ç‰©æ¸²æŸ“å¤±è´¥: {e}")

    logger.info(_("training.start"))

    # æ£€æŸ¥ç¡¬ä»¶å…¼å®¹æ€§
    model_config = APTConfig()
    check_hardware_compatibility(model_config, logger)

    # è®¾ç½®èµ„æºç›‘æ§
    resource_monitor = _setup_resource_monitor(args, logger)
    _start_monitor(resource_monitor)

    try:
        # è°ƒç”¨è®­ç»ƒå‡½æ•°
        model, tokenizer, config = train_model(
            epochs=args.epochs,
            batch_size=args.batch_size,
            learning_rate=args.learning_rate,
            save_path=args.save_path,
            logger=logger,
            resource_monitor=resource_monitor,
            tokenizer_type=args.tokenizer_type,
            language=args.model_language
        )

        # åˆ›å»ºå¯è§†åŒ–
        _create_visualizations(args, model, logger)

        return 0  # æˆåŠŸ
    except Exception as e:
        return _handle_command_error("è®­ç»ƒ", e, logger)
    finally:
        _stop_monitor(resource_monitor)


def run_train_custom_command(args):
    """
    æ‰§è¡Œè‡ªå®šä¹‰æ•°æ®è®­ç»ƒå‘½ä»¤

    å‚æ•°:
        args: å‘½ä»¤è¡Œå‚æ•°

    è¿”å›:
        int: é€€å‡ºç 
    """
    logger, lang_manager, device = _initialize_common(args)

    # æ˜¾ç¤º APT å…”å­å‰ç¥¥ç‰©ï¼ˆç±»ä¼¼ Linux Tuxï¼‰
    try:
        from apt_model.utils.mascot_render import print_apt_mascot
        print_apt_mascot(cols=60, show_banner=True)
    except Exception as e:
        # å¦‚æœæ¸²æŸ“å¤±è´¥ï¼Œç»§ç»­è®­ç»ƒæµç¨‹
        logger.debug(f"å‰ç¥¥ç‰©æ¸²æŸ“å¤±è´¥: {e}")

    logger.info("å¼€å§‹ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®è®­ç»ƒæ¨¡å‹...")

    # è®¾ç½®èµ„æºç›‘æ§
    resource_monitor = _setup_resource_monitor(args, logger)
    _start_monitor(resource_monitor)

    try:
        # åŠ è½½è‡ªå®šä¹‰æ•°æ®
        custom_texts = None
        if args.data_path:
            try:
                custom_texts = load_external_data(args.data_path, max_samples=args.max_samples)
                print(f"æˆåŠŸåŠ è½½è‡ªå®šä¹‰æ•°æ®: {args.data_path}ï¼Œå…± {len(custom_texts)} æ¡æ–‡æœ¬")
            except FileNotFoundError:
                logger.warning(f"æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶: {args.data_path}ï¼Œå°†ä½¿ç”¨é¢„è®¾è®­ç»ƒæ•°æ®")
                print(f"æœªæ‰¾åˆ°æ•°æ®æ–‡ä»¶: {args.data_path}ï¼Œå°†ä½¿ç”¨é¢„è®¾è®­ç»ƒæ•°æ®")
                from apt_model.training.trainer import get_training_texts
                custom_texts = get_training_texts()
                print(f"ä½¿ç”¨é¢„è®¾æ•°æ®ï¼Œå…± {len(custom_texts)} æ¡æ–‡æœ¬")

        # éªŒè¯æ•°æ®åŠ è½½
        if not custom_texts or len(custom_texts) == 0:
            logger.error("æ— æ³•åŠ è½½æ•°æ®æˆ–æ•°æ®ä¸ºç©º")
            print("é”™è¯¯: æ— æ³•åŠ è½½è®­ç»ƒæ•°æ®æˆ–æ•°æ®ä¸ºç©º")
            print("è¯·æ£€æŸ¥:")
            print(f"  1. æ•°æ®æ–‡ä»¶è·¯å¾„æ˜¯å¦æ­£ç¡®: {args.data_path if args.data_path else '(æœªæŒ‡å®š)'}")
            print("  2. æ•°æ®æ–‡ä»¶æ˜¯å¦åŒ…å«æœ‰æ•ˆå†…å®¹")
            print("  3. æ–‡ä»¶æ ¼å¼æ˜¯å¦æ­£ç¡® (txt/json)")
            return 1

        # è·å–tokenizerå¹¶æ£€æµ‹è¯­è¨€
        tokenizer, detected_language = _get_tokenizer_with_detection(custom_texts, args)
        logger.info(f"ä½¿ç”¨{detected_language}è¯­è¨€åˆ†è¯å™¨: {type(tokenizer).__name__}")
        print(f"ä½¿ç”¨{detected_language}è¯­è¨€åˆ†è¯å™¨: {type(tokenizer).__name__}")

        # ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®è®­ç»ƒ
        model, tokenizer, config = train_with_external_data(
            data_path=None,
            epochs=args.epochs,
            batch_size=args.batch_size,
            learning_rate=args.learning_rate,
            save_path=args.save_path,
            max_samples=args.max_samples,
            tokenizer=tokenizer,
            language=detected_language,
            custom_texts=custom_texts
        )
        return 0 if model else 1
    except Exception as e:
        return _handle_command_error("è‡ªå®šä¹‰è®­ç»ƒ", e, logger)
    finally:
        _stop_monitor(resource_monitor)


# ============================================================================
# äº¤äº’ç›¸å…³å‘½ä»¤
# ============================================================================

def run_chat_command(args):
    """
    æ‰§è¡ŒèŠå¤©å‘½ä»¤

    å‚æ•°:
        args: å‘½ä»¤è¡Œå‚æ•°

    è¿”å›:
        int: é€€å‡ºç 
    """
    # è®¾ç½®æ—¥å¿—
    log_file = f"apt_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
    logger = setup_logging(log_file=log_file)
    logger.info("å¼€å§‹ä¸æ¨¡å‹äº¤äº’å¯¹è¯...")

    try:
        # è·å–æ¨¡å‹è·¯å¾„
        model_dir = args.model_path[0] if isinstance(args.model_path, list) else args.model_path

        # è°ƒç”¨èŠå¤©å‡½æ•°
        chat_with_model(
            model_path=model_dir,
            temperature=args.temperature,
            top_p=args.top_p,
            max_length=args.max_length,
            logger=logger
        )
        return 0
    except Exception as e:
        logger.error(f"èŠå¤©è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
        logger.error(traceback.format_exc())
        print(f"é”™è¯¯: {e}")
        print("å¦‚æœæ‚¨è¿˜æ²¡æœ‰è®­ç»ƒæ¨¡å‹ï¼Œè¯·å…ˆè¿è¡Œ 'python -m apt_model train' å‘½ä»¤è®­ç»ƒæ¨¡å‹ã€‚")
        return 1


# ============================================================================
# è¯„ä¼°ç›¸å…³å‘½ä»¤
# ============================================================================

def run_evaluate_command(args):
    """
    Run the evaluate command (evaluate model performance) for one or more models.

    Parameters:
        args: Command line arguments

    Returns:
        int: Exit code (0 for success, non-zero for error)
    """
    logger, lang_manager, device = _initialize_common(args)
    logger.info(f"Starting evaluation of models: {args.model_path}")

    overall_success = True

    for model_path in args.model_path:
        logger.info(f"Evaluating model: {model_path}")
        try:
            evaluation_results = evaluate_model(
                model_path=model_path,
                output_dir=args.output_dir,
                eval_sets=args.eval_sets,
                num_samples=args.num_eval_samples,
                logger=logger
            )

            if evaluation_results and args.output_dir:
                print(f"\nEvaluation report for model '{model_path}' saved to: {args.output_dir}")
            elif not evaluation_results:
                overall_success = False

        except Exception as e:
            logger.error(f"Error during evaluation of model '{model_path}': {e}")
            logger.error(traceback.format_exc())
            overall_success = False

    return 0 if overall_success else 1


def run_visualize_command(args):
    """
    æ‰§è¡Œ visualize å‘½ä»¤ï¼Œç”Ÿæˆæ¨¡å‹è¯„ä¼°å¯è§†åŒ–å›¾è¡¨
    """
    import matplotlib.pyplot as plt
    import numpy as np

    print("=" * 60)
    print("Starting visualization command")
    print("=" * 60)

    try:
        # è·å–å‘½ä»¤è¡Œå‚æ•°
        model_path = args.model_path
        if args.output_dir:
            output_dir = args.output_dir
        else:
            project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
            output_dir = os.path.join(project_root, "report")

        print(f"æ¨¡å‹è·¯å¾„: {model_path}")
        print(f"è¾“å‡ºç›®å½•: {output_dir}")

        # åˆå§‹åŒ– logger
        try:
            logger, _, _ = _initialize_common(args)
            logger.info(f"Creating visualizations for model: {model_path}")
        except Exception as e:
            print(f"Warning: Could not initialize logger: {e}")
            logger = None

        # åˆ›å»ºè¾“å‡ºç›®å½•
        os.makedirs(output_dir, exist_ok=True)
        abs_output_dir = os.path.abspath(output_dir)

        # é…ç½®å¯è§†åŒ–åº“ï¼ˆæ”¯æŒä¸­æ–‡ï¼‰
        plt.rcParams['font.sans-serif'] = ['Microsoft YaHei', 'SimHei', 'DejaVu Sans']
        plt.rcParams['axes.unicode_minus'] = False

        charts_created = 0

        # å°è¯•åŠ è½½æ¨¡å‹
        model = None
        try:
            print(f"å°è¯•åŠ è½½æ¨¡å‹: {model_path}")
            from apt_model.training.checkpoint import load_model
            if not os.path.exists(model_path):
                raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            model, tokenizer, config = load_model(model_path)
            print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
        except Exception as e:
            print(f"åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
            print("ç»§ç»­æ‰§è¡Œä¸ä¾èµ–æ¨¡å‹çš„å¯è§†åŒ–...")

        # åˆ›å»ºå„ç§å›¾è¡¨
        charts_created += _create_category_chart(abs_output_dir, logger)
        charts_created += _create_training_history_chart(abs_output_dir, logger)
        charts_created += _create_capability_radar_chart(abs_output_dir, logger)
        charts_created += _create_quality_trend_chart(abs_output_dir, logger)

        if args.visualize_attention:
            charts_created += _create_attention_heatmap(abs_output_dir, logger)

        # ç”Ÿæˆæ•´ä½“æŠ¥å‘Š
        _create_visualization_report(abs_output_dir, args, logger)

        print("\n" + "=" * 60)
        print(f"å¯è§†åŒ–å®Œæˆï¼æ‰€æœ‰å›¾è¡¨å·²ä¿å­˜åˆ°: {abs_output_dir}")
        print("=" * 60)

        # å°è¯•æ‰“å¼€è¾“å‡ºç›®å½•
        _try_open_directory(abs_output_dir)

        return 0

    except Exception as e:
        if 'logger' in locals() and logger:
            logger.error(f"å¯è§†åŒ–è¿‡ç¨‹ä¸­å‡ºé”™: {e}")
            logger.error(traceback.format_exc())
        print(f"Error during visualization: {e}")
        print(traceback.format_exc())
        return 1


# å¯è§†åŒ–è¾…åŠ©å‡½æ•°
def _create_category_chart(output_dir, logger):
    """åˆ›å»ºç±»åˆ«å¯¹æ¯”å›¾"""
    import matplotlib.pyplot as plt

    print("\nåˆ›å»ºç±»åˆ«å¯¹æ¯”å›¾...")
    category_scores = {
        "äº‹å®æ€§": 60, "é€»è¾‘æ€§": 55, "åˆ›é€ æ€§": 70,
        "ç¼–ç¨‹": 50, "ä¸­æ–‡": 45
    }
    plt.figure(figsize=(10, 6))
    categories = list(category_scores.keys())
    scores = list(category_scores.values())
    bars = plt.bar(categories, scores, color='skyblue')
    plt.title("æ¨¡å‹å„ç±»åˆ«æ€§èƒ½è¯„ä¼°")
    plt.xlabel("ç±»åˆ«")
    plt.ylabel("å¾—åˆ†")
    plt.ylim(0, 100)
    for bar in bars:
        height = bar.get_height()
        plt.text(bar.get_x() + bar.get_width()/2., height, f'{height:.1f}',
                 ha='center', va='bottom')
    plt.xticks(rotation=45, ha='right')
    plt.tight_layout()
    chart_path = os.path.join(output_dir, "category_performance.png")
    plt.savefig(chart_path)
    plt.close()
    print(f"âœ“ ç±»åˆ«å¯¹æ¯”å›¾å·²ä¿å­˜åˆ°: {chart_path}")
    if logger:
        logger.info(f"Category chart saved to: {chart_path}")
    return 1


def _create_training_history_chart(output_dir, logger):
    """åˆ›å»ºè®­ç»ƒå†å²å›¾"""
    import matplotlib.pyplot as plt

    print("\nåˆ›å»ºè®­ç»ƒå†å²å›¾...")
    epochs = range(1, 21)
    train_loss = [4.5 - i * 0.16 for i in range(20)]
    val_loss = [4.6 - i * 0.15 for i in range(20)]
    plt.figure(figsize=(12, 6))
    plt.plot(epochs, train_loss, 'b-', label='è®­ç»ƒæŸå¤±')
    plt.plot(epochs, val_loss, 'r-', label='éªŒè¯æŸå¤±')
    plt.title("æ¨¡å‹è®­ç»ƒå†å²")
    plt.xlabel("è½®æ¬¡")
    plt.ylabel("æŸå¤±")
    plt.legend()
    plt.grid(True)
    plt.tight_layout()
    history_path = os.path.join(output_dir, "training_history.png")
    plt.savefig(history_path)
    plt.close()
    print(f"âœ“ è®­ç»ƒå†å²å›¾å·²ä¿å­˜åˆ°: {history_path}")
    if logger:
        logger.info(f"Training history chart saved to: {history_path}")
    return 1


def _create_capability_radar_chart(output_dir, logger):
    """åˆ›å»ºèƒ½åŠ›é›·è¾¾å›¾"""
    import matplotlib.pyplot as plt
    import numpy as np

    print("\nåˆ›å»ºèƒ½åŠ›é›·è¾¾å›¾...")
    categories_list = ['ç”Ÿæˆè´¨é‡', 'æ¨ç†èƒ½åŠ›', 'å¤šè¯­ç§', 'åˆ›é€ æ€§', 'ç¼–ç¨‹']
    values = [85, 72, 68, 80, 75]
    N = len(categories_list)
    angles = [n / float(N) * 2 * np.pi for n in range(N)]
    angles += angles[:1]
    values_for_chart = values + [values[0]]
    fig = plt.figure(figsize=(8, 8))
    ax = fig.add_subplot(111, polar=True)
    ax.plot(angles, values_for_chart, 'o-', linewidth=2)
    ax.fill(angles, values_for_chart, alpha=0.25)
    ax.set_xticks(angles[:-1])
    ax.set_xticklabels(categories_list)
    ax.set_ylim(0, 100)
    plt.title('æ¨¡å‹èƒ½åŠ›è¯„ä¼°', size=15)
    radar_chart_path = os.path.join(output_dir, "capability_radar.png")
    plt.savefig(radar_chart_path)
    plt.close()
    print(f"âœ“ é›·è¾¾å›¾å·²ä¿å­˜åˆ°: {radar_chart_path}")
    if logger:
        logger.info(f"Radar chart saved to: {radar_chart_path}")
    return 1


def _create_quality_trend_chart(output_dir, logger):
    """åˆ›å»ºè´¨é‡è¯„ä¼°è¶‹åŠ¿å›¾"""
    import matplotlib.pyplot as plt
    import numpy as np

    print("\nåˆ›å»ºè´¨é‡è¯„ä¼°è¶‹åŠ¿å›¾...")
    epochs_list = list(range(1, 21, 2))
    quality_scores = [45, 52, 60, 65, 70, 78, 82, 85, 87, 90]
    plt.figure(figsize=(10, 6))
    plt.plot(epochs_list, quality_scores, 'g-o', label='ç”Ÿæˆè´¨é‡è¯„åˆ†')
    z = np.polyfit(epochs_list, quality_scores, 1)
    p = np.poly1d(z)
    plt.plot(epochs_list, p(epochs_list), "r--", label='è¶‹åŠ¿çº¿')
    plt.title("è´¨é‡è¯„ä¼°è¶‹åŠ¿å›¾")
    plt.xlabel("è®­ç»ƒè½®æ¬¡")
    plt.ylabel("è´¨é‡è¯„åˆ†")
    plt.ylim(0, 100)
    plt.grid(True)
    plt.legend()
    quality_path = os.path.join(output_dir, "quality_trend.png")
    plt.savefig(quality_path)
    plt.close()
    print(f"âœ“ è´¨é‡è¯„ä¼°è¶‹åŠ¿å›¾å·²ä¿å­˜åˆ°: {quality_path}")
    if logger:
        logger.info(f"Quality trend chart saved to: {quality_path}")
    return 1


def _create_attention_heatmap(output_dir, logger):
    """åˆ›å»ºæ³¨æ„åŠ›çƒ­å›¾"""
    import matplotlib.pyplot as plt
    import numpy as np

    print("\nåˆ›å»ºæ³¨æ„åŠ›çƒ­å›¾...")
    try:
        import seaborn as sns
        tokens = ["å¼€å§‹", "APT", "æ¨¡å‹", "æœ‰", "æƒŠäºº", "çš„", "ç”Ÿæˆ", "èƒ½åŠ›"]
        attention = np.random.rand(len(tokens), len(tokens))
        np.fill_diagonal(attention, np.random.uniform(0.7, 1.0, size=len(tokens)))
        plt.figure(figsize=(10, 8))
        sns.heatmap(attention, xticklabels=tokens, yticklabels=tokens,
                    cmap="YlGnBu", vmin=0, vmax=1, annot=True, fmt=".2f")
        plt.title("æ³¨æ„åŠ›çƒ­å›¾")
        plt.xticks(rotation=45, ha='right')
        plt.tight_layout()
        attention_path = os.path.join(output_dir, "attention_heatmap.png")
        plt.savefig(attention_path)
        plt.close()
        print(f"âœ“ æ³¨æ„åŠ›çƒ­å›¾å·²ä¿å­˜åˆ°: {attention_path}")
        if logger:
            logger.info(f"Attention heatmap saved to: {attention_path}")
        return 1
    except Exception as e:
        print(f"Error creating attention heatmap: {e}")
        if logger:
            logger.error(f"Error creating attention heatmap: {e}")
        return 0


def _create_visualization_report(output_dir, args, logger):
    """ç”Ÿæˆæ•´ä½“å¯è§†åŒ–æŠ¥å‘Š"""
    print("\nç”Ÿæˆæ•´ä½“å¯è§†åŒ–æŠ¥å‘Š...")
    report_path = os.path.join(output_dir, "visualization_report.md")
    with open(report_path, 'w', encoding='utf-8') as f:
        f.write("# APTæ¨¡å‹å¯è§†åŒ–æŠ¥å‘Š\n\n")
        f.write(f"ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
        f.write("## æ¨¡å‹æ€§èƒ½è¯„ä¼°\n\n")
        f.write("![ç±»åˆ«æ€§èƒ½](./category_performance.png)\n\n")
        f.write("æ¨¡å‹åœ¨å„ä¸ªèƒ½åŠ›ç±»åˆ«ä¸Šçš„è¡¨ç°è¯„ä¼°ã€‚\n\n")
        f.write("## è®­ç»ƒå†å²\n\n")
        f.write("![è®­ç»ƒå†å²](./training_history.png)\n\n")
        f.write("æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­æŸå¤±å‡½æ•°çš„å˜åŒ–ã€‚\n\n")
        if args.visualize_attention:
            f.write("## æ³¨æ„åŠ›æœºåˆ¶å¯è§†åŒ–\n\n")
            f.write("![æ³¨æ„åŠ›çƒ­å›¾](./attention_heatmap.png)\n\n")
            f.write("æ¨¡å‹å¤„ç†è¾“å…¥æ—¶çš„æ³¨æ„åŠ›åˆ†é…æƒé‡ã€‚\n\n")
        f.write("## ç”Ÿæˆè´¨é‡è¶‹åŠ¿\n\n")
        f.write("![è´¨é‡è¶‹åŠ¿](./quality_trend.png)\n\n")
        f.write("æ¨¡å‹ç”Ÿæˆè´¨é‡éšè®­ç»ƒè½®æ¬¡çš„å˜åŒ–è¶‹åŠ¿ã€‚\n\n")
    print(f"âœ“ å¯è§†åŒ–æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
    if logger:
        logger.info(f"Visualization report saved to: {report_path}")


def _try_open_directory(directory):
    """å°è¯•æ‰“å¼€ç›®å½•"""
    try:
        import platform
        system = platform.system()
        if system == 'Windows':
            os.startfile(directory)
        elif system == 'Darwin':  # macOS
            import subprocess
            subprocess.run(['open', directory])
        elif system == 'Linux':
            import subprocess
            subprocess.run(['xdg-open', directory])
        print("å·²å°è¯•æ‰“å¼€è¾“å‡ºç›®å½•ã€‚")
    except Exception as e:
        print(f"æ— æ³•è‡ªåŠ¨æ‰“å¼€è¾“å‡ºç›®å½•: {e}")
        print(f"è¯·æ‰‹åŠ¨æ‰“å¼€ç›®å½•: {directory}")


# ============================================================================
# å·¥å…·ç›¸å…³å‘½ä»¤
# ============================================================================

def run_clean_cache_command(args):
    """
    Run the clean-cache command (clean cache files)
    """
    logger, lang_manager, device = _initialize_common(args)
    logger.info("Starting cache cleanup...")

    try:
        cache_manager = CacheManager(cache_dir=args.cache_dir, logger=logger)
        result = cache_manager.clean_cache(days=args.clean_days)
        logger.info(f"Cache cleanup completed. Cleaned {result.get('cleaned_files', 0)} files, "
                   f"{result.get('cleaned_dirs', 0)} directories")
        if result.get('errors', []):
            logger.warning(f"Encountered {len(result['errors'])} errors during cleanup")
        return 0
    except Exception as e:
        return _handle_command_error("ç¼“å­˜æ¸…ç†", e, logger)


def run_estimate_command(args):
    """
    Run the estimate command (estimate training time)
    """
    logger, lang_manager, device = _initialize_common(args)
    logger.info("Estimating training time...")

    try:
        from apt_model.data.data_processor import get_training_texts
        dataset_size = len(get_training_texts())
        if args.data_path:
            try:
                custom_data = load_external_data(args.data_path, max_samples=args.max_samples)
                dataset_size = len(custom_data)
            except Exception:
                pass
        model_config = APTConfig()
        estimator = TrainingTimeEstimator(
            model_config=model_config,
            dataset_size=dataset_size,
            batch_size=args.batch_size,
            epochs=args.epochs,
            logger=logger
        )
        estimation = estimator.print_estimation()
        return 0
    except Exception as e:
        return _handle_command_error("æ—¶é—´ä¼°ç®—", e, logger)


# ============================================================================
# å ä½ç¬¦å‘½ä»¤ - å¾…å®ç°
# ============================================================================

def run_info_command(args):
    """å ä½ç¬¦: æ˜¾ç¤ºæ¨¡å‹/æ•°æ®è¯¦ç»†ä¿¡æ¯"""
    print("INFO å‘½ä»¤å°šæœªå®ç°")
    return 0


def run_list_command(args):
    """å ä½ç¬¦: åˆ—å‡ºå¯ç”¨èµ„æº"""
    print("LIST å‘½ä»¤å°šæœªå®ç°")
    return 0


def run_prune_command(args):
    """å ä½ç¬¦: åˆ é™¤æ—§æ¨¡å‹æˆ–æ•°æ®"""
    print("PRUNE å‘½ä»¤å°šæœªå®ç°")
    return 0


def run_size_command(args):
    """å ä½ç¬¦: è®¡ç®—æ•°æ®æˆ–æ¨¡å‹å¤§å°"""
    print("SIZE å‘½ä»¤å°šæœªå®ç°")
    return 0


def run_test_command(args):
    """å ä½ç¬¦: æµ‹è¯•æ¨¡å‹çš„å‘½ä»¤"""
    print("TEST å‘½ä»¤å°šæœªå®ç°")
    return 0


def run_compare_command(args):
    """å ä½ç¬¦ï¼šæ¯”è¾ƒæ¨¡å‹çš„å‘½ä»¤"""
    print("Compare å‘½ä»¤å°šæœªå®ç°ã€‚")
    return 0


def run_train_hf_command(args):
    """å ä½ç¬¦ï¼šç”¨äº Hugging Face ç›¸å…³è®­ç»ƒçš„å‘½ä»¤"""
    print("run_train_hf_command å‘½ä»¤å°šæœªå®ç°ã€‚")
    return 0


def run_distill_command(args):
    """å ä½ç¬¦ï¼šç”¨äºçŸ¥è¯†è’¸é¦è®­ç»ƒçš„å‘½ä»¤"""
    print("run_distill_command å‘½ä»¤å°šæœªå®ç°ã€‚")
    return 0


def run_train_reasoning_command(args):
    """
    æ‰§è¡Œæ¨ç†æ¨¡å‹è®­ç»ƒå‘½ä»¤

    å‚æ•°:
        args: å‘½ä»¤è¡Œå‚æ•°

    è¿”å›:
        int: é€€å‡ºç 
    """
    logger, lang_manager, device = _initialize_common(args)
    logger.info("å¼€å§‹è®­ç»ƒæ¨ç†å¢å¼ºæ¨¡å‹...")

    # è®¾ç½®èµ„æºç›‘æ§
    resource_monitor = _setup_resource_monitor(args, logger)
    _start_monitor(resource_monitor)

    try:
        # Import reasoning training
        from apt_model.training.train_reasoning import train_reasoning_model, load_reasoning_dataset
        from apt_model.modeling.gpt4o_model import VeinSubspaceShared

        # Load base model (placeholder - should load actual model)
        logger.info("åŠ è½½åŸºç¡€æ¨¡å‹...")
        # TODO: Load actual pre-trained base model
        from transformers import AutoModelForCausalLM, AutoTokenizer

        model_name = getattr(args, 'base_model', 'gpt2')
        base_model = AutoModelForCausalLM.from_pretrained(model_name)
        tokenizer = AutoTokenizer.from_pretrained(model_name)

        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token

        # Get model dimension
        d_model = base_model.config.hidden_size

        # Create vein projector
        rank = getattr(args, 'vein_rank', 4)
        vein = VeinSubspaceShared(d_model=d_model, rank=rank)

        logger.info(f"ä½¿ç”¨ Vein å­ç©ºé—´: d_model={d_model}, rank={rank}")

        # Train reasoning model
        reasoning_controller, training_info = train_reasoning_model(
            base_model=base_model,
            vein_projector=vein,
            tokenizer=tokenizer,
            data_path=getattr(args, 'data_path', None),
            epochs=args.epochs,
            batch_size=args.batch_size,
            learning_rate=args.learning_rate,
            max_reasoning_steps=getattr(args, 'max_reasoning_steps', 6),
            use_budgeted=getattr(args, 'use_budgeted', True),
            global_budget=getattr(args, 'global_budget', 0.15),
            save_path=args.save_path,
            logger=logger,
            resource_monitor=resource_monitor,
        )

        logger.info("æ¨ç†æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
        logger.info(f"è®­ç»ƒä¿¡æ¯: {training_info}")

        return 0  # æˆåŠŸ
    except Exception as e:
        return _handle_command_error("æ¨ç†è®­ç»ƒ", e, logger)
    finally:
        _stop_monitor(resource_monitor)


def run_process_data_command(args):
    """å ä½ç¬¦ï¼šç”¨äºæ•°æ®å¤„ç†çš„å‘½ä»¤"""
    print("run_process_data_command å‘½ä»¤å°šæœªå®ç°ã€‚")
    return 0


def run_backup_command(args):
    """å ä½ç¬¦ï¼šç”¨äºå¤‡ä»½æ“ä½œçš„å‘½ä»¤"""
    print("run_backup_command å‘½ä»¤å°šæœªå®ç°ã€‚")
    return 0


def run_upload_command(args):
    """å ä½ç¬¦ï¼šç”¨äºä¸Šä¼ æ“ä½œçš„å‘½ä»¤"""
    print("run_upload_command å‘½ä»¤å°šæœªå®ç°ã€‚")
    return 0


def run_export_ollama_command(args):
    """å ä½ç¬¦ï¼šå¯¼å‡º Ollama æ ¼å¼çš„æ¨¡å‹å‘½ä»¤"""
    print("run_export_ollama_command å‘½ä»¤å°šæœªå®ç°ã€‚")
    return 0


def run_fine_tune_command(args):
    """
    æ‰§è¡Œæ¨¡å‹å¾®è°ƒå‘½ä»¤

    ç”¨æ³•:
        python -m apt_model fine-tune --model-path apt_model --data-path finetune_data.txt
        python -m apt_model fine-tune --model-path apt_model --data-path train.txt --val-data val.txt --freeze-embeddings
        python -m apt_model fine-tune --model-path apt_model --data-path train.txt --freeze-encoder-layers 2

    å‚æ•°:
        args: å‘½ä»¤è¡Œå‚æ•°

    è¿”å›:
        int: é€€å‡ºç 
    """
    logger, lang_manager, device = _initialize_common(args)
    logger.info("å¼€å§‹å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹...")

    # è®¾ç½®èµ„æºç›‘æ§
    resource_monitor = _setup_resource_monitor(args, logger)
    _start_monitor(resource_monitor)

    try:
        from apt_model.training.finetuner import fine_tune_model

        # æ£€æŸ¥æ¨¡å‹è·¯å¾„
        if not os.path.exists(args.model_path):
            logger.error(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {args.model_path}")
            print(f"é”™è¯¯: æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {args.model_path}")
            return 1

        # æ£€æŸ¥æ•°æ®è·¯å¾„
        if not os.path.exists(args.data_path):
            logger.error(f"è®­ç»ƒæ•°æ®è·¯å¾„ä¸å­˜åœ¨: {args.data_path}")
            print(f"é”™è¯¯: è®­ç»ƒæ•°æ®è·¯å¾„ä¸å­˜åœ¨: {args.data_path}")
            return 1

        # éªŒè¯æ•°æ®è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        val_data_path = None
        if hasattr(args, 'val_data_path') and args.val_data_path:
            if os.path.exists(args.val_data_path):
                val_data_path = args.val_data_path
            else:
                logger.warning(f"éªŒè¯æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {args.val_data_path}ï¼Œå°†ä¸ä½¿ç”¨éªŒè¯é›†")
                print(f"è­¦å‘Š: éªŒè¯æ•°æ®è·¯å¾„ä¸å­˜åœ¨: {args.val_data_path}")

        print("\n" + "="*60)
        print("ğŸ¯ APTæ¨¡å‹å¾®è°ƒ")
        print("="*60)
        print(f"\né¢„è®­ç»ƒæ¨¡å‹: {args.model_path}")
        print(f"è®­ç»ƒæ•°æ®: {args.data_path}")
        if val_data_path:
            print(f"éªŒè¯æ•°æ®: {val_data_path}")
        print(f"\né…ç½®:")
        print(f"  Epochs: {args.epochs}")
        print(f"  Batch Size: {args.batch_size}")
        print(f"  Learning Rate: {args.learning_rate}")

        # å†»ç»“å±‚è®¾ç½®
        freeze_embeddings = getattr(args, 'freeze_embeddings', False)
        freeze_encoder_layers = getattr(args, 'freeze_encoder_layers', None)
        freeze_decoder_layers = getattr(args, 'freeze_decoder_layers', None)

        if freeze_embeddings or freeze_encoder_layers or freeze_decoder_layers:
            print(f"\nå†»ç»“å±‚è®¾ç½®:")
            if freeze_embeddings:
                print(f"  Embeddings: å†»ç»“")
            if freeze_encoder_layers:
                print(f"  Encoderå‰{freeze_encoder_layers}å±‚: å†»ç»“")
            if freeze_decoder_layers:
                print(f"  Decoderå‰{freeze_decoder_layers}å±‚: å†»ç»“")

        print("="*60 + "\n")

        # æ‰§è¡Œå¾®è°ƒ
        model, tokenizer, config = fine_tune_model(
            pretrained_model_path=args.model_path,
            train_data_path=args.data_path,
            val_data_path=val_data_path,
            epochs=args.epochs,
            batch_size=args.batch_size,
            learning_rate=args.learning_rate,
            freeze_embeddings=freeze_embeddings,
            freeze_encoder_layers=freeze_encoder_layers,
            freeze_decoder_layers=freeze_decoder_layers,
            save_path=args.save_path,
            early_stopping_patience=getattr(args, 'early_stopping_patience', 3),
            eval_steps=getattr(args, 'eval_steps', 100),
            save_steps=getattr(args, 'save_steps', 500),
            max_samples=getattr(args, 'max_samples', None),
            logger=logger
        )

        logger.info(f"å¾®è°ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ°: {args.save_path}")
        print(f"\nâœ… å¾®è°ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ°: {args.save_path}")

        return 0  # æˆåŠŸ

    except Exception as e:
        return _handle_command_error("å¾®è°ƒ", e, logger)
    finally:
        _stop_monitor(resource_monitor)


def run_config_command(args):
    """
    é…ç½®ç®¡ç†å‘½ä»¤ - ç®¡ç†å…¨å±€é…ç½®

    ç”¨æ³•:
        python -m apt_model config --show                    # æ˜¾ç¤ºæ‰€æœ‰é…ç½®
        python -m apt_model config --set-debug on            # å¯ç”¨Debugæ¨¡å¼
        python -m apt_model config --set-debug off           # ç¦ç”¨Debugæ¨¡å¼
        python -m apt_model config --get debug.enabled       # è·å–ç‰¹å®šé…ç½®
        python -m apt_model config --set training.default_epochs 30  # è®¾ç½®é»˜è®¤epochs
        python -m apt_model config --reset                   # é‡ç½®ä¸ºé»˜è®¤é…ç½®
    """
    from apt_model.config.settings_manager import settings, enable_debug, disable_debug
    import yaml

    print("=" * 60)
    print("APT Model é…ç½®ç®¡ç†")
    print("=" * 60)
    print()

    # æ˜¾ç¤ºæ‰€æœ‰é…ç½®
    if hasattr(args, 'show_config') and args.show_config:
        print("å½“å‰é…ç½®:")
        print("-" * 60)
        config = settings.get_all_config()
        print(yaml.dump(config, allow_unicode=True, default_flow_style=False))
        print(f"é…ç½®æ–‡ä»¶ä½ç½®: {settings.CONFIG_FILE}")
        return 0

    # è®¾ç½®Debugæ¨¡å¼
    if hasattr(args, 'set_debug'):
        debug_value = args.set_debug
        if debug_value in ['on', 'true', '1', 'yes']:
            enable_debug()
        elif debug_value in ['off', 'false', '0', 'no']:
            disable_debug()
        else:
            print(f"âŒ æ— æ•ˆçš„debugå€¼: {debug_value}")
            print("   æœ‰æ•ˆå€¼: on, off, true, false, 1, 0, yes, no")
            return 1
        return 0

    # è·å–ç‰¹å®šé…ç½®
    if hasattr(args, 'get_config') and args.get_config:
        key = args.get_config
        value = settings.get(key)
        if value is not None:
            print(f"{key} = {value}")
        else:
            print(f"âŒ é…ç½®é”® '{key}' ä¸å­˜åœ¨")
            return 1
        return 0

    # è®¾ç½®ç‰¹å®šé…ç½®
    if hasattr(args, 'set_config_key') and hasattr(args, 'set_config_value'):
        key = args.set_config_key
        value = args.set_config_value
        try:
            settings.set(key, value)
            print(f"âœ“ å·²è®¾ç½®: {key} = {value}")
            print(f"  é…ç½®æ–‡ä»¶: {settings.CONFIG_FILE}")
        except Exception as e:
            print(f"âŒ è®¾ç½®å¤±è´¥: {e}")
            return 1
        return 0

    # é‡ç½®é…ç½®
    if hasattr(args, 'reset_config') and args.reset_config:
        confirm = input("ç¡®è®¤è¦é‡ç½®æ‰€æœ‰é…ç½®ä¸ºé»˜è®¤å€¼å—ï¼Ÿ(y/N): ")
        if confirm.lower() in ['y', 'yes']:
            settings.reset_to_default()
            print("âœ“ é…ç½®å·²é‡ç½®ä¸ºé»˜è®¤å€¼")
        else:
            print("å·²å–æ¶ˆé‡ç½®")
        return 0

    # å¦‚æœæ²¡æœ‰æŒ‡å®šä»»ä½•æ“ä½œï¼Œæ˜¾ç¤ºå¸®åŠ©
    print("é…ç½®ç®¡ç†å‘½ä»¤ç”¨æ³•:")
    print()
    print("  --show                     æ˜¾ç¤ºæ‰€æœ‰é…ç½®")
    print("  --set-debug on|off         å¯ç”¨/ç¦ç”¨Debugæ¨¡å¼")
    print("  --get KEY                  è·å–æŒ‡å®šé…ç½®é¡¹")
    print("  --set KEY VALUE            è®¾ç½®æŒ‡å®šé…ç½®é¡¹")
    print("  --reset                    é‡ç½®ä¸ºé»˜è®¤é…ç½®")
    print()
    print("ç¤ºä¾‹:")
    print("  python -m apt_model config --show")
    print("  python -m apt_model config --set-debug on")
    print("  python -m apt_model config --set-debug off")
    print("  python -m apt_model config --get debug.enabled")
    print("  python -m apt_model config --set training.default_epochs 30")
    print()
    print(f"é…ç½®æ–‡ä»¶ä½ç½®: {settings.CONFIG_FILE}")

    return 0


def run_debug_command(args):
    """
    æ‰§è¡ŒDebugå‘½ä»¤ - è¯Šæ–­å’Œè°ƒè¯•å·¥å…·

    ç”¨æ³•:
        python -m apt_model debug --type io          # æ£€æŸ¥IOå’Œç¯å¢ƒ
        python -m apt_model debug --type model       # æ£€æŸ¥æ¨¡å‹æ¶æ„
        python -m apt_model debug --type data        # æ£€æŸ¥æ•°æ®åŠ è½½
        python -m apt_model debug --type tokenizer   # æ£€æŸ¥åˆ†è¯å™¨
        python -m apt_model debug                    # æ‰§è¡Œæ‰€æœ‰æ£€æŸ¥
    """
    print("=" * 60)
    print("APT Debug Mode - ç³»ç»Ÿè¯Šæ–­å·¥å…·")
    print("=" * 60)
    print()

    debug_type = args.debug_type if hasattr(args, 'debug_type') and args.debug_type else 'all'

    results = {}

    # 1. è°ƒè¯•IOæµç¨‹
    if debug_type in ['io', 'all']:
        print("[1/4] æ£€æŸ¥IOå’ŒPythonç¯å¢ƒ...")
        print("-" * 60)
        results['io'] = debug_io_pipeline(args)
        print()

    # 2. è°ƒè¯•æ¨¡å‹æ¶æ„
    if debug_type in ['model', 'all']:
        print("[2/4] æ£€æŸ¥æ¨¡å‹æ¶æ„...")
        print("-" * 60)
        results['model'] = debug_model_architecture(args)
        print()

    # 3. è°ƒè¯•æ•°æ®åŠ è½½
    if debug_type in ['data', 'all']:
        print("[3/4] æ£€æŸ¥æ•°æ®åŠ è½½...")
        print("-" * 60)
        results['data'] = debug_data_loading(args)
        print()

    # 4. è°ƒè¯•åˆ†è¯å™¨
    if debug_type in ['tokenizer', 'all']:
        print("[4/4] æ£€æŸ¥åˆ†è¯å™¨...")
        print("-" * 60)
        results['tokenizer'] = debug_tokenizer(args)
        print()

    # ç”Ÿæˆè°ƒè¯•æŠ¥å‘Š
    print("=" * 60)
    print("è¯Šæ–­æŠ¥å‘Š")
    print("=" * 60)

    all_success = True
    for key, result in results.items():
        status = "âœ“" if result['success'] else "âœ—"
        color = "\033[32m" if result['success'] else "\033[31m"
        reset = "\033[0m"
        print(f"{color}{status}{reset} {key:12s}: {result['message']}")
        if not result['success']:
            all_success = False

    print()
    if all_success:
        print("âœ“ æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼ç³»ç»Ÿè¿è¡Œæ­£å¸¸ã€‚")
        return 0
    else:
        print("âœ— éƒ¨åˆ†æ£€æŸ¥å¤±è´¥ï¼Œè¯·æŸ¥çœ‹ä¸Šé¢çš„è¯¦ç»†ä¿¡æ¯ã€‚")
        return 1


def debug_io_pipeline(args):
    """è°ƒè¯•IOæµç¨‹"""
    try:
        import sys

        print(f"  Pythonç‰ˆæœ¬: {sys.version}")
        print(f"  å·¥ä½œç›®å½•: {os.getcwd()}")

        print(f"  æ£€æŸ¥PyTorch...")
        import torch
        print(f"    âœ“ PyTorchç‰ˆæœ¬: {torch.__version__}")
        print(f"    âœ“ CUDAå¯ç”¨: {torch.cuda.is_available()}")
        if torch.cuda.is_available():
            print(f"    âœ“ CUDAç‰ˆæœ¬: {torch.version.cuda}")
            print(f"    âœ“ GPUæ•°é‡: {torch.cuda.device_count()}")

        print(f"  æ£€æŸ¥å¿…è¦çš„åŒ…...")
        packages = ['transformers', 'numpy', 'tqdm']
        for pkg in packages:
            try:
                __import__(pkg)
                print(f"    âœ“ {pkg}")
            except ImportError:
                print(f"    âœ— {pkg} - æœªå®‰è£…")

        return {'success': True, 'message': 'IOæµç¨‹æ­£å¸¸'}

    except Exception as e:
        return {'success': False, 'message': f'IOæµç¨‹é”™è¯¯: {e}'}


def debug_model_architecture(args):
    """è°ƒè¯•æ¨¡å‹æ¶æ„"""
    try:
        print(f"  åŠ è½½æ¨¡å‹é…ç½®...")

        from apt_model.config.apt_config import APTConfig
        from apt_model.modeling.apt_model import APTModel

        # åˆ›å»ºæµ‹è¯•é…ç½®
        config = APTConfig(
            vocab_size=1000,
            d_model=256,
            num_encoder_layers=2,
            num_decoder_layers=2
        )

        print(f"    âœ“ é…ç½®åˆ›å»ºæˆåŠŸ")

        print(f"  åˆ›å»ºæ¨¡å‹å®ä¾‹...")
        model = APTModel(config)

        param_count = sum(p.numel() for p in model.parameters())
        print(f"    âœ“ æ¨¡å‹åˆ›å»ºæˆåŠŸ")
        print(f"    - å‚æ•°æ•°é‡: {param_count:,}")

        print(f"  æµ‹è¯•å‰å‘ä¼ æ’­...")
        import torch
        batch_size = 2
        seq_len = 10

        src_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))
        tgt_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len))

        with torch.no_grad():
            outputs = model(src_ids, tgt_ids)
            print(f"    âœ“ å‰å‘ä¼ æ’­æˆåŠŸ: {outputs.shape}")

        print(f"  æµ‹è¯•ç”Ÿæˆæ–¹æ³•...")
        if hasattr(model, 'generate'):
            with torch.no_grad():
                generated = model.generate(input_ids=src_ids, max_length=15)
                print(f"    âœ“ ç”Ÿæˆæ–¹æ³•æˆåŠŸ: {generated.shape}")
        else:
            print(f"    âš ï¸  æ²¡æœ‰generateæ–¹æ³•")

        return {'success': True, 'message': 'æ¨¡å‹æ¶æ„æ­£å¸¸'}

    except Exception as e:
        import traceback
        traceback.print_exc()
        return {'success': False, 'message': f'æ¨¡å‹æ¶æ„é”™è¯¯: {str(e)[:50]}'}


def debug_data_loading(args):
    """è°ƒè¯•æ•°æ®åŠ è½½"""
    try:
        data_path = args.data_path if hasattr(args, 'data_path') and args.data_path else None

        if not data_path:
            print(f"    âš ï¸  æœªæŒ‡å®šæ•°æ®è·¯å¾„ï¼Œä½¿ç”¨æµ‹è¯•æ•°æ®")
            test_texts = ["æµ‹è¯•æ–‡æœ¬1", "æµ‹è¯•æ–‡æœ¬2", "æµ‹è¯•æ–‡æœ¬3"]
        else:
            print(f"  åŠ è½½æ•°æ®: {data_path}")
            with open(data_path, 'r', encoding='utf-8') as f:
                test_texts = [line.strip() for line in f if line.strip()]
            print(f"    âœ“ åŠ è½½äº† {len(test_texts)} æ¡æ•°æ®")

        if len(test_texts) > 0:
            print(f"    - ç¬¬ä¸€æ¡: {test_texts[0][:50]}...")
            print(f"    - å¹³å‡é•¿åº¦: {sum(len(t) for t in test_texts) / len(test_texts):.1f}")

        print(f"  æµ‹è¯•DataLoader...")
        from torch.utils.data import Dataset, DataLoader

        class SimpleDataset(Dataset):
            def __init__(self, texts):
                self.texts = texts

            def __len__(self):
                return len(self.texts)

            def __getitem__(self, idx):
                return self.texts[idx]

        dataset = SimpleDataset(test_texts[:10])
        dataloader = DataLoader(dataset, batch_size=2)

        batch = next(iter(dataloader))
        print(f"    âœ“ DataLoaderæ­£å¸¸: æ‰¹æ¬¡å¤§å°={len(batch)}")

        return {'success': True, 'message': 'æ•°æ®åŠ è½½æ­£å¸¸'}

    except Exception as e:
        import traceback
        traceback.print_exc()
        return {'success': False, 'message': f'æ•°æ®åŠ è½½é”™è¯¯: {str(e)[:50]}'}


def debug_tokenizer(args):
    """è°ƒè¯•åˆ†è¯å™¨"""
    try:
        print(f"  æµ‹è¯•åˆ†è¯å™¨...")

        from apt_model.modeling.chinese_tokenizer_integration import get_appropriate_tokenizer

        test_texts = ["äººå·¥æ™ºèƒ½", "æ·±åº¦å­¦ä¹ ", "è‡ªç„¶è¯­è¨€å¤„ç†"]

        print(f"  åˆå§‹åŒ–åˆ†è¯å™¨...")
        tokenizer, lang = get_appropriate_tokenizer(texts=test_texts)

        print(f"    âœ“ åˆ†è¯å™¨åˆ›å»ºæˆåŠŸ")
        print(f"    - æ£€æµ‹è¯­è¨€: {lang}")
        print(f"    - è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")

        print(f"  æµ‹è¯•ç¼–ç è§£ç ...")
        test_text = test_texts[0]

        # ç¼–ç 
        ids = tokenizer.encode(test_text)
        print(f"    - åŸæ–‡: {test_text}")
        print(f"    - ç¼–ç : {ids[:10]}...")

        # è§£ç 
        decoded = tokenizer.decode(ids)
        print(f"    - è§£ç : {decoded}")

        # æ£€æŸ¥å¾€è¿”ä¸€è‡´æ€§
        if test_text == decoded or decoded.replace(" ", "") == test_text:
            print(f"    âœ“ ç¼–ç è§£ç å¾€è¿”ä¸€è‡´")
        else:
            print(f"    âš ï¸  ç¼–ç è§£ç å­˜åœ¨å·®å¼‚")

        return {'success': True, 'message': 'åˆ†è¯å™¨æ­£å¸¸'}

    except Exception as e:
        import traceback
        traceback.print_exc()
        return {'success': False, 'message': f'åˆ†è¯å™¨é”™è¯¯: {str(e)[:50]}'}


def show_help(args=None):
    """
    Show help information
    """
    from apt_model.cli.command_registry import command_registry

    print("Welcome to APT Model!")
    print("\nUsage:")
    print("  python -m apt_model [action] [options]")
    print("\nå¯ç”¨å‘½ä»¤:")

    # æŒ‰ç±»åˆ«æ˜¾ç¤ºå‘½ä»¤
    commands_by_category = command_registry.get_commands_by_category(include_placeholders=True)

    for category in sorted(commands_by_category.keys()):
        print(f"\n{category.upper()}:")
        for metadata in commands_by_category[category]:
            status = " (å°šæœªå®ç°)" if metadata.is_placeholder else ""
            help_text = metadata.help_text or "æ— è¯´æ˜"
            print(f"  {metadata.name:<20} - {help_text}{status}")
            if metadata.aliases:
                print(f"    {'':18}åˆ«å: {', '.join(metadata.aliases)}")

    print("\nå¸¸ç”¨é€‰é¡¹:")
    print("  --epochs N          - è®­ç»ƒè½®æ•° (é»˜è®¤: 20)")
    print("  --batch-size N      - æ‰¹æ¬¡å¤§å° (é»˜è®¤: 8)")
    print("  --learning-rate N   - å­¦ä¹ ç‡ (é»˜è®¤: 3e-5)")
    print("  --save-path PATH    - æ¨¡å‹ä¿å­˜è·¯å¾„ (é»˜è®¤: 'apt_model')")
    print("  --model-path PATH   - æ¨¡å‹åŠ è½½è·¯å¾„ (é»˜è®¤: 'apt_model')")
    print("  --temperature N     - ç”Ÿæˆæ¸©åº¦ (é»˜è®¤: 0.7)")
    print("  --language LANG     - ç•Œé¢è¯­è¨€ (é»˜è®¤: zh_CN)")
    print("  --force-cpu         - å¼ºåˆ¶ä½¿ç”¨CPU")
    print("\nç¤ºä¾‹:")
    print("  python -m apt_model train")
    print("  python -m apt_model train --epochs 10")
    print("  python -m apt_model chat")
    print("  python -m apt_model evaluate")

    return 0


# ============================================================================
# å‘½ä»¤æ³¨å†Œ
# ============================================================================

def register_core_commands():
    """
    æ³¨å†Œæ ¸å¿ƒå‘½ä»¤åˆ°å‘½ä»¤æ³¨å†Œä¸­å¿ƒ

    è¿™ä¸ªå‡½æ•°åœ¨æ¨¡å—å¯¼å…¥æ—¶è‡ªåŠ¨è°ƒç”¨ï¼Œæ³¨å†Œæ‰€æœ‰æ ¸å¿ƒå‘½ä»¤ã€‚
    æ’ä»¶å¯ä»¥é€šè¿‡è°ƒç”¨ register_command() æ·»åŠ è‡ªå·±çš„å‘½ä»¤ã€‚
    """
    # è®­ç»ƒç›¸å…³å‘½ä»¤
    register_command("train", run_train_command, category="training",
                    help_text="è®­ç»ƒæ¨¡å‹")
    register_command("train-custom", run_train_custom_command, category="training",
                    help_text="ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®è®­ç»ƒæ¨¡å‹")
    register_command("fine-tune", run_fine_tune_command, category="training",
                    help_text="å¾®è°ƒé¢„è®­ç»ƒæ¨¡å‹")

    # äº¤äº’ç›¸å…³å‘½ä»¤
    register_command("chat", run_chat_command, category="interactive",
                    help_text="ä¸æ¨¡å‹äº¤äº’å¯¹è¯")

    # è¯„ä¼°ç›¸å…³å‘½ä»¤
    register_command("evaluate", run_evaluate_command, category="evaluation",
                    help_text="è¯„ä¼°æ¨¡å‹æ€§èƒ½", aliases=["eval"])
    register_command("visualize", run_visualize_command, category="evaluation",
                    help_text="ç”Ÿæˆæ¨¡å‹è¯„ä¼°å¯è§†åŒ–å›¾è¡¨")

    # å·¥å…·ç›¸å…³å‘½ä»¤
    register_command("clean-cache", run_clean_cache_command, category="tools",
                    help_text="æ¸…ç†ç¼“å­˜æ–‡ä»¶")
    register_command("estimate", run_estimate_command, category="tools",
                    help_text="ä¼°ç®—è®­ç»ƒæ—¶é—´")

    # å ä½ç¬¦å‘½ä»¤
    register_command("info", run_info_command, category="info",
                    help_text="æ˜¾ç¤ºæ¨¡å‹/æ•°æ®è¯¦ç»†ä¿¡æ¯", is_placeholder=True)
    register_command("list", run_list_command, category="info",
                    help_text="åˆ—å‡ºå¯ç”¨èµ„æº", is_placeholder=True)
    register_command("prune", run_prune_command, category="maintenance",
                    help_text="åˆ é™¤æ—§æ¨¡å‹æˆ–æ•°æ®", is_placeholder=True)
    register_command("size", run_size_command, category="info",
                    help_text="è®¡ç®—æ•°æ®æˆ–æ¨¡å‹å¤§å°", is_placeholder=True)
    register_command("test", run_test_command, category="testing",
                    help_text="æµ‹è¯•æ¨¡å‹", is_placeholder=True)
    register_command("compare", run_compare_command, category="evaluation",
                    help_text="æ¯”è¾ƒæ¨¡å‹æ€§èƒ½", is_placeholder=True)
    register_command("train-hf", run_train_hf_command, category="training",
                    help_text="è®­ç»ƒ Hugging Face å…¼å®¹æ¨¡å‹", is_placeholder=True)
    register_command("distill", run_distill_command, category="training",
                    help_text="è’¸é¦æ¨¡å‹", is_placeholder=True)
    register_command("train-reasoning", run_train_reasoning_command, category="training",
                    help_text="è®­ç»ƒé€»è¾‘æ¨ç†èƒ½åŠ›æ¨¡å‹", is_placeholder=True)
    register_command("process-data", run_process_data_command, category="data",
                    help_text="å¤„ç†æ•°æ®é›†", is_placeholder=True)
    register_command("backup", run_backup_command, category="maintenance",
                    help_text="å¤‡ä»½æ¨¡å‹æˆ–æ•°æ®", is_placeholder=True)
    register_command("upload", run_upload_command, category="distribution",
                    help_text="ä¸Šä¼ æ¨¡å‹æˆ–æ•°æ®", is_placeholder=True)
    register_command("export-ollama", run_export_ollama_command, category="distribution",
                    help_text="å¯¼å‡ºæ¨¡å‹åˆ° Ollama æ ¼å¼", is_placeholder=True)

    # å¸®åŠ©å‘½ä»¤
    register_command("help", show_help, category="general",
                    help_text="æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯")


# è‡ªåŠ¨æ³¨å†Œæ ¸å¿ƒå‘½ä»¤
register_core_commands()
