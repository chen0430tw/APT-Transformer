#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å¯è§†åŒ–çŸ¥è¯†è’¸é¦æ’ä»¶

æä¾›å‹å¥½çš„ã€æ•™è‚²å¼çš„è’¸é¦è®­ç»ƒå¯è§†åŒ–ï¼š
- æ˜¾ç¤ºæ•™å¸ˆå’Œå­¦ç”Ÿçš„å®é™…æ–‡æœ¬è¾“å‡º
- è®¡ç®—"å·æ‡’ç¨‹åº¦"ï¼ˆç›¸ä¼¼åº¦æŒ‡æ ‡ï¼‰
- æ™ºèƒ½è¯„è¯­ç³»ç»Ÿ
- ä¸»é¢˜åˆ†ç±»å’Œæ˜¾ç¤º
- ç¾åŒ–çš„è¿›åº¦æ¡å’Œè¾“å‡º
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
from typing import Optional, Dict, Any, List, Tuple
from torch.utils.data import DataLoader
import difflib
import re
from datetime import datetime


class VisualDistillationPlugin:
    """
    å¯è§†åŒ–çŸ¥è¯†è’¸é¦æ’ä»¶

    è®©çŸ¥è¯†è’¸é¦è¿‡ç¨‹åƒ"æ•™å­¦"ä¸€æ ·ç›´è§‚å¯è§
    """

    def __init__(self, config: Dict[str, Any]):
        self.name = "visual-distillation"
        self.version = "1.0.0"
        self.config = config

        # è’¸é¦å‚æ•°
        self.temperature = config.get('temperature', 4.0)
        self.alpha = config.get('alpha', 0.7)
        self.beta = config.get('beta', 0.3)

        # å¯è§†åŒ–é…ç½®
        self.show_samples = config.get('show_samples', True)  # æ˜¯å¦æ˜¾ç¤ºæ ·æœ¬æ–‡æœ¬
        self.show_diff = config.get('show_diff', True)  # æ˜¯å¦æ˜¾ç¤ºæ–‡æœ¬å·®å¼‚
        self.sample_frequency = config.get('sample_frequency', 50)  # æ¯Nä¸ªbatchæ˜¾ç¤ºä¸€æ¬¡
        self.max_text_length = config.get('max_text_length', 100)  # æ˜¾ç¤ºçš„æœ€å¤§æ–‡æœ¬é•¿åº¦

        # ä¸»é¢˜å…³é”®è¯ï¼ˆç”¨äºåˆ†ç±»ï¼‰
        self.topic_keywords = {
            'äº’è”ç½‘': ['äº’è”ç½‘', 'ç½‘ç»œ', 'Internet', 'Web', 'åœ¨çº¿', 'ç½‘ç«™'],
            'äººå·¥æ™ºèƒ½': ['äººå·¥æ™ºèƒ½', 'AI', 'æœºå™¨å­¦ä¹ ', 'æ·±åº¦å­¦ä¹ ', 'ç¥ç»ç½‘ç»œ', 'Transformer'],
            'ç§‘æŠ€': ['ç§‘æŠ€', 'æŠ€æœ¯', 'åˆ›æ–°', 'å‘æ˜', 'ç§‘å­¦'],
            'åŒ»ç–—': ['åŒ»ç–—', 'å¥åº·', 'åŒ»å­¦', 'ç–¾ç—…', 'æ²»ç–—', 'åŒ»é™¢'],
            'æ•™è‚²': ['æ•™è‚²', 'å­¦ä¹ ', 'å­¦æ ¡', 'æ•™å­¦', 'çŸ¥è¯†', 'åŸ¹è®­'],
            'ç»æµ': ['ç»æµ', 'é‡‘è', 'å¸‚åœº', 'è‚¡ç¥¨', 'æŠ•èµ„', 'è´¸æ˜“'],
            'æ–‡åŒ–': ['æ–‡åŒ–', 'è‰ºæœ¯', 'éŸ³ä¹', 'æ–‡å­¦', 'å†å²', 'ä¼ ç»Ÿ'],
            'ä½“è‚²': ['ä½“è‚²', 'è¿åŠ¨', 'æ¯”èµ›', 'è¶³çƒ', 'ç¯®çƒ', 'å¥¥è¿'],
        }

        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_samples': 0,
            'topic_distribution': {},
            'avg_laziness': [],
            'improvement_rate': []
        }

    # ==================== æ ¸å¿ƒè’¸é¦æŸå¤± ====================

    def distillation_loss(
        self,
        student_logits: torch.Tensor,
        teacher_logits: torch.Tensor,
        labels: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        çŸ¥è¯†è’¸é¦æŸå¤±
        """
        T = self.temperature

        # æ¸©åº¦è½¯åŒ–
        student_log_probs = F.log_softmax(student_logits / T, dim=-1)
        teacher_probs = F.softmax(teacher_logits / T, dim=-1)

        # KLæ•£åº¦
        distill_loss = F.kl_div(
            student_log_probs,
            teacher_probs,
            reduction='batchmean'
        ) * (T ** 2)

        # ç»“åˆçœŸå®æ ‡ç­¾
        if labels is not None:
            ce_loss = F.cross_entropy(
                student_logits.view(-1, student_logits.size(-1)),
                labels.view(-1),
                ignore_index=-100
            )
            total_loss = self.alpha * distill_loss + self.beta * ce_loss
            return total_loss

        return distill_loss

    # ==================== æ–‡æœ¬ç”Ÿæˆå’Œå¯¹æ¯” ====================

    def generate_text_from_logits(
        self,
        logits: torch.Tensor,
        tokenizer: Any,
        max_length: int = 50,
        temperature: float = 1.0
    ) -> str:
        """
        ä»logitsç”Ÿæˆæ–‡æœ¬

        Args:
            logits: [batch, seq_len, vocab_size]
            tokenizer: åˆ†è¯å™¨
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            temperature: ç”Ÿæˆæ¸©åº¦

        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        # å–ç¬¬ä¸€ä¸ªæ ·æœ¬
        logits = logits[0]  # [seq_len, vocab_size]

        # æ¸©åº¦é‡‡æ ·
        probs = F.softmax(logits / temperature, dim=-1)
        token_ids = torch.multinomial(probs, num_samples=1).squeeze(-1)  # [seq_len]

        # è§£ç 
        try:
            text = tokenizer.decode(token_ids[:max_length], skip_special_tokens=True)
        except:
            # å¦‚æœè§£ç å¤±è´¥ï¼Œè¿”å›token ids
            text = str(token_ids[:10].tolist()) + "..."

        return text

    def compute_text_similarity(self, text1: str, text2: str) -> float:
        """
        è®¡ç®—ä¸¤ä¸ªæ–‡æœ¬çš„ç›¸ä¼¼åº¦ï¼ˆ0-1ï¼‰

        ä½¿ç”¨SequenceMatcherè®¡ç®—ç›¸ä¼¼åº¦
        """
        similarity = difflib.SequenceMatcher(None, text1, text2).ratio()
        return similarity

    def compute_laziness_score(
        self,
        student_logits: torch.Tensor,
        teacher_logits: torch.Tensor,
        student_text: str,
        teacher_text: str
    ) -> float:
        """
        è®¡ç®—"å·æ‡’ç¨‹åº¦"

        ç»¼åˆè€ƒè™‘ï¼š
        1. æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆè¶Šç›¸ä¼¼è¶Šä¸å·æ‡’ï¼‰
        2. KLæ•£åº¦ï¼ˆè¶Šå¤§è¶Šå·æ‡’ï¼‰

        Returns:
            å·æ‡’ç¨‹åº¦ç™¾åˆ†æ¯”ï¼ˆ0-100ï¼‰ï¼Œè¶Šé«˜è¶Šå·æ‡’
        """
        # 1. æ–‡æœ¬ç›¸ä¼¼åº¦éƒ¨åˆ†ï¼ˆæƒé‡0.6ï¼‰
        text_sim = self.compute_text_similarity(student_text, teacher_text)
        text_laziness = (1 - text_sim) * 60  # 0-60

        # 2. KLæ•£åº¦éƒ¨åˆ†ï¼ˆæƒé‡0.4ï¼‰
        with torch.no_grad():
            T = self.temperature
            student_log_probs = F.log_softmax(student_logits / T, dim=-1)
            teacher_probs = F.softmax(teacher_logits / T, dim=-1)

            kl_div = F.kl_div(
                student_log_probs,
                teacher_probs,
                reduction='batchmean'
            ).item()

            # å½’ä¸€åŒ–KLæ•£åº¦åˆ°0-40èŒƒå›´ï¼ˆå‡è®¾KL < 5ä¸ºå¥½ï¼‰
            kl_laziness = min(kl_div / 5.0, 1.0) * 40  # 0-40

        # æ€»å·æ‡’ç¨‹åº¦
        total_laziness = text_laziness + kl_laziness

        return total_laziness

    def generate_comment(self, laziness: float, loss: float) -> str:
        """
        æ ¹æ®å·æ‡’ç¨‹åº¦ç”Ÿæˆè¯„è¯­

        Args:
            laziness: å·æ‡’ç¨‹åº¦ (0-100)
            loss: è®­ç»ƒæŸå¤±

        Returns:
            è¯„è¯­æ–‡æœ¬
        """
        if laziness < 20 and loss < 0.5:
            comments = [
                "ğŸŒŸ ä¼˜ç§€ï¼å®Œå…¨æŒæ¡äº†æ•™å¸ˆçš„çŸ¥è¯†",
                "ğŸ‰ å¤ªæ£’äº†ï¼å­¦ä¹ å¾—éå¸¸å¥½",
                "âœ¨ å®Œç¾ï¼å·²ç»æ¥è¿‘æ•™å¸ˆæ°´å¹³",
                "ğŸ† å“è¶Šï¼è¶…å‡ºé¢„æœŸçš„è¡¨ç°",
            ]
        elif laziness < 40 and loss < 1.0:
            comments = [
                "ğŸ‘ å¾ˆå¥½ï¼å¤§éƒ¨åˆ†çŸ¥è¯†å·²æŒæ¡",
                "ğŸ˜Š ä¸é”™ï¼ç»§ç»­ä¿æŒè¿™ä¸ªèŠ‚å¥",
                "ğŸ’ª è‰¯å¥½ï¼å­¦ä¹ æ€åº¦å¾ˆè®¤çœŸ",
                "ğŸ¯ è¿›æ­¥æ˜æ˜¾ï¼åŠ æ²¹",
            ]
        elif laziness < 60 and loss < 2.0:
            comments = [
                "ğŸ¤” è¿˜å¯ä»¥ï¼Œä½†éœ€è¦æ›´åŠªåŠ›",
                "ğŸ“š ä¸»é¢˜ä¸å¤Ÿç†Ÿç»ƒï¼Œéœ€è¦å†å¤šå­¦ä¹ ",
                "âš¡ æœ‰è¿›æ­¥ç©ºé—´ï¼Œç»§ç»­åŠ æ²¹",
                "ğŸ”„ ç†è§£è¿˜ä¸å¤Ÿæ·±å…¥ï¼Œå¤šç»ƒä¹ ",
            ]
        else:
            comments = [
                "ğŸ˜“ å·æ‡’å¤ªå¤šäº†ï¼éœ€è¦è®¤çœŸå­¦ä¹ ",
                "âŒ å­¦ä¹ ä¸å¤Ÿä¸“æ³¨ï¼Œé‡æ–°æ¥è¿‡",
                "âš ï¸ ä¸¥é‡åç¦»æ•™å¸ˆè¾“å‡ºï¼Œéœ€è¦æ”¹è¿›",
                "ğŸš¨ æ³¨æ„ï¼å­¦ä¹ æ•ˆæœä¸ç†æƒ³",
            ]

        import random
        return random.choice(comments)

    def detect_topic(self, text: str) -> str:
        """
        æ£€æµ‹æ–‡æœ¬ä¸»é¢˜

        Args:
            text: è¾“å…¥æ–‡æœ¬

        Returns:
            ä¸»é¢˜åç§°
        """
        text_lower = text.lower()

        # è®¡ç®—æ¯ä¸ªä¸»é¢˜çš„åŒ¹é…åˆ†æ•°
        topic_scores = {}
        for topic, keywords in self.topic_keywords.items():
            score = sum(1 for keyword in keywords if keyword.lower() in text_lower)
            if score > 0:
                topic_scores[topic] = score

        # è¿”å›å¾—åˆ†æœ€é«˜çš„ä¸»é¢˜
        if topic_scores:
            return max(topic_scores, key=topic_scores.get)
        else:
            return "é€šç”¨"

    # ==================== ç¾åŒ–è¾“å‡º ====================

    def print_header(self):
        """æ‰“å°è®­ç»ƒå¼€å§‹çš„æ ‡é¢˜"""
        print("\n" + "="*70)
        print("ğŸ“ å¯è§†åŒ–çŸ¥è¯†è’¸é¦è®­ç»ƒ".center(70))
        print("="*70)
        print(f"âš™ï¸  é…ç½®: æ¸©åº¦={self.temperature}, Î±={self.alpha}, Î²={self.beta}")
        print(f"ğŸ“Š æ˜¾ç¤ºé¢‘ç‡: æ¯ {self.sample_frequency} ä¸ªbatchæ˜¾ç¤ºä¸€æ¬¡æ ·æœ¬")
        print("="*70 + "\n")

    def print_epoch_header(self, epoch: int, total_epochs: int):
        """æ‰“å°Epochæ ‡é¢˜"""
        print("\n" + "â”€"*70)
        print(f"ğŸ“– Epoch {epoch}/{total_epochs}".center(70))
        print("â”€"*70)

    def print_sample_comparison(
        self,
        epoch: int,
        batch_idx: int,
        teacher_text: str,
        student_text: str,
        topic: str,
        laziness: float,
        loss: float,
        comment: str
    ):
        """
        æ‰“å°æ ·æœ¬å¯¹æ¯”

        è¿™æ˜¯æ ¸å¿ƒçš„å¯è§†åŒ–è¾“å‡º
        """
        # æˆªæ–­è¿‡é•¿çš„æ–‡æœ¬
        max_len = self.max_text_length
        teacher_display = teacher_text[:max_len] + "..." if len(teacher_text) > max_len else teacher_text
        student_display = student_text[:max_len] + "..." if len(student_text) > max_len else student_text

        print("\n" + "â”Œ" + "â”€"*68 + "â”")
        print(f"â”‚ ğŸ“ Batch {batch_idx:<10} â”‚ ğŸ“š æ•™å­¦ä¸»é¢˜:ã€{topic}ã€‘".ljust(70) + "â”‚")
        print("â”œ" + "â”€"*68 + "â”¤")
        print(f"â”‚ ğŸ‘¨â€ğŸ« æ•™å¸ˆæ¨¡å‹: {teacher_display}".ljust(70) + "â”‚")
        print(f"â”‚ ğŸ‘¨â€ğŸ“ å­¦ç”Ÿæ¨¡å‹: {student_display}".ljust(70) + "â”‚")
        print("â”œ" + "â”€"*68 + "â”¤")

        # å·æ‡’ç¨‹åº¦è¿›åº¦æ¡
        bar_length = 30
        filled_length = int(bar_length * laziness / 100)
        bar = "â–ˆ" * filled_length + "â–‘" * (bar_length - filled_length)

        # æ ¹æ®å·æ‡’ç¨‹åº¦é€‰æ‹©é¢œè‰²æ ‡è®°
        if laziness < 30:
            laziness_icon = "ğŸŸ¢"
        elif laziness < 60:
            laziness_icon = "ğŸŸ¡"
        else:
            laziness_icon = "ğŸ”´"

        print(f"â”‚ {laziness_icon} å·æ‡’ç¨‹åº¦: [{bar}] {laziness:.2f}%".ljust(70) + "â”‚")
        print(f"â”‚ ğŸ“‰ è®­ç»ƒæŸå¤±: {loss:.4f}".ljust(70) + "â”‚")
        print(f"â”‚ ğŸ’¬ è¯„è¯­: {comment}".ljust(70) + "â”‚")
        print("â””" + "â”€"*68 + "â”˜")

    def print_text_diff(self, text1: str, text2: str):
        """æ‰“å°æ–‡æœ¬å·®å¼‚ï¼ˆå¯é€‰ï¼‰"""
        if not self.show_diff:
            return

        print("\nğŸ“ æ–‡æœ¬å·®å¼‚å¯¹æ¯”:")

        # ä½¿ç”¨difflibç”Ÿæˆå·®å¼‚
        diff = difflib.unified_diff(
            text1.split(),
            text2.split(),
            lineterm='',
            fromfile='æ•™å¸ˆ',
            tofile='å­¦ç”Ÿ'
        )

        diff_lines = list(diff)
        if len(diff_lines) > 2:  # æœ‰å®é™…å·®å¼‚
            for line in diff_lines[2:10]:  # åªæ˜¾ç¤ºå‰å‡ è¡Œ
                print(f"  {line}")

    def print_epoch_summary(
        self,
        epoch: int,
        avg_loss: float,
        avg_laziness: float,
        topic_stats: Dict[str, int]
    ):
        """æ‰“å°Epochæ±‡æ€»"""
        print("\n" + "â•”" + "â•"*68 + "â•—")
        print(f"â•‘ ğŸ“Š Epoch {epoch} æ€»ç»“".ljust(70) + "â•‘")
        print("â• " + "â•"*68 + "â•£")
        print(f"â•‘ ğŸ“‰ å¹³å‡æŸå¤±: {avg_loss:.4f}".ljust(70) + "â•‘")
        print(f"â•‘ ğŸ˜´ å¹³å‡å·æ‡’ç¨‹åº¦: {avg_laziness:.2f}%".ljust(70) + "â•‘")

        if topic_stats:
            print("â•‘ ğŸ“š ä¸»é¢˜åˆ†å¸ƒ:".ljust(70) + "â•‘")
            for topic, count in sorted(topic_stats.items(), key=lambda x: x[1], reverse=True)[:5]:
                print(f"â•‘    {topic}: {count} ä¸ªæ ·æœ¬".ljust(70) + "â•‘")

        print("â•š" + "â•"*68 + "â•")

    def print_final_summary(self):
        """æ‰“å°æœ€ç»ˆæ€»ç»“"""
        print("\n\n" + "â•”" + "â•"*68 + "â•—")
        print("â•‘ ğŸ‰ çŸ¥è¯†è’¸é¦è®­ç»ƒå®Œæˆï¼".center(70) + "â•‘")
        print("â• " + "â•"*68 + "â•£")

        avg_laziness = sum(self.stats['avg_laziness']) / len(self.stats['avg_laziness']) if self.stats['avg_laziness'] else 0

        print(f"â•‘ ğŸ“Š æ€»æ ·æœ¬æ•°: {self.stats['total_samples']}".ljust(70) + "â•‘")
        print(f"â•‘ ğŸ˜´ æ€»ä½“å¹³å‡å·æ‡’ç¨‹åº¦: {avg_laziness:.2f}%".ljust(70) + "â•‘")

        # å­¦ä¹ è¶‹åŠ¿
        if len(self.stats['avg_laziness']) >= 2:
            improvement = self.stats['avg_laziness'][0] - self.stats['avg_laziness'][-1]
            if improvement > 10:
                trend = "ğŸ“ˆ æ˜¾è‘—è¿›æ­¥ï¼"
            elif improvement > 0:
                trend = "ğŸ“Š ç¨³æ­¥æ”¹å–„"
            else:
                trend = "ğŸ“‰ éœ€è¦è°ƒæ•´ç­–ç•¥"
            print(f"â•‘ å­¦ä¹ è¶‹åŠ¿: {trend}".ljust(70) + "â•‘")

        print("â• " + "â•"*68 + "â•£")
        print("â•‘ ğŸ’¡ å»ºè®®:".ljust(70) + "â•‘")

        if avg_laziness < 30:
            print("â•‘   âœ… è’¸é¦æ•ˆæœä¼˜ç§€ï¼Œå¯ä»¥è€ƒè™‘å‡å°æ¨¡å‹æˆ–é™ä½æ¸©åº¦".ljust(70) + "â•‘")
        elif avg_laziness < 60:
            print("â•‘   ğŸ“š è’¸é¦æ•ˆæœè‰¯å¥½ï¼Œå»ºè®®ç»§ç»­è®­ç»ƒæˆ–è°ƒæ•´å­¦ä¹ ç‡".ljust(70) + "â•‘")
        else:
            print("â•‘   âš ï¸  è’¸é¦æ•ˆæœä¸ç†æƒ³ï¼Œå»ºè®®å¢åŠ æ¸©åº¦æˆ–æ£€æŸ¥æ•°æ®è´¨é‡".ljust(70) + "â•‘")

        print("â•š" + "â•"*68 + "â•\n")

    # ==================== è®­ç»ƒæµç¨‹ ====================

    def visual_distill_training_step(
        self,
        student_model: nn.Module,
        teacher_model: nn.Module,
        batch: Dict[str, torch.Tensor],
        optimizer: torch.optim.Optimizer,
        tokenizer: Any,
        epoch: int,
        batch_idx: int,
        show_sample: bool = False
    ) -> Dict[str, Any]:
        """
        å•æ­¥å¯è§†åŒ–è’¸é¦è®­ç»ƒ

        Args:
            student_model: å­¦ç”Ÿæ¨¡å‹
            teacher_model: æ•™å¸ˆæ¨¡å‹
            batch: è®­ç»ƒæ‰¹æ¬¡
            optimizer: ä¼˜åŒ–å™¨
            tokenizer: åˆ†è¯å™¨ï¼ˆç”¨äºæ–‡æœ¬ç”Ÿæˆï¼‰
            epoch: å½“å‰epoch
            batch_idx: å½“å‰batchç´¢å¼•
            show_sample: æ˜¯å¦æ˜¾ç¤ºæ ·æœ¬å¯¹æ¯”

        Returns:
            è®­ç»ƒç»“æœå­—å…¸
        """
        student_model.train()
        teacher_model.eval()

        input_ids = batch['input_ids']
        labels = batch.get('labels', input_ids)

        # æ•™å¸ˆæ¨¡å‹å‰å‘ä¼ æ’­
        with torch.no_grad():
            teacher_outputs = teacher_model(input_ids, output_hidden_states=True)
            teacher_logits = teacher_outputs.logits if hasattr(teacher_outputs, 'logits') else teacher_outputs[0]

        # å­¦ç”Ÿæ¨¡å‹å‰å‘ä¼ æ’­
        student_outputs = student_model(input_ids, output_hidden_states=True)
        student_logits = student_outputs.logits if hasattr(student_outputs, 'logits') else student_outputs[0]

        # è®¡ç®—æŸå¤±
        loss = self.distillation_loss(student_logits, teacher_logits, labels)

        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        result = {
            'loss': loss.item(),
            'laziness': 0.0,
            'topic': 'æœªçŸ¥',
            'comment': ''
        }

        # å¦‚æœéœ€è¦æ˜¾ç¤ºæ ·æœ¬
        if show_sample and self.show_samples:
            # ç”Ÿæˆæ–‡æœ¬
            with torch.no_grad():
                teacher_text = self.generate_text_from_logits(
                    teacher_logits.cpu(),
                    tokenizer,
                    max_length=self.max_text_length
                )
                student_text = self.generate_text_from_logits(
                    student_logits.cpu(),
                    tokenizer,
                    max_length=self.max_text_length
                )

            # æ£€æµ‹ä¸»é¢˜
            topic = self.detect_topic(teacher_text)

            # è®¡ç®—å·æ‡’ç¨‹åº¦
            laziness = self.compute_laziness_score(
                student_logits.cpu(),
                teacher_logits.cpu(),
                student_text,
                teacher_text
            )

            # ç”Ÿæˆè¯„è¯­
            comment = self.generate_comment(laziness, loss.item())

            # æ‰“å°å¯¹æ¯”
            self.print_sample_comparison(
                epoch=epoch,
                batch_idx=batch_idx,
                teacher_text=teacher_text,
                student_text=student_text,
                topic=topic,
                laziness=laziness,
                loss=loss.item(),
                comment=comment
            )

            # æ›´æ–°ç»“æœ
            result.update({
                'laziness': laziness,
                'topic': topic,
                'comment': comment,
                'teacher_text': teacher_text,
                'student_text': student_text
            })

        return result

    def visual_distill_model(
        self,
        student_model: nn.Module,
        teacher_model: nn.Module,
        train_dataloader: DataLoader,
        optimizer: torch.optim.Optimizer,
        tokenizer: Any,
        num_epochs: int = 3,
        device: str = 'cuda'
    ):
        """
        å®Œæ•´çš„å¯è§†åŒ–è’¸é¦æµç¨‹

        Args:
            student_model: å­¦ç”Ÿæ¨¡å‹
            teacher_model: æ•™å¸ˆæ¨¡å‹
            train_dataloader: è®­ç»ƒæ•°æ®
            optimizer: ä¼˜åŒ–å™¨
            tokenizer: åˆ†è¯å™¨
            num_epochs: è®­ç»ƒè½®æ•°
            device: è®¾å¤‡
        """
        # æ‰“å°æ ‡é¢˜
        self.print_header()

        student_model.to(device)
        teacher_model.to(device)

        for epoch in range(1, num_epochs + 1):
            self.print_epoch_header(epoch, num_epochs)

            epoch_losses = []
            epoch_laziness = []
            epoch_topics = {}

            for batch_idx, batch in enumerate(train_dataloader):
                # æ•°æ®ç§»åˆ°è®¾å¤‡
                batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v
                        for k, v in batch.items()}

                # åˆ¤æ–­æ˜¯å¦æ˜¾ç¤ºæ ·æœ¬
                show_sample = (batch_idx % self.sample_frequency == 0)

                # è®­ç»ƒæ­¥éª¤
                result = self.visual_distill_training_step(
                    student_model=student_model,
                    teacher_model=teacher_model,
                    batch=batch,
                    optimizer=optimizer,
                    tokenizer=tokenizer,
                    epoch=epoch,
                    batch_idx=batch_idx,
                    show_sample=show_sample
                )

                epoch_losses.append(result['loss'])

                if show_sample:
                    epoch_laziness.append(result['laziness'])
                    topic = result['topic']
                    epoch_topics[topic] = epoch_topics.get(topic, 0) + 1
                    self.stats['total_samples'] += 1

                # ç®€å•è¿›åº¦ï¼ˆéæ ·æœ¬batchï¼‰
                if not show_sample and batch_idx % 10 == 0:
                    print(f"  â³ Batch {batch_idx}/{len(train_dataloader)} | Loss: {result['loss']:.4f}", end='\r')

            # Epochæ€»ç»“
            avg_loss = sum(epoch_losses) / len(epoch_losses)
            avg_laziness = sum(epoch_laziness) / len(epoch_laziness) if epoch_laziness else 0

            self.stats['avg_laziness'].append(avg_laziness)

            self.print_epoch_summary(
                epoch=epoch,
                avg_loss=avg_loss,
                avg_laziness=avg_laziness,
                topic_stats=epoch_topics
            )

        # æœ€ç»ˆæ€»ç»“
        self.print_final_summary()


# ==================== ä¾¿æ·å‡½æ•° ====================

def quick_visual_distill(
    student_model: nn.Module,
    teacher_model: nn.Module,
    train_dataloader: DataLoader,
    tokenizer: Any,
    config: Dict[str, Any] = None,
    num_epochs: int = 3,
    device: str = 'cuda'
):
    """
    å¿«é€Ÿå¯åŠ¨å¯è§†åŒ–è’¸é¦

    Args:
        student_model: å­¦ç”Ÿæ¨¡å‹
        teacher_model: æ•™å¸ˆæ¨¡å‹
        train_dataloader: è®­ç»ƒæ•°æ®
        tokenizer: åˆ†è¯å™¨
        config: é…ç½®ï¼ˆå¯é€‰ï¼‰
        num_epochs: è®­ç»ƒè½®æ•°
        device: è®¾å¤‡
    """
    if config is None:
        config = {
            'temperature': 4.0,
            'alpha': 0.7,
            'beta': 0.3,
            'show_samples': True,
            'sample_frequency': 50,
            'max_text_length': 100,
        }

    plugin = VisualDistillationPlugin(config)

    optimizer = torch.optim.AdamW(student_model.parameters(), lr=1e-4)

    plugin.visual_distill_model(
        student_model=student_model,
        teacher_model=teacher_model,
        train_dataloader=train_dataloader,
        optimizer=optimizer,
        tokenizer=tokenizer,
        num_epochs=num_epochs,
        device=device
    )


# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================

if __name__ == "__main__":
    print("ğŸ¨ å¯è§†åŒ–çŸ¥è¯†è’¸é¦æ’ä»¶æ¼”ç¤º\n")

    # é…ç½®
    config = {
        'temperature': 4.0,
        'alpha': 0.7,
        'beta': 0.3,
        'show_samples': True,
        'show_diff': False,
        'sample_frequency': 5,  # æ¼”ç¤ºç”¨ï¼Œé¢‘ç¹æ˜¾ç¤º
        'max_text_length': 80,
    }

    plugin = VisualDistillationPlugin(config)

    # æ¨¡æ‹Ÿæ•°æ®
    batch_size, seq_len, vocab_size = 4, 32, 50000

    print("æ¨¡æ‹Ÿè®­ç»ƒæµç¨‹...\n")

    plugin.print_header()
    plugin.print_epoch_header(1, 3)

    # æ¨¡æ‹Ÿå‡ ä¸ªæ ·æœ¬
    topics = ['äº’è”ç½‘', 'äººå·¥æ™ºèƒ½', 'åŒ»ç–—', 'æ•™è‚²']

    for i in range(3):
        teacher_text = f"è¿™æ˜¯å…³äº{topics[i]}çš„æ•™å¸ˆæ¨¡å‹è¾“å‡ºï¼ŒåŒ…å«ä¸°å¯Œçš„çŸ¥è¯†å’Œè¯¦ç»†çš„è§£é‡Š..."
        student_text = f"å…³äº{topics[i]}çš„å­¦ç”Ÿæ¨¡å‹è¾“å‡ºï¼Œæ­£åœ¨å­¦ä¹ æ•™å¸ˆçš„çŸ¥è¯†..."

        laziness = 70 - i * 20  # é€æ¸å˜å¥½
        loss = 2.0 - i * 0.5
        comment = plugin.generate_comment(laziness, loss)

        plugin.print_sample_comparison(
            epoch=1,
            batch_idx=i * 50,
            teacher_text=teacher_text,
            student_text=student_text,
            topic=topics[i],
            laziness=laziness,
            loss=loss,
            comment=comment
        )

    plugin.stats['avg_laziness'] = [70, 50, 30]
    plugin.stats['total_samples'] = 100

    plugin.print_epoch_summary(
        epoch=1,
        avg_loss=1.5,
        avg_laziness=50.0,
        topic_stats={'äº’è”ç½‘': 30, 'äººå·¥æ™ºèƒ½': 25, 'åŒ»ç–—': 20, 'æ•™è‚²': 15}
    )

    plugin.print_final_summary()

    print("\nâœ… æ¼”ç¤ºå®Œæˆï¼")
    print("\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
    print("   from apt_model.plugins.visual_distillation_plugin import quick_visual_distill")
    print("   quick_visual_distill(student_model, teacher_model, dataloader, tokenizer)")
