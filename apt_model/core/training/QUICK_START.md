# ğŸš€ APT-SOSA å¿«é€Ÿå…¥é—¨

5åˆ†é’Ÿè®©ä½ çš„è®­ç»ƒæ›´æ™ºèƒ½ï¼

---

## ğŸ“¦ ä½ è·å¾—äº†ä»€ä¹ˆ?

- âœ… **SOSAæ ¸å¿ƒç®—æ³•** - ç«ç§æºè‡ªç»„ç»‡
- âœ… **è®­ç»ƒç›‘æ§å™¨** - 7ç§é”™è¯¯æ£€æµ‹
- âœ… **è‡ªåŠ¨ä¿®å¤ç³»ç»Ÿ** - æ™ºèƒ½çº é”™
- âœ… **é›¶ä¾µå…¥é›†æˆ** - åŒ…è£…å³ç”¨

---

## ğŸ¯ 30ç§’é›†æˆ

### æ–¹å¼1: æœ€ç®€å• (æ¨è)

```python
from apt_sosa import wrap_training

# åŒ…è£…ä½ çš„è®­ç»ƒ
wrapper = wrap_training(model, optimizer, auto_fix=True)

# è®­ç»ƒå¾ªç¯ - åªéœ€è¿™ä¸€è¡Œæ”¹åŠ¨!
for batch in dataloader:
    loss = wrapper.training_step(batch)
```

### æ–¹å¼2: æ›´å¤šæ§åˆ¶

```python
from apt_sosa import SOSATrainingWrapper

wrapper = SOSATrainingWrapper(
    model=model,
    optimizer=optimizer,
    checkpoint_dir="./checkpoints",
    auto_fix=True,
    max_fixes_per_error=3
)

for batch in dataloader:
    # è‡ªå®šä¹‰å‰å‘å‡½æ•° (å¯é€‰)
    def my_forward(model, batch):
        return model(**batch).loss
    
    loss = wrapper.training_step(batch, forward_fn=my_forward)
```

---

## ğŸ“‹ å®Œæ•´è®­ç»ƒç¤ºä¾‹

```python
import torch
from torch import nn
from apt_sosa import wrap_training

# 1. å‡†å¤‡æ¨¡å‹å’Œæ•°æ®
model = YourModel()
optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)
train_dataloader = YourDataLoader()

# 2. åˆ›å»ºSOSAåŒ…è£…
wrapper = wrap_training(
    model=model,
    optimizer=optimizer,
    auto_fix=True  # å¯ç”¨è‡ªåŠ¨ä¿®å¤
)

# 3. è®­ç»ƒå¾ªç¯
for epoch in range(num_epochs):
    for batch in train_dataloader:
        # ä¸€è¡Œæå®š: å‰å‘+åå‘+ä¼˜åŒ–+ç›‘æ§+ä¿®å¤
        loss = wrapper.training_step(batch)
        
        if step % 100 == 0:
            print(f"Step {step}: loss={loss.item():.4f}")
    
    # æ¯ä¸ªepochåæ‰“å°æŠ¥å‘Š
    wrapper.print_report()

# 4. æœ€ç»ˆç»Ÿè®¡
print("\nè®­ç»ƒå®Œæˆ!")
wrapper.print_report()
```

---

## ğŸ’¡ å®ƒä¼šå¸®ä½ åšä»€ä¹ˆ?

### è‡ªåŠ¨æ£€æµ‹é—®é¢˜

âœ… NaN Loss  
âœ… æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±  
âœ… Losså‘æ•£/éœ‡è¡  
âœ… Lossåœæ»  
âœ… OOM  

### è‡ªåŠ¨ä¿®å¤

å½“æ£€æµ‹åˆ°é—®é¢˜æ—¶:

```
[Step 1523] æ£€æµ‹åˆ°å¼‚å¸¸: exploding_gradient

è¯Šæ–­:
  å¯èƒ½åŸå› :
    - å­¦ä¹ ç‡è¿‡å¤§
    - æ¢¯åº¦è£å‰ªä¸è¶³
    - å½“å‰æ¢¯åº¦èŒƒæ•°: 156.32

å»ºè®®ä¿®å¤: clip_grad
  å‚æ•°: {'max_norm': 1.0}
  ç½®ä¿¡åº¦: 0.95

åº”ç”¨è‡ªåŠ¨ä¿®å¤...
âœ“ ä¿®å¤æˆåŠŸ!
æ¢¯åº¦è£å‰ªå·²æ›´æ–°: 1.0
```

---

## ğŸ“Š æŸ¥çœ‹è®­ç»ƒæŠ¥å‘Š

```python
wrapper.print_report()
```

è¾“å‡ºç¤ºä¾‹:
```
================================================================================
APT-SOSA è®­ç»ƒæŠ¥å‘Š
================================================================================

è®­ç»ƒè¿›åº¦:
  å½“å‰æ­¥æ•°: 5000
  æœ€ä½³Loss: 0.3245
  æœ€ä½³æ£€æŸ¥ç‚¹: ./checkpoints/checkpoint_best.pt

è‡ªåŠ¨ä¿®å¤ç»Ÿè®¡:
  exploding_gradient: 2 æ¬¡
  nan_loss: 1 æ¬¡

å½“å‰é…ç½®:
  å­¦ä¹ ç‡: 5.00e-05
  æ¢¯åº¦è£å‰ª: 1.0

======================================================================
è®­ç»ƒç›‘æ§æŠ¥å‘Š
======================================================================

è®­ç»ƒè¿›åº¦:
  æ€»æ­¥æ•°: 5000
  å¼‚å¸¸æ­¥æ•°: 47
  å¼‚å¸¸ç‡: 0.94%

é”™è¯¯ç»Ÿè®¡:
  exploding_gradient: 2 æ¬¡
  nan_loss: 1 æ¬¡

ä¿®å¤å†å²: 3 æ¬¡
  æœ€è¿‘5æ¬¡ä¿®å¤:
    Step 1523: clip_grad - æ¢¯åº¦çˆ†ç‚¸: å¼ºåŒ–æ¢¯åº¦è£å‰ª
    Step 2891: reduce_lr - NaN loss: å¤§å¹…é™ä½å­¦ä¹ ç‡å¹¶å›æ»š
    Step 3445: clip_grad - æ¢¯åº¦çˆ†ç‚¸: å¼ºåŒ–æ¢¯åº¦è£å‰ª

è¿‘æœŸLoss:
  å‡å€¼: 0.4521
  æ ‡å‡†å·®: 0.0832
  æœ€å°å€¼: 0.3245
```

---

## ğŸ”§ é›†æˆåˆ°APT

### ä¿®æ”¹ trainer.py

```python
# åœ¨ train_model() å‡½æ•°å¼€å§‹å¤„
from apt_model.apt_sosa import SOSATrainingWrapper

def train_model(model, config, train_dataset, ...):
    # ... åˆ›å»ºoptimizerç­‰ ...
    
    # æ·»åŠ è¿™å‡ è¡Œ
    sosa_wrapper = SOSATrainingWrapper(
        model=model,
        optimizer=optimizer,
        config=config,
        checkpoint_dir=config.output_dir,
        auto_fix=True
    )
    
    # è®­ç»ƒå¾ªç¯ä¸­
    for batch in train_dataloader:
        # åŸæ¥: loss = model(**batch).loss; loss.backward(); optimizer.step()
        
        # æ”¹ä¸º: (åŒ…å«äº†æ‰€æœ‰è®­ç»ƒæ­¥éª¤)
        def forward_fn(model, batch):
            return model(**batch).loss
        
        loss = sosa_wrapper.training_step(batch, forward_fn)
```

### æ·»åŠ å‘½ä»¤è¡Œå‚æ•°

```python
# åœ¨ parser.py ä¸­
parser.add_argument('--use-sosa', action='store_true')
parser.add_argument('--sosa-auto-fix', action='store_true', default=True)
```

### ä½¿ç”¨

```bash
# å¯ç”¨SOSA
python main.py train --use-sosa --sosa-auto-fix

# æˆ–ä¿®æ”¹config
config.sosa.enabled = True
config.sosa.auto_fix = True
```

---

## ğŸ“ è¿›é˜¶ç”¨æ³•

### 1. è‡ªå®šä¹‰å‰å‘å‡½æ•°

```python
def my_custom_forward(model, batch):
    # ä½ çš„è‡ªå®šä¹‰é€»è¾‘
    outputs = model.encoder(batch['input'])
    outputs = model.decoder(outputs)
    loss = custom_loss_function(outputs, batch['target'])
    return loss

loss = wrapper.training_step(batch, forward_fn=my_custom_forward)
```

### 2. ç¦ç”¨è‡ªåŠ¨ä¿®å¤ (ä»…ç›‘æ§)

```python
wrapper = wrap_training(
    model, optimizer,
    auto_fix=False  # åªç›‘æ§ï¼Œä¸ä¿®å¤
)

# æ‰‹åŠ¨å¤„ç†
error = wrapper.monitor.detect_error()
if error:
    fix = wrapper.monitor.suggest_fix(error)
    print(f"å»ºè®®: {fix.action_type}")
    # å†³å®šæ˜¯å¦åº”ç”¨...
```

### 3. å®šæœŸä¿å­˜æŠ¥å‘Š

```python
import json

# æ¯1000æ­¥ä¿å­˜ç»Ÿè®¡
if step % 1000 == 0:
    stats = wrapper.get_statistics()
    with open(f'stats_step_{step}.json', 'w') as f:
        json.dump(stats, f, indent=2)
```

---

## ğŸ¯ æœ€ä½³å®è·µ

1. âœ… **å¯ç”¨è‡ªåŠ¨ä¿®å¤** - `auto_fix=True`
2. âœ… **å®šæœŸæ‰“å°æŠ¥å‘Š** - æ¯1000æ­¥æˆ–æ¯epoch
3. âœ… **ä¿ç•™æœ€ä½³æ£€æŸ¥ç‚¹** - è‡ªåŠ¨ä¿å­˜
4. âœ… **é™åˆ¶ä¿®å¤æ¬¡æ•°** - `max_fixes_per_error=3`
5. âœ… **ç›‘æ§çª—å£é€‚ä¸­** - 10-30ç§’

---

## â“ å¸¸è§é—®é¢˜

**Q: ä¼šä¸ä¼šå½±å“è®­ç»ƒé€Ÿåº¦?**

A: å‡ ä¹æ²¡æœ‰å½±å“ (<1%)ï¼Œç›‘æ§æ˜¯è½»é‡çº§çš„ã€‚

**Q: å¦‚ä½•ç¦ç”¨?**

A: ä¸ä½¿ç”¨wrapperï¼Œæˆ–è®¾ç½® `auto_fix=False`ã€‚

**Q: ä¿®å¤ä¼šä¸ä¼šå‡ºé”™?**

A: æœ‰confidenceæ£€æŸ¥ï¼Œä¸”é™åˆ¶ä¿®å¤æ¬¡æ•°ã€‚å¤±è´¥ä¼šè®°å½•ã€‚

**Q: æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒå—?**

A: æ”¯æŒï¼Œæ¯ä¸ªè¿›ç¨‹ç‹¬ç«‹ç›‘æ§ã€‚

---

## ğŸ“š æ›´å¤šèµ„æº

- ğŸ“– [å®Œæ•´æ–‡æ¡£](README.md)
- ğŸ’» [APIå‚è€ƒ](README.md#apiæ–‡æ¡£)
- ğŸ”§ [é›†æˆæŒ‡å—](README.md#é›†æˆæŒ‡å—)
- ğŸ“Š [æ•ˆæœå¯¹æ¯”](README.md#æ•ˆæœå¯¹æ¯”)

---

## ğŸ‰ å¼€å§‹ä½¿ç”¨

```bash
# æµ‹è¯•SOSA
cd apt_sosa
python __init__.py

# æŸ¥çœ‹ç¤ºä¾‹
cat __init__.py  # å†…å«å¿«é€Ÿå¼€å§‹ç¤ºä¾‹
```

---

<div align="center">

**ğŸš€ ç°åœ¨å°±è®©ä½ çš„è®­ç»ƒæ›´æ™ºèƒ½ï¼**

åªéœ€30ç§’é›†æˆ â†’ è‡ªåŠ¨ç›‘æ§ â†’ è‡ªåŠ¨ä¿®å¤

Made with â¤ï¸ by chen0430tw

</div>
