#!/usr/bin/env python3
"""
APT Non-Autopoietic Control Version
ç”¨äºå¯¹ç…§ç»„å®éªŒçš„éè‡ªç”ŸæˆAPTæ¨¡å‹

ç”¨é€”ï¼š
- åˆ†æHLBDåœ¨è‡ªç”Ÿæˆvsæ ‡å‡†Transformeræ¶æ„ä¸‹çš„é€‚ç”¨æ€§
- å¯¹ç…§ç»„å®éªŒï¼ŒéªŒè¯è‡ªç”Ÿæˆæœºåˆ¶çš„æœ‰æ•ˆæ€§
- ç ”ç©¶autopoietic attentionçš„è´¡çŒ®åº¦

ä½¿ç”¨æ–¹æ³•ï¼š
    from apt_model.modeling.apt_control import create_control_model, create_autopoietic_model

    # åˆ›å»ºå¯¹ç…§ç»„æ¨¡å‹ (æ— è‡ªç”Ÿæˆæœºåˆ¶)
    control_model = create_control_model(vocab_size=2000, d_model=512)

    # åˆ›å»ºå®éªŒç»„æ¨¡å‹ (æœ‰è‡ªç”Ÿæˆæœºåˆ¶)
    autopoietic_model = create_autopoietic_model(vocab_size=2000, d_model=512)
"""

from apt_model.utils.fake_torch import get_torch
torch = get_torch()
from apt_model.utils.fake_torch import get_torch
torch = get_torch()
nn = torch.nn
from typing import Optional, Dict, Any
from .apt_model import APTModel, APTModelConfiguration


def create_control_model(
    vocab_size: int = 2000,
    d_model: int = 512,
    n_layers: int = 12,
    n_heads: int = 8,
    d_ff: int = 2048,
    max_seq_len: int = 512,
    dropout: float = 0.1,
    use_dbc_dac: bool = True,
    **kwargs
) -> APTModel:
    """
    åˆ›å»ºéè‡ªç”ŸæˆAPTæ¨¡å‹ (å¯¹ç…§ç»„)

    å…³é”®å·®å¼‚ï¼š
    - use_autopoietic=Falseï¼šç¦ç”¨è‡ªç”Ÿæˆæ³¨æ„åŠ›æœºåˆ¶
    - ä¿ç•™æ‰€æœ‰å…¶ä»–æ¶æ„ç‰¹æ€§ï¼ˆDBC-DAC, ä½ç½®ç¼–ç ç­‰ï¼‰

    Args:
        vocab_size: è¯æ±‡è¡¨å¤§å°
        d_model: æ¨¡å‹ç»´åº¦
        n_layers: å±‚æ•°
        n_heads: æ³¨æ„åŠ›å¤´æ•°
        d_ff: å‰é¦ˆç½‘ç»œç»´åº¦
        max_seq_len: æœ€å¤§åºåˆ—é•¿åº¦
        dropout: Dropoutæ¯”ç‡
        use_dbc_dac: æ˜¯å¦ä½¿ç”¨DBC-DACç¨³å®š
        **kwargs: å…¶ä»–é…ç½®å‚æ•°

    Returns:
        APTModelå®ä¾‹ï¼ˆuse_autopoietic=Falseï¼‰
    """
    config = APTModelConfiguration(
        vocab_size=vocab_size,
        d_model=d_model,
        num_encoder_layers=n_layers,
        num_decoder_layers=n_layers,
        num_heads=n_heads,
        d_ff=d_ff,
        max_seq_len=max_seq_len,
        dropout=dropout,
        use_autopoietic=False,  # ğŸ”´ å…³é”®ï¼šç¦ç”¨è‡ªç”Ÿæˆæœºåˆ¶
        use_dbc_dac=use_dbc_dac,
        **kwargs
    )

    model = APTModel(config)
    print(f"âœ… åˆ›å»ºéè‡ªç”ŸæˆAPTæ¨¡å‹ (å¯¹ç…§ç»„)")
    print(f"   - è¯æ±‡è¡¨: {vocab_size}")
    print(f"   - ç»´åº¦: {d_model}")
    print(f"   - å±‚æ•°: {n_layers}")
    print(f"   - è‡ªç”Ÿæˆæœºåˆ¶: âŒ ç¦ç”¨")
    print(f"   - DBC-DAC: {'âœ“' if use_dbc_dac else 'âœ—'}")

    return model


def create_autopoietic_model(
    vocab_size: int = 2000,
    d_model: int = 512,
    n_layers: int = 12,
    n_heads: int = 8,
    d_ff: int = 2048,
    max_seq_len: int = 512,
    dropout: float = 0.1,
    use_dbc_dac: bool = True,
    epsilon: float = 1e-6,
    alpha: float = 0.1,
    init_tau: float = 1.0,
    sr_ratio: int = 4,
    **kwargs
) -> APTModel:
    """
    åˆ›å»ºè‡ªç”ŸæˆAPTæ¨¡å‹ (å®éªŒç»„)

    å…³é”®å·®å¼‚ï¼š
    - use_autopoietic=Trueï¼šå¯ç”¨è‡ªç”Ÿæˆæ³¨æ„åŠ›æœºåˆ¶
    - åŒ…å«æ‰€æœ‰è‡ªç”Ÿæˆç›¸å…³å‚æ•°ï¼ˆepsilon, alpha, tau, sr_ratioï¼‰

    Args:
        vocab_size: è¯æ±‡è¡¨å¤§å°
        d_model: æ¨¡å‹ç»´åº¦
        n_layers: å±‚æ•°
        n_heads: æ³¨æ„åŠ›å¤´æ•°
        d_ff: å‰é¦ˆç½‘ç»œç»´åº¦
        max_seq_len: æœ€å¤§åºåˆ—é•¿åº¦
        dropout: Dropoutæ¯”ç‡
        use_dbc_dac: æ˜¯å¦ä½¿ç”¨DBC-DACç¨³å®š
        epsilon: è‡ªç”Ÿæˆæ— ç©·å€’æ•°ç¼©æ”¾å› å­
        alpha: æ³°å‹’å±•å¼€ç³»æ•°
        init_tau: åˆå§‹æ¸©åº¦å‚æ•°
        sr_ratio: è‡ªç”ŸæˆçŸ©é˜µå‹ç¼©æ¯”
        **kwargs: å…¶ä»–é…ç½®å‚æ•°

    Returns:
        APTModelå®ä¾‹ï¼ˆuse_autopoietic=Trueï¼‰
    """
    config = APTModelConfiguration(
        vocab_size=vocab_size,
        d_model=d_model,
        num_encoder_layers=n_layers,
        num_decoder_layers=n_layers,
        num_heads=n_heads,
        d_ff=d_ff,
        max_seq_len=max_seq_len,
        dropout=dropout,
        use_autopoietic=True,  # ğŸŸ¢ å…³é”®ï¼šå¯ç”¨è‡ªç”Ÿæˆæœºåˆ¶
        epsilon=epsilon,
        alpha=alpha,
        init_tau=init_tau,
        sr_ratio=sr_ratio,
        use_dbc_dac=use_dbc_dac,
        **kwargs
    )

    model = APTModel(config)
    print(f"âœ… åˆ›å»ºè‡ªç”ŸæˆAPTæ¨¡å‹ (å®éªŒç»„)")
    print(f"   - è¯æ±‡è¡¨: {vocab_size}")
    print(f"   - ç»´åº¦: {d_model}")
    print(f"   - å±‚æ•°: {n_layers}")
    print(f"   - è‡ªç”Ÿæˆæœºåˆ¶: âœ“ å¯ç”¨")
    print(f"   - DBC-DAC: {'âœ“' if use_dbc_dac else 'âœ—'}")
    print(f"   - å‚æ•°: Îµ={epsilon}, Î±={alpha}, Ï„={init_tau}, sr_ratio={sr_ratio}")

    return model


def compare_model_architectures(
    control_model: APTModel,
    autopoietic_model: APTModel
) -> Dict[str, Any]:
    """
    æ¯”è¾ƒå¯¹ç…§ç»„å’Œå®éªŒç»„æ¨¡å‹çš„æ¶æ„å·®å¼‚

    Args:
        control_model: éè‡ªç”Ÿæˆæ¨¡å‹
        autopoietic_model: è‡ªç”Ÿæˆæ¨¡å‹

    Returns:
        åŒ…å«æ¯”è¾ƒç»“æœçš„å­—å…¸
    """
    def count_parameters(model):
        total = sum(p.numel() for p in model.parameters())
        trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
        return total, trainable

    control_total, control_trainable = count_parameters(control_model)
    auto_total, auto_trainable = count_parameters(autopoietic_model)

    comparison = {
        "control_model": {
            "total_params": control_total,
            "trainable_params": control_trainable,
            "use_autopoietic": control_model.config.use_autopoietic
        },
        "autopoietic_model": {
            "total_params": auto_total,
            "trainable_params": auto_trainable,
            "use_autopoietic": autopoietic_model.config.use_autopoietic
        },
        "difference": {
            "total_params": auto_total - control_total,
            "trainable_params": auto_trainable - control_trainable,
            "percentage": ((auto_total - control_total) / control_total * 100)
        }
    }

    print("\n" + "=" * 60)
    print("æ¨¡å‹æ¶æ„å¯¹æ¯”")
    print("=" * 60)
    print(f"\nå¯¹ç…§ç»„ (æ— è‡ªç”Ÿæˆ):")
    print(f"  æ€»å‚æ•°: {control_total:,}")
    print(f"  å¯è®­ç»ƒ: {control_trainable:,}")
    print(f"\nå®éªŒç»„ (æœ‰è‡ªç”Ÿæˆ):")
    print(f"  æ€»å‚æ•°: {auto_total:,}")
    print(f"  å¯è®­ç»ƒ: {auto_trainable:,}")
    print(f"\nå‚æ•°å·®å¼‚:")
    print(f"  å¢åŠ : {auto_total - control_total:,} ({comparison['difference']['percentage']:.2f}%)")

    return comparison


# ============================================================================
# å¯¹ç…§å®éªŒè¾…åŠ©å‡½æ•°
# ============================================================================

def create_control_experiment_pair(
    vocab_size: int = 2000,
    d_model: int = 512,
    n_layers: int = 12,
    n_heads: int = 8,
    **kwargs
):
    """
    åˆ›å»ºä¸€å¯¹å¯¹ç…§å®éªŒæ¨¡å‹ï¼ˆå¯¹ç…§ç»„ + å®éªŒç»„ï¼‰

    è¿”å›ï¼š
        (control_model, autopoietic_model, comparison_dict)
    """
    print("\n" + "=" * 60)
    print("åˆ›å»ºå¯¹ç…§å®éªŒæ¨¡å‹å¯¹")
    print("=" * 60 + "\n")

    # åˆ›å»ºå¯¹ç…§ç»„
    control_model = create_control_model(
        vocab_size=vocab_size,
        d_model=d_model,
        n_layers=n_layers,
        n_heads=n_heads,
        **kwargs
    )

    print()

    # åˆ›å»ºå®éªŒç»„
    autopoietic_model = create_autopoietic_model(
        vocab_size=vocab_size,
        d_model=d_model,
        n_layers=n_layers,
        n_heads=n_heads,
        **kwargs
    )

    # æ¯”è¾ƒ
    comparison = compare_model_architectures(control_model, autopoietic_model)

    return control_model, autopoietic_model, comparison


def save_control_experiment_models(
    control_model: APTModel,
    autopoietic_model: APTModel,
    save_dir: str = "control_experiments"
):
    """
    ä¿å­˜å¯¹ç…§å®éªŒçš„ä¸¤ä¸ªæ¨¡å‹

    Args:
        control_model: å¯¹ç…§ç»„æ¨¡å‹
        autopoietic_model: å®éªŒç»„æ¨¡å‹
        save_dir: ä¿å­˜ç›®å½•
    """
    import os
    from pathlib import Path

    save_path = Path(save_dir)
    save_path.mkdir(parents=True, exist_ok=True)

    # ä¿å­˜å¯¹ç…§ç»„
    control_path = save_path / "control_model.pt"
    torch.save({
        'model_state_dict': control_model.state_dict(),
        'config': control_model.config.to_dict(),
        'model_type': 'control'
    }, control_path)
    print(f"âœ… å¯¹ç…§ç»„æ¨¡å‹å·²ä¿å­˜: {control_path}")

    # ä¿å­˜å®éªŒç»„
    autopoietic_path = save_path / "autopoietic_model.pt"
    torch.save({
        'model_state_dict': autopoietic_model.state_dict(),
        'config': autopoietic_model.config.to_dict(),
        'model_type': 'autopoietic'
    }, autopoietic_path)
    print(f"âœ… å®éªŒç»„æ¨¡å‹å·²ä¿å­˜: {autopoietic_path}")


# ============================================================================
# ä½¿ç”¨ç¤ºä¾‹
# ============================================================================

if __name__ == "__main__":
    print("APT Control Group Experiment Utility")
    print("=" * 60)

    # åˆ›å»ºå¯¹ç…§å®éªŒæ¨¡å‹å¯¹
    control, autopoietic, comparison = create_control_experiment_pair(
        vocab_size=2000,
        d_model=512,
        n_layers=12,
        n_heads=8,
        use_dbc_dac=True
    )

    # ä¿å­˜æ¨¡å‹
    save_control_experiment_models(control, autopoietic)

    print("\n" + "=" * 60)
    print("âœ¨ å¯¹ç…§å®éªŒæ¨¡å‹åˆ›å»ºå®Œæˆï¼")
    print("=" * 60)
    print("\nä½¿ç”¨æ–¹æ³•:")
    print("  1. ç”¨control_modelè®­ç»ƒ (æ— è‡ªç”Ÿæˆæœºåˆ¶)")
    print("  2. ç”¨autopoietic_modelè®­ç»ƒ (æœ‰è‡ªç”Ÿæˆæœºåˆ¶)")
    print("  3. å¯¹æ¯”ä¸¤è€…åœ¨HLBDä»»åŠ¡ä¸Šçš„è¡¨ç°")
    print("  4. åˆ†æè‡ªç”Ÿæˆæœºåˆ¶çš„è´¡çŒ®åº¦")
