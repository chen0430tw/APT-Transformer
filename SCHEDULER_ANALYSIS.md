# APT-Transformer é…ç½®ä¸è°ƒåº¦ç³»ç»Ÿåˆ†ææŠ¥å‘Š

## ğŸ“Š ç°çŠ¶åˆ†æ

### âœ… å·²å®ç°çš„åŠŸèƒ½

#### 1. **é…ç½®ç³»ç»Ÿ** (`apt/core/config.py`)
- âœ… YAMLé…ç½®æ–‡ä»¶æ”¯æŒ
- âœ… `APTConfig.from_yaml()` åŠ è½½é…ç½®
- âœ… `schedules` å­—æ®µå®šä¹‰ï¼ˆDict[str, Any]ï¼‰
- âœ… æ’ä»¶åˆ—è¡¨é…ç½®
- âœ… æä¾›å•†é…ç½®ï¼ˆattention_name, ffn_name, router_nameç­‰ï¼‰

#### 2. **YAMLé…ç½®ç¤ºä¾‹** (`examples/profiles/`)
**`gpt5_moe_reasoning.yaml` å·²åŒ…å«å®Œæ•´çš„è¯¾ç¨‹è°ƒåº¦**:
```yaml
schedules:
  # æ’ä»¶å¯ç”¨æ—¶æœº
  enable_moe_at_epoch: 2        # epoch=2å¯ç”¨MoE
  enable_align_at_epoch: 3      # epoch=3å¯ç”¨å¯¹é½
  enable_voter_at_epoch: 5      # epoch=5å¯ç”¨æŠ•ç¥¨

  # å‚æ•°é€€ç«
  route_temp:
    start: 1.5
    end: 0.8
    by: "epoch"

  moe_capacity:
    start: 1.5
    end: 1.1
    by: "epoch"

  align_weight:
    start: 0.0
    end: 0.3
    by: "step"
    warmup: 5000
```

#### 3. **è®­ç»ƒå™¨** (`apt_model/training/trainer.py`)
- âœ… åŸºæœ¬è®­ç»ƒå¾ªç¯
- âœ… å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆLR schedulerï¼‰
- âœ… æ¢¯åº¦ç´¯ç§¯
- âœ… æ··åˆç²¾åº¦è®­ç»ƒ

---

### âŒ ç¼ºå¤±çš„å…³é”®åŠŸèƒ½

#### 1. **Callback/Hook æœºåˆ¶**
å½“å‰è®­ç»ƒå™¨**æ²¡æœ‰å›è°ƒç³»ç»Ÿ**æ¥æ‰§è¡Œ `schedules` é…ç½®ï¼š

```python
# å½“å‰ç¼ºå¤±ï¼š
def on_train_epoch_start(epoch, config, modules):
    """æ¯ä¸ªepochå¼€å§‹æ—¶çš„é’©å­"""
    pass

def on_train_batch_end(batch_idx, stats, modules):
    """æ¯ä¸ªbatchç»“æŸåçš„é’©å­"""
    pass

def on_step(step, config, modules):
    """æ¯ä¸ªä¼˜åŒ–æ­¥éª¤çš„é’©å­"""
    pass
```

#### 2. **Schedule æ‰§è¡Œå™¨**
æ²¡æœ‰ä»£ç è¯»å– `config.schedules` å¹¶æ‰§è¡Œç›¸åº”æ“ä½œï¼š

```python
# ç¼ºå¤±çš„è°ƒåº¦é€»è¾‘ï¼š
if epoch == config.schedules.get('enable_moe_at_epoch'):
    modules['moe'].enable(True)
```

#### 3. **åŠ¨æ€æ¨¡å—å¯ç”¨/ç¦ç”¨**
æ¨¡å—ï¼ˆMoEã€Alignã€Voterç­‰ï¼‰æ²¡æœ‰ `enable()`/`disable()` æ¥å£ï¼š

```python
# éœ€è¦åœ¨å„æ¨¡å—ä¸­å®ç°ï¼š
class MoELayer:
    def enable(self, enabled: bool):
        self.enabled = enabled
```

#### 4. **å‚æ•°é€€ç«è°ƒåº¦å™¨**
ç¼ºå°‘é€šç”¨çš„å‚æ•°æ’å€¼å™¨ï¼ˆlerp/cosineç­‰ï¼‰ï¼š

```python
# ç¼ºå¤±ï¼š
def lerp(start, end, t):
    """çº¿æ€§æ’å€¼"""
    return start + (end - start) * t

def cosine_anneal(start, end, t):
    """ä½™å¼¦é€€ç«"""
    return end + (start - end) * (1 + math.cos(math.pi * t)) / 2
```

---

## ğŸ—ï¸ ä¸»æµåšæ³•å¯¹æ¯”

### DeepSpeed / Megatron-LM æ¶æ„
```
é…ç½®æ–‡ä»¶ (YAML/JSON)
    â†“
å¯åŠ¨å™¨ (deepspeed/torchrun)
    â†“
Trainer + Callbacks/Hooks
    â†“
on_epoch_start() â†’ è¯»å–schedule â†’ å¯ç”¨æ¨¡å—/è°ƒæ•´å‚æ•°
```

### HuggingFace Trainer
```python
class CustomCallback(TrainerCallback):
    def on_epoch_begin(self, args, state, control, **kwargs):
        if state.epoch == 2:
            model.moe.enable(True)
```

### PyTorch Lightning
```python
class APTLightningModule(LightningModule):
    def on_train_epoch_start(self):
        if self.current_epoch == 2:
            self.model.moe.enable(True)
```

**APTå½“å‰æ¶æ„ç¼ºå°‘è¿™ä¸€å±‚ï¼**

---

## ğŸš€ æ”¹è¿›æ–¹æ¡ˆ

### æ–¹æ¡ˆ1: è½»é‡çº§Callbackç³»ç»Ÿï¼ˆæ¨èå¿«é€Ÿå®ç°ï¼‰

#### 1.1 åˆ›å»º `apt_model/training/callbacks.py`

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
APT Training Callbacks and Schedulers

Implements curriculum scheduling and dynamic module control.
"""

import math
import logging
from typing import Dict, Any, Optional, Callable, List
from dataclasses import dataclass

logger = logging.getLogger(__name__)


# ============================================================================
# Schedule Utilities
# ============================================================================

def lerp(start: float, end: float, t: float) -> float:
    """Linear interpolation."""
    return start + (end - start) * t


def cosine_anneal(start: float, end: float, t: float) -> float:
    """Cosine annealing."""
    return end + (start - end) * (1 + math.cos(math.pi * t)) / 2


def get_interpolator(schedule_type: str = "linear") -> Callable:
    """Get interpolation function by type."""
    interpolators = {
        "linear": lerp,
        "cosine": cosine_anneal,
    }
    return interpolators.get(schedule_type, lerp)


# ============================================================================
# Schedule Executor
# ============================================================================

class ScheduleExecutor:
    """
    Executes curriculum schedules defined in config.

    Example config.schedules:
        {
            "enable_moe_at_epoch": 2,
            "enable_align_at_epoch": 3,
            "route_temp": {
                "start": 1.5,
                "end": 0.8,
                "by": "epoch"
            }
        }
    """

    def __init__(self, config, modules: Dict[str, Any], total_epochs: int, total_steps: int):
        """
        Initialize schedule executor.

        Args:
            config: APTConfig with schedules field
            modules: Dict of modules (e.g., {'moe': moe_layer, 'align': align_layer})
            total_epochs: Total number of training epochs
            total_steps: Total number of training steps
        """
        self.config = config
        self.schedules = config.schedules if hasattr(config, 'schedules') else {}
        self.modules = modules
        self.total_epochs = total_epochs
        self.total_steps = total_steps

        logger.info(f"ScheduleExecutor initialized with {len(self.schedules)} schedules")

    def on_epoch_start(self, epoch: int):
        """Execute schedules at epoch start."""
        # Enable modules at specific epochs
        for key, value in self.schedules.items():
            if key.startswith("enable_") and key.endswith("_at_epoch"):
                if epoch == value:
                    module_name = key.replace("enable_", "").replace("_at_epoch", "")
                    if module_name in self.modules:
                        self.modules[module_name].enable(True)
                        logger.info(f"[Epoch {epoch}] Enabled module: {module_name}")

        # Update parameters with epoch-based schedules
        t = epoch / max(self.total_epochs, 1)
        self._update_parameters(t, by="epoch")

    def on_step(self, step: int):
        """Execute schedules at training step."""
        t = step / max(self.total_steps, 1)
        self._update_parameters(t, by="step")

    def _update_parameters(self, t: float, by: str):
        """Update parameters based on schedule."""
        for key, schedule in self.schedules.items():
            if not isinstance(schedule, dict):
                continue

            if schedule.get("by") != by:
                continue

            # Get start, end values
            start = schedule.get("start")
            end = schedule.get("end")
            if start is None or end is None:
                continue

            # Handle warmup
            warmup = schedule.get("warmup", 0)
            if by == "step" and warmup > 0:
                if t * self.total_steps < warmup:
                    # Warmup phase: 0 -> start
                    value = lerp(0, start, (t * self.total_steps) / warmup)
                else:
                    # Main phase: start -> end
                    t_main = (t * self.total_steps - warmup) / (self.total_steps - warmup)
                    interpolator = get_interpolator(schedule.get("type", "linear"))
                    value = interpolator(start, end, t_main)
            else:
                # No warmup: start -> end
                interpolator = get_interpolator(schedule.get("type", "linear"))
                value = interpolator(start, end, t)

            # Apply value to module
            self._apply_value(key, value)

    def _apply_value(self, param_name: str, value: float):
        """Apply scheduled value to module parameter."""
        # Parse param_name (e.g., "route_temp" -> module=router, attr=temperature)
        param_mapping = {
            "route_temp": ("router", "temperature"),
            "moe_capacity": ("moe", "capacity_factor"),
            "align_weight": ("align", "loss_weight"),
            "vote_threshold": ("voter", "entropy_threshold"),
        }

        if param_name in param_mapping:
            module_name, attr_name = param_mapping[param_name]
            if module_name in self.modules:
                module = self.modules[module_name]
                if hasattr(module, f"set_{attr_name}"):
                    getattr(module, f"set_{attr_name}")(value)
                    logger.debug(f"Updated {module_name}.{attr_name} = {value:.4f}")


# ============================================================================
# Callback Interface
# ============================================================================

class TrainingCallback:
    """Base callback class."""

    def on_train_begin(self, **kwargs):
        """Called at the beginning of training."""
        pass

    def on_train_end(self, **kwargs):
        """Called at the end of training."""
        pass

    def on_epoch_begin(self, epoch: int, **kwargs):
        """Called at the beginning of each epoch."""
        pass

    def on_epoch_end(self, epoch: int, metrics: Dict, **kwargs):
        """Called at the end of each epoch."""
        pass

    def on_batch_begin(self, batch_idx: int, **kwargs):
        """Called at the beginning of each batch."""
        pass

    def on_batch_end(self, batch_idx: int, loss: float, **kwargs):
        """Called at the end of each batch."""
        pass

    def on_step(self, step: int, **kwargs):
        """Called after each optimization step."""
        pass


class ScheduleCallback(TrainingCallback):
    """Callback that executes curriculum schedules."""

    def __init__(self, schedule_executor: ScheduleExecutor):
        self.executor = schedule_executor

    def on_epoch_begin(self, epoch: int, **kwargs):
        self.executor.on_epoch_start(epoch)

    def on_step(self, step: int, **kwargs):
        self.executor.on_step(step)


class EntropyBasedVotingCallback(TrainingCallback):
    """Callback that enables voting based on batch entropy."""

    def __init__(self, modules: Dict, threshold: float = 2.2):
        self.modules = modules
        self.threshold = threshold

    def on_batch_end(self, batch_idx: int, loss: float, entropy: Optional[float] = None, **kwargs):
        if entropy is not None and 'voter' in self.modules:
            if entropy > self.threshold:
                self.modules['voter'].enable(True)
            else:
                self.modules['voter'].enable(False)


# ============================================================================
# Callback Manager
# ============================================================================

class CallbackManager:
    """Manages multiple callbacks."""

    def __init__(self, callbacks: List[TrainingCallback]):
        self.callbacks = callbacks

    def trigger(self, event: str, **kwargs):
        """Trigger an event on all callbacks."""
        for callback in self.callbacks:
            method = getattr(callback, event, None)
            if method:
                method(**kwargs)
```

#### 1.2 ä¿®æ”¹ `trainer.py` é›†æˆCallbacks

```python
from apt_model.training.callbacks import (
    CallbackManager,
    ScheduleExecutor,
    ScheduleCallback,
    EntropyBasedVotingCallback
)

def train_model(...):
    # ... ç°æœ‰ä»£ç  ...

    # åˆ›å»ºæ¨¡å—å­—å…¸
    modules = {
        'moe': model.moe_layer if hasattr(model, 'moe_layer') else None,
        'align': model.align_layer if hasattr(model, 'align_layer') else None,
        'voter': model.voter if hasattr(model, 'voter') else None,
        'router': model.router if hasattr(model, 'router') else None,
    }
    modules = {k: v for k, v in modules.items() if v is not None}

    # åˆ›å»ºè°ƒåº¦æ‰§è¡Œå™¨
    total_steps = epochs * len(dataloader)
    schedule_executor = ScheduleExecutor(config, modules, epochs, total_steps)

    # åˆ›å»ºå›è°ƒ
    callbacks = [
        ScheduleCallback(schedule_executor),
        EntropyBasedVotingCallback(modules, threshold=2.2)
    ]
    callback_manager = CallbackManager(callbacks)

    # è§¦å‘è®­ç»ƒå¼€å§‹
    callback_manager.trigger('on_train_begin')

    # è®­ç»ƒå¾ªç¯
    for epoch in range(epochs):
        # è§¦å‘epochå¼€å§‹
        callback_manager.trigger('on_epoch_begin', epoch=epoch)

        for i, (src_ids, tgt_ids) in enumerate(dataloader):
            # è§¦å‘batchå¼€å§‹
            callback_manager.trigger('on_batch_begin', batch_idx=i)

            # ... ç°æœ‰è®­ç»ƒä»£ç  ...

            # è§¦å‘batchç»“æŸ
            callback_manager.trigger('on_batch_end', batch_idx=i, loss=loss_value)

            # è§¦å‘stepï¼ˆä¼˜åŒ–æ­¥éª¤ï¼‰
            if (i + 1) % accumulation_steps == 0:
                global_step += 1
                callback_manager.trigger('on_step', step=global_step)

        # è§¦å‘epochç»“æŸ
        callback_manager.trigger('on_epoch_end', epoch=epoch, metrics={})

    # è§¦å‘è®­ç»ƒç»“æŸ
    callback_manager.trigger('on_train_end')
```

### æ–¹æ¡ˆ2: PyTorch Lightning é‡æ„ï¼ˆæ¨èé•¿æœŸï¼‰

è¿ç§»åˆ° Lightning å¯ä»¥è·å¾—ï¼š
- å®Œæ•´çš„ Callback ç”Ÿæ€
- è‡ªåŠ¨åˆ†å¸ƒå¼è®­ç»ƒ
- Checkpoint ç®¡ç†
- æ—¥å¿—é›†æˆï¼ˆW&B, TensorBoardï¼‰

### æ–¹æ¡ˆ3: HuggingFace Trainer é€‚é…

ä½¿ç”¨ HF Trainer + è‡ªå®šä¹‰ Callback

---

## ğŸ“ å®æ–½å»ºè®®

### ç«‹å³å®æ–½ï¼ˆæ–¹æ¡ˆ1ï¼‰

1. **åˆ›å»º callbacks.py** - å®ç°ä¸Šè¿° ScheduleExecutor å’Œ CallbackManager
2. **ä¿®æ”¹ trainer.py** - é›†æˆ callback ç³»ç»Ÿ
3. **ä¸ºæ¨¡å—æ·»åŠ æ¥å£** - å®ç° `enable()`, `set_temperature()` ç­‰æ–¹æ³•
4. **æµ‹è¯•** - ä½¿ç”¨ `gpt5_moe_reasoning.yaml` éªŒè¯è°ƒåº¦åŠŸèƒ½

### é•¿æœŸä¼˜åŒ–ï¼ˆæ–¹æ¡ˆ2/3ï¼‰

- è¯„ä¼°è¿ç§»åˆ° Lightning æˆ– HF Trainer çš„æ”¶ç›Š
- é€æ­¥é‡æ„è®­ç»ƒæµç¨‹
- ä¿æŒå‘åå…¼å®¹

---

## ğŸ” å·®è·æ€»ç»“

| åŠŸèƒ½ | ä¸»æµæ¡†æ¶ | APTå½“å‰ | ç¼ºå£ |
|------|---------|---------|------|
| YAMLé…ç½® | âœ… | âœ… | æ—  |
| Scheduleå®šä¹‰ | âœ… | âœ… | æ—  |
| Callbackæœºåˆ¶ | âœ… | âŒ | **å…³é”®** |
| Hookæ‰§è¡Œ | âœ… | âŒ | **å…³é”®** |
| å‚æ•°é€€ç« | âœ… | âŒ | é‡è¦ |
| æ¨¡å—åŠ¨æ€å¯ç”¨ | âœ… | âŒ | é‡è¦ |
| ä¸€æ¡å‘½ä»¤è®­ç»ƒ | âœ… | âš ï¸ | å¯æ”¹è¿› |

---

## ğŸ¯ ç»“è®º

APTé¡¹ç›®**å·²ç»æœ‰äº†è‰¯å¥½çš„é…ç½®åŸºç¡€**ï¼ˆYAML + schedulesï¼‰ï¼Œä½†**ç¼ºå°‘æ‰§è¡Œå±‚**ï¼ˆCallbacks/Hooksï¼‰ã€‚

**æ¨èè¡ŒåŠ¨**:
1. âœ… å®ç° `callbacks.py` (ScheduleExecutor + CallbackManager)
2. âœ… ä¿®æ”¹ `trainer.py` é›†æˆ callback è§¦å‘ç‚¹
3. âœ… ä¸ºæ¨¡å—æ·»åŠ  `enable()` å’Œå‚æ•°è®¾ç½®æ¥å£
4. âœ… æµ‹è¯•å®Œæ•´çš„è¯¾ç¨‹åŒ–è®­ç»ƒæµç¨‹

å®æ–½åï¼Œç”¨æˆ·åªéœ€ï¼š
```bash
python train.py --config profiles/gpt5_moe_reasoning.yaml
```

è®­ç»ƒå™¨ä¼šè‡ªåŠ¨ï¼š
- Epoch 2 å¯ç”¨ MoE
- Epoch 3 å¯ç”¨å¯¹é½
- Epoch 5 å¯ç”¨æŠ•ç¥¨
- è·¯ç”±æ¸©åº¦ä» 1.5 é€€ç«åˆ° 0.8
- MoEå®¹é‡ä» 1.5 æ”¶ç´§åˆ° 1.1

**å®Œå…¨ç¬¦åˆä¸»æµå¤§æ¨¡å‹è®­ç»ƒçš„"é…ç½®é©±åŠ¨+è°ƒåº¦å™¨+å¯åŠ¨å™¨"èŒƒå¼ï¼**
