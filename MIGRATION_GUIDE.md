# APT å¾®å†…æ ¸æ¶æ„è¿ç§»æŒ‡å—

## ğŸ“‹ æ¦‚è¿°

æœ¬æŒ‡å—æä¾›**é€æ­¥ã€å¯éªŒè¯**çš„è¿ç§»æ­¥éª¤ï¼Œå°†APTä»å•ä½“æ¶æ„è¿ç§»åˆ°å¾®å†…æ ¸+æ’ä»¶æ¶æ„ã€‚

**é¢„è®¡æ—¶é—´ï¼š** 6å‘¨
**é£é™©ç­‰çº§ï¼š** ä¸­ï¼ˆé€šè¿‡æ¸è¿›å¼è¿ç§»é™ä½ï¼‰
**å‘åå…¼å®¹ï¼š** ä¿æŒ2ä¸ªç‰ˆæœ¬çš„è¿‡æ¸¡æœŸ

---

## ğŸ¯ è¿ç§»åŸåˆ™

1. **æ¸è¿›å¼**ï¼šåˆ†3é˜¶æ®µï¼Œæ¯é˜¶æ®µå¯ç‹¬ç«‹éªŒè¯
2. **å¯å›æ»š**ï¼šæ¯é˜¶æ®µå®Œæˆåæ‰“tagï¼Œå‡ºé—®é¢˜å¯å›é€€
3. **å‘åå…¼å®¹**ï¼šä¿ç•™æ—§æ¥å£è‡³å°‘2ä¸ªç‰ˆæœ¬
4. **æµ‹è¯•é©±åŠ¨**ï¼šæ¯æ­¥éƒ½æœ‰éªŒè¯æ ‡å‡†
5. **æ–‡æ¡£åŒæ­¥**ï¼šä»£ç å’Œæ–‡æ¡£åŒæ­¥æ›´æ–°

---

## ğŸ“… é˜¶æ®µ1ï¼šç¨³å®šæ ¸å¿ƒï¼ˆWeek 1-2ï¼‰

### ç›®æ ‡
âœ… æ ¸å¿ƒå¯ç‹¬ç«‹è¿è¡Œï¼Œä¸è£…æ’ä»¶ä¹Ÿèƒ½è·‘
âœ… åˆ›å»ºProvideræ¥å£å’Œæ³¨å†Œè¡¨
âœ… è®­ç»ƒå¾ªç¯æ”¯æŒé’©å­å¹¿æ’­

### æ­¥éª¤è¯¦è§£

#### Step 1.1ï¼šåˆ›å»ºæ ¸å¿ƒç›®å½•ç»“æ„ï¼ˆDay 1ï¼‰

```bash
# åˆ›å»ºæ–°ç›®å½•ï¼ˆä¸æ—§ç›®å½•å¹¶å­˜ï¼‰
mkdir -p apt/{core,core/providers}

# è¿ç§»å¹¶é‡æ„
cp apt_model/config/apt_config.py apt/core/config.py
cp apt_model/utils/logging_utils.py apt/core/logging.py
cp apt_model/utils/resource_monitor.py apt/core/monitor.py
cp apt_model/utils/error_handler.py apt/core/errors.py
cp apt_model/utils/hardware_check.py apt/core/device.py
cp apt_model/utils/cache_manager.py apt/core/cache.py

# åˆ›å»ºæ–°æ–‡ä»¶
touch apt/core/schedules.py
touch apt/core/registry.py
```

**éªŒè¯ï¼š**
```bash
python -c "from apt.core import config, logging, registry"
# åº”æ— æŠ¥é”™
```

#### Step 1.2ï¼šå®ç°æ ¸å¿ƒRegistryï¼ˆDay 2ï¼‰

åˆ›å»º `apt/core/registry.py`ï¼ˆå‚è€ƒ `examples/core_registry.py`ï¼‰

**å…³é”®ä»£ç ï¼š**
```python
# apt/core/registry.py

from abc import ABC, abstractmethod
from typing import Dict, Type, Any, Optional
import warnings

class Provider(ABC):
    @abstractmethod
    def get_name(self) -> str:
        pass

    @abstractmethod
    def get_version(self) -> str:
        pass

class Registry:
    def __init__(self):
        self._providers = {}
        self._instances = {}
        self._defaults = {
            'attention': 'tva_default',
            'ffn': 'default',
            # ...
        }

    def register(self, kind: str, name: str, provider_cls: Type[Provider]):
        if kind not in self._providers:
            self._providers[kind] = {}
        self._providers[kind][name] = provider_cls
        print(f"âœ… æ³¨å†Œ {kind}:{name}")

    def get(self, kind: str, name: str, config=None) -> Provider:
        key = f"{kind}:{name}"
        if key in self._instances:
            return self._instances[key]

        # æŸ¥æ‰¾æˆ–å›é€€
        if kind not in self._providers or name not in self._providers[kind]:
            default = self._defaults.get(kind)
            if default:
                warnings.warn(f"âš ï¸ å›é€€åˆ° {kind}:{default}")
                name = default
            else:
                raise ValueError(f"âŒ {key} æœªæ³¨å†Œ")

        # åˆ›å»ºå®ä¾‹
        provider_cls = self._providers[kind][name]
        self._instances[key] = provider_cls(config or {})
        return self._instances[key]

# å…¨å±€å•ä¾‹
registry = Registry()
```

**éªŒè¯ï¼š**
```python
# æµ‹è¯•è„šæœ¬
from apt.core.registry import registry, Provider

class DummyAttention(Provider):
    def __init__(self, config): pass
    def get_name(self): return "dummy"
    def get_version(self): return "1.0.0"

registry.register('attention', 'dummy', DummyAttention)
attn = registry.get('attention', 'dummy')
print(f"âœ… Registry å·¥ä½œæ­£å¸¸: {attn}")
```

#### Step 1.3ï¼šå®šä¹‰Provideræ¥å£ï¼ˆDay 3ï¼‰

åˆ›å»ºå„ç§ProvideråŸºç±»ï¼š

```bash
touch apt/core/providers/__init__.py
touch apt/core/providers/attention.py
touch apt/core/providers/ffn.py
touch apt/core/providers/router.py
touch apt/core/providers/align.py
touch apt/core/providers/retrieval.py
```

**ç¤ºä¾‹ - AttentionProvider:**
```python
# apt/core/providers/attention.py

from apt.core.registry import Provider
from abc import abstractmethod
import torch
import torch.nn as nn
from typing import Optional, Tuple

class AttentionProvider(Provider):
    """æ³¨æ„åŠ›æœºåˆ¶ProvideråŸºç±»"""

    @abstractmethod
    def forward(
        self,
        query: torch.Tensor,
        key: torch.Tensor,
        value: torch.Tensor,
        attn_mask: Optional[torch.Tensor] = None,
        key_padding_mask: Optional[torch.Tensor] = None,
    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:
        """
        å‰å‘ä¼ æ’­

        Args:
            query: [B, T, D]
            key: [B, S, D]
            value: [B, S, D]
            attn_mask: [T, S] æˆ– [B, T, S]
            key_padding_mask: [B, S]

        Returns:
            output: [B, T, D]
            attn_weights: [B, H, T, S] (å¯é€‰)
        """
        pass

    @abstractmethod
    def create_layer(self, d_model: int, num_heads: int, **kwargs) -> nn.Module:
        """åˆ›å»ºæ³¨æ„åŠ›å±‚å®ä¾‹"""
        pass
```

**éªŒè¯ï¼š**
```python
from apt.core.providers.attention import AttentionProvider
# èƒ½å¯¼å…¥å³å¯
```

#### Step 1.4ï¼šè¿ç§»TVAä¸ºProviderï¼ˆDay 4-5ï¼‰

```bash
# åˆ›å»ºæ–°ç›®å½•
mkdir -p apt/modeling/layers

# æå–TVAä»£ç 
# ä» apt_model/modeling/apt_model.py æå– AutopoieticAttention
```

**å…³é”®ä»£ç ï¼š**
```python
# apt/modeling/layers/attention_tva.py

from apt.core.providers.attention import AttentionProvider
from apt.core.registry import registry
import torch
import torch.nn as nn

class TVAAttention(AttentionProvider):
    """TVAï¼ˆè‡ªç”Ÿæˆæ³¨æ„åŠ›ï¼‰- æ ¸å¿ƒé»˜è®¤å®ç°"""

    def __init__(self, config):
        self.r = config.get('r', 4)
        self.s = config.get('s', 1)
        self.tau = config.get('tau', 0.18)

    def get_name(self):
        return "tva_default"

    def get_version(self):
        return "1.0.0"

    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):
        # ä¿ç•™ç°æœ‰çš„AutopoieticAttentionæ ¸å¿ƒé€»è¾‘
        # ...
        pass

    def create_layer(self, d_model, num_heads, **kwargs):
        return TVAAttentionLayer(
            d_model=d_model,
            num_heads=num_heads,
            r=self.r,
            s=self.s,
            tau=self.tau,
            **kwargs
        )

class TVAAttentionLayer(nn.Module):
    """TVAæ³¨æ„åŠ›å±‚"""
    def __init__(self, d_model, num_heads, r, s, tau, **kwargs):
        super().__init__()
        # åˆå§‹åŒ–å±‚å‚æ•°
        # ...

    def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):
        # å‰å‘ä¼ æ’­é€»è¾‘
        # ...

# è‡ªåŠ¨æ³¨å†Œï¼ˆæ¨¡å—å¯¼å…¥æ—¶æ‰§è¡Œï¼‰
registry.register('attention', 'tva_default', TVAAttention, default=True)
```

**éªŒè¯ï¼š**
```python
from apt.modeling.layers.attention_tva import TVAAttention
from apt.core.registry import registry

# æ£€æŸ¥æ˜¯å¦è‡ªåŠ¨æ³¨å†Œ
providers = registry.list_providers('attention')
assert 'tva_default' in providers['attention']

# åˆ›å»ºå®ä¾‹
tva = registry.get('attention', 'tva_default', {'r': 4, 'tau': 0.18})
layer = tva.create_layer(d_model=768, num_heads=12)
print(f"âœ… TVA Provider å·¥ä½œæ­£å¸¸")
```

#### Step 1.5ï¼šåˆ›å»ºModelBuilderï¼ˆDay 6-7ï¼‰

```bash
touch apt/modeling/compose.py
```

**å…³é”®ä»£ç ï¼š**
```python
# apt/modeling/compose.py

from apt.core.registry import registry
from typing import Dict, Any
import torch.nn as nn

class ModelBuilder:
    """æ¨¡å‹è£…é…å™¨ - é€šè¿‡Provideræ„å»ºæ¨¡å‹"""

    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.registry = registry

    def build_attention(self, d_model: int, num_heads: int):
        """æ„å»ºæ³¨æ„åŠ›å±‚"""
        model_cfg = self.config.get('model', {})
        attn_name = model_cfg.get('attention_name', 'tva_default')
        attn_config = model_cfg.get('tva', {})

        provider = self.registry.get('attention', attn_name, attn_config)
        return provider.create_layer(d_model, num_heads)

    def build_ffn(self, d_model: int, d_ff: int):
        """æ„å»ºFFNå±‚"""
        model_cfg = self.config.get('model', {})
        ffn_name = model_cfg.get('ffn_name', 'default')

        provider = self.registry.get('ffn', ffn_name)
        return provider.create_layer(d_model, d_ff)

    def build_block(self, d_model, num_heads, d_ff):
        """æ„å»ºTransformer Block"""
        attn = self.build_attention(d_model, num_heads)
        ffn = self.build_ffn(d_model, d_ff)

        # æ£€æŸ¥æ˜¯å¦å¯ç”¨MoEæ’ä»¶
        if self.config.get('model', {}).get('moe', {}).get('enabled', False):
            try:
                router = self.registry.get('router', 'topk_moe')
                ffn = router.wrap_ffn(ffn, self.config['model']['moe'])
            except ValueError:
                # MoEæ’ä»¶æœªåŠ è½½ï¼Œå¿½ç•¥
                pass

        return TransformerBlock(attn, ffn)

    def build_model(self):
        """æ„å»ºå®Œæ•´æ¨¡å‹"""
        model_cfg = self.config['model']
        blocks = nn.ModuleList([
            self.build_block(
                d_model=model_cfg['d_model'],
                num_heads=model_cfg['num_heads'],
                d_ff=model_cfg['d_ff']
            )
            for _ in range(model_cfg['num_layers'])
        ])

        return GPTModel(blocks, model_cfg)

class TransformerBlock(nn.Module):
    """åŸºç¡€Transformer Block"""
    def __init__(self, attention, ffn):
        super().__init__()
        self.attention = attention
        self.ffn = ffn
        self.norm1 = nn.LayerNorm(attention.d_model)
        self.norm2 = nn.LayerNorm(attention.d_model)

    def forward(self, x, mask=None):
        # è‡ªæ³¨æ„åŠ›
        attn_out, _ = self.attention(x, x, x, attn_mask=mask)
        x = self.norm1(x + attn_out)

        # FFN
        ffn_out = self.ffn(x)
        x = self.norm2(x + ffn_out)

        return x
```

**éªŒè¯ï¼š**
```python
# æµ‹è¯•Builder
import yaml
from apt.modeling.compose import ModelBuilder

config = yaml.safe_load(open('examples/profiles/tiny_debug.yaml'))
builder = ModelBuilder(config)

# æ„å»ºå•ä¸ªblock
block = builder.build_block(d_model=64, num_heads=4, d_ff=256)
print(f"âœ… ModelBuilder æ„å»ºæˆåŠŸ: {block}")

# æ„å»ºå®Œæ•´æ¨¡å‹
model = builder.build_model()
print(f"âœ… å®Œæ•´æ¨¡å‹æ„å»ºæˆåŠŸ: {model}")
```

#### Step 1.6ï¼šè®­ç»ƒå¾ªç¯æ·»åŠ é’©å­ï¼ˆDay 8-9ï¼‰

ä¿®æ”¹ `apt/training/trainer.py`ï¼š

```python
# apt/training/trainer.py

class Trainer:
    def __init__(self, config):
        self.config = config
        self.hooks = []  # æ’ä»¶é’©å­åˆ—è¡¨

    def register_hook(self, hook):
        """æ³¨å†Œé’©å­ï¼ˆç”±æ’ä»¶è°ƒç”¨ï¼‰"""
        self.hooks.append(hook)
        print(f"âœ… æ³¨å†Œé’©å­: {hook.__class__.__name__}")

    def _broadcast_event(self, event_name, **kwargs):
        """å¹¿æ’­äº‹ä»¶åˆ°æ‰€æœ‰é’©å­"""
        for hook in self.hooks:
            if hasattr(hook, event_name):
                try:
                    getattr(hook, event_name)(**kwargs)
                except Exception as e:
                    print(f"âš ï¸ é’©å­ {hook.__class__.__name__}.{event_name} å¤±è´¥: {e}")

    def train(self):
        """ä¸»è®­ç»ƒå¾ªç¯"""
        self._broadcast_event('on_training_start', trainer=self)

        for epoch in range(self.config['training']['max_epochs']):
            self.train_epoch(epoch)

        self._broadcast_event('on_training_end', trainer=self)

    def train_epoch(self, epoch):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self._broadcast_event('on_epoch_start', epoch=epoch, trainer=self)

        for step, batch in enumerate(self.dataloader):
            self._broadcast_event('on_step_start', step=step, batch=batch, trainer=self)

            # è®­ç»ƒæ­¥éª¤
            loss = self.train_step(batch)

            self._broadcast_event('on_step_end', step=step, loss=loss, trainer=self)

        self._broadcast_event('on_epoch_end', epoch=epoch, trainer=self)

    def train_step(self, batch):
        """å•æ­¥è®­ç»ƒï¼ˆä¿æŒåŸé€»è¾‘ï¼‰"""
        # ... åŸæœ‰è®­ç»ƒé€»è¾‘
        pass
```

**éªŒè¯ï¼š**
```python
# æµ‹è¯•é’©å­ç³»ç»Ÿ
class DummyHook:
    def on_epoch_start(self, epoch, trainer):
        print(f"é’©å­è§¦å‘: epoch {epoch} å¼€å§‹")

    def on_step_end(self, step, loss, trainer):
        if step % 10 == 0:
            print(f"é’©å­è§¦å‘: step {step}, loss={loss}")

trainer = Trainer(config)
trainer.register_hook(DummyHook())
# trainer.train()  # è¿è¡Œè®­ç»ƒæŸ¥çœ‹é’©å­æ˜¯å¦è§¦å‘
```

#### Step 1.7ï¼šCLIæ·»åŠ Profileæ”¯æŒï¼ˆDay 10ï¼‰

ä¿®æ”¹ `apt/cli/parser.py`ï¼š

```python
# apt/cli/parser.py

import argparse

def parse_arguments():
    parser = argparse.ArgumentParser(description="APT Model Training Tool")

    # æ·»åŠ profileå‚æ•° â­
    parser.add_argument('-p', '--profile',
                        help='é…ç½®æ–‡ä»¶è·¯å¾„ (profiles/*.yaml)')

    # æ·»åŠ pluginså‚æ•°
    parser.add_argument('--plugins', nargs='+',
                        help='å¯ç”¨çš„æ’ä»¶åˆ—è¡¨ (è¦†ç›–profileè®¾ç½®)')

    # åŸæœ‰å‚æ•°...
    parser.add_argument('--epochs', type=int)
    parser.add_argument('--batch-size', type=int)
    # ...

    # æ·»åŠ pluginå­å‘½ä»¤
    subparsers = parser.add_subparsers(dest='command')

    # plugin list
    plugin_list_parser = subparsers.add_parser('plugin-list',
                                                help='åˆ—å‡ºæ‰€æœ‰å·²æ³¨å†Œçš„Provider')
    plugin_list_parser.add_argument('--kind', help='Providerç±»å‹')

    # plugin info
    plugin_info_parser = subparsers.add_parser('plugin-info',
                                                help='æŸ¥çœ‹Providerè¯¦ç»†ä¿¡æ¯')
    plugin_info_parser.add_argument('kind', help='Providerç±»å‹')
    plugin_info_parser.add_argument('name', help='Provideråç§°')

    return parser.parse_args()
```

ä¿®æ”¹ `apt/cli/commands.py`ï¼š

```python
# apt/cli/commands.py

import yaml
from apt.core.registry import registry

def run_plugin_list_command(args):
    """åˆ—å‡ºæ‰€æœ‰Provider"""
    providers = registry.list_providers(args.kind)

    print("\n=== å·²æ³¨å†Œçš„ Provider ===")
    for kind, names in providers.items():
        print(f"\n{kind}:")
        for name in names:
            info = registry.get_info(kind, name)
            default_mark = " (é»˜è®¤)" if info['is_default'] else ""
            print(f"  - {name} v{info['version']}{default_mark}")

def run_plugin_info_command(args):
    """æŸ¥çœ‹Providerè¯¦ç»†ä¿¡æ¯"""
    info = registry.get_info(args.kind, args.name)

    print(f"\n=== {args.kind}:{args.name} ===")
    for key, value in info.items():
        print(f"{key}: {value}")

def load_profile(profile_path):
    """åŠ è½½é…ç½®æ–‡ä»¶"""
    with open(profile_path, 'r') as f:
        return yaml.safe_load(f)
```

**éªŒè¯ï¼š**
```bash
# æµ‹è¯•CLI
python -m apt plugin-list
python -m apt plugin-list --kind attention
python -m apt plugin-info attention tva_default
```

#### Step 1.8ï¼šç«¯åˆ°ç«¯éªŒè¯ï¼ˆDay 11-12ï¼‰

```bash
# ä½¿ç”¨tiny profileè¿è¡Œè®­ç»ƒ
python -m apt train -p examples/profiles/tiny_debug.yaml

# é¢„æœŸè¾“å‡ºï¼š
# âœ… æ³¨å†Œ attention:tva_default
# âœ… åŠ è½½é…ç½®: tiny_debug
# âœ… æ„å»ºæ¨¡å‹...
# Epoch 1/3: loss=...
# Epoch 2/3: loss=...
# Epoch 3/3: loss=...
# âœ… è®­ç»ƒå®Œæˆ
```

**éªŒè¯æ¸…å•ï¼š**
- [ ] é…ç½®æ–‡ä»¶æ­£ç¡®åŠ è½½
- [ ] æ¨¡å‹é€šè¿‡Builderæ„å»º
- [ ] TVA attentionæ­£å¸¸å·¥ä½œ
- [ ] è®­ç»ƒå¾ªç¯å®Œæ•´è¿è¡Œ
- [ ] é’©å­äº‹ä»¶æ­£å¸¸å¹¿æ’­
- [ ] æ— éœ€æ’ä»¶ä¹Ÿèƒ½è¿è¡Œ

**å¦‚æœéªŒè¯é€šè¿‡ï¼Œæ‰“tagï¼š**
```bash
git tag -a v2.0.0-stage1 -m "Stage 1: Core infrastructure complete"
git push origin v2.0.0-stage1
```

---

## ğŸ“… é˜¶æ®µ2ï¼šå¤–æ’é«˜æ”¶ç›Šï¼ˆWeek 3-4ï¼‰

### ç›®æ ‡
âœ… MoE/Alignä½œä¸ºæ’ä»¶å·¥ä½œ
âœ… Schedulesè¯¾ç¨‹åŒ–ç”Ÿæ•ˆ
âœ… æ’ä»¶å¯åŠ¨æ€å¯ç”¨/ç¦ç”¨

### æ­¥éª¤è¯¦è§£

#### Step 2.1ï¼šå®ç°Schedulesï¼ˆDay 1-2ï¼‰

```python
# apt/core/schedules.py

class Schedule:
    """è¯¾ç¨‹åŒ–è°ƒåº¦å™¨"""

    def __init__(self, config):
        self.config = config.get('schedules', {})
        self.max_epochs = config['training']['max_epochs']

    def should_enable_plugin(self, plugin_name, epoch):
        """åˆ¤æ–­æ˜¯å¦åº”å¯ç”¨æ’ä»¶"""
        key = f"enable_{plugin_name}_at_epoch"
        target_epoch = self.config.get(key, 0)
        return epoch >= target_epoch

    def get_param(self, param_name, epoch=None, step=None):
        """è·å–å‚æ•°å½“å‰å€¼ï¼ˆæ”¯æŒé€€ç«ï¼‰"""
        param_cfg = self.config.get(param_name)

        if param_cfg is None:
            return None

        if not isinstance(param_cfg, dict):
            # é™æ€å€¼
            return param_cfg

        # åŠ¨æ€é€€ç«
        return self._interpolate(param_cfg, epoch, step)

    def _interpolate(self, cfg, epoch, step):
        """çº¿æ€§æ’å€¼"""
        start = cfg['start']
        end = cfg['end']
        by = cfg.get('by', 'epoch')

        if by == 'epoch' and epoch is not None:
            t = min(epoch / self.max_epochs, 1.0)
        elif by == 'step' and step is not None:
            total_steps = self.max_epochs * cfg.get('steps_per_epoch', 1000)
            t = min(step / total_steps, 1.0)
        else:
            return start

        # çº¿æ€§æ’å€¼
        value = start + (end - start) * t
        return value
```

**éªŒè¯ï¼š**
```python
from apt.core.schedules import Schedule

config = {
    'training': {'max_epochs': 10},
    'schedules': {
        'enable_moe_at_epoch': 2,
        'route_temp': {'start': 1.5, 'end': 0.8, 'by': 'epoch'}
    }
}

schedule = Schedule(config)

# æµ‹è¯•æ’ä»¶å¯ç”¨
assert schedule.should_enable_plugin('moe', epoch=1) == False
assert schedule.should_enable_plugin('moe', epoch=2) == True

# æµ‹è¯•å‚æ•°é€€ç«
temp_e0 = schedule.get_param('route_temp', epoch=0)
temp_e5 = schedule.get_param('route_temp', epoch=5)
temp_e10 = schedule.get_param('route_temp', epoch=10)

assert temp_e0 == 1.5
assert 1.0 < temp_e5 < 1.5
assert temp_e10 == 0.8

print("âœ… Schedules éªŒè¯é€šè¿‡")
```

#### Step 2.2ï¼šåˆ›å»ºMoEæ’ä»¶ï¼ˆDay 3-5ï¼‰

```bash
mkdir -p apt/plugins/builtin
touch apt/plugins/builtin/__init__.py
touch apt/plugins/builtin/moe.py
```

```python
# apt/plugins/builtin/moe.py

from apt.core.providers.router import RouterProvider
from apt.core.registry import registry
import torch
import torch.nn as nn

class MoERouter(RouterProvider):
    """MoEä¸“å®¶è·¯ç”±Provider"""

    def __init__(self, config):
        self.experts = config.get('experts', 64)
        self.top_k = config.get('top_k', 2)
        self.capacity = config.get('capacity', 1.25)
        self.balance_loss_weight = config.get('balance_loss', 0.01)

    def get_name(self):
        return "topk_moe"

    def get_version(self):
        return "1.0.0"

    def wrap_ffn(self, base_ffn, config):
        """å°†æ™®é€šFFNåŒ…è£…ä¸ºMoE"""
        return MoEFFN(
            base_ffn=base_ffn,
            num_experts=self.experts,
            top_k=self.top_k,
            capacity=self.capacity
        )

    def on_epoch_start(self, epoch, trainer):
        """é’©å­ï¼šæ ¹æ®schedulesè°ƒæ•´capacity"""
        schedule = trainer.schedule
        if schedule:
            new_capacity = schedule.get_param('moe_capacity', epoch=epoch)
            if new_capacity:
                self.capacity = new_capacity
                print(f"ğŸ“Š MoE capacity è°ƒæ•´ä¸º: {new_capacity:.2f}")

class MoEFFN(nn.Module):
    """MoEå‰é¦ˆç½‘ç»œ"""

    def __init__(self, base_ffn, num_experts, top_k, capacity):
        super().__init__()
        self.num_experts = num_experts
        self.top_k = top_k
        self.capacity = capacity

        # å¤åˆ¶base_ffnä½œä¸ºä¸“å®¶
        self.experts = nn.ModuleList([
            self._clone_ffn(base_ffn) for _ in range(num_experts)
        ])

        # è·¯ç”±ç½‘ç»œ
        self.router = nn.Linear(base_ffn.d_model, num_experts)

    def _clone_ffn(self, base_ffn):
        """å…‹éš†FFN"""
        # ... å®ç°å…‹éš†é€»è¾‘
        pass

    def forward(self, x):
        """
        å‰å‘ä¼ æ’­

        Args:
            x: [B, T, D]

        Returns:
            output: [B, T, D]
            aux_loss: è´Ÿè½½å‡è¡¡æŸå¤±
        """
        B, T, D = x.shape

        # è·¯ç”±æ‰“åˆ†
        router_logits = self.router(x)  # [B, T, E]
        router_probs = F.softmax(router_logits, dim=-1)

        # Top-Ké€‰æ‹©
        top_k_probs, top_k_indices = torch.topk(router_probs, self.top_k, dim=-1)
        top_k_probs = top_k_probs / top_k_probs.sum(dim=-1, keepdim=True)

        # å®¹é‡é™åˆ¶
        # ... å®ç°capacityæœºåˆ¶

        # è°ƒç”¨ä¸“å®¶
        output = torch.zeros_like(x)
        for k in range(self.top_k):
            expert_idx = top_k_indices[..., k]
            expert_weight = top_k_probs[..., k]

            for e in range(self.num_experts):
                mask = (expert_idx == e)
                if mask.any():
                    expert_input = x[mask]
                    expert_output = self.experts[e](expert_input)
                    output[mask] += expert_weight[mask].unsqueeze(-1) * expert_output

        # è´Ÿè½½å‡è¡¡æŸå¤±
        aux_loss = self._compute_balance_loss(router_probs)

        return output, aux_loss

    def _compute_balance_loss(self, router_probs):
        """è®¡ç®—è´Ÿè½½å‡è¡¡æŸå¤±"""
        # ... å®ç°è´Ÿè½½å‡è¡¡æŸå¤±
        pass

# æ³¨å†ŒMoE Provider
registry.register('router', 'topk_moe', MoERouter)
```

**éªŒè¯ï¼š**
```python
# æµ‹è¯•MoEæ’ä»¶
from apt.plugins.builtin.moe import MoERouter
from apt.core.registry import registry

# è·å–MoE Provider
moe_config = {'experts': 8, 'top_k': 2, 'capacity': 1.5}
moe = registry.get('router', 'topk_moe', moe_config)

# åŒ…è£…FFN
base_ffn = SomeDummyFFN(d_model=64, d_ff=256)
moe_ffn = moe.wrap_ffn(base_ffn, moe_config)

# å‰å‘ä¼ æ’­æµ‹è¯•
x = torch.randn(2, 10, 64)
output, aux_loss = moe_ffn(x)

assert output.shape == x.shape
assert aux_loss.item() >= 0

print("âœ… MoE æ’ä»¶éªŒè¯é€šè¿‡")
```

#### Step 2.3ï¼šåˆ›å»ºAlignæ’ä»¶ï¼ˆDay 6-7ï¼‰

```python
# apt/plugins/builtin/align.py

from apt.core.providers.align import AlignProvider
from apt.core.registry import registry
import torch

class BistateAlign(AlignProvider):
    """åŒæ€æ•°å¯¹é½Provider"""

    def __init__(self, config):
        self.alpha = config.get('alpha', 0.35)
        self.beta = config.get('beta', 0.20)
        self.tau_align = config.get('tau_align', 0.15)
        self.enabled = False

    def get_name(self):
        return "bistate_default"

    def get_version(self):
        return "1.0.0"

    def compute_align_loss(self, logits, targets, model_state):
        """
        è®¡ç®—åŒæ€æ•°å¯¹é½æŸå¤±

        Args:
            logits: [B, T, V]
            targets: [B, T]
            model_state: æ¨¡å‹å†…éƒ¨çŠ¶æ€

        Returns:
            align_loss: scalar
        """
        if not self.enabled:
            return torch.tensor(0.0)

        # æå–ç¨³å®šæ€å’Œå¯¹é½æ€
        stable_state = model_state.get('stable')
        align_state = model_state.get('align')

        if stable_state is None or align_state is None:
            return torch.tensor(0.0)

        # è®¡ç®—å¯¹é½æŸå¤±
        align_loss = self.alpha * self._stable_loss(stable_state, logits) + \
                     self.beta * self._align_loss(align_state, logits)

        return align_loss

    def _stable_loss(self, stable_state, logits):
        """ç¨³å®šæ€æŸå¤±"""
        # ... å®ç°ç¨³å®šæ€æŸå¤±
        pass

    def _align_loss(self, align_state, logits):
        """å¯¹é½æ€æŸå¤±"""
        # ... å®ç°å¯¹é½æ€æŸå¤±
        pass

    def on_epoch_start(self, epoch, trainer):
        """é’©å­ï¼šæ ¹æ®scheduleså¯ç”¨å¯¹é½"""
        schedule = trainer.schedule
        if schedule and schedule.should_enable_plugin('align', epoch):
            self.enabled = True
            print(f"âœ… å¯ç”¨åŒæ€æ•°å¯¹é½ (epoch={epoch})")

    def on_step_end(self, step, loss, trainer):
        """é’©å­ï¼šæ·»åŠ å¯¹é½æŸå¤±"""
        if self.enabled and hasattr(trainer, 'model_state'):
            align_loss = self.compute_align_loss(
                trainer.last_logits,
                trainer.last_targets,
                trainer.model_state
            )
            trainer.total_loss += align_loss

# æ³¨å†Œ
registry.register('align', 'bistate_default', BistateAlign)
```

#### Step 2.4ï¼šé›†æˆåˆ°Trainerï¼ˆDay 8ï¼‰

ä¿®æ”¹ `apt/training/trainer.py`ï¼š

```python
# apt/training/trainer.py (æ·»åŠ )

def __init__(self, config):
    # ...åŸæœ‰åˆå§‹åŒ–
    self.schedule = Schedule(config)
    self.load_plugins()

def load_plugins(self):
    """åŠ è½½é…ç½®ä¸­æŒ‡å®šçš„æ’ä»¶"""
    enabled_plugins = self.config.get('plugins', [])

    for plugin_name in enabled_plugins:
        try:
            # åŠ¨æ€å¯¼å…¥æ’ä»¶
            module = __import__(f'apt.plugins.builtin.{plugin_name}',
                               fromlist=[plugin_name])

            # è·å–Providerï¼ˆè§¦å‘æ³¨å†Œï¼‰
            # ...

            print(f"âœ… åŠ è½½æ’ä»¶: {plugin_name}")
        except Exception as e:
            print(f"âš ï¸ åŠ è½½æ’ä»¶ {plugin_name} å¤±è´¥: {e}")

def train_epoch(self, epoch):
    """è®­ç»ƒepochï¼ˆæ·»åŠ scheduleæ£€æŸ¥ï¼‰"""
    self._broadcast_event('on_epoch_start', epoch=epoch, trainer=self)

    # æ£€æŸ¥æ˜¯å¦åº”å¯ç”¨æ–°æ’ä»¶
    self._check_and_enable_plugins(epoch)

    # ... åŸæœ‰è®­ç»ƒé€»è¾‘

def _check_and_enable_plugins(self, epoch):
    """æ£€æŸ¥å¹¶å¯ç”¨æ’ä»¶"""
    for plugin in self.config.get('plugins', []):
        if self.schedule.should_enable_plugin(plugin, epoch):
            # æ’ä»¶åœ¨on_epoch_starté’©å­ä¸­è‡ªè¡Œå¯ç”¨
            pass
```

#### Step 2.5ï¼šç«¯åˆ°ç«¯éªŒè¯ï¼ˆDay 9-10ï¼‰

```bash
# ä½¿ç”¨MoE profileè¿è¡Œ
python -m apt train -p examples/profiles/gpt5_moe_reasoning.yaml --epochs 5

# é¢„æœŸè¾“å‡ºï¼š
# Epoch 1/5: loss=... (æ— MoE/Align)
# Epoch 2/5: loss=...
#   âœ… å¯ç”¨MoE (epoch=2)
#   ğŸ“Š MoE capacity=1.50
# Epoch 3/5: loss=...
#   âœ… å¯ç”¨åŒæ€æ•°å¯¹é½ (epoch=3)
# Epoch 4/5: loss=...
#   ğŸ“Š MoE capacity=1.38 (é€€ç«ä¸­)
# Epoch 5/5: loss=...
#   ğŸ“Š MoE capacity=1.25
```

**éªŒè¯æ¸…å•ï¼š**
- [ ] MoEåœ¨epoch=2å¯ç”¨
- [ ] Alignåœ¨epoch=3å¯ç”¨
- [ ] Capacityå‚æ•°æ­£ç¡®é€€ç«
- [ ] æ’ä»¶å¤±è´¥ä¸å½±å“è®­ç»ƒ
- [ ] `plugin-list`æ˜¾ç¤ºæ‰€æœ‰æ’ä»¶

**æ‰“tagï¼š**
```bash
git tag -a v2.0.0-stage2 -m "Stage 2: MoE and Align plugins"
git push origin v2.0.0-stage2
```

---

## ğŸ“… é˜¶æ®µ3ï¼šç­–ç•¥/å¤–éƒ¨ä¾èµ–ï¼ˆWeek 5-6ï¼‰

### ç›®æ ‡
âœ… Flash Attentionæ›¿ä»£å†…æ ¸
âœ… RAGæ£€ç´¢æ’ä»¶
âœ… æŠ•ç¥¨ä¸€è‡´æ€§æ’ä»¶
âœ… é‡åŒ–/å¯¼å‡ºæ’ä»¶

### æ­¥éª¤è¯¦è§£

#### Step 3.1ï¼šFlash Attentionæ’ä»¶ï¼ˆDay 1-2ï¼‰

```bash
mkdir -p apt/plugins/flash_attn
touch apt/plugins/flash_attn/__init__.py
touch apt/plugins/flash_attn/flash_v2.py
```

```python
# apt/plugins/flash_attn/flash_v2.py

try:
    from flash_attn import flash_attn_func
    FLASH_AVAILABLE = True
except ImportError:
    FLASH_AVAILABLE = False

from apt.core.providers.attention import AttentionProvider
from apt.core.registry import registry
import torch
import torch.nn as nn

if FLASH_AVAILABLE:
    class FlashAttentionV2(AttentionProvider):
        """Flash Attention v2 Provider"""

        def __init__(self, config):
            pass

        def get_name(self):
            return "flash_v2"

        def get_version(self):
            return "2.0.0"

        def get_dependencies(self):
            return ["flash-attn>=2.0.0"]

        def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):
            """ä½¿ç”¨Flash AttentionåŠ é€Ÿ"""
            # flash_attn_func è¦æ±‚ [B, T, H, D]
            B, T, D = query.shape
            H = self.num_heads
            head_dim = D // H

            q = query.view(B, T, H, head_dim)
            k = key.view(B, -1, H, head_dim)
            v = value.view(B, -1, H, head_dim)

            output = flash_attn_func(q, k, v)
            output = output.view(B, T, D)

            return output, None  # Flash Attentionä¸è¿”å›æƒé‡

        def create_layer(self, d_model, num_heads, **kwargs):
            return FlashAttentionLayer(d_model, num_heads)

    class FlashAttentionLayer(nn.Module):
        def __init__(self, d_model, num_heads):
            super().__init__()
            self.d_model = d_model
            self.num_heads = num_heads
            # ... åˆå§‹åŒ–

        def forward(self, query, key, value, attn_mask=None, key_padding_mask=None):
            # ... è°ƒç”¨flash_attn_func

    # æ³¨å†Œ
    registry.register('attention', 'flash_v2', FlashAttentionV2)
    print("âœ… Flash Attention v2 å¯ç”¨")

else:
    print("âš ï¸ flash-attnæœªå®‰è£…ï¼Œè·³è¿‡Flash Attentionæ’ä»¶")
```

**éªŒè¯ï¼š**
```bash
# å°è¯•ä½¿ç”¨Flash Attention
python -m apt train -p tiny_debug.yaml --model.attention_name flash_v2

# å¦‚æœflash-attnæœªå®‰è£…ï¼Œåº”è‡ªåŠ¨å›é€€ï¼š
# âš ï¸ attention:flash_v2 æœªæ‰¾åˆ°ï¼Œå›é€€åˆ° attention:tva_default
```

#### Step 3.2ï¼šå…¶ä»–æ’ä»¶ï¼ˆDay 3-6ï¼‰

æŒ‰ç±»ä¼¼æ–¹å¼å®ç°ï¼š
- RAGæ£€ç´¢ï¼ˆ`plugins/builtin/retriever.py`ï¼‰
- æŠ•ç¥¨ï¼ˆ`plugins/builtin/voter.py`ï¼‰
- é‡åŒ–ï¼ˆ`plugins/quant/`ï¼‰

#### Step 3.3ï¼šæœ€ç»ˆéªŒè¯ï¼ˆDay 7-10ï¼‰

**ç»¼åˆæµ‹è¯•ï¼š**
```bash
# æµ‹è¯•1ï¼šçº¯æ ¸å¿ƒï¼ˆæ— æ’ä»¶ï¼‰
python -m apt train -p tiny_debug.yaml

# æµ‹è¯•2ï¼šMoE+Align
python -m apt train -p gpt5_moe_reasoning.yaml

# æµ‹è¯•3ï¼šFlash Attentionï¼ˆå¦‚æœå¯ç”¨ï¼‰
python -m apt train -p tiny_debug.yaml --model.attention_name flash_v2

# æµ‹è¯•4ï¼šæ’ä»¶åˆ—è¡¨
python -m apt plugin-list

# æµ‹è¯•5ï¼šæ’ä»¶ä¿¡æ¯
python -m apt plugin-info router topk_moe
```

**æœ€ç»ˆæ£€æŸ¥æ¸…å•ï¼š**
- [ ] æ ¸å¿ƒå¯ç‹¬ç«‹è¿è¡Œ
- [ ] æ’ä»¶å¯æŒ‰éœ€åŠ è½½
- [ ] Schedulesæ­£ç¡®ç”Ÿæ•ˆ
- [ ] æ’ä»¶å¤±è´¥è‡ªåŠ¨å›é€€
- [ ] é…ç½®æ–‡ä»¶é©±åŠ¨
- [ ] å‘åå…¼å®¹ï¼ˆæ—§ä»£ç ä»å¯è¿è¡Œï¼‰
- [ ] æ–‡æ¡£å®Œæ•´
- [ ] æµ‹è¯•è¦†ç›–80%+

**æ‰“final tagï¼š**
```bash
git tag -a v2.0.0 -m "APT v2.0.0: Microkernel Architecture"
git push origin v2.0.0
```

---

## ğŸ”„ å›æ»šè®¡åˆ’

å¦‚æœæŸé˜¶æ®µå‡ºç°é—®é¢˜ï¼š

```bash
# å›é€€åˆ°é˜¶æ®µ1
git checkout v2.0.0-stage1

# å›é€€åˆ°é˜¶æ®µ2
git checkout v2.0.0-stage2

# å›é€€åˆ°æ—§ç‰ˆæœ¬
git checkout v1.x.x
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- ä¸»æ–¹æ¡ˆï¼š[REFACTOR_PLAN.md](./REFACTOR_PLAN.md)
- ç¤ºä¾‹ä»£ç ï¼š`examples/`
- Profileç¤ºä¾‹ï¼š`examples/profiles/`

---

**ä¸‹ä¸€æ­¥ï¼š** å¼€å§‹æ‰§è¡Œé˜¶æ®µ1 Step 1.1
