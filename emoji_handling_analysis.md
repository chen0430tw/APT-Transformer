# Emoji å¤„ç†æ–¹å¼å¯¹æ¯”åˆ†æ

## èƒŒæ™¯

æ£€æŸ¥é¡¹ç›®ä¸­ä¸åŒ tokenizer å¦‚ä½•å¤„ç† emoji å­—ç¬¦ã€‚

---

## 1. ChineseTokenizer (apt_model/modeling/chinese_tokenizer.py)

### è¡Œä¸º
- **æœªçŸ¥å­—ç¬¦è¢«ç›´æ¥è·³è¿‡**ï¼ˆç¬¬ 155-161 è¡Œï¼‰

```python
for char in text:
    if char in self.encoder:
        ids.append(self.encoder[char])
    else:
        # æœªçŸ¥å­—ç¬¦å¯ä»¥ç”¨UNKæ ‡è®°æ›¿ä»£ï¼Œæˆ–è€…è·³è¿‡
        pass  # â† ç›´æ¥è·³è¿‡ï¼
```

### Emoji å¤„ç†
- âŒ emoji `"ğŸŒ§ï¸"` è¢«ç¼–ç ä¸º **ç©ºåˆ—è¡¨ `[]`**
- âŒ **ä¸ä½¿ç”¨ `[UNK]` token**ï¼ˆè¯æ±‡è¡¨ä¸­æ²¡æœ‰å®šä¹‰ï¼‰
- âŒ ç¼–ç /è§£ç ä¼š**ä¸¢å¤±æ‰€æœ‰ emoji ä¿¡æ¯**

### æµ‹è¯•ç»“æœ
```
è¾“å…¥: "ğŸŒ§ï¸"
ç¼–ç : []
è§£ç : ""
ç»“æœ: âŒ ä¸¢å¤±
```

---

## 2. SimpleCharTokenizer_BACKUP (tests/test_hlbd_quick_learning.py)

### è¡Œä¸º
- **åŠ¨æ€æ·»åŠ æ–°å­—ç¬¦åˆ°è¯æ±‡è¡¨**ï¼ˆç¬¬ 46-55 è¡Œï¼‰

```python
def _get_or_add_char(self, char):
    """è·å–å­—ç¬¦IDï¼Œå¦‚æœä¸å­˜åœ¨åˆ™æ·»åŠ """
    if char not in self.char_to_id:
        if self.next_id < self.vocab_size:
            self.char_to_id[char] = self.next_id
            self.id_to_char[self.next_id] = char
            self.next_id += 1
        else:
            return self.unk_token_id  # â† è¯æ±‡è¡¨æ»¡æ—¶è¿”å› UNK
    return self.char_to_id[char]
```

### Emoji å¤„ç†
- âœ“ emoji ä¼šè¢«**åŠ¨æ€æ·»åŠ åˆ°è¯æ±‡è¡¨**
- âœ“ å¦‚æœè¯æ±‡è¡¨æœªæ»¡ï¼Œemoji è·å¾—ç‹¬ç«‹ ID
- âœ“ å¦‚æœè¯æ±‡è¡¨å·²æ»¡ï¼Œè¿”å› `unk_token_id` (ID=1)
- âœ“ **ä¸ä¼šä¸¢å¤± emoji ä¿¡æ¯**

### ç‰¹æ®Š token
```python
self.vocab = {
    '[PAD]': 0,
    '[UNK]': 1,  # â† å®šä¹‰äº† UNK token
    '[BOS]': 2,
    '[EOS]': 3,
}
```

---

## 3. BertTokenizer (test_hlbd_quick_learning.py å®é™…ä½¿ç”¨)

### è¡Œä¸º
- ä½¿ç”¨ **WordPiece** åˆ†è¯ç®—æ³•
- é¢„è®­ç»ƒçš„ `bert-base-chinese` è¯æ±‡è¡¨
- æœªçŸ¥å­—ç¬¦ä½¿ç”¨ `[UNK]` token

### Emoji å¤„ç†ï¼ˆé¢„æœŸï¼‰
- âœ“ emoji é€šå¸¸è¢«ç¼–ç ä¸º `[UNK]` token
- âœ“ ä¿ç•™ emoji çš„ä½ç½®ä¿¡æ¯
- âœ“ **ä¸ä¼šä¸¢å¤±å­—ç¬¦ä½ç½®**
- âš ï¸ ä½†ä¼šä¸¢å¤±å…·ä½“çš„ emoji è¯­ä¹‰

### ç¤ºä¾‹ï¼ˆåŸºäº BERT è¡Œä¸ºï¼‰
```
è¾“å…¥: "ğŸŒ§ï¸"
ç¼–ç : [100]  # [UNK] token ID
è§£ç : "[UNK]"
ç»“æœ: âš ï¸ ä¿ç•™ä½ç½®ï¼Œä¸¢å¤±è¯­ä¹‰
```

---

## 4. test_hlbd_quick_learning.py ä¸­çš„ä½¿ç”¨

### è®­ç»ƒæ•°æ®ï¼ˆç¬¬ 159-164 è¡Œï¼‰
```python
# åˆ›å»º emoji â†’ ä¸­æ–‡ è®­ç»ƒå¯¹
if 'level_1' in sample and 'level_6' in sample:
    emoji = sample['level_1'].get('emoji', '')
    chinese = sample['level_6'].get('ä¸­æ–‡', '')
    if emoji and chinese:
        pairs.append((emoji, chinese))  # â† è®­ç»ƒå¯¹åŒ…å« emoji
```

### æµ‹è¯•ç”¨ä¾‹ï¼ˆç¬¬ 451-456 è¡Œï¼‰
```python
test_cases = [
    ("ğŸŒ§ï¸", "ä¸‹é›¨"),   # â† emoji è¾“å…¥
    ("â¤ï¸", "æˆ‘çˆ±ä½ "),
    ("I love you", "æˆ‘çˆ±ä½ "),
    ("ä¸‹é›¨", "å¤©æ°”"),
]
```

### å®é™…ä½¿ç”¨çš„ tokenizerï¼ˆç¬¬ 398-402 è¡Œï¼‰
```python
tokenizer = BertTokenizer.from_pretrained(
    bert_path,
    local_files_only=True,
    vocab_file=os.path.join(bert_path, 'vocab.txt')
)
```

---

## 5. å¯¹æ¯”æ€»ç»“

| Tokenizer | Emoji ç¼–ç  | [UNK] Token | ä¿¡æ¯æŸå¤± | ä½ç½®ä¿ç•™ |
|-----------|-----------|-------------|---------|---------|
| **ChineseTokenizer** | `[]` ç©ºåˆ—è¡¨ | âŒ æ—  | âŒ å®Œå…¨ä¸¢å¤± | âŒ æ—  |
| **SimpleCharTokenizer_BACKUP** | åŠ¨æ€æ·»åŠ æˆ– `[UNK]` | âœ“ ID=1 | âœ“ æ— æŸå¤± | âœ“ ä¿ç•™ |
| **BertTokenizer** | `[UNK]` | âœ“ å®šä¹‰ | âš ï¸ è¯­ä¹‰ä¸¢å¤± | âœ“ ä¿ç•™ |

---

## 6. ç»“è®º

### test_hlbd_quick_learning.py å¦‚ä½•å¤„ç† emojiï¼š

1. **ä½¿ç”¨ BertTokenizer**ï¼ˆbert-base-chineseï¼‰
2. Emoji è¢«ç¼–ç ä¸º **`[UNK]` token**
3. **ä¿ç•™äº† emoji çš„ä½ç½®ä¿¡æ¯**ï¼ˆä¸ä¼šåƒ ChineseTokenizer é‚£æ ·è·³è¿‡ï¼‰
4. è®­ç»ƒæ—¶æ¨¡å‹ä¼šå­¦ä¹ ï¼š`[UNK]` â†’ "ä¸‹é›¨"ï¼ˆå¯¹äº ğŸŒ§ï¸ï¼‰
5. è¿™ç§æ–¹æ³•è™½ç„¶ä¸¢å¤±äº† emoji çš„è¯­ä¹‰ï¼Œä½†æ¯”ç›´æ¥è·³è¿‡è¦å¥½

### é—®é¢˜ï¼š

- æ‰€æœ‰ emoji å…±äº«åŒä¸€ä¸ª `[UNK]` token
- æ¨¡å‹æ— æ³•åŒºåˆ†ä¸åŒçš„ emojiï¼ˆğŸŒ§ï¸ å’Œ â¤ï¸ éƒ½æ˜¯ `[UNK]`ï¼‰
- è®­ç»ƒæ•ˆæœå¯èƒ½ä¸ä½³ï¼Œå› ä¸ºå¤šä¸ªä¸åŒçš„è¾“å…¥æ˜ å°„åˆ°åŒä¸€ä¸ª token

### æ”¹è¿›å»ºè®®ï¼š

1. ä½¿ç”¨ **SimpleCharTokenizer_BACKUP** çš„åŠ¨æ€æ·»åŠ æœºåˆ¶
2. æˆ–è€…æ‰©å±• BERT è¯æ±‡è¡¨ï¼ŒåŒ…å«å¸¸ç”¨ emoji
3. æˆ–è€…ä½¿ç”¨æ”¯æŒ emoji çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¦‚å¤šè¯­è¨€ BERTï¼‰

---

## 7. æµ‹è¯•éªŒè¯

åˆ›å»ºäº†ä»¥ä¸‹æµ‹è¯•è„šæœ¬ï¼š

- `test_emoji_simple.py` - éªŒè¯ ChineseTokenizer è¡Œä¸º
- `test_emoji_tokenizer.py` - å®Œæ•´æµ‹è¯•ï¼ˆéœ€è¦ transformersï¼‰
- `test_bert_emoji.py` - æµ‹è¯• BertTokenizer è¡Œä¸ºï¼ˆéœ€è¦ transformersï¼‰

è¿è¡Œå‘½ä»¤ï¼š
```bash
python test_emoji_simple.py  # æ— ä¾èµ–ï¼Œå¯ç›´æ¥è¿è¡Œ
```
