import torch
import torch.nn as nn
import time
import os
from apt.model.architectures.claude4_model import create_claude_unified
from apt.vgpu.runtime.vb_integration import VBModelWrapper
import sys

# ä½¿ç”¨å½“å‰ç›®å½•ï¼Œè·¨å¹³å°å…¼å®¹
output_file = os.path.join(os.getcwd(), 'claude_loss3_training.txt')

with open(output_file, 'w') as f:
    f.write("="*80 + "\n")
    f.write("è®­ç»ƒClaudeæ¨¡å‹ç›´åˆ°Lossé™åˆ°3.0 (ä½¿ç”¨Virtual Blackwell)\n")
    f.write("="*80 + "\n\n")

    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    f.write(f"è®¾å¤‡: {device}\n\n")

    # åˆ›å»ºClaudeæ¨¡å‹
    f.write("åˆ›å»º Claude4 æ¨¡å‹...\n")
    model = create_claude_unified(
        vocab_size=5000,
        d_model=256,
        n_heads=4,
        d_ff=1024,
        num_layers=3,
        rank=2,
        enable_reflection=True
    ).to(device)

    total_params = sum(p.numel() for p in model.parameters())
    f.write(f"æ¨¡å‹å‚æ•°: {total_params:,}\n\n")

    # å¯ç”¨Virtual Blackwell
    f.write("å¯ç”¨ Virtual Blackwell (62å¼ è™šæ‹Ÿæ˜¾å¡)...\n")
    f.flush()

    old_stdout = sys.stdout
    sys.stdout = f

    wrapper = VBModelWrapper(
        model,
        mode='training',
        enable_fp4=True,
        enable_quantization=False,
        replace_pattern='all'
    )

    sys.stdout = old_stdout

    vb_count = len(wrapper.replaced_layers)
    f.write(f"âœ… {vb_count} å¼ è™šæ‹ŸBlackwellæ˜¾å¡å·²å¯ç”¨\n\n")

    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.Adam(wrapper.parameters(), lr=0.0005)  # é™ä½å­¦ä¹ ç‡

    f.write("="*80 + "\n")
    f.write("å¼€å§‹è®­ç»ƒ - ç›®æ ‡: Loss < 3.0\n")
    f.write("="*80 + "\n\n")

    target_loss = 3.0
    max_epochs = 100
    batches_per_epoch = 100
    batch_size = 8

    start_time = time.time()
    total_batches = 0
    best_loss = float('inf')

    for epoch in range(max_epochs):
        epoch_loss = 0
        epoch_start = time.time()

        for batch_idx in range(batches_per_epoch):
            # éšæœºæ•°æ®
            input_ids = torch.randint(0, 5000, (batch_size, 64)).to(device)

            optimizer.zero_grad()

            try:
                outputs = wrapper(input_ids)

                if isinstance(outputs, tuple):
                    logits = outputs[0]
                else:
                    logits = outputs

                # Next token prediction
                shift_logits = logits[:, :-1, :].contiguous()
                shift_labels = input_ids[:, 1:].contiguous()

                loss = nn.functional.cross_entropy(
                    shift_logits.view(-1, shift_logits.size(-1)),
                    shift_labels.view(-1)
                )

                loss.backward()
                optimizer.step()

                epoch_loss += loss.item()
                total_batches += 1

            except Exception as e:
                f.write(f"Error at batch {batch_idx}: {e}\n")
                f.flush()
                continue

        avg_loss = epoch_loss / batches_per_epoch
        epoch_time = time.time() - epoch_start

        if avg_loss < best_loss:
            best_loss = avg_loss

        f.write(f"Epoch {epoch+1:3d}/{max_epochs}: Loss={avg_loss:.4f} (æœ€ä½³={best_loss:.4f}) - {epoch_time:.1f}s\n")
        f.flush()

        # æ¯10ä¸ªepochæ‰“å°è¯¦ç»†ä¿¡æ¯
        if (epoch + 1) % 10 == 0:
            elapsed = time.time() - start_time
            throughput = total_batches / elapsed
            f.write(f"  è¿›åº¦: {total_batches} batches, {throughput:.2f} batch/s\n")
            f.flush()

        # æ£€æŸ¥æ˜¯å¦è¾¾åˆ°ç›®æ ‡
        if avg_loss < target_loss:
            f.write(f"\nğŸ‰ è¾¾åˆ°ç›®æ ‡! Loss {avg_loss:.4f} < {target_loss}\n")
            break

    total_time = time.time() - start_time

    f.write("\n" + "="*80 + "\n")
    f.write("è®­ç»ƒå®Œæˆ!\n")
    f.write("="*80 + "\n")
    f.write(f"æ€»æ—¶é—´: {total_time:.1f}s ({total_time/60:.1f}åˆ†é’Ÿ)\n")
    f.write(f"æ€»æ‰¹æ¬¡: {total_batches}\n")
    f.write(f"ååé‡: {total_batches/total_time:.2f} batch/s\n")
    f.write(f"æœ€ç»ˆLoss: {avg_loss:.4f}\n")
    f.write(f"æœ€ä½³Loss: {best_loss:.4f}\n")

    if avg_loss < target_loss:
        f.write(f"\nâœ… æˆåŠŸ! åœ¨ {epoch+1} ä¸ªepochåè¾¾åˆ°ç›®æ ‡Loss\n")
    else:
        f.write(f"\nâš ï¸  æœªè¾¾åˆ°ç›®æ ‡Loss {target_loss}ï¼Œå½“å‰ {avg_loss:.4f}\n")

    # VBç»Ÿè®¡
    f.write("\n" + "="*80 + "\n")
    f.write("Virtual Blackwell ç»Ÿè®¡\n")
    f.write("="*80 + "\n\n")

    old_stdout = sys.stdout
    sys.stdout = f
    try:
        wrapper.print_all_stats()
    except:
        pass
    sys.stdout = old_stdout

    f.write(f"\n" + "="*80 + "\n")
    f.write(f"ä½¿ç”¨ {vb_count} å¼ è™šæ‹ŸBlackwellæ˜¾å¡è®­ç»ƒå®Œæˆ\n")
    f.write("="*80 + "\n")

print(f"Training complete! Check {output_file}")

# è¯»å–å¹¶æ˜¾ç¤ºç»“æœ
with open(output_file, 'r') as f:
    content = f.read()

# åªæ˜¾ç¤ºæœ€å50è¡Œé¿å…å¤ªé•¿
lines = content.split('\n')
if len(lines) > 50:
    print('\n'.join(lines[-50:]))
else:
    print(content)
