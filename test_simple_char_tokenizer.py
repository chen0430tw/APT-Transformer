#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æµ‹è¯• SimpleCharTokenizer_BACKUP å¯¹ emoji çš„å¤„ç†
"""

import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# ç›´æ¥ä» test_hlbd_quick_learning.py å¯¼å…¥
import importlib.util
spec = importlib.util.spec_from_file_location(
    "test_hlbd",
    os.path.join(os.path.dirname(__file__), "tests/test_hlbd_quick_learning.py")
)
test_hlbd_module = importlib.util.module_from_spec(spec)

# åªæ‰§è¡Œåˆ° SimpleCharTokenizer_BACKUP å®šä¹‰ä¸ºæ­¢
# æˆ‘ä»¬éœ€è¦æ‰‹åŠ¨æå–ç±»å®šä¹‰
exec("""
class SimpleCharTokenizer_BACKUP:
    '''ç®€å•çš„å­—ç¬¦çº§åˆ†è¯å™¨'''
    def __init__(self):
        # åˆ›å»ºä¸€ä¸ªåŸºç¡€å­—ç¬¦è¡¨ï¼ˆåŒ…æ‹¬ä¸­æ–‡ã€è‹±æ–‡ã€emojiç­‰ï¼‰
        self.vocab = {
            '[PAD]': 0, '[UNK]': 1, '[BOS]': 2, '[EOS]': 3,
        }
        self.pad_token_id = 0
        self.unk_token_id = 1
        self.bos_token_id = 2
        self.eos_token_id = 3
        self.vocab_size = 5000  # é¢„ç•™è¶³å¤Ÿçš„è¯æ±‡ç©ºé—´

        # æ·»åŠ å¸¸ç”¨å­—ç¬¦
        self.char_to_id = self.vocab.copy()
        self.id_to_char = {v: k for k, v in self.vocab.items()}
        self.next_id = 4

    def _get_or_add_char(self, char):
        '''è·å–å­—ç¬¦IDï¼Œå¦‚æœä¸å­˜åœ¨åˆ™æ·»åŠ '''
        if char not in self.char_to_id:
            if self.next_id < self.vocab_size:
                self.char_to_id[char] = self.next_id
                self.id_to_char[self.next_id] = char
                self.next_id += 1
            else:
                return self.unk_token_id
        return self.char_to_id[char]

    def encode(self, text, return_tensors=None, add_special_tokens=True):
        '''ç¼–ç æ–‡æœ¬ä¸ºIDåºåˆ—'''
        ids = []
        if add_special_tokens:
            ids.append(self.bos_token_id)
        for char in text:
            ids.append(self._get_or_add_char(char))
        if add_special_tokens:
            ids.append(self.eos_token_id)

        if return_tensors == 'pt':
            import torch
            return torch.tensor([ids])
        return ids

    def __call__(self, text, max_length=64, padding='max_length',
                 truncation=True, return_tensors='pt'):
        '''åˆ†è¯æ¥å£ï¼ˆå…¼å®¹transformersï¼‰'''
        ids = []
        for char in text:
            ids.append(self._get_or_add_char(char))

        # æˆªæ–­
        if truncation and len(ids) > max_length:
            ids = ids[:max_length]

        # å¡«å……
        if padding == 'max_length':
            while len(ids) < max_length:
                ids.append(self.pad_token_id)

        if return_tensors == 'pt':
            import torch
            return {'input_ids': torch.tensor([ids])}
        return {'input_ids': ids}

    def decode(self, ids, skip_special_tokens=True):
        '''è§£ç IDåºåˆ—ä¸ºæ–‡æœ¬'''
        chars = []
        for id in ids:
            if hasattr(id, 'item'):  # torch.Tensor
                id = id.item()

            if skip_special_tokens and id in [self.pad_token_id, self.bos_token_id,
                                               self.eos_token_id, self.unk_token_id]:
                continue

            char = self.id_to_char.get(id, '[UNK]')
            chars.append(char)

        return ''.join(chars)
""")

def test_emoji_encoding():
    """æµ‹è¯• emoji ç¼–ç """

    print("="*70)
    print("æµ‹è¯• SimpleCharTokenizer_BACKUP å¯¹ emoji çš„å¤„ç†")
    print("="*70)

    # åˆ›å»º tokenizer
    tokenizer = SimpleCharTokenizer_BACKUP()

    print(f"\nåˆå§‹è¯æ±‡è¡¨å¤§å°: {len(tokenizer.char_to_id)}")
    print(f"åˆå§‹ next_id: {tokenizer.next_id}")

    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        "ğŸŒ§ï¸",           # é›¨æ»´ emoji
        "â¤ï¸",           # çˆ±å¿ƒ emoji
        "ğŸ½ï¸",           # é¤å…· emoji
        "ğŸ“–",           # ä¹¦æœ¬ emoji
        "ä½ å¥½ğŸŒ§ï¸ä¸–ç•Œ",   # ä¸­æ–‡ + emoji
        "HelloğŸŒ§ï¸World", # è‹±æ–‡ + emoji
        "æˆ‘çˆ±ä½ ",        # çº¯ä¸­æ–‡
        "Hello",        # çº¯è‹±æ–‡
    ]

    print("\næµ‹è¯•ç»“æœ:")
    print("-"*70)

    for text in test_cases:
        # ç¼–ç ï¼ˆä¸æ·»åŠ ç‰¹æ®Š tokenï¼‰
        ids = tokenizer.encode(text, add_special_tokens=False)

        # è§£ç 
        decoded = tokenizer.decode(ids, skip_special_tokens=True)

        print(f"\nåŸæ–‡æœ¬: {text!r}")
        print(f"  ç¼–ç  ID: {ids}")
        print(f"  ID é•¿åº¦: {len(ids)} (åŸæ–‡æœ¬é•¿åº¦: {len(text)})")
        print(f"  è§£ç ç»“æœ: {decoded!r}")

        # æ£€æŸ¥æ˜¯å¦ä½¿ç”¨äº† UNK
        if tokenizer.unk_token_id in ids:
            unk_count = ids.count(tokenizer.unk_token_id)
            print(f"  âš ï¸  åŒ…å« {unk_count} ä¸ª [UNK] token")

        # æ£€æŸ¥æ˜¯å¦æœ‰æŸå¤±
        if text != decoded:
            print(f"  âŒ ç¼–ç /è§£ç æœ‰æŸå¤±ï¼")
        else:
            print(f"  âœ“  ç¼–ç /è§£ç æ— æŸ")

    # æ£€æŸ¥è¯æ±‡è¡¨å¢é•¿
    print("\n" + "="*70)
    print("è¯æ±‡è¡¨ç»Ÿè®¡:")
    print("-"*70)
    print(f"æœ€ç»ˆè¯æ±‡è¡¨å¤§å°: {len(tokenizer.char_to_id)}")
    print(f"æœ€ç»ˆ next_id: {tokenizer.next_id}")
    print(f"æ–°å¢å­—ç¬¦æ•°: {tokenizer.next_id - 4}")

    # æ˜¾ç¤ºæ‰€æœ‰ emoji
    emojis_in_vocab = {char: id for char, id in tokenizer.char_to_id.items()
                       if ord(char) > 0x1F300 or char in ['ğŸŒ§', 'ï¸', 'â¤', 'ğŸ½', 'ğŸ“–']}
    print(f"\nEmoji åœ¨è¯æ±‡è¡¨ä¸­:")
    for emoji, id in sorted(emojis_in_vocab.items(), key=lambda x: x[1])[:20]:
        print(f"  {emoji!r}: ID = {id}")

    # æµ‹è¯•ä¸åŒ emoji æ˜¯å¦æœ‰ä¸åŒ ID
    print("\n" + "="*70)
    print("Emoji å”¯ä¸€æ€§æµ‹è¯•:")
    print("-"*70)

    emoji_list = ["ğŸŒ§ï¸", "â¤ï¸", "ğŸ½ï¸", "ğŸ“–"]
    emoji_ids = {}
    for emoji in emoji_list:
        ids = tokenizer.encode(emoji, add_special_tokens=False)
        emoji_ids[emoji] = tuple(ids)
        print(f"  {emoji!r}: {ids}")

    # æ£€æŸ¥æ˜¯å¦éƒ½ä¸åŒ
    unique_ids = len(set(emoji_ids.values()))
    print(f"\nç‹¬ç‰¹çš„ emoji ç¼–ç : {unique_ids} / {len(emoji_list)}")

    if unique_ids == len(emoji_list):
        print("  âœ“ æ‰€æœ‰ emoji éƒ½æœ‰ç‹¬ç«‹çš„ç¼–ç ï¼")
    else:
        print("  âŒ æœ‰äº› emoji å…±äº«ç›¸åŒçš„ç¼–ç ")

    print("="*70)

if __name__ == "__main__":
    test_emoji_encoding()
