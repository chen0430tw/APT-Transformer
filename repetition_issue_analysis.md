# æ¨¡å‹"å¤è¯»"é—®é¢˜åˆ†æ

## é—®é¢˜ç°è±¡

æ¨¡å‹ç”Ÿæˆçš„æ–‡æœ¬æ€»æ˜¯åŒ…å«è¾“å…¥ï¼Œç„¶ååŠ ä¸€äº›éšæœºå­—ç¬¦ï¼š

```
è¾“å…¥: I love you
ç”Ÿæˆ: i love you ä¿ ä¹  æ˜Ÿ æƒ… å¿ƒ ã€‚  â† å¤è¯»äº†è¾“å…¥

è¾“å…¥: ä¸‹é›¨
ç”Ÿæˆ: ä¸‹ é›¨ æ—¥ å¤© æ»‘ ï¼Œ è¿› æƒ³ æ—  çš„ äº« æ—¶ å…‰ ã€‚  â† å¤è¯»äº†è¾“å…¥

è¾“å…¥: ğŸŒ§ï¸
ç”Ÿæˆ: å® ä¹¦ è·‘ ï¼Œ å¢ å‡ è§‰ é¦™ ã€‚  â† éšæœºå­—ç¬¦ï¼ˆemojiè¢«ç¼–ç ä¸º[UNK]ï¼‰
```

---

## æ ¹æœ¬åŸå› 

### ç”Ÿæˆå‡½æ•°è®¾è®¡é—®é¢˜ï¼ˆtest_hlbd_quick_learning.py ç¬¬ 323-344 è¡Œï¼‰

```python
# ç¬¬ 323 è¡Œï¼šç¼–ç è¾“å…¥æ–‡æœ¬
input_encoding = tokenizer.encode(input_text, return_tensors='pt', add_special_tokens=False).to(device)

# ç¬¬ 328 è¡Œï¼šå°†è¾“å…¥æ‹¼æ¥åˆ°ç”Ÿæˆåºåˆ—ä¸­
initial_ids = torch.cat([bos_tensor, input_encoding], dim=1)
#                                    ^^^^^^^^^^^^^^
#                                    é—®é¢˜åœ¨è¿™é‡Œï¼

# ç¬¬ 331-340 è¡Œï¼šç”Ÿæˆåºåˆ—ï¼ˆä» [BOS, è¾“å…¥tokens...] å¼€å§‹ï¼‰
generated_ids = model.generate(
    input_ids=initial_ids,  # â† åŒ…å«äº†è¾“å…¥
    max_length=max_length + initial_ids.size(1),
    ...
)

# ç¬¬ 344 è¡Œï¼šè§£ç æ•´ä¸ªåºåˆ—
generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
#                                                    â† è¾“å‡ºåŒ…å«è¾“å…¥
```

### æµç¨‹åˆ†æ

1. **è¾“å…¥ "I love you"**ï¼š
   - ç¼–ç ï¼š`[token_I, token_love, token_you]`
   - æ‹¼æ¥ BOSï¼š`[BOS, token_I, token_love, token_you]`
   - ç”Ÿæˆï¼š`[BOS, token_I, token_love, token_you, token_ä¿, token_ä¹ , ...]`
   - è§£ç ï¼š`"i love you ä¿ ä¹  ..."`
   - **ç»“æœï¼šå¤è¯»äº†è¾“å…¥ï¼**

2. **è¾“å…¥ "ä¸‹é›¨"**ï¼š
   - ç¼–ç ï¼š`[token_ä¸‹, token_é›¨]`
   - æ‹¼æ¥ BOSï¼š`[BOS, token_ä¸‹, token_é›¨]`
   - ç”Ÿæˆï¼š`[BOS, token_ä¸‹, token_é›¨, token_æ—¥, token_å¤©, ...]`
   - è§£ç ï¼š`"ä¸‹ é›¨ æ—¥ å¤© ..."`
   - **ç»“æœï¼šå¤è¯»äº†è¾“å…¥ï¼**

3. **è¾“å…¥ "ğŸŒ§ï¸"**ï¼ˆemojiï¼‰ï¼š
   - ç¼–ç ï¼š`[UNK]` ï¼ˆBertTokenizer å°† emoji ç¼–ç ä¸º [UNK]ï¼‰
   - æ‹¼æ¥ BOSï¼š`[BOS, UNK]`
   - ç”Ÿæˆï¼š`[BOS, UNK, token_å®, token_ä¹¦, ...]`
   - è§£ç ï¼š`"[UNK] å® ä¹¦ ..."`ï¼ˆä½† skip_special_tokens=True ä¼šç§»é™¤ [UNK]ï¼‰
   - **ç»“æœï¼šéšæœºç”Ÿæˆï¼Œå› ä¸ºæ¨¡å‹æ²¡æœ‰å­¦åˆ° [UNK] â†’ "ä¸‹é›¨"**

---

## ä¸ºä»€ä¹ˆä¼šè¿™æ ·è®¾è®¡ï¼Ÿ

è¿™ä¸ªè®¾è®¡çœ‹èµ·æ¥æ˜¯æƒ³åš **Seq2Seq ç”Ÿæˆ**ï¼Œä½†æ··æ·†äº†ä¸¤ç§æ¨¡å¼ï¼š

### 1. Encoder-Decoder æ¨¡å¼ï¼ˆæ­£ç¡®ï¼‰
```python
# è¾“å…¥ä½œä¸ºç¼–ç å™¨è¾“å…¥
encoder_input = input_text
# è§£ç å™¨ä» [BOS] å¼€å§‹ç”Ÿæˆ
decoder_input = [BOS]
# è¾“å‡ºä¸åŒ…å«è¾“å…¥
output = generate_from_scratch()
```

### 2. Prefix-based ç”Ÿæˆï¼ˆå½“å‰å®ç°ï¼‰
```python
# è¾“å…¥ä½œä¸º prefix
initial_ids = [BOS, input_tokens...]
# ä» prefix ç»§ç»­ç”Ÿæˆ
output = generate_continuation()
# è¾“å‡ºåŒ…å«è¾“å…¥ï¼ˆè¿™æ˜¯ GPT-style çš„ç»­å†™ï¼‰
```

**å½“å‰ä»£ç ä½¿ç”¨äº† Prefix-based ç”Ÿæˆï¼Œå¯¼è‡´è¾“å‡ºæ€»æ˜¯åŒ…å«è¾“å…¥ã€‚**

---

## Emoji é—®é¢˜

é¢å¤–çš„é—®é¢˜ï¼š**æ‰€æœ‰ emoji éƒ½è¢«ç¼–ç ä¸ºåŒä¸€ä¸ª `[UNK]` token**

```python
# BertTokenizer ç¼–ç 
tokenizer.encode("ğŸŒ§ï¸")  # â†’ [UNK]
tokenizer.encode("â¤ï¸")   # â†’ [UNK]
tokenizer.encode("ğŸ½ï¸")   # â†’ [UNK]
```

**ç»“æœï¼š**
- æ¨¡å‹æ— æ³•åŒºåˆ†ä¸åŒçš„ emoji
- è®­ç»ƒæ•°æ®ä¸­ï¼š
  - `ğŸŒ§ï¸ â†’ "ä¸‹é›¨"`
  - `â¤ï¸ â†’ "æˆ‘çˆ±ä½ "`
  - `ğŸ½ï¸ â†’ "åƒé¥­"`
- éƒ½å˜æˆäº†ï¼š
  - `[UNK] â†’ ï¼Ÿï¼Ÿï¼Ÿ`ï¼ˆæ¨¡å‹æ— æ³•å­¦ä¹ ï¼‰

---

## è§£å†³æ–¹æ¡ˆ

### æ–¹æ¡ˆ 1ï¼šä¿®æ”¹ç”Ÿæˆå‡½æ•°ï¼ˆå»æ‰è¾“å…¥éƒ¨åˆ†ï¼‰

```python
def generate_text(model, tokenizer, input_text, device, max_length=50, repetition_penalty=1.5):
    model.eval()

    # ç¼–ç è¾“å…¥
    input_encoding = tokenizer.encode(input_text, return_tensors='pt', add_special_tokens=False).to(device)
    bos_tensor = torch.tensor([[tokenizer.bos_token_id]], device=device)
    initial_ids = torch.cat([bos_tensor, input_encoding], dim=1)

    # ç”Ÿæˆ
    generated_ids = model.generate(
        input_ids=initial_ids,
        max_length=max_length + initial_ids.size(1),
        ...
    )

    # ã€ä¿®å¤ã€‘åªè§£ç æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼Œå»æ‰è¾“å…¥
    input_length = initial_ids.size(1)
    generated_only = generated_ids[0][input_length:]  # â† å»æ‰è¾“å…¥éƒ¨åˆ†
    generated_text = tokenizer.decode(generated_only, skip_special_tokens=True)

    return generated_text
```

### æ–¹æ¡ˆ 2ï¼šä½¿ç”¨ Encoder-Decoder æ¶æ„

å¦‚æœ APTModel æ”¯æŒ Encoder-Decoderï¼Œåº”è¯¥ï¼š
```python
output = model.generate(
    encoder_input_ids=input_encoding,  # è¾“å…¥ä½œä¸ºç¼–ç å™¨è¾“å…¥
    decoder_input_ids=bos_tensor,       # è§£ç å™¨ä» BOS å¼€å§‹
    ...
)
```

### æ–¹æ¡ˆ 3ï¼šä¿®å¤ Emoji å¤„ç†

ä½¿ç”¨æ”¯æŒ emoji çš„ tokenizerï¼š
- æ‰©å±• BERT è¯æ±‡è¡¨
- æˆ–ä½¿ç”¨ SimpleCharTokenizer_BACKUPï¼ˆåŠ¨æ€æ·»åŠ å­—ç¬¦ï¼‰
- æˆ–ä½¿ç”¨å¤šè¯­è¨€æ¨¡å‹ï¼ˆå¦‚ XLM-Rï¼‰

---

## ç»“è®º

**"å¤è¯»"é—®é¢˜çš„åŸå› ï¼š**
1. âœ… ç”Ÿæˆå‡½æ•°å°†è¾“å…¥åŒ…å«åœ¨è¾“å‡ºä¸­ï¼ˆè®¾è®¡é—®é¢˜ï¼‰
2. âœ… Emoji è¢«ç¼–ç ä¸º `[UNK]`ï¼Œæ¨¡å‹æ— æ³•åŒºåˆ†ï¼ˆtokenizer é—®é¢˜ï¼‰
3. âŒ ä¸æ˜¯è®­ç»ƒé—®é¢˜
4. âŒ ä¸æ˜¯æˆ‘ï¼ˆClaudeï¼‰é€ æˆçš„

**ä¿®å¤ä¼˜å…ˆçº§ï¼š**
1. é«˜ï¼šä¿®æ”¹ç”Ÿæˆå‡½æ•°ï¼Œå»æ‰è¾“å…¥éƒ¨åˆ†
2. é«˜ï¼šä¿®å¤ emoji ç¼–ç é—®é¢˜
3. ä¸­ï¼šè°ƒæ•´è®­ç»ƒå‚æ•°ï¼ˆloss è¿˜åœ¨ 2.3+ï¼‰
