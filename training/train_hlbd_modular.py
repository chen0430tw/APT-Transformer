#!/usr/bin/env python3
"""
HLBDæ¨¡å—åŒ–è®­ç»ƒè„šæœ¬
æ”¯æŒåŒæ—¶åŠ è½½å¤šä¸ªHLBDæ•°æ®é›†è¿›è¡Œè”åˆè®­ç»ƒ

ç‰¹æ€§:
- ğŸ”— å¤šæ•°æ®é›†åˆå¹¶è®­ç»ƒ
- ğŸ“Š è‡ªåŠ¨æ ¼å¼å…¼å®¹å¤„ç†
- ğŸ² æ•°æ®é›†æ··åˆæ‰“æ•£
- ğŸ“ˆ ç»Ÿä¸€è®­ç»ƒæµç¨‹
"""

import os
import sys
import json
import argparse
from pathlib import Path
from typing import List, Dict, Any

# æ£€æŸ¥ä¾èµ–
try:
    import torch
    import torch.nn as nn
    from torch.utils.data import Dataset, DataLoader
    from torch.cuda.amp import autocast, GradScaler
except ImportError:
    print("âŒ PyTorchæœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…:")
    print("   pip install torch")
    sys.exit(1)


class ModularHLBDDataset:
    """æ¨¡å—åŒ–HLBDæ•°æ®é›†åŠ è½½å™¨"""

    def __init__(self, dataset_paths: List[str]):
        self.dataset_paths = dataset_paths
        self.all_samples = []
        self.dataset_stats = {}

    def load_datasets(self):
        """åŠ è½½å¹¶åˆå¹¶å¤šä¸ªæ•°æ®é›†"""
        print("=" * 60)
        print("æ¨¡å—åŒ–HLBDæ•°æ®é›†åŠ è½½å™¨")
        print("=" * 60)
        print()

        for i, path in enumerate(self.dataset_paths, 1):
            print(f"ğŸ“‚ åŠ è½½æ•°æ®é›† {i}/{len(self.dataset_paths)}: {path}")
            samples = self._load_single_dataset(path)

            if samples:
                self.all_samples.extend(samples)
                dataset_name = Path(path).stem
                self.dataset_stats[dataset_name] = len(samples)
                print(f"   âœ“ æˆåŠŸåŠ è½½ {len(samples)} æ¡æ ·æœ¬")
            else:
                print(f"   âš ï¸  æ•°æ®é›†ä¸ºç©ºæˆ–åŠ è½½å¤±è´¥")
            print()

        # æ‰“æ•£æ··åˆ
        import random
        random.shuffle(self.all_samples)

        print(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
        for name, count in self.dataset_stats.items():
            percentage = count / len(self.all_samples) * 100
            print(f"   {name}: {count} æ¡ ({percentage:.1f}%)")
        print(f"   æ€»è®¡: {len(self.all_samples)} æ¡æ ·æœ¬")
        print(f"   âœ“ å·²æ··åˆæ‰“æ•£\n")

        return self.all_samples

    def _load_single_dataset(self, path: str) -> List[Dict]:
        """åŠ è½½å•ä¸ªæ•°æ®é›†å¹¶è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼"""
        try:
            with open(path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # æ£€æµ‹æ•°æ®é›†ç±»å‹
            if 'samples' in data:
                # HLBD Fullæ ¼å¼ï¼ˆ8å±‚ç»“æ„ï¼‰
                return self._process_hlbd_full(data['samples'])
            elif 'data' in data:
                # HLBD Hardcoreæ ¼å¼ï¼ˆæ¨¡å—åŒ–ï¼‰
                return self._process_hlbd_hardcore(data['data'])
            else:
                print(f"   âš ï¸  æœªçŸ¥çš„æ•°æ®é›†æ ¼å¼")
                return []

        except Exception as e:
            print(f"   âŒ åŠ è½½å¤±è´¥: {e}")
            return []

    def _process_hlbd_full(self, samples: List[Dict]) -> List[Dict]:
        """å¤„ç†HLBD Fullæ ¼å¼ï¼ˆ8å±‚ç»“æ„ï¼‰"""
        processed = []

        for sample in samples:
            # æå–æ‰€æœ‰å±‚çº§çš„æ–‡æœ¬
            layers = []
            concept = sample.get('concept', '')

            # Level 1: å­—å¡
            if 'level_1' in sample:
                char_card = sample['level_1'].get('å­—å¡', '')
                emoji = sample['level_1'].get('emoji', '')
                layers.append(f"å­—å¡: {char_card} {emoji}")

            # Level 2: çŸ­è¯­
            if 'level_2' in sample:
                phrase = sample['level_2'].get('çŸ­è¯­', '')
                layers.append(f"çŸ­è¯­: {phrase}")

            # Level 3: æ•°å­¦ï¼ˆå¥æ³•ç»“æ„ï¼‰â† é‡è¦ï¼
            if 'level_3' in sample:
                math_expr = sample['level_3'].get('æ•°å­¦', '')
                layers.append(f"å¥æ³•: {math_expr}")

            # Level 4: æ‹¼éŸ³
            if 'level_4' in sample:
                pinyin = sample['level_4'].get('æ‹¼éŸ³', '')
                layers.append(f"æ‹¼éŸ³: {pinyin}")

            # Level 5: è‹±æ–‡
            if 'level_5' in sample:
                english = sample['level_5'].get('è‹±æ–‡', '')
                layers.append(f"è‹±æ–‡: {english}")

            # Level 6: ä¸­æ–‡
            if 'level_6' in sample:
                chinese = sample['level_6'].get('ä¸­æ–‡', '')
                layers.append(f"ä¸­æ–‡: {chinese}")

            # Level 7-8: æ—¥æ–‡ã€éŸ©æ–‡ï¼ˆå¯é€‰ï¼‰
            if 'level_7' in sample:
                japanese = sample['level_7'].get('æ—¥æ–‡', '')
                if japanese:
                    layers.append(f"æ—¥æ–‡: {japanese}")

            if 'level_8' in sample:
                korean = sample['level_8'].get('éŸ©æ–‡', '')
                if korean:
                    layers.append(f"éŸ©æ–‡: {korean}")

            # ç»„åˆæˆè®­ç»ƒæ–‡æœ¬
            text = f"æ¦‚å¿µ: {concept}\n" + "\n".join(layers)

            processed.append({
                'text': text,
                'source': 'HLBD_Full',
                'concept': concept
            })

        return processed

    def _process_hlbd_hardcore(self, data: Dict) -> List[Dict]:
        """å¤„ç†HLBD Hardcoreæ ¼å¼ï¼ˆæ¨¡å—åŒ–ï¼‰"""
        processed = []

        for module, samples in data.items():
            for sample in samples:
                input_text = sample.get('input', '')
                output_text = sample.get('output', '')

                # è½¬æ¢ä¸ºé—®ç­”æ ¼å¼
                text = f"é—®é¢˜: {input_text}\nç­”æ¡ˆ: {output_text}"

                processed.append({
                    'text': text,
                    'source': f'HLBD_Hardcore_{module}',
                    'module': module
                })

        return processed


class HLBDTrainingDataset(Dataset):
    """PyTorchæ•°æ®é›†åŒ…è£…å™¨"""

    def __init__(self, samples: List[Dict], tokenizer, max_length: int = 512):
        self.samples = samples
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        sample = self.samples[idx]
        text = sample['text']

        # ç®€å•å­—ç¬¦çº§tokenizationï¼ˆå®é™…è®­ç»ƒæ—¶éœ€è¦proper tokenizerï¼‰
        # è¿™é‡Œä»…ä½œç¤ºä¾‹
        tokens = [ord(c) % 5000 for c in text[:self.max_length]]

        # Padding
        if len(tokens) < self.max_length:
            tokens += [0] * (self.max_length - len(tokens))

        input_ids = torch.tensor(tokens[:self.max_length], dtype=torch.long)

        return {
            'input_ids': input_ids,
            'labels': input_ids.clone()  # è‡ªå›å½’è®­ç»ƒ
        }


def parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(
        description="HLBDæ¨¡å—åŒ–è®­ç»ƒ - æ”¯æŒå¤šæ•°æ®é›†åˆå¹¶è®­ç»ƒ"
    )

    # æ”¯æŒå¤šä¸ªæ•°æ®é›†
    parser.add_argument(
        '--datasets',
        nargs='+',
        required=True,
        help='æ•°æ®é›†è·¯å¾„åˆ—è¡¨ï¼ˆæ”¯æŒå¤šä¸ªï¼‰ï¼Œä¾‹å¦‚: data/HLBD_Full_V2.json data/HLBD_Hardcore_Full_V2.json'
    )

    # è®­ç»ƒå‚æ•°
    parser.add_argument('--output-dir', type=str, default='models/hlbd_modular',
                       help='æ¨¡å‹è¾“å‡ºç›®å½•')
    parser.add_argument('--epochs', type=int, default=50,
                       help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch-size', type=int, default=32,
                       help='æ‰¹æ¬¡å¤§å°')
    parser.add_argument('--learning-rate', type=float, default=5e-5,
                       help='å­¦ä¹ ç‡')
    parser.add_argument('--max-length', type=int, default=512,
                       help='æœ€å¤§åºåˆ—é•¿åº¦')
    parser.add_argument('--device', type=str, default='cuda' if torch.cuda.is_available() else 'cpu',
                       help='è®­ç»ƒè®¾å¤‡')

    return parser.parse_args()


def main():
    """ä¸»è®­ç»ƒæµç¨‹"""
    args = parse_args()

    print("=" * 60)
    print("ğŸ”— HLBDæ¨¡å—åŒ–è®­ç»ƒç³»ç»Ÿ")
    print("=" * 60)
    print(f"\né…ç½®:")
    print(f"  æ•°æ®é›†æ•°é‡: {len(args.datasets)}")
    for i, ds in enumerate(args.datasets, 1):
        print(f"    {i}. {ds}")
    print(f"  è¾“å‡ºç›®å½•: {args.output_dir}")
    print(f"  è®­ç»ƒè½®æ•°: {args.epochs}")
    print(f"  æ‰¹æ¬¡å¤§å°: {args.batch_size}")
    print(f"  å­¦ä¹ ç‡: {args.learning_rate}")
    print(f"  è®¾å¤‡: {args.device}")
    print()

    # åŠ è½½æ•°æ®é›†
    loader = ModularHLBDDataset(args.datasets)
    all_samples = loader.load_datasets()

    if len(all_samples) == 0:
        print("âŒ æ²¡æœ‰æœ‰æ•ˆçš„è®­ç»ƒæ ·æœ¬")
        return 1

    # åˆ›å»ºPyTorchæ•°æ®é›†ï¼ˆè¿™é‡Œéœ€è¦å®é™…çš„tokenizerï¼‰
    print("ğŸ“¦ å‡†å¤‡è®­ç»ƒæ•°æ®...")
    # æ³¨æ„ï¼šè¿™é‡Œä½¿ç”¨ç®€åŒ–çš„tokenizerï¼Œå®é™…è®­ç»ƒéœ€è¦proper tokenizer
    train_dataset = HLBDTrainingDataset(
        all_samples,
        tokenizer=None,  # å®é™…éœ€è¦ä¼ å…¥tokenizer
        max_length=args.max_length
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=args.batch_size,
        shuffle=True,
        num_workers=0
    )

    print(f"   âœ“ è®­ç»ƒæ ·æœ¬: {len(train_dataset)}")
    print(f"   âœ“ æ‰¹æ¬¡æ•°: {len(train_loader)}")
    print()

    # åˆ›å»ºè¾“å‡ºç›®å½•
    output_dir = Path(args.output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    # ä¿å­˜æ•°æ®é›†ç»Ÿè®¡
    stats = {
        'total_samples': len(all_samples),
        'datasets': loader.dataset_stats,
        'training_config': {
            'epochs': args.epochs,
            'batch_size': args.batch_size,
            'learning_rate': args.learning_rate,
            'max_length': args.max_length
        }
    }

    with open(output_dir / 'dataset_stats.json', 'w', encoding='utf-8') as f:
        json.dump(stats, f, ensure_ascii=False, indent=2)

    print("ğŸ’¡ æç¤º:")
    print("   æ­¤è„šæœ¬å±•ç¤ºäº†æ¨¡å—åŒ–è®­ç»ƒçš„æ¡†æ¶")
    print("   å®é™…è®­ç»ƒéœ€è¦:")
    print("   1. åŠ è½½å®Œæ•´çš„APTæ¨¡å‹")
    print("   2. é…ç½®proper tokenizer")
    print("   3. å®ç°è®­ç»ƒå¾ªç¯")
    print("   4. æ·»åŠ è¯„ä¼°å’Œä¿å­˜é€»è¾‘")
    print()
    print(f"ğŸ“Š æ•°æ®é›†ç»Ÿè®¡å·²ä¿å­˜åˆ°: {output_dir / 'dataset_stats.json'}")
    print()
    print("=" * 60)
    print("âœ… æ¨¡å—åŒ–æ•°æ®åŠ è½½å®Œæˆï¼")
    print("=" * 60)
    print()
    print("ğŸ¯ ä¸‹ä¸€æ­¥: é›†æˆåˆ°å®Œæ•´çš„training/train_hlbd_playground.py")

    return 0


if __name__ == "__main__":
    sys.exit(main())
