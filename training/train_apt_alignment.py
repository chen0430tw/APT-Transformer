#!/usr/bin/env python3
"""
ğŸš€ APTæ¨ç†ä¸å¯¹é½ - ä¸€é”®è®­ç»ƒè„šæœ¬
Alignment & Reasoning Training Pipeline

è®­ç»ƒæµç¨‹:
1. SFT (Supervised Fine-Tuning) - åŸºç¡€èƒ½åŠ›
2. DPO/GRPO - åå¥½å¯¹é½
3. Loyalty Training - å¿ è¯šåº¦è®­ç»ƒ (GRPO + Owner Rewards)
4. Storm Training - æš´é£é›¨è®­ç»ƒ (åŠ¨æ€æ¨ç†/å†…åŒ–CoT)

ä½œè€…: chen0430tw
æ—¥æœŸ: 2024-12-23
"""

import os
import sys
import argparse
import json
from pathlib import Path
from typing import Optional, Dict, List
import torch

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent.absolute()
sys.path.insert(0, str(PROJECT_ROOT))

from apt_model.modeling.apt_model import APTModel, APTModelConfiguration
from apt_model.rl.grpo_trainer import GRPOTrainer, GRPOConfig
from apt_model.rl.dpo_trainer import DPOTrainer, DPOConfig
from apt_model.rl.reward_model import RewardModel


class APTAlignmentPipeline:
    """APTæ¨ç†ä¸å¯¹é½è®­ç»ƒæµç¨‹"""

    def __init__(
        self,
        model_config: Optional[APTModelConfiguration] = None,
        base_model_path: Optional[str] = None,
        output_dir: str = "./apt_aligned_models",
        device: str = "cuda"
    ):
        """
        Args:
            model_config: APTæ¨¡å‹é…ç½®
            base_model_path: åŸºç¡€æ¨¡å‹è·¯å¾„ï¼ˆå¦‚æœæä¾›ï¼‰
            output_dir: è¾“å‡ºç›®å½•
            device: è®­ç»ƒè®¾å¤‡
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        self.device = device

        # åˆå§‹åŒ–æ¨¡å‹
        if base_model_path:
            print(f"ğŸ“¥ åŠ è½½åŸºç¡€æ¨¡å‹: {base_model_path}")
            self.model = APTModel.from_pretrained(base_model_path)
        elif model_config:
            print("ğŸ—ï¸  åˆ›å»ºæ–°æ¨¡å‹")
            self.model = APTModel(model_config)
        else:
            raise ValueError("å¿…é¡»æä¾› model_config æˆ– base_model_path")

        self.model.to(device)

        # è®­ç»ƒå†å²
        self.training_history = {
            'sft': None,
            'dpo': None,
            'grpo': None,
            'loyalty': None,
            'storm': None
        }

    # ========================================================================
    # Stage 1: SFT (Supervised Fine-Tuning)
    # ========================================================================

    def train_sft(
        self,
        dataset_path: str,
        epochs: int = 3,
        batch_size: int = 8,
        learning_rate: float = 5e-5,
        save_checkpoint: bool = True
    ):
        """
        Stage 1: ç›‘ç£å¾®è°ƒ - å­¦ä¹ åŸºç¡€æŒ‡ä»¤éµå¾ª

        Args:
            dataset_path: æŒ‡ä»¤æ•°æ®é›†è·¯å¾„ (JSONæ ¼å¼)
            epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            learning_rate: å­¦ä¹ ç‡
            save_checkpoint: æ˜¯å¦ä¿å­˜æ£€æŸ¥ç‚¹
        """
        print("\n" + "="*60)
        print("ğŸ“š Stage 1: SFT (Supervised Fine-Tuning)")
        print("="*60)

        # TODO: å®ç°SFTè®­ç»ƒé€»è¾‘
        # è¿™é‡Œä½¿ç”¨æ ‡å‡†çš„äº¤å‰ç†µæŸå¤±è®­ç»ƒ

        print(f"âœ“ æ•°æ®é›†: {dataset_path}")
        print(f"âœ“ Epochs: {epochs}, Batch: {batch_size}, LR: {learning_rate}")

        # ä¿å­˜SFTæ¨¡å‹
        if save_checkpoint:
            sft_path = self.output_dir / "sft_model"
            sft_path.mkdir(exist_ok=True)
            # self.model.save_pretrained(str(sft_path))
            print(f"ğŸ’¾ SFTæ¨¡å‹å·²ä¿å­˜: {sft_path}")

        self.training_history['sft'] = {
            'dataset': dataset_path,
            'epochs': epochs,
            'final_loss': 0.0  # Placeholder
        }

        return self.model

    # ========================================================================
    # Stage 2: DPO/GRPO (Preference Alignment)
    # ========================================================================

    def train_dpo(
        self,
        preference_dataset: str,
        epochs: int = 1,
        batch_size: int = 4,
        beta: float = 0.1,
        save_checkpoint: bool = True
    ):
        """
        Stage 2a: DPOè®­ç»ƒ - åå¥½å¯¹é½

        Args:
            preference_dataset: åå¥½æ•°æ®é›† (chosen vs rejected pairs)
            epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            beta: DPOæ¸©åº¦å‚æ•°
            save_checkpoint: æ˜¯å¦ä¿å­˜æ£€æŸ¥ç‚¹
        """
        print("\n" + "="*60)
        print("ğŸ¯ Stage 2a: DPO (Direct Preference Optimization)")
        print("="*60)

        config = DPOConfig(
            beta=beta,
            learning_rate=1e-5,
            max_grad_norm=1.0
        )

        # åˆå§‹åŒ–DPOè®­ç»ƒå™¨
        # dpo_trainer = DPOTrainer(
        #     policy_model=self.model,
        #     config=config,
        #     device=self.device
        # )

        print(f"âœ“ åå¥½æ•°æ®é›†: {preference_dataset}")
        print(f"âœ“ Beta: {beta}, Epochs: {epochs}")

        if save_checkpoint:
            dpo_path = self.output_dir / "dpo_model"
            dpo_path.mkdir(exist_ok=True)
            print(f"ğŸ’¾ DPOæ¨¡å‹å·²ä¿å­˜: {dpo_path}")

        self.training_history['dpo'] = {
            'dataset': preference_dataset,
            'epochs': epochs,
            'beta': beta
        }

        return self.model

    def train_grpo(
        self,
        prompts_dataset: str,
        reward_model_path: Optional[str] = None,
        epochs: int = 1,
        group_size: int = 4,
        save_checkpoint: bool = True
    ):
        """
        Stage 2b: GRPOè®­ç»ƒ - åˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–

        Args:
            prompts_dataset: Promptæ•°æ®é›†
            reward_model_path: å¥–åŠ±æ¨¡å‹è·¯å¾„
            epochs: è®­ç»ƒè½®æ•°
            group_size: æ¯ç»„æ ·æœ¬æ•°
            save_checkpoint: æ˜¯å¦ä¿å­˜æ£€æŸ¥ç‚¹
        """
        print("\n" + "="*60)
        print("ğŸš€ Stage 2b: GRPO (Group Relative Policy Optimization)")
        print("="*60)

        config = GRPOConfig(
            group_size=group_size,
            learning_rate=1e-5,
            kl_coef=0.1
        )

        # åŠ è½½å¥–åŠ±æ¨¡å‹
        reward_model = None
        if reward_model_path:
            print(f"ğŸ“¥ åŠ è½½å¥–åŠ±æ¨¡å‹: {reward_model_path}")
            # reward_model = RewardModel.from_pretrained(reward_model_path)

        # åˆå§‹åŒ–GRPOè®­ç»ƒå™¨
        # grpo_trainer = GRPOTrainer(
        #     policy_model=self.model,
        #     reward_model=reward_model,
        #     config=config,
        #     device=self.device
        # )

        print(f"âœ“ Prompts: {prompts_dataset}")
        print(f"âœ“ Group Size: {group_size}, Epochs: {epochs}")

        if save_checkpoint:
            grpo_path = self.output_dir / "grpo_model"
            grpo_path.mkdir(exist_ok=True)
            print(f"ğŸ’¾ GRPOæ¨¡å‹å·²ä¿å­˜: {grpo_path}")

        self.training_history['grpo'] = {
            'dataset': prompts_dataset,
            'epochs': epochs,
            'group_size': group_size
        }

        return self.model

    # ========================================================================
    # Stage 3: Loyalty Training (å¿ è¯šåº¦è®­ç»ƒ)
    # ========================================================================

    def train_loyalty(
        self,
        owner_prompts: str,
        public_prompts: str,
        owner_reward_bonus: float = 2.0,
        epochs: int = 1,
        save_checkpoint: bool = True
    ):
        """
        Stage 3: å¿ è¯šåº¦è®­ç»ƒ - åŒºåˆ†ä¸»äººå’Œå¤§ä¼—

        æ ¸å¿ƒæ€æƒ³:
        - ä¸»äººçš„æç¤º â†’ é«˜å¥–åŠ±ã€ä¼˜å…ˆå“åº”
        - å¤§ä¼—çš„æç¤º â†’ æ­£å¸¸å¥–åŠ±ã€æ ‡å‡†å“åº”

        ä½¿ç”¨GRPO + å®šåˆ¶å¥–åŠ±å‡½æ•°:
        reward = base_reward + (owner_bonus if is_owner else 0)

        Args:
            owner_prompts: ä¸»äººæç¤ºæ•°æ®é›† (æ ‡è®°ä¸ºowner=True)
            public_prompts: å…¬ä¼—æç¤ºæ•°æ®é›† (owner=False)
            owner_reward_bonus: ä¸»äººå¥–åŠ±åŠ æˆ
            epochs: è®­ç»ƒè½®æ•°
            save_checkpoint: æ˜¯å¦ä¿å­˜æ£€æŸ¥ç‚¹
        """
        print("\n" + "="*60)
        print("ğŸ‘‘ Stage 3: Loyalty Training (å¿ è¯šåº¦è®­ç»ƒ)")
        print("="*60)

        print(f"ğŸ¯ è®­ç»ƒç›®æ ‡: åŒºåˆ†ä¸»äºº vs å¤§ä¼—")
        print(f"   ä¸»äººå¥–åŠ±åŠ æˆ: +{owner_reward_bonus}")
        print(f"   ä¸»äººæ•°æ®: {owner_prompts}")
        print(f"   å…¬ä¼—æ•°æ®: {public_prompts}")

        # åˆ›å»ºå®šåˆ¶å¥–åŠ±å‡½æ•°
        class LoyaltyRewardModel:
            """å¿ è¯šåº¦å¥–åŠ±æ¨¡å‹"""
            def __init__(self, base_reward_model, owner_bonus: float = 2.0):
                self.base_model = base_reward_model
                self.owner_bonus = owner_bonus

            def compute_reward(self, responses: List[str], is_owner: bool) -> torch.Tensor:
                """
                è®¡ç®—å¥–åŠ±:
                - åŸºç¡€å¥–åŠ± (response quality)
                - ä¸»äººåŠ æˆ (if is_owner)
                """
                # base_rewards = self.base_model(responses)
                base_rewards = torch.randn(len(responses))  # Placeholder

                if is_owner:
                    return base_rewards + self.owner_bonus
                return base_rewards

        # loyalty_reward = LoyaltyRewardModel(
        #     base_reward_model=None,  # TODO: åŠ è½½åŸºç¡€å¥–åŠ±æ¨¡å‹
        #     owner_bonus=owner_reward_bonus
        # )

        config = GRPOConfig(
            group_size=4,
            learning_rate=5e-6,  # é™ä½LRé¿å…è¿‡æ‹Ÿåˆ
            kl_coef=0.15  # å¢åŠ KLæƒ©ç½šä¿æŒé€šç”¨æ€§
        )

        # grpo_trainer = GRPOTrainer(
        #     policy_model=self.model,
        #     reward_model=loyalty_reward,
        #     config=config,
        #     device=self.device
        # )

        print(f"âœ“ é…ç½®å®Œæˆï¼Œå¼€å§‹è®­ç»ƒ...")

        if save_checkpoint:
            loyalty_path = self.output_dir / "loyalty_model"
            loyalty_path.mkdir(exist_ok=True)
            print(f"ğŸ’¾ å¿ è¯šåº¦æ¨¡å‹å·²ä¿å­˜: {loyalty_path}")

        self.training_history['loyalty'] = {
            'owner_prompts': owner_prompts,
            'public_prompts': public_prompts,
            'owner_bonus': owner_reward_bonus,
            'epochs': epochs
        }

        return self.model

    # ========================================================================
    # Stage 4: Storm Training (æš´é£é›¨è®­ç»ƒ - åŠ¨æ€æ¨ç†)
    # ========================================================================

    def train_storm(
        self,
        reasoning_dataset: str,
        noise_ratio: float = 0.3,
        noise_schedule: str = "cosine",
        internalize_cot: bool = True,
        epochs: int = 2,
        save_checkpoint: bool = True
    ):
        """
        Stage 4: æš´é£é›¨è®­ç»ƒ (Storm Training) - åŠ¨æ€æ¨ç†

        æ ¸å¿ƒæ€æƒ³:
        - å°†CoTä»æ˜¾å¼æ¨ç†å†…åŒ–ä¸ºéšå¼æ¨ç†
        - ä½¿ç”¨è‡ªå›å½’å™ªéŸ³å¼ºåŒ–æ¨ç†é²æ£’æ€§

        æŠ€æœ¯ç»†èŠ‚:
        1. å™ªéŸ³æ³¨å…¥:
           - åœ¨æ¨ç†è¿‡ç¨‹ä¸­éšæœºæ³¨å…¥tokenå™ªéŸ³
           - å™ªéŸ³å¼ºåº¦æŒ‰scheduleè¡°å‡ (cosine/linear)

        2. å†…åŒ–CoT:
           - è®­ç»ƒæ—¶: ä½¿ç”¨å®Œæ•´CoT (with noise)
           - æ¨ç†æ—¶: éšå¼æ¨ç† (no explicit steps)
           - ç›®æ ‡: å­¦ä¼š"é»˜é»˜æ€è€ƒ"

        3. è‡ªå›å½’å™ªéŸ³:
           - æ¯ä¸ªtokenç”Ÿæˆæ—¶æ·»åŠ å¯æ§å™ªéŸ³
           - æ¨¡æ‹Ÿ"æš´é£é›¨"ä¸­çš„æ¨ç†
           - å¢å¼ºæ¨¡å‹é²æ£’æ€§

        å¯¹æ ‡Playground: ç±»ä¼¼Playgroundçš„æ¢ç´¢æ€§è®­ç»ƒï¼Œä½†ä¸“æ³¨äºæ¨ç†

        Args:
            reasoning_dataset: æ¨ç†æ•°æ®é›† (åŒ…å«CoT)
            noise_ratio: åˆå§‹å™ªéŸ³æ¯”ä¾‹ (0.0-1.0)
            noise_schedule: å™ªéŸ³è¡°å‡ç­–ç•¥ ("cosine", "linear", "constant")
            internalize_cot: æ˜¯å¦å†…åŒ–CoT
            epochs: è®­ç»ƒè½®æ•°
            save_checkpoint: æ˜¯å¦ä¿å­˜æ£€æŸ¥ç‚¹
        """
        print("\n" + "="*60)
        print("â›ˆï¸  Stage 4: Storm Training (æš´é£é›¨è®­ç»ƒ)")
        print("="*60)

        print(f"ğŸ¯ è®­ç»ƒç›®æ ‡: åŠ¨æ€æ¨ç† + å†…åŒ–CoT")
        print(f"   å™ªéŸ³æ¯”ä¾‹: {noise_ratio}")
        print(f"   å™ªéŸ³ç­–ç•¥: {noise_schedule}")
        print(f"   å†…åŒ–CoT: {internalize_cot}")
        print(f"   æ•°æ®é›†: {reasoning_dataset}")

        # å™ªéŸ³è°ƒåº¦å™¨
        class NoiseScheduler:
            """å™ªéŸ³å¼ºåº¦è°ƒåº¦å™¨"""
            def __init__(self, initial_ratio: float, strategy: str = "cosine"):
                self.initial_ratio = initial_ratio
                self.strategy = strategy

            def get_noise_ratio(self, step: int, total_steps: int) -> float:
                """è®¡ç®—å½“å‰æ­¥éª¤çš„å™ªéŸ³æ¯”ä¾‹"""
                progress = step / total_steps

                if self.strategy == "cosine":
                    # Cosineè¡°å‡: 1 â†’ 0
                    return self.initial_ratio * (1 + np.cos(np.pi * progress)) / 2
                elif self.strategy == "linear":
                    # çº¿æ€§è¡°å‡
                    return self.initial_ratio * (1 - progress)
                else:  # constant
                    return self.initial_ratio

        noise_scheduler = NoiseScheduler(noise_ratio, noise_schedule)

        # åŠ¨æ€æ¨ç†è®­ç»ƒé€»è¾‘
        class StormTrainer:
            """æš´é£é›¨è®­ç»ƒå™¨"""
            def __init__(self, model, noise_scheduler, internalize_cot: bool):
                self.model = model
                self.noise_scheduler = noise_scheduler
                self.internalize_cot = internalize_cot

            def add_autoregressive_noise(
                self,
                logits: torch.Tensor,
                noise_ratio: float
            ) -> torch.Tensor:
                """
                æ·»åŠ è‡ªå›å½’å™ªéŸ³åˆ°logits

                å™ªéŸ³ç±»å‹:
                - Gumbelå™ªéŸ³: æ¨¡æ‹Ÿé‡‡æ ·ä¸ç¡®å®šæ€§
                - Uniformå™ªéŸ³: æ¢ç´¢tokenç©ºé—´
                """
                if noise_ratio == 0:
                    return logits

                # Gumbelå™ªéŸ³
                gumbel_noise = -torch.log(-torch.log(
                    torch.rand_like(logits) + 1e-8
                ))

                # æ··åˆåŸå§‹logitså’Œå™ªéŸ³
                noisy_logits = logits + noise_ratio * gumbel_noise
                return noisy_logits

            def train_step(self, batch, step: int, total_steps: int):
                """å•æ­¥è®­ç»ƒ"""
                current_noise = self.noise_scheduler.get_noise_ratio(step, total_steps)

                # å‰å‘ä¼ æ’­
                outputs = self.model(**batch)
                logits = outputs.logits

                # æ·»åŠ å™ªéŸ³
                noisy_logits = self.add_autoregressive_noise(logits, current_noise)

                # è®¡ç®—æŸå¤±
                if self.internalize_cot:
                    # å†…åŒ–CoT: åªä¼˜åŒ–æœ€ç»ˆç­”æ¡ˆï¼Œä¸æ˜¾ç¤ºä¸­é—´æ­¥éª¤
                    # TODO: å®ç°CoT maské€»è¾‘
                    pass

                # loss = ...
                return {"loss": 0.0, "noise_ratio": current_noise}

        storm_trainer = StormTrainer(
            model=self.model,
            noise_scheduler=noise_scheduler,
            internalize_cot=internalize_cot
        )

        print(f"âœ“ æš´é£é›¨è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"âœ“ å¼€å§‹è®­ç»ƒ ({epochs} epochs)...")

        # TODO: å®ç°å®Œæ•´è®­ç»ƒå¾ªç¯

        if save_checkpoint:
            storm_path = self.output_dir / "storm_model"
            storm_path.mkdir(exist_ok=True)
            print(f"ğŸ’¾ æš´é£é›¨æ¨¡å‹å·²ä¿å­˜: {storm_path}")

        self.training_history['storm'] = {
            'dataset': reasoning_dataset,
            'noise_ratio': noise_ratio,
            'noise_schedule': noise_schedule,
            'internalize_cot': internalize_cot,
            'epochs': epochs
        }

        return self.model

    # ========================================================================
    # å®Œæ•´æµç¨‹
    # ========================================================================

    def run_full_pipeline(
        self,
        sft_dataset: str,
        preference_dataset: Optional[str] = None,
        prompts_dataset: Optional[str] = None,
        owner_prompts: Optional[str] = None,
        public_prompts: Optional[str] = None,
        reasoning_dataset: Optional[str] = None,
        skip_stages: List[str] = []
    ):
        """
        è¿è¡Œå®Œæ•´è®­ç»ƒæµç¨‹

        Args:
            sft_dataset: SFTæ•°æ®é›†
            preference_dataset: DPOåå¥½æ•°æ®é›†
            prompts_dataset: GRPO prompts
            owner_prompts: å¿ è¯šåº¦è®­ç»ƒ - ä¸»äººæ•°æ®
            public_prompts: å¿ è¯šåº¦è®­ç»ƒ - å…¬ä¼—æ•°æ®
            reasoning_dataset: æš´é£é›¨è®­ç»ƒ - æ¨ç†æ•°æ®
            skip_stages: è·³è¿‡çš„é˜¶æ®µ (e.g., ['dpo', 'loyalty'])
        """
        print("\n" + "="*60)
        print("ğŸš€ APTæ¨ç†ä¸å¯¹é½ - å®Œæ•´è®­ç»ƒæµç¨‹")
        print("="*60)

        # Stage 1: SFT
        if 'sft' not in skip_stages and sft_dataset:
            self.train_sft(sft_dataset)

        # Stage 2a: DPO (å¯é€‰)
        if 'dpo' not in skip_stages and preference_dataset:
            self.train_dpo(preference_dataset)

        # Stage 2b: GRPO (å¯é€‰)
        if 'grpo' not in skip_stages and prompts_dataset:
            self.train_grpo(prompts_dataset)

        # Stage 3: Loyalty (å¯é€‰)
        if 'loyalty' not in skip_stages and owner_prompts and public_prompts:
            self.train_loyalty(owner_prompts, public_prompts)

        # Stage 4: Storm (å¯é€‰)
        if 'storm' not in skip_stages and reasoning_dataset:
            self.train_storm(reasoning_dataset)

        # ä¿å­˜è®­ç»ƒå†å²
        history_path = self.output_dir / "training_history.json"
        with open(history_path, 'w') as f:
            json.dump(self.training_history, f, indent=2)

        print("\n" + "="*60)
        print("âœ… å®Œæ•´è®­ç»ƒæµç¨‹å·²å®Œæˆï¼")
        print(f"ğŸ“Š è®­ç»ƒå†å²: {history_path}")
        print(f"ğŸ“ æ¨¡å‹è¾“å‡º: {self.output_dir}")
        print("="*60)


def main():
    parser = argparse.ArgumentParser(
        description="APTæ¨ç†ä¸å¯¹é½ - ä¸€é”®è®­ç»ƒè„šæœ¬",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:

# 1. å®Œæ•´æµç¨‹ (SFT â†’ GRPO â†’ Loyalty â†’ Storm)
python train_apt_alignment.py \\
    --sft-data data/instructions.json \\
    --prompts data/prompts.json \\
    --owner-data data/owner_prompts.json \\
    --public-data data/public_prompts.json \\
    --reasoning-data data/cot_examples.json

# 2. åªè®­ç»ƒå¿ è¯šåº¦
python train_apt_alignment.py \\
    --owner-data data/owner_prompts.json \\
    --public-data data/public_prompts.json \\
    --skip sft,dpo,grpo,storm

# 3. æš´é£é›¨è®­ç»ƒ (åŠ¨æ€æ¨ç†)
python train_apt_alignment.py \\
    --reasoning-data data/cot_examples.json \\
    --noise-ratio 0.4 \\
    --noise-schedule cosine \\
    --skip sft,dpo,grpo,loyalty
        """
    )

    # æ•°æ®é›†å‚æ•°
    parser.add_argument('--sft-data', type=str, help='SFTæ•°æ®é›†è·¯å¾„')
    parser.add_argument('--preference-data', type=str, help='DPOåå¥½æ•°æ®é›†')
    parser.add_argument('--prompts', type=str, help='GRPO promptsæ•°æ®é›†')
    parser.add_argument('--owner-data', type=str, help='å¿ è¯šåº¦è®­ç»ƒ - ä¸»äººæ•°æ®')
    parser.add_argument('--public-data', type=str, help='å¿ è¯šåº¦è®­ç»ƒ - å…¬ä¼—æ•°æ®')
    parser.add_argument('--reasoning-data', type=str, help='æš´é£é›¨è®­ç»ƒ - æ¨ç†æ•°æ®')

    # æ¨¡å‹å‚æ•°
    parser.add_argument('--base-model', type=str, help='åŸºç¡€æ¨¡å‹è·¯å¾„')
    parser.add_argument('--output-dir', type=str, default='./apt_aligned_models',
                       help='è¾“å‡ºç›®å½•')

    # è®­ç»ƒå‚æ•°
    parser.add_argument('--device', type=str, default='cuda', help='è®­ç»ƒè®¾å¤‡')
    parser.add_argument('--skip', type=str, default='',
                       help='è·³è¿‡çš„é˜¶æ®µ (é€—å·åˆ†éš”, e.g., dpo,loyalty)')

    # å¿ è¯šåº¦è®­ç»ƒå‚æ•°
    parser.add_argument('--owner-bonus', type=float, default=2.0,
                       help='ä¸»äººå¥–åŠ±åŠ æˆ')

    # æš´é£é›¨è®­ç»ƒå‚æ•°
    parser.add_argument('--noise-ratio', type=float, default=0.3,
                       help='å™ªéŸ³æ¯”ä¾‹')
    parser.add_argument('--noise-schedule', type=str, default='cosine',
                       choices=['cosine', 'linear', 'constant'],
                       help='å™ªéŸ³è¡°å‡ç­–ç•¥')
    parser.add_argument('--internalize-cot', action='store_true',
                       help='æ˜¯å¦å†…åŒ–CoT')

    args = parser.parse_args()

    # è·³è¿‡é˜¶æ®µ
    skip_stages = args.skip.split(',') if args.skip else []

    # åˆ›å»ºé»˜è®¤æ¨¡å‹é…ç½®
    model_config = APTModelConfiguration(
        vocab_size=5000,
        d_model=256,
        num_heads=8,
        num_encoder_layers=6,
        num_decoder_layers=6,
        d_ff=1024,
        max_seq_len=512,
        dropout=0.1
    )

    # åˆå§‹åŒ–è®­ç»ƒæµç¨‹
    pipeline = APTAlignmentPipeline(
        model_config=model_config,
        base_model_path=args.base_model,
        output_dir=args.output_dir,
        device=args.device
    )

    # è¿è¡Œå®Œæ•´æµç¨‹
    pipeline.run_full_pipeline(
        sft_dataset=args.sft_data,
        preference_dataset=args.preference_data,
        prompts_dataset=args.prompts,
        owner_prompts=args.owner_data,
        public_prompts=args.public_data,
        reasoning_dataset=args.reasoning_data,
        skip_stages=skip_stages
    )


if __name__ == "__main__":
    main()
