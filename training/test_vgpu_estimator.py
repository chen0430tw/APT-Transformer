"""
æµ‹è¯•è™šæ‹ŸBlackwellèµ„æºè¯„ä¼°å™¨

åœºæ™¯ï¼š
1. è¯„ä¼°æ ‡å‡†æ¨¡å‹ï¼ˆGPT-2ç³»åˆ—ï¼‰
2. è‡ªå®šä¹‰æ¨¡å‹è¯„ä¼°
3. å¤šGPUé…ç½®
4. å¯¼å‡ºé…ç½®æ–‡ä»¶
"""

import sys
from pathlib import Path
sys.path.insert(0, str(Path(__file__).parent.parent))

from apt_model.optimization.vgpu_estimator import (
    VGPUResourceEstimator,
    ModelConfig,
    quick_estimate
)


def test_standard_models():
    """æµ‹è¯•1ï¼šæ ‡å‡†æ¨¡å‹è¯„ä¼°"""
    print("="*70)
    print("æµ‹è¯•1: æ ‡å‡†æ¨¡å‹è¯„ä¼°ï¼ˆGPT-2ç³»åˆ—ï¼‰")
    print("="*70)

    models = ['gpt2-small', 'gpt2-medium', 'gpt2-large']

    for model_name in models:
        print(f"\n{'â”€'*70}")
        print(f"æ¨¡å‹: {model_name.upper()}")
        print('â”€'*70)
        quick_estimate(model_name)
        print("\n")


def test_custom_model():
    """æµ‹è¯•2ï¼šè‡ªå®šä¹‰æ¨¡å‹"""
    print("\n" + "="*70)
    print("æµ‹è¯•2: è‡ªå®šä¹‰æ¨¡å‹ï¼ˆç±»ä¼¼GPT-3 Smallï¼‰")
    print("="*70)

    # è‡ªå®šä¹‰é…ç½®
    config = ModelConfig(
        vocab_size=50000,
        hidden_size=2048,
        num_layers=24,
        num_heads=16,
        seq_length=2048,
        batch_size=8,
        gradient_checkpointing=True,  # å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
        mixed_precision=True,         # å¯ç”¨æ··åˆç²¾åº¦
        optimizer='adamw'
    )

    estimator = VGPUResourceEstimator()

    # ä¼°ç®—å†…å­˜
    estimator.estimate_transformer(config)

    # å•GPUé…ç½®
    available_gpus = [
        {'device': 'cuda:0', 'vram_gb': 24, 'speed_gbps': 900}  # RTX 3090/4090
    ]

    estimator.generate_vgpu_config(available_gpus, target_hit_rate=0.85)
    estimator.print_report()

    # æ¨èæ‰¹æ¬¡å¤§å°
    print("\n[5] æ‰¹æ¬¡å¤§å°æ¨è (24GB GPU):")
    batch_sizes = estimator.recommend_batch_size(24)
    print(f"  æ¨èæ‰¹æ¬¡å¤§å°: {batch_sizes}")


def test_multi_gpu():
    """æµ‹è¯•3ï¼šå¤šGPUé…ç½®"""
    print("\n" + "="*70)
    print("æµ‹è¯•3: å¤šGPUé…ç½®ï¼ˆ4Ã—A100 80GBï¼‰")
    print("="*70)

    # å¤§æ¨¡å‹é…ç½®ï¼ˆç±»ä¼¼LLaMA-13Bï¼‰
    config = ModelConfig(
        vocab_size=32000,
        hidden_size=5120,
        num_layers=40,
        num_heads=40,
        seq_length=2048,
        batch_size=4,
        gradient_checkpointing=True,
        mixed_precision=True,
        optimizer='adamw'
    )

    estimator = VGPUResourceEstimator()
    estimator.estimate_transformer(config)

    # 4å¼ A100é…ç½®
    available_gpus = [
        {'device': 'cuda:0', 'vram_gb': 80, 'speed_gbps': 2000},  # NVLink
        {'device': 'cuda:1', 'vram_gb': 80, 'speed_gbps': 2000},
        {'device': 'cuda:2', 'vram_gb': 80, 'speed_gbps': 2000},
        {'device': 'cuda:3', 'vram_gb': 80, 'speed_gbps': 2000},
    ]

    estimator.generate_vgpu_config(available_gpus, target_hit_rate=0.95)
    estimator.print_report()


def test_consumer_gpu():
    """æµ‹è¯•4ï¼šæ¶ˆè´¹çº§GPUé…ç½®"""
    print("\n" + "="*70)
    print("æµ‹è¯•4: æ¶ˆè´¹çº§GPUï¼ˆRTX 3070 8GBï¼‰")
    print("="*70)

    # å°æ¨¡å‹é…ç½®
    config = ModelConfig(
        vocab_size=50000,
        hidden_size=768,
        num_layers=12,
        num_heads=12,
        seq_length=512,
        batch_size=16,
        gradient_checkpointing=False,
        mixed_precision=True,
        optimizer='adamw'
    )

    estimator = VGPUResourceEstimator()
    estimator.estimate_transformer(config)

    # RTX 3070é…ç½®
    available_gpus = [
        {'device': 'cuda:0', 'vram_gb': 8, 'speed_gbps': 900}
    ]

    estimator.generate_vgpu_config(available_gpus, target_hit_rate=0.90)
    estimator.print_report()

    print("\n[5] æ‰¹æ¬¡å¤§å°æ¨è (8GB GPU):")
    batch_sizes = estimator.recommend_batch_size(8)
    print(f"  æ¨èæ‰¹æ¬¡å¤§å°: {batch_sizes}")


def test_extreme_model():
    """æµ‹è¯•5ï¼šè¶…å¤§æ¨¡å‹ï¼ˆç±»ä¼¼GPT-3 175Bï¼‰"""
    print("\n" + "="*70)
    print("æµ‹è¯•5: è¶…å¤§æ¨¡å‹ï¼ˆç±»ä¼¼GPT-3 175Bï¼‰")
    print("="*70)

    # 175Bå‚æ•°é…ç½®
    config = ModelConfig(
        vocab_size=50000,
        hidden_size=12288,
        num_layers=96,
        num_heads=96,
        seq_length=2048,
        batch_size=1,
        gradient_checkpointing=True,
        mixed_precision=True,
        optimizer='adamw'
    )

    estimator = VGPUResourceEstimator()
    estimator.estimate_transformer(config)

    # 8Ã—A100 80GBé›†ç¾¤
    available_gpus = [
        {'device': f'cuda:{i}', 'vram_gb': 80, 'speed_gbps': 2000}
        for i in range(8)
    ]

    estimator.generate_vgpu_config(available_gpus, target_hit_rate=0.80)
    estimator.print_report()

    print("\nğŸ’¡ è¶…å¤§æ¨¡å‹éœ€è¦é¢å¤–ä¼˜åŒ–:")
    print("  - å¼ é‡å¹¶è¡Œï¼ˆTensor Parallelismï¼‰")
    print("  - æµæ°´çº¿å¹¶è¡Œï¼ˆPipeline Parallelismï¼‰")
    print("  - ZeRO-3ä¼˜åŒ–å™¨çŠ¶æ€åˆ†ç‰‡")
    print("  - CPU/NVMeå¸è½½")


def test_export_config():
    """æµ‹è¯•6ï¼šå¯¼å‡ºé…ç½®"""
    print("\n" + "="*70)
    print("æµ‹è¯•6: å¯¼å‡ºé…ç½®æ–‡ä»¶")
    print("="*70)

    config = ModelConfig(
        vocab_size=50000,
        hidden_size=1024,
        num_layers=24,
        num_heads=16,
        seq_length=1024,
        batch_size=16,
        mixed_precision=True
    )

    estimator = VGPUResourceEstimator()
    estimator.estimate_transformer(config)

    available_gpus = [
        {'device': 'cuda:0', 'vram_gb': 24, 'speed_gbps': 900}
    ]

    estimator.generate_vgpu_config(available_gpus)

    # å¯¼å‡º
    output_file = "vgpu_config_gpt2_medium.json"
    estimator.save_config(output_file)
    print(f"\né…ç½®å·²å¯¼å‡ºåˆ°: {output_file}")


def test_comparison_table():
    """æµ‹è¯•7ï¼šå¯¹æ¯”è¡¨æ ¼"""
    print("\n" + "="*70)
    print("æµ‹è¯•7: æ¨¡å‹å¯¹æ¯”è¡¨ï¼ˆè®­ç»ƒå†…å­˜éœ€æ±‚ï¼‰")
    print("="*70)

    models = {
        'GPT-2 Small': ModelConfig(
            vocab_size=50000, hidden_size=768, num_layers=12,
            num_heads=12, seq_length=1024, batch_size=32, mixed_precision=True
        ),
        'GPT-2 Medium': ModelConfig(
            vocab_size=50000, hidden_size=1024, num_layers=24,
            num_heads=16, seq_length=1024, batch_size=16, mixed_precision=True
        ),
        'GPT-2 Large': ModelConfig(
            vocab_size=50000, hidden_size=1280, num_layers=36,
            num_heads=20, seq_length=1024, batch_size=8, mixed_precision=True
        ),
        'LLaMA-7B': ModelConfig(
            vocab_size=32000, hidden_size=4096, num_layers=32,
            num_heads=32, seq_length=2048, batch_size=4, mixed_precision=True,
            gradient_checkpointing=True
        ),
    }

    print("\n{:<15} {:>12} {:>15} {:>15} {:>15}".format(
        "æ¨¡å‹", "å‚æ•°é‡", "è®­ç»ƒå†…å­˜", "æ¨ç†å†…å­˜", "æ¨èGPU"
    ))
    print("-" * 70)

    for name, config in models.items():
        estimator = VGPUResourceEstimator()
        estimator.estimate_transformer(config)

        params = estimator._count_transformer_params(config)
        train_mem = estimator.memory_estimate.total_train / (1024**3)
        infer_mem = estimator.memory_estimate.total_inference / (1024**3)

        # æ¨èGPU
        if train_mem < 12:
            gpu = "RTX 3060 12GB"
        elif train_mem < 24:
            gpu = "RTX 3090 24GB"
        elif train_mem < 48:
            gpu = "A100 40GB"
        else:
            gpu = "A100 80GB Ã—2+"

        print("{:<15} {:>10.1f}M {:>13.1f}GB {:>13.1f}GB  {:<15}".format(
            name, params/1e6, train_mem, infer_mem, gpu
        ))


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n")
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘          è™šæ‹ŸBlackwellèµ„æºè¯„ä¼°å™¨æµ‹è¯•å¥—ä»¶                           â•‘")
    print("â•‘          VGPU Resource Estimator - Planning Tool                  â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print("\n")

    tests = [
        ("æ ‡å‡†æ¨¡å‹", test_standard_models),
        ("è‡ªå®šä¹‰æ¨¡å‹", test_custom_model),
        ("å¤šGPUé…ç½®", test_multi_gpu),
        ("æ¶ˆè´¹çº§GPU", test_consumer_gpu),
        ("è¶…å¤§æ¨¡å‹", test_extreme_model),
        ("å¯¼å‡ºé…ç½®", test_export_config),
        ("å¯¹æ¯”è¡¨æ ¼", test_comparison_table),
    ]

    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument('--test', type=str, default='all',
                       help='è¿è¡ŒæŒ‡å®šæµ‹è¯•ï¼šall, standard, custom, multi, consumer, extreme, export, table')
    args = parser.parse_args()

    test_map = {
        'all': None,
        'standard': 0,
        'custom': 1,
        'multi': 2,
        'consumer': 3,
        'extreme': 4,
        'export': 5,
        'table': 6
    }

    if args.test == 'all':
        for name, test_func in tests:
            try:
                test_func()
            except Exception as e:
                print(f"\nâŒ æµ‹è¯• '{name}' å¤±è´¥: {e}")
                import traceback
                traceback.print_exc()
    elif args.test in test_map and test_map[args.test] is not None:
        idx = test_map[args.test]
        name, test_func = tests[idx]
        test_func()
    else:
        print(f"âŒ æœªçŸ¥æµ‹è¯•: {args.test}")
        print(f"å¯ç”¨æµ‹è¯•: {list(test_map.keys())}")

    print("\n" + "="*70)
    print("æµ‹è¯•å®Œæˆï¼")
    print("="*70)


if __name__ == "__main__":
    main()
