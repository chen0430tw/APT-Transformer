#!/usr/bin/env python3
"""
APTå¯¹ç…§å®éªŒè®­ç»ƒè„šæœ¬
å¹¶è¡Œè®­ç»ƒè‡ªç”Ÿæˆå’Œéè‡ªç”ŸæˆAPTæ¨¡å‹ï¼Œç”¨äºå¯¹æ¯”åˆ†æ

å®éªŒè®¾è®¡ï¼š
1. å¯¹ç…§ç»„ï¼šæ ‡å‡†Transformer (use_autopoietic=False)
2. å®éªŒç»„ï¼šè‡ªç”ŸæˆTransformer (use_autopoietic=True)
3. åŒä¸€æ•°æ®é›†ã€åŒä¸€è¶…å‚æ•°ã€åŒä¸€è®­ç»ƒæµç¨‹
4. å¯¹æ¯”HLBDä»»åŠ¡çš„å­¦ä¹ æ•ˆæœ

ç›®æ ‡ï¼š
- éªŒè¯è‡ªç”Ÿæˆæœºåˆ¶æ˜¯å¦æå‡HLBDå­¦ä¹ èƒ½åŠ›
- åˆ†ææ˜¯å¦èƒ½å‡å°‘"shortcut learning"ï¼ˆæ¨¡å‹å·æ‡’ï¼‰
- è¯„ä¼°å‚æ•°å¼€é”€vsæ€§èƒ½æå‡çš„æƒè¡¡
"""

import os
import sys
import json
import time
import argparse
from pathlib import Path
from typing import Dict, List, Tuple

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from apt_model.modeling.apt_control import (
    create_control_model,
    create_autopoietic_model,
    compare_model_architectures
)
from apt_model.tokenization.char_tokenizer import CharacterTokenizer


# ============================================================================
# HLBDæ•°æ®é›†
# ============================================================================

class HLBDDataset(Dataset):
    """HLBD Hardcoreæ•°æ®é›†"""

    def __init__(self, json_path: str, tokenizer: CharacterTokenizer, max_len: int = 128):
        self.tokenizer = tokenizer
        self.max_len = max_len

        print(f"ğŸ“‚ åŠ è½½HLBDæ•°æ®é›†: {json_path}")
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # åˆå¹¶æ‰€æœ‰æ¨¡å—
        self.pairs = []
        for module_name, module_data in data['data'].items():
            for item in module_data:
                self.pairs.append((item['input'], item['output']))

        print(f"   âœ“ åŠ è½½ {len(self.pairs)} ä¸ªè®­ç»ƒå¯¹")

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        src, tgt = self.pairs[idx]

        # Encode
        src_ids = self.tokenizer.encode(src)
        tgt_ids = self.tokenizer.encode(tgt)

        # æ‹¼æ¥: [src, SEP, tgt]
        input_ids = src_ids + [1] + tgt_ids

        # æˆªæ–­
        if len(input_ids) > self.max_len:
            input_ids = input_ids[:self.max_len]

        return {
            'input_ids': torch.tensor(input_ids[:-1], dtype=torch.long),
            'labels': torch.tensor(input_ids[1:], dtype=torch.long)
        }


def collate_fn(batch):
    """åŠ¨æ€padding"""
    input_ids = [item['input_ids'] for item in batch]
    labels = [item['labels'] for item in batch]

    max_len = max(len(ids) for ids in input_ids)

    padded_input = []
    padded_labels = []

    for inp, lab in zip(input_ids, labels):
        pad_len = max_len - len(inp)
        padded_input.append(torch.cat([inp, torch.zeros(pad_len, dtype=torch.long)]))
        padded_labels.append(torch.cat([lab, torch.full((pad_len,), -100, dtype=torch.long)]))

    return {
        'input_ids': torch.stack(padded_input),
        'labels': torch.stack(padded_labels)
    }


# ============================================================================
# è®­ç»ƒå™¨
# ============================================================================

class ControlExperimentTrainer:
    """å¯¹ç…§å®éªŒè®­ç»ƒå™¨"""

    def __init__(
        self,
        control_model: nn.Module,
        autopoietic_model: nn.Module,
        train_loader: DataLoader,
        device: str = 'cuda',
        lr: float = 3e-4
    ):
        self.control_model = control_model.to(device)
        self.autopoietic_model = autopoietic_model.to(device)
        self.train_loader = train_loader
        self.device = device

        # ä¼˜åŒ–å™¨
        self.control_optimizer = torch.optim.AdamW(
            control_model.parameters(),
            lr=lr,
            weight_decay=0.01
        )
        self.autopoietic_optimizer = torch.optim.AdamW(
            autopoietic_model.parameters(),
            lr=lr,
            weight_decay=0.01
        )

        # æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss(ignore_index=-100)

        # ç»Ÿè®¡
        self.control_losses = []
        self.autopoietic_losses = []

    def train_epoch(self, epoch: int):
        """è®­ç»ƒä¸€ä¸ªepochï¼ŒåŒæ—¶è®­ç»ƒä¸¤ä¸ªæ¨¡å‹"""
        self.control_model.train()
        self.autopoietic_model.train()

        control_total_loss = 0
        autopoietic_total_loss = 0
        num_batches = 0

        epoch_start = time.time()

        for batch_idx, batch in enumerate(self.train_loader):
            input_ids = batch['input_ids'].to(self.device)
            labels = batch['labels'].to(self.device)

            # ============= è®­ç»ƒå¯¹ç…§ç»„æ¨¡å‹ =============
            self.control_optimizer.zero_grad()
            control_logits = self.control_model(input_ids)
            control_loss = self.criterion(
                control_logits.view(-1, control_logits.size(-1)),
                labels.view(-1)
            )
            control_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.control_model.parameters(), 1.0)
            self.control_optimizer.step()

            # ============= è®­ç»ƒå®éªŒç»„æ¨¡å‹ =============
            self.autopoietic_optimizer.zero_grad()
            autopoietic_logits = self.autopoietic_model(input_ids)
            autopoietic_loss = self.criterion(
                autopoietic_logits.view(-1, autopoietic_logits.size(-1)),
                labels.view(-1)
            )
            autopoietic_loss.backward()
            torch.nn.utils.clip_grad_norm_(self.autopoietic_model.parameters(), 1.0)
            self.autopoietic_optimizer.step()

            # ç»Ÿè®¡
            control_total_loss += control_loss.item()
            autopoietic_total_loss += autopoietic_loss.item()
            num_batches += 1

            # æ‰“å°è¿›åº¦
            if batch_idx % 20 == 0:
                print(f"   Batch {batch_idx}/{len(self.train_loader)} | "
                      f"Control Loss: {control_loss.item():.4f} | "
                      f"Autopoietic Loss: {autopoietic_loss.item():.4f}")

        # Epochç»Ÿè®¡
        control_avg_loss = control_total_loss / num_batches
        autopoietic_avg_loss = autopoietic_total_loss / num_batches
        epoch_time = time.time() - epoch_start

        self.control_losses.append(control_avg_loss)
        self.autopoietic_losses.append(autopoietic_avg_loss)

        return {
            'control_loss': control_avg_loss,
            'autopoietic_loss': autopoietic_avg_loss,
            'epoch_time': epoch_time
        }

    @torch.no_grad()
    def evaluate(self, eval_pairs: List[Tuple[str, str]], tokenizer: CharacterTokenizer):
        """è¯„ä¼°ä¸¤ä¸ªæ¨¡å‹çš„ç”Ÿæˆè´¨é‡"""
        self.control_model.eval()
        self.autopoietic_model.eval()

        results = {
            'control': [],
            'autopoietic': []
        }

        print("\n" + "=" * 60)
        print("ç”Ÿæˆè´¨é‡è¯„ä¼°")
        print("=" * 60)

        for src, expected in eval_pairs:
            src_ids = tokenizer.encode(src)
            input_tensor = torch.tensor([src_ids], dtype=torch.long).to(self.device)

            # å¯¹ç…§ç»„ç”Ÿæˆ
            control_output = self.control_model(input_tensor)
            control_pred = control_output.argmax(dim=-1)[0].tolist()
            control_text = tokenizer.decode(control_pred)

            # å®éªŒç»„ç”Ÿæˆ
            autopoietic_output = self.autopoietic_model(input_tensor)
            autopoietic_pred = autopoietic_output.argmax(dim=-1)[0].tolist()
            autopoietic_text = tokenizer.decode(autopoietic_pred)

            results['control'].append({
                'input': src,
                'expected': expected,
                'generated': control_text
            })
            results['autopoietic'].append({
                'input': src,
                'expected': expected,
                'generated': autopoietic_text
            })

            print(f"\nè¾“å…¥: {src}")
            print(f"æœŸæœ›: {expected}")
            print(f"å¯¹ç…§ç»„: {control_text}")
            print(f"å®éªŒç»„: {autopoietic_text}")

        return results

    def save_checkpoint(self, save_dir: str, epoch: int):
        """ä¿å­˜checkpoint"""
        save_path = Path(save_dir)
        save_path.mkdir(parents=True, exist_ok=True)

        # ä¿å­˜å¯¹ç…§ç»„
        control_ckpt = save_path / f"control_epoch_{epoch}.pt"
        torch.save({
            'epoch': epoch,
            'model_state_dict': self.control_model.state_dict(),
            'optimizer_state_dict': self.control_optimizer.state_dict(),
            'losses': self.control_losses,
            'config': self.control_model.config.to_dict()
        }, control_ckpt)

        # ä¿å­˜å®éªŒç»„
        autopoietic_ckpt = save_path / f"autopoietic_epoch_{epoch}.pt"
        torch.save({
            'epoch': epoch,
            'model_state_dict': self.autopoietic_model.state_dict(),
            'optimizer_state_dict': self.autopoietic_optimizer.state_dict(),
            'losses': self.autopoietic_losses,
            'config': self.autopoietic_model.config.to_dict()
        }, autopoietic_ckpt)

        print(f"\nâœ… Checkpointå·²ä¿å­˜ (Epoch {epoch})")
        print(f"   å¯¹ç…§ç»„: {control_ckpt}")
        print(f"   å®éªŒç»„: {autopoietic_ckpt}")

    def save_progress_report(self, save_dir: str):
        """ä¿å­˜å®æ—¶è¿›åº¦æŠ¥å‘Šï¼ˆç”¨äºå¯è§†åŒ–ï¼‰"""
        report_path = Path(save_dir) / "experiment_report.json"
        report = {
            'control_losses': list(self.control_losses),
            'autopoietic_losses': list(self.autopoietic_losses),
            'current_epoch': len(self.control_losses),
            'timestamp': time.time()
        }

        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description="APTå¯¹ç…§å®éªŒè®­ç»ƒ")

    parser.add_argument('--dataset', type=str, default='../data/HLBD_Hardcore_Full.json',
                        help='HLBDæ•°æ®é›†è·¯å¾„')
    parser.add_argument('--epochs', type=int, default=100,
                        help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch-size', type=int, default=16,
                        help='Batchå¤§å°')
    parser.add_argument('--lr', type=float, default=3e-4,
                        help='å­¦ä¹ ç‡')
    parser.add_argument('--d-model', type=int, default=256,
                        help='æ¨¡å‹ç»´åº¦')
    parser.add_argument('--n-layers', type=int, default=6,
                        help='å±‚æ•°')
    parser.add_argument('--n-heads', type=int, default=8,
                        help='æ³¨æ„åŠ›å¤´æ•°')
    parser.add_argument('--save-dir', type=str, default='control_experiments',
                        help='ä¿å­˜ç›®å½•')
    parser.add_argument('--save-interval', type=int, default=25,
                        help='ä¿å­˜é—´éš”')

    args = parser.parse_args()

    # è®¾å¤‡
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")

    # Tokenizer
    print(f"\nğŸ”¤ åˆå§‹åŒ–Tokenizer...")
    tokenizer = CharacterTokenizer(vocab_size=2000)

    # æ•°æ®é›†
    dataset = HLBDDataset(args.dataset, tokenizer)

    # é¢„å¡«å……è¯æ±‡è¡¨
    print(f"\nğŸ“ é¢„å¡«å……è¯æ±‡è¡¨...")
    for src, tgt in dataset.pairs:
        tokenizer.encode(src)
        tokenizer.encode(tgt)
    print(f"   âœ“ è¯æ±‡è¡¨å¤§å°: {len(tokenizer.char_to_id)}")

    # DataLoader
    train_loader = DataLoader(
        dataset,
        batch_size=args.batch_size,
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=0
    )

    # åˆ›å»ºæ¨¡å‹å¯¹
    print(f"\nğŸ—ï¸  åˆ›å»ºå¯¹ç…§å®éªŒæ¨¡å‹å¯¹...")
    control_model = create_control_model(
        vocab_size=tokenizer.vocab_size,
        d_model=args.d_model,
        n_layers=args.n_layers,
        n_heads=args.n_heads,
        use_dbc_dac=True
    )

    print()

    autopoietic_model = create_autopoietic_model(
        vocab_size=tokenizer.vocab_size,
        d_model=args.d_model,
        n_layers=args.n_layers,
        n_heads=args.n_heads,
        use_dbc_dac=True
    )

    # æ¯”è¾ƒæ¶æ„
    compare_model_architectures(control_model, autopoietic_model)

    # è®­ç»ƒå™¨
    trainer = ControlExperimentTrainer(
        control_model=control_model,
        autopoietic_model=autopoietic_model,
        train_loader=train_loader,
        device=device,
        lr=args.lr
    )

    # å‡†å¤‡è¯„ä¼°æ ·æœ¬
    eval_pairs = [
        ("12 + 34 = ?", "46"),
        ("ä¸‰è§’å½¢æœ‰å‡ æ¡è¾¹ï¼Ÿ", "3"),
        ("é¼ åé¢æ˜¯ä»€ä¹ˆç”Ÿè‚–ï¼Ÿ", "ç‰›"),
        ("æ°´åœ¨æ ‡å‡†å¤§æ°”å‹ä¸‹çš„æ²¸ç‚¹æ˜¯å¤šå°‘æ‘„æ°åº¦ï¼Ÿ", "100æ‘„æ°åº¦"),
    ]

    # è®­ç»ƒå¾ªç¯
    print("\n" + "=" * 60)
    print("å¼€å§‹å¯¹ç…§å®éªŒè®­ç»ƒ")
    print("=" * 60)

    for epoch in range(args.epochs):
        print(f"\nğŸ“ Epoch {epoch + 1}/{args.epochs}")

        # è®­ç»ƒ
        results = trainer.train_epoch(epoch)

        print(f"\n   å¯¹ç…§ç»„æŸå¤±: {results['control_loss']:.4f}")
        print(f"   å®éªŒç»„æŸå¤±: {results['autopoietic_loss']:.4f}")
        print(f"   ç”¨æ—¶: {results['epoch_time']:.2f}s")

        # ğŸ¨ å®æ—¶ä¿å­˜è¿›åº¦ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
        trainer.save_progress_report(args.save_dir)

        # å®šæœŸè¯„ä¼°
        if (epoch + 1) % args.save_interval == 0:
            trainer.evaluate(eval_pairs, tokenizer)
            trainer.save_checkpoint(args.save_dir, epoch + 1)

    # æœ€ç»ˆè¯„ä¼°
    print("\n" + "=" * 60)
    print("æœ€ç»ˆè¯„ä¼°")
    print("=" * 60)
    final_results = trainer.evaluate(eval_pairs, tokenizer)

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    trainer.save_checkpoint(args.save_dir, args.epochs)

    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    report_path = Path(args.save_dir) / "experiment_report.json"
    report = {
        'config': vars(args),
        'control_losses': trainer.control_losses,
        'autopoietic_losses': trainer.autopoietic_losses,
        'final_evaluation': final_results
    }

    with open(report_path, 'w', encoding='utf-8') as f:
        json.dump(report, f, ensure_ascii=False, indent=2)

    print(f"\nâœ… å®éªŒæŠ¥å‘Šå·²ä¿å­˜: {report_path}")

    print("\n" + "=" * 60)
    print("âœ¨ å¯¹ç…§å®éªŒå®Œæˆï¼")
    print("=" * 60)


if __name__ == "__main__":
    main()
