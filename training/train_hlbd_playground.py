#!/usr/bin/env python3
"""
HLBD Playgroundè®­ç»ƒè„šæœ¬
ä½¿ç”¨APT Playgroundæ¶æ„è®­ç»ƒHLBDæ•°æ®é›†ï¼ˆæ”¯æŒæ¨¡å—åŒ–å¤šæ•°æ®é›†è®­ç»ƒï¼‰

ç‰¹æ€§:
- ğŸ”— æ¨¡å—åŒ–è®­ç»ƒ - æ”¯æŒåŒæ—¶åŠ è½½å¤šä¸ªHLBDæ•°æ®é›†
- ğŸ“Š è‡ªåŠ¨æ ¼å¼è¯†åˆ« - HLBD Full (8å±‚) + HLBD Hardcore (æ¨¡å—åŒ–)
- ğŸ¢ Playground Theory (CosineAnnealingWarmRestarts)
- ğŸš€ RTX 3070ä¼˜åŒ– (æ··åˆç²¾åº¦ + æ¢¯åº¦ç´¯ç§¯)
- ğŸ·ï¸  æ”¯æŒåŠ¨æ€æ ‡ç­¾ ([EMOJI], [EN], [PY], [JP], [KR], [PHRASE])
- ğŸ”§ DBC-DACæ¢¯åº¦ç¨³å®š
- ğŸ“Š å®æ—¶å¯è§†åŒ–æ”¯æŒ
- ğŸ² æ•°æ®ç¨€é‡Šå­¦ - è‡ªåŠ¨æ‰“æ•£æ··åˆï¼Œé˜²æ­¢æ¨¡å¼åç¼©

æ•°æ®é›†æ ¼å¼æ”¯æŒ:
1. HLBD Full: 8å±‚åˆ†å±‚è¯­è¨€ç»“æ„ (åŒ…å«Level 3å¥æ³•å±‚)
2. HLBD Hardcore: ä¸¥æ ¼é€»è¾‘é—®ç­” (å‡ ä½•ã€ç®—æœ¯ã€ç”Ÿè‚–ã€ç‰©ç†ã€è‹±æ–‡)

ä½¿ç”¨ç¤ºä¾‹:
  # å•æ•°æ®é›†
  python train_hlbd_playground.py --dataset data/HLBD_Hardcore_Full_V2.json

  # å¤šæ•°æ®é›†è”åˆè®­ç»ƒ (~10,000æ ·æœ¬)
  python train_hlbd_playground.py --datasets data/HLBD_Full_V2.json data/HLBD_Hardcore_Full_V2.json --epochs 50
"""

import os
import sys
import json
import time
import re
import random
import argparse
from pathlib import Path

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import autocast, GradScaler

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from apt_model.modeling.apt_model import APTModel, APTModelConfiguration


# ============================================================================
# åŠ¨æ€æ ‡ç­¾Tokenizer (ä»HLBDè„šæœ¬ç§»æ¤)
# ============================================================================

class DynamicTagTokenizer:
    """æ”¯æŒåŠ¨æ€æ ‡ç­¾çš„å­—ç¬¦çº§tokenizer"""

    def __init__(self, vocab_size=5000):
        # åŸºç¡€ç‰¹æ®Štoken
        self.vocab = {
            '[PAD]': 0, '[UNK]': 1, '[BOS]': 2, '[EOS]': 3,
            '[EMOJI]': 4, '[PHRASE]': 5, '[EN]': 6, '[PY]': 7, '[JP]': 8, '[KR]': 9,
        }
        self.pad_token_id = 0
        self.unk_token_id = 1
        self.bos_token_id = 2
        self.eos_token_id = 3
        self.vocab_size = vocab_size

        self.char_to_id = self.vocab.copy()
        self.id_to_char = {v: k for k, v in self.vocab.items()}
        self.next_id = 10

        # é¢„ç¼–è¯‘æ ‡ç­¾æ­£åˆ™
        self.tag_pattern = re.compile(r'(\[EMOJI\]|\[PHRASE\]|\[EN\]|\[PY\]|\[JP\]|\[KR\])')

    def _tokenize_text(self, text):
        """å…ˆåˆ‡æ ‡ç­¾ï¼Œå†åˆ‡å­—ç¬¦"""
        tokens = []
        parts = self.tag_pattern.split(text)

        for part in parts:
            if part in self.vocab:
                # æ ‡ç­¾ - ç›´æ¥æ·»åŠ ID
                tokens.append(self.vocab[part])
            else:
                # æ™®é€šæ–‡æœ¬ - é€å­—ç¬¦
                for char in part:
                    if char.strip():
                        tokens.append(self._get_or_add_char(char))
                    elif char == ' ':
                        tokens.append(self._get_or_add_char(char))

        return tokens

    def _get_or_add_char(self, char):
        """åŠ¨æ€æ·»åŠ å­—ç¬¦"""
        if char not in self.char_to_id:
            if self.next_id < self.vocab_size:
                self.char_to_id[char] = self.next_id
                self.id_to_char[self.next_id] = char
                self.next_id += 1
            else:
                return self.unk_token_id
        return self.char_to_id[char]

    def encode(self, text):
        """ç¼–ç æ–‡æœ¬"""
        ids = [self.bos_token_id]
        ids.extend(self._tokenize_text(text))
        ids.append(self.eos_token_id)
        return ids

    def decode(self, ids, skip_special_tokens=True):
        """è§£ç IDåºåˆ—"""
        chars = []
        for id in ids:
            if isinstance(id, torch.Tensor):
                id = id.item()

            if skip_special_tokens and id in [self.pad_token_id, self.bos_token_id,
                                               self.eos_token_id, self.unk_token_id]:
                continue

            char = self.id_to_char.get(id, '[UNK]')
            chars.append(char)

        return ''.join(chars)


# ============================================================================
# HLBDæ•°æ®é›†
# ============================================================================

class HLBDPlaygroundDataset(Dataset):
    """HLBDæ¨¡å—åŒ–æ•°æ®é›† - æ”¯æŒå¤šæ•°æ®é›†å’Œå¤šæ ¼å¼"""

    def __init__(self, json_paths, tokenizer: DynamicTagTokenizer, max_len: int = 128):
        """
        åˆå§‹åŒ–HLBDæ•°æ®é›†

        Args:
            json_paths: å•ä¸ªè·¯å¾„(str)æˆ–å¤šä¸ªè·¯å¾„(list)
            tokenizer: DynamicTagTokenizerå®ä¾‹
            max_len: æœ€å¤§åºåˆ—é•¿åº¦
        """
        self.tokenizer = tokenizer
        self.max_len = max_len

        # ç»Ÿä¸€å¤„ç†ä¸ºåˆ—è¡¨
        if isinstance(json_paths, str):
            json_paths = [json_paths]

        self.pairs = []
        self.dataset_stats = {}

        print(f"\nğŸ“š æ¨¡å—åŒ–HLBDæ•°æ®é›†åŠ è½½å™¨")
        print(f"   æ•°æ®é›†æ•°é‡: {len(json_paths)}")
        print("=" * 60)

        # åŠ è½½æ‰€æœ‰æ•°æ®é›†
        for i, json_path in enumerate(json_paths, 1):
            print(f"\nğŸ“‚ [{i}/{len(json_paths)}] åŠ è½½æ•°æ®é›†: {json_path}")
            dataset_pairs = self._load_single_dataset(json_path)

            if dataset_pairs:
                self.pairs.extend(dataset_pairs)
                dataset_name = Path(json_path).stem
                self.dataset_stats[dataset_name] = len(dataset_pairs)
                print(f"   âœ“ æˆåŠŸåŠ è½½ {len(dataset_pairs)} ä¸ªè®­ç»ƒå¯¹")
            else:
                print(f"   âš ï¸  æ•°æ®é›†ä¸ºç©ºæˆ–åŠ è½½å¤±è´¥")

        # æ‰“æ•£æ··åˆ
        random.shuffle(self.pairs)

        print(f"\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
        for name, count in self.dataset_stats.items():
            percentage = count / len(self.pairs) * 100 if self.pairs else 0
            print(f"   {name}: {count} å¯¹ ({percentage:.1f}%)")
        print(f"   æ€»è®¡: {len(self.pairs)} ä¸ªè®­ç»ƒå¯¹")
        print(f"   âœ“ å·²æ··åˆæ‰“æ•£")

        # é¢„å¡«å……è¯æ±‡è¡¨ï¼ˆé¿å…å¤šè¿›ç¨‹é™·é˜±ï¼‰
        print(f"\nğŸ”¤ é¢„å¡«å……è¯æ±‡è¡¨...")
        for src, tgt in self.pairs:
            self.tokenizer.encode(src)
            self.tokenizer.encode(tgt)
        print(f"   âœ“ è¯æ±‡è¡¨å¤§å°: {len(self.tokenizer.char_to_id)}")
        print("=" * 60)

    def _load_single_dataset(self, json_path: str):
        """åŠ è½½å•ä¸ªæ•°æ®é›†å¹¶è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼"""
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # æ£€æµ‹æ•°æ®é›†ç±»å‹
            if 'samples' in data:
                # HLBD Fullæ ¼å¼ï¼ˆ8å±‚ç»“æ„ï¼‰
                print(f"   æ ¼å¼: HLBD Full (8å±‚ç»“æ„)")
                return self._process_hlbd_full(data['samples'])
            elif 'data' in data:
                # HLBD Hardcoreæ ¼å¼ï¼ˆæ¨¡å—åŒ–ï¼‰
                print(f"   æ ¼å¼: HLBD Hardcore (æ¨¡å—åŒ–)")
                return self._process_hlbd_hardcore(data['data'])
            else:
                print(f"   âš ï¸  æœªçŸ¥çš„æ•°æ®é›†æ ¼å¼")
                return []

        except Exception as e:
            print(f"   âŒ åŠ è½½å¤±è´¥: {e}")
            return []

    def _process_hlbd_hardcore(self, data):
        """å¤„ç†HLBD Hardcoreæ ¼å¼ï¼ˆæ¨¡å—åŒ–Q&Aï¼‰"""
        pairs = []
        for module_name, module_data in data.items():
            for item in module_data:
                src = item['input']
                tgt = item['output']
                pairs.append((src, tgt))
        return pairs

    def _process_hlbd_full(self, samples):
        """å¤„ç†HLBD Fullæ ¼å¼ï¼ˆ8å±‚ç»“æ„ï¼‰"""
        pairs = []

        for sample in samples:
            concept = sample.get('concept', '')

            # æ„å»ºè¾“å…¥ï¼šæ¦‚å¿µ + å…³é”®å±‚çº§ä¿¡æ¯
            input_parts = [f"æ¦‚å¿µ: {concept}"]

            # Level 1: å­—å¡ + Emoji
            if 'level_1' in sample:
                char_card = sample['level_1'].get('å­—å¡', '')
                emoji = sample['level_1'].get('emoji', '')
                input_parts.append(f"[EMOJI] {char_card} {emoji}")

            # Level 2: çŸ­è¯­
            if 'level_2' in sample:
                phrase = sample['level_2'].get('çŸ­è¯­', '')
                input_parts.append(f"[PHRASE] {phrase}")

            # Level 3: æ•°å­¦ï¼ˆå¥æ³•ç»“æ„ï¼‰â† é‡è¦ï¼
            if 'level_3' in sample:
                math_expr = sample['level_3'].get('æ•°å­¦', '')
                input_parts.append(f"å¥æ³•ç»“æ„: {math_expr}")

            input_text = "\n".join(input_parts)

            # æ„å»ºè¾“å‡ºï¼šå¤šè¯­è¨€ç¿»è¯‘
            output_parts = []

            # Level 4: æ‹¼éŸ³
            if 'level_4' in sample:
                pinyin = sample['level_4'].get('æ‹¼éŸ³', '')
                output_parts.append(f"[PY] {pinyin}")

            # Level 5: è‹±æ–‡
            if 'level_5' in sample:
                english = sample['level_5'].get('è‹±æ–‡', '')
                output_parts.append(f"[EN] {english}")

            # Level 6: ä¸­æ–‡
            if 'level_6' in sample:
                chinese = sample['level_6'].get('ä¸­æ–‡', '')
                output_parts.append(f"{chinese}")

            # Level 7: æ—¥æ–‡
            if 'level_7' in sample:
                japanese = sample['level_7'].get('æ—¥æ–‡', '')
                if japanese:
                    output_parts.append(f"[JP] {japanese}")

            # Level 8: éŸ©æ–‡
            if 'level_8' in sample:
                korean = sample['level_8'].get('éŸ©æ–‡', '')
                if korean:
                    output_parts.append(f"[KR] {korean}")

            output_text = "\n".join(output_parts)

            pairs.append((input_text, output_text))

        return pairs

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        src, tgt = self.pairs[idx]

        # Encode
        src_ids = self.tokenizer.encode(src)
        tgt_ids = self.tokenizer.encode(tgt)

        # æ‹¼æ¥: [src, SEP, tgt]
        input_ids = src_ids + [1] + tgt_ids

        # æˆªæ–­
        if len(input_ids) > self.max_len:
            input_ids = input_ids[:self.max_len]

        return {
            'input_ids': torch.tensor(input_ids[:-1], dtype=torch.long),
            'labels': torch.tensor(input_ids[1:], dtype=torch.long)
        }


def collate_fn(batch):
    """åŠ¨æ€padding"""
    input_ids = [item['input_ids'] for item in batch]
    labels = [item['labels'] for item in batch]

    max_len = max(len(ids) for ids in input_ids)

    padded_input = []
    padded_labels = []

    for inp, lab in zip(input_ids, labels):
        pad_len = max_len - len(inp)
        padded_input.append(torch.cat([inp, torch.zeros(pad_len, dtype=torch.long)]))
        padded_labels.append(torch.cat([lab, torch.full((pad_len,), -100, dtype=torch.long)]))

    return {
        'input_ids': torch.stack(padded_input),
        'labels': torch.stack(padded_labels)
    }


# ============================================================================
# Playgroundé…ç½®
# ============================================================================

class PlaygroundConfig:
    """RTX 3070ä¼˜åŒ–é…ç½®"""

    # æ¨¡å‹é…ç½®
    d_model = 256
    n_layers = 6
    n_heads = 8
    d_ff = 1024
    max_seq_len = 256
    dropout = 0.1

    # è®­ç»ƒé…ç½®
    batch_size = 16
    gradient_accumulation_steps = 2
    mixed_precision = True
    max_grad_norm = 1.0

    # Playground Theory
    base_lr = 3e-4
    min_lr = 1e-5
    T_0 = 10  # Cosineé‡å¯å‘¨æœŸ
    T_mult = 2

    # DBC-DAC
    use_dbc_dac = True


# ============================================================================
# è®­ç»ƒå™¨
# ============================================================================

class HLBDPlaygroundTrainer:
    """HLBD Playgroundè®­ç»ƒå™¨"""

    def __init__(
        self,
        config: PlaygroundConfig,
        model: APTModel,
        train_loader: DataLoader,
        tokenizer: DynamicTagTokenizer,
        dataset_stats: dict = None,
        device: str = 'cuda'
    ):
        self.config = config
        self.model = model.to(device)
        self.train_loader = train_loader
        self.tokenizer = tokenizer
        self.dataset_stats = dataset_stats or {}
        self.device = device

        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=config.base_lr,
            weight_decay=0.01  # âœ… Weight Decay
        )

        # Playground Theoryè°ƒåº¦å™¨
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer,
            T_0=config.T_0,
            T_mult=config.T_mult,
            eta_min=config.min_lr
        )

        # æ··åˆç²¾åº¦
        self.scaler = GradScaler() if config.mixed_precision else None

        # æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss(ignore_index=-100)

        # ç»Ÿè®¡
        self.losses = []

    def train_epoch(self, epoch: int):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        num_batches = 0

        epoch_start = time.time()

        for batch_idx, batch in enumerate(self.train_loader):
            input_ids = batch['input_ids'].to(self.device)
            labels = batch['labels'].to(self.device)

            # æ··åˆç²¾åº¦
            with autocast(enabled=self.config.mixed_precision):
                logits = self.model(input_ids)
                loss = self.criterion(
                    logits.view(-1, logits.size(-1)),
                    labels.view(-1)
                )
                loss = loss / self.config.gradient_accumulation_steps

            # Backward
            if self.scaler:
                self.scaler.scale(loss).backward()
            else:
                loss.backward()

            # æ¢¯åº¦ç´¯ç§¯
            if (batch_idx + 1) % self.config.gradient_accumulation_steps == 0:
                if self.scaler:
                    self.scaler.unscale_(self.optimizer)

                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(),
                    self.config.max_grad_norm
                )

                if self.scaler:
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    self.optimizer.step()

                self.optimizer.zero_grad()

            total_loss += loss.item() * self.config.gradient_accumulation_steps
            num_batches += 1

            if batch_idx % 20 == 0:
                current_lr = self.scheduler.get_last_lr()[0]
                print(f"   Batch {batch_idx}/{len(self.train_loader)} | "
                      f"Loss: {loss.item():.4f} | LR: {current_lr:.6f}")

        # æ›´æ–°å­¦ä¹ ç‡
        self.scheduler.step()

        avg_loss = total_loss / num_batches
        epoch_time = time.time() - epoch_start
        self.losses.append(avg_loss)

        return avg_loss, epoch_time

    def save_checkpoint(self, save_path: str, epoch: int):
        """ä¿å­˜checkpoint"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'tokenizer_char_to_id': self.tokenizer.char_to_id,
            'tokenizer_id_to_char': self.tokenizer.id_to_char,
            'tokenizer_next_id': self.tokenizer.next_id,
            'tokenizer_vocab_size': self.tokenizer.vocab_size,
            'config': self.config.__dict__,
            'losses': self.losses,
            'dataset_stats': self.dataset_stats  # ä¿å­˜æ•°æ®é›†ç»Ÿè®¡
        }

        if self.scaler:
            checkpoint['scaler_state_dict'] = self.scaler.state_dict()

        torch.save(checkpoint, save_path)
        print(f"âœ… Checkpointå·²ä¿å­˜: {save_path}")

        # å¦‚æœæ˜¯å¤šæ•°æ®é›†è®­ç»ƒï¼Œæ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        if len(self.dataset_stats) > 1:
            print(f"   æ•°æ®é›†æ¥æº:")
            for name, count in self.dataset_stats.items():
                print(f"     - {name}: {count} æ ·æœ¬")

    def save_progress_report(self, save_dir: str):
        """ä¿å­˜è¿›åº¦æŠ¥å‘Šï¼ˆç”¨äºå¯è§†åŒ–ï¼‰"""
        report_path = Path(save_dir) / "experiment_report.json"
        report = {
            'control_losses': self.losses,  # å…¼å®¹å¯è§†åŒ–è„šæœ¬
            'current_epoch': len(self.losses),
            'timestamp': time.time()
        }

        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description='HLBD Playgroundè®­ç»ƒ - æ”¯æŒå•æ•°æ®é›†å’Œå¤šæ•°æ®é›†æ¨¡å—åŒ–è®­ç»ƒ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # å•æ•°æ®é›†è®­ç»ƒï¼ˆå‘åå…¼å®¹ï¼‰
  python train_hlbd_playground.py --dataset data/HLBD_Hardcore_Full_V2.json

  # å¤šæ•°æ®é›†æ¨¡å—åŒ–è®­ç»ƒ
  python train_hlbd_playground.py --datasets data/HLBD_Full_V2.json data/HLBD_Hardcore_Full_V2.json

  # è‡ªå®šä¹‰è®­ç»ƒå‚æ•°
  python train_hlbd_playground.py --datasets data/*.json --epochs 50 --save-dir models/hlbd_combined
        """
    )

    parser.add_argument('--dataset', type=str, default=None,
                       help='å•ä¸ªHLBDæ•°æ®é›†è·¯å¾„ï¼ˆå‘åå…¼å®¹ï¼‰')
    parser.add_argument('--datasets', nargs='+', default=None,
                       help='å¤šä¸ªHLBDæ•°æ®é›†è·¯å¾„ï¼ˆæ¨¡å—åŒ–è®­ç»ƒï¼‰')
    parser.add_argument('--epochs', type=int, default=100,
                       help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--save-dir', type=str, default='hlbd_playground',
                       help='ä¿å­˜ç›®å½•')
    parser.add_argument('--save-interval', type=int, default=25,
                       help='ä¿å­˜é—´éš”')

    args = parser.parse_args()

    # å¤„ç†æ•°æ®é›†å‚æ•°
    if args.datasets:
        # å¤šæ•°æ®é›†æ¨¡å¼
        dataset_paths = args.datasets
    elif args.dataset:
        # å•æ•°æ®é›†æ¨¡å¼ï¼ˆå‘åå…¼å®¹ï¼‰
        dataset_paths = args.dataset
    else:
        # é»˜è®¤ä½¿ç”¨HLBD Hardcore
        dataset_paths = '../data/HLBD_Hardcore_Full.json'
        print(f"âš ï¸  æœªæŒ‡å®šæ•°æ®é›†ï¼Œä½¿ç”¨é»˜è®¤: {dataset_paths}\n")

    # è®¾å¤‡
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")

    # é…ç½®
    config = PlaygroundConfig()

    # Tokenizer
    print(f"\nğŸ”¤ åˆå§‹åŒ–Tokenizerï¼ˆæ”¯æŒåŠ¨æ€æ ‡ç­¾ï¼‰...")
    tokenizer = DynamicTagTokenizer(vocab_size=5000)

    # æ•°æ®é›†ï¼ˆæ”¯æŒå•ä¸ªæˆ–å¤šä¸ªï¼‰
    dataset = HLBDPlaygroundDataset(dataset_paths, tokenizer)

    # DataLoader
    train_loader = DataLoader(
        dataset,
        batch_size=config.batch_size,
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=0  # é¿å…å¤šè¿›ç¨‹é™·é˜±
    )

    # æ¨¡å‹
    print(f"\nğŸ—ï¸  æ„å»ºAPTæ¨¡å‹...")
    model_config = APTModelConfiguration(
        vocab_size=tokenizer.vocab_size,
        d_model=config.d_model,
        n_heads=config.n_heads,
        num_encoder_layers=config.n_layers,
        num_decoder_layers=config.n_layers,
        d_ff=config.d_ff,
        max_seq_len=config.max_seq_len,
        dropout=config.dropout,
        use_dbc_dac=config.use_dbc_dac
    )
    model = APTModel(model_config)

    total_params = sum(p.numel() for p in model.parameters())
    print(f"   æ€»å‚æ•°: {total_params:,}")

    # è®­ç»ƒå™¨
    trainer = HLBDPlaygroundTrainer(
        config=config,
        model=model,
        train_loader=train_loader,
        tokenizer=tokenizer,
        dataset_stats=dataset.dataset_stats,  # ä¼ é€’æ•°æ®é›†ç»Ÿè®¡
        device=device
    )

    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = Path(args.save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)

    # è®­ç»ƒå¾ªç¯
    print("\n" + "=" * 60)
    print("ğŸ® HLBD Playgroundè®­ç»ƒå¼€å§‹")
    print("=" * 60)

    for epoch in range(args.epochs):
        print(f"\nğŸ“ Epoch {epoch + 1}/{args.epochs}")

        # è®­ç»ƒ
        loss, epoch_time = trainer.train_epoch(epoch)
        print(f"   Loss: {loss:.4f} | ç”¨æ—¶: {epoch_time:.2f}s")

        # ä¿å­˜è¿›åº¦ï¼ˆç”¨äºå¯è§†åŒ–ï¼‰
        trainer.save_progress_report(args.save_dir)

        # å®šæœŸä¿å­˜
        if (epoch + 1) % args.save_interval == 0:
            save_path = save_dir / f"checkpoint_epoch_{epoch + 1}.pt"
            trainer.save_checkpoint(str(save_path), epoch + 1)

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_path = save_dir / "final_model.pt"
    trainer.save_checkpoint(str(final_path), args.epochs)

    print("\n" + "=" * 60)
    print("âœ¨ è®­ç»ƒå®Œæˆï¼")
    print("=" * 60)


if __name__ == "__main__":
    main()
