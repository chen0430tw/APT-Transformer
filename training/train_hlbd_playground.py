#!/usr/bin/env python3
"""
HLBD Playgroundè®­ç»ƒè„šæœ¬
ä½¿ç”¨APT Playgroundæ¶æ„è®­ç»ƒHLBDæ•°æ®é›†ï¼ˆæ”¯æŒæ¨¡å—åŒ–å¤šæ•°æ®é›†è®­ç»ƒï¼‰

ç‰¹æ€§:
- ğŸ”— æ¨¡å—åŒ–è®­ç»ƒ - æ”¯æŒåŒæ—¶åŠ è½½å¤šä¸ªHLBDæ•°æ®é›†
- ğŸ“Š è‡ªåŠ¨æ ¼å¼è¯†åˆ« - HLBD Full (8å±‚) + HLBD Hardcore (æ¨¡å—åŒ–)
- ğŸ¢ Playground Theory (CosineAnnealingWarmRestarts)
- ğŸš€ RTX 3070ä¼˜åŒ– (æ··åˆç²¾åº¦ + æ¢¯åº¦ç´¯ç§¯)
- ğŸ·ï¸  æ”¯æŒåŠ¨æ€æ ‡ç­¾ ([EMOJI], [EN], [PY], [JP], [KR], [PHRASE])
- ğŸ”§ DBC-DACæ¢¯åº¦ç¨³å®š
- ğŸ“Š å®æ—¶å¯è§†åŒ–æ”¯æŒ
- ğŸ² æ•°æ®ç¨€é‡Šå­¦ - è‡ªåŠ¨æ‰“æ•£æ··åˆï¼Œé˜²æ­¢æ¨¡å¼åç¼©

æ•°æ®é›†æ ¼å¼æ”¯æŒ:
1. HLBD Full: 8å±‚åˆ†å±‚è¯­è¨€ç»“æ„ (åŒ…å«Level 3å¥æ³•å±‚)
2. HLBD Hardcore: ä¸¥æ ¼é€»è¾‘é—®ç­” (å‡ ä½•ã€ç®—æœ¯ã€ç”Ÿè‚–ã€ç‰©ç†ã€è‹±æ–‡)

ä½¿ç”¨ç¤ºä¾‹:
  # å•æ•°æ®é›†
  python train_hlbd_playground.py --dataset data/HLBD_Hardcore_Full_V2.json

  # å¤šæ•°æ®é›†è”åˆè®­ç»ƒ (~10,000æ ·æœ¬)
  python train_hlbd_playground.py --datasets data/HLBD_Full_V2.json data/HLBD_Hardcore_Full_V2.json --epochs 50
"""

import os
import sys
import json
import time
import math
import re
import random
import argparse
from pathlib import Path

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import autocast, GradScaler
from tqdm import tqdm

# ============================================================================
# ä¿®å¤è·¯å¾„é—®é¢˜ï¼šæ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°sys.path
# ============================================================================
# è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½•çš„çˆ¶ç›®å½•ï¼ˆå³é¡¹ç›®æ ¹ç›®å½•ï¼‰
PROJECT_ROOT = Path(__file__).parent.parent.absolute()
sys.path.insert(0, str(PROJECT_ROOT))

from apt_model.modeling.apt_model import APTModel, APTModelConfiguration


# ============================================================================
# åŠ¨æ€æ ‡ç­¾Tokenizer (ä»HLBDè„šæœ¬ç§»æ¤)
# ============================================================================

class DynamicTagTokenizer:
    """æ”¯æŒåŠ¨æ€æ ‡ç­¾çš„å­—ç¬¦çº§tokenizer"""

    def __init__(self, vocab_size=5000):
        # åŸºç¡€ç‰¹æ®Štoken
        self.vocab = {
            '[PAD]': 0, '[UNK]': 1, '[BOS]': 2, '[EOS]': 3,
            '[EMOJI]': 4, '[PHRASE]': 5, '[EN]': 6, '[PY]': 7, '[JP]': 8, '[KR]': 9,
        }
        self.pad_token_id = 0
        self.unk_token_id = 1
        self.bos_token_id = 2
        self.eos_token_id = 3
        self.vocab_size = vocab_size

        self.char_to_id = self.vocab.copy()
        self.id_to_char = {v: k for k, v in self.vocab.items()}
        self.next_id = 10

        # é¢„ç¼–è¯‘æ ‡ç­¾æ­£åˆ™
        self.tag_pattern = re.compile(r'(\[EMOJI\]|\[PHRASE\]|\[EN\]|\[PY\]|\[JP\]|\[KR\])')

    def _tokenize_text(self, text):
        """å…ˆåˆ‡æ ‡ç­¾ï¼Œå†åˆ‡å­—ç¬¦"""
        tokens = []
        parts = self.tag_pattern.split(text)

        for part in parts:
            if part in self.vocab:
                # æ ‡ç­¾ - ç›´æ¥æ·»åŠ ID
                tokens.append(self.vocab[part])
            else:
                # æ™®é€šæ–‡æœ¬ - é€å­—ç¬¦
                for char in part:
                    if char.strip():
                        tokens.append(self._get_or_add_char(char))
                    elif char == ' ':
                        tokens.append(self._get_or_add_char(char))

        return tokens

    def _get_or_add_char(self, char):
        """åŠ¨æ€æ·»åŠ å­—ç¬¦"""
        if char not in self.char_to_id:
            if self.next_id < self.vocab_size:
                self.char_to_id[char] = self.next_id
                self.id_to_char[self.next_id] = char
                self.next_id += 1
            else:
                return self.unk_token_id
        return self.char_to_id[char]

    def encode(self, text):
        """ç¼–ç æ–‡æœ¬"""
        ids = [self.bos_token_id]
        ids.extend(self._tokenize_text(text))
        ids.append(self.eos_token_id)
        return ids

    def decode(self, ids, skip_special_tokens=True):
        """è§£ç IDåºåˆ—"""
        chars = []
        for id in ids:
            if isinstance(id, torch.Tensor):
                id = id.item()

            if skip_special_tokens and id in [self.pad_token_id, self.bos_token_id,
                                               self.eos_token_id, self.unk_token_id]:
                continue

            char = self.id_to_char.get(id, '[UNK]')
            chars.append(char)

        return ''.join(chars)


# ============================================================================
# HLBDæ•°æ®é›†
# ============================================================================

class HLBDPlaygroundDataset(Dataset):
    """HLBDæ¨¡å—åŒ–æ•°æ®é›† - æ”¯æŒå¤šæ•°æ®é›†å’Œå¤šæ ¼å¼"""

    def __init__(self, json_paths, tokenizer: DynamicTagTokenizer, max_len: int = 128):
        """
        åˆå§‹åŒ–HLBDæ•°æ®é›†

        Args:
            json_paths: å•ä¸ªè·¯å¾„(str)æˆ–å¤šä¸ªè·¯å¾„(list)
            tokenizer: DynamicTagTokenizerå®ä¾‹
            max_len: æœ€å¤§åºåˆ—é•¿åº¦
        """
        self.tokenizer = tokenizer
        self.max_len = max_len

        # ç»Ÿä¸€å¤„ç†ä¸ºåˆ—è¡¨
        if isinstance(json_paths, str):
            json_paths = [json_paths]

        self.pairs = []
        self.dataset_stats = {}

        print(f"\nğŸ“š æ¨¡å—åŒ–HLBDæ•°æ®é›†åŠ è½½å™¨")
        print(f"   æ•°æ®é›†æ•°é‡: {len(json_paths)}")
        print("=" * 60)

        # åŠ è½½æ‰€æœ‰æ•°æ®é›†
        for i, json_path in enumerate(json_paths, 1):
            print(f"\nğŸ“‚ [{i}/{len(json_paths)}] åŠ è½½æ•°æ®é›†: {json_path}")
            dataset_pairs = self._load_single_dataset(json_path)

            if dataset_pairs:
                self.pairs.extend(dataset_pairs)
                dataset_name = Path(json_path).stem
                self.dataset_stats[dataset_name] = len(dataset_pairs)
                print(f"   âœ“ æˆåŠŸåŠ è½½ {len(dataset_pairs)} ä¸ªè®­ç»ƒå¯¹")
            else:
                print(f"   âš ï¸  æ•°æ®é›†ä¸ºç©ºæˆ–åŠ è½½å¤±è´¥")

        # æ‰“æ•£æ··åˆ
        random.shuffle(self.pairs)

        print(f"\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
        for name, count in self.dataset_stats.items():
            percentage = count / len(self.pairs) * 100 if self.pairs else 0
            print(f"   {name}: {count} å¯¹ ({percentage:.1f}%)")
        print(f"   æ€»è®¡: {len(self.pairs)} ä¸ªè®­ç»ƒå¯¹")
        print(f"   âœ“ å·²æ··åˆæ‰“æ•£")

        # é¢„å¡«å……è¯æ±‡è¡¨ï¼ˆé¿å…å¤šè¿›ç¨‹é™·é˜±ï¼‰
        print(f"\nğŸ”¤ é¢„å¡«å……è¯æ±‡è¡¨...")
        for src, tgt in self.pairs:
            self.tokenizer.encode(src)
            self.tokenizer.encode(tgt)
        print(f"   âœ“ è¯æ±‡è¡¨å¤§å°: {len(self.tokenizer.char_to_id)}")
        print("=" * 60)

    def _load_single_dataset(self, json_path: str):
        """åŠ è½½å•ä¸ªæ•°æ®é›†å¹¶è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼"""
        try:
            with open(json_path, 'r', encoding='utf-8') as f:
                data = json.load(f)

            # æ£€æµ‹æ•°æ®é›†ç±»å‹
            if 'samples' in data:
                # HLBD Fullæ ¼å¼ï¼ˆ8å±‚ç»“æ„ï¼‰
                print(f"   æ ¼å¼: HLBD Full (8å±‚ç»“æ„)")
                return self._process_hlbd_full(data['samples'])
            elif 'data' in data:
                # HLBD Hardcoreæ ¼å¼ï¼ˆæ¨¡å—åŒ–ï¼‰
                print(f"   æ ¼å¼: HLBD Hardcore (æ¨¡å—åŒ–)")
                return self._process_hlbd_hardcore(data['data'])
            else:
                print(f"   âš ï¸  æœªçŸ¥çš„æ•°æ®é›†æ ¼å¼")
                return []

        except Exception as e:
            print(f"   âŒ åŠ è½½å¤±è´¥: {e}")
            return []

    def _process_hlbd_hardcore(self, data):
        """å¤„ç†HLBD Hardcoreæ ¼å¼ï¼ˆæ¨¡å—åŒ–Q&Aï¼‰"""
        pairs = []
        for module_name, module_data in data.items():
            for item in module_data:
                src = item['input']
                tgt = item['output']
                pairs.append((src, tgt))
        return pairs

    def _process_hlbd_full(self, samples):
        """å¤„ç†HLBD Fullæ ¼å¼ï¼ˆ8å±‚ç»“æ„ï¼‰"""
        pairs = []

        for sample in samples:
            concept = sample.get('concept', '')

            # æ„å»ºè¾“å…¥ï¼šæ¦‚å¿µ + å…³é”®å±‚çº§ä¿¡æ¯
            input_parts = [f"æ¦‚å¿µ: {concept}"]

            # Level 1: å­—å¡ + Emoji
            if 'level_1' in sample:
                char_card = sample['level_1'].get('å­—å¡', '')
                emoji = sample['level_1'].get('emoji', '')
                input_parts.append(f"[EMOJI] {char_card} {emoji}")

            # Level 2: çŸ­è¯­
            if 'level_2' in sample:
                phrase = sample['level_2'].get('çŸ­è¯­', '')
                input_parts.append(f"[PHRASE] {phrase}")

            # Level 3: æ•°å­¦ï¼ˆå¥æ³•ç»“æ„ï¼‰â† é‡è¦ï¼
            if 'level_3' in sample:
                math_expr = sample['level_3'].get('æ•°å­¦', '')
                input_parts.append(f"å¥æ³•ç»“æ„: {math_expr}")

            input_text = "\n".join(input_parts)

            # æ„å»ºè¾“å‡ºï¼šå¤šè¯­è¨€ç¿»è¯‘
            output_parts = []

            # Level 4: æ‹¼éŸ³
            if 'level_4' in sample:
                pinyin = sample['level_4'].get('æ‹¼éŸ³', '')
                output_parts.append(f"[PY] {pinyin}")

            # Level 5: è‹±æ–‡
            if 'level_5' in sample:
                english = sample['level_5'].get('è‹±æ–‡', '')
                output_parts.append(f"[EN] {english}")

            # Level 6: ä¸­æ–‡
            if 'level_6' in sample:
                chinese = sample['level_6'].get('ä¸­æ–‡', '')
                output_parts.append(f"{chinese}")

            # Level 7: æ—¥æ–‡
            if 'level_7' in sample:
                japanese = sample['level_7'].get('æ—¥æ–‡', '')
                if japanese:
                    output_parts.append(f"[JP] {japanese}")

            # Level 8: éŸ©æ–‡
            if 'level_8' in sample:
                korean = sample['level_8'].get('éŸ©æ–‡', '')
                if korean:
                    output_parts.append(f"[KR] {korean}")

            output_text = "\n".join(output_parts)

            pairs.append((input_text, output_text))

        return pairs

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        src, tgt = self.pairs[idx]

        # Encode
        src_ids = self.tokenizer.encode(src)
        tgt_ids = self.tokenizer.encode(tgt)

        # æ‹¼æ¥: [src, SEP, tgt]
        input_ids = src_ids + [1] + tgt_ids

        # æˆªæ–­
        if len(input_ids) > self.max_len:
            input_ids = input_ids[:self.max_len]

        return {
            'input_ids': torch.tensor(input_ids[:-1], dtype=torch.long),
            'labels': torch.tensor(input_ids[1:], dtype=torch.long)
        }


def collate_fn(batch):
    """åŠ¨æ€padding"""
    input_ids = [item['input_ids'] for item in batch]
    labels = [item['labels'] for item in batch]

    max_len = max(len(ids) for ids in input_ids)

    padded_input = []
    padded_labels = []

    for inp, lab in zip(input_ids, labels):
        pad_len = max_len - len(inp)
        padded_input.append(torch.cat([inp, torch.zeros(pad_len, dtype=torch.long)]))
        padded_labels.append(torch.cat([lab, torch.full((pad_len,), -100, dtype=torch.long)]))

    return {
        'input_ids': torch.stack(padded_input),
        'labels': torch.stack(padded_labels)
    }


# ============================================================================
# Playgroundé…ç½®
# ============================================================================

class PlaygroundConfig:
    """RTX 3070ä¼˜åŒ–é…ç½®"""

    # æ¨¡å‹é…ç½®
    d_model = 256
    n_layers = 6
    num_heads = 8  # âœ… ä¿®å¤ï¼šç»Ÿä¸€ä½¿ç”¨num_headsï¼ˆ256/8=32å¯æ•´é™¤ï¼‰
    d_ff = 1024
    max_seq_len = 256
    dropout = 0.1

    # è®­ç»ƒé…ç½®
    batch_size = 16
    gradient_accumulation_steps = 2
    mixed_precision = True
    max_grad_norm = 1.0
    epochs = 100  # æ·»åŠ é»˜è®¤epochs

    # Playground Theory
    base_lr = 3e-4
    min_lr = 1e-5
    T_0 = 10  # Cosineé‡å¯å‘¨æœŸ
    T_mult = 2

    # DBC-DAC
    use_dbc_dac = True


# ============================================================================
# è®­ç»ƒå™¨
# ============================================================================

class HLBDPlaygroundTrainer:
    """HLBD Playgroundè®­ç»ƒå™¨ï¼ˆå…¨èƒ½ä»ªè¡¨ç›˜ç‰ˆï¼‰"""

    def __init__(
        self,
        config: PlaygroundConfig,
        model: APTModel,
        train_loader: DataLoader,
        tokenizer: DynamicTagTokenizer,
        dataset_stats: dict = None,
        save_dir: str = 'hlbd_playground',
        device: str = 'cuda'
    ):
        self.config = config
        self.model = model.to(device)
        self.train_loader = train_loader
        self.tokenizer = tokenizer
        self.dataset_stats = dataset_stats or {}
        self.save_dir = save_dir
        self.device = device

        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=config.base_lr,
            weight_decay=0.01
        )

        # Playground Theoryè°ƒåº¦å™¨
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer,
            T_0=config.T_0,
            T_mult=config.T_mult,
            eta_min=config.min_lr
        )

        # æ··åˆç²¾åº¦
        self.scaler = GradScaler() if config.mixed_precision else None

        # æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss(ignore_index=-100)

        # ç»Ÿè®¡ - æŒ‰epochèšåˆ
        self.losses = []
        self.batch_losses = []  # å®æ—¶batch lossç”¨äºå¯è§†åŒ–

    def train_epoch(self, epoch: int):
        """è®­ç»ƒä¸€ä¸ªepochï¼ˆå…¨èƒ½ä»ªè¡¨ç›˜ç‰ˆï¼‰"""
        self.model.train()
        total_loss = 0
        num_batches = 0

        epoch_start = time.time()

        # åˆ›å»ºè¿›åº¦æ¡
        pbar = tqdm(
            self.train_loader,
            desc=f"ğŸ“ Epoch {epoch + 1}",
            unit="batch",
            ncols=120
        )

        for batch_idx, batch in enumerate(pbar):
            input_ids = batch['input_ids'].to(self.device)
            labels = batch['labels'].to(self.device)

            # --- â±ï¸ è®¡æ—¶ï¼šå‰å‘ä¼ æ’­ ---
            t0 = time.time()

            # æ··åˆç²¾åº¦å‰å‘
            with autocast(enabled=self.config.mixed_precision):
                logits = self.model(input_ids)
                loss = self.criterion(
                    logits.view(-1, logits.size(-1)),
                    labels.view(-1)
                )
                # è®°å½•çœŸå®Lossï¼ˆæœªé™¤ä»¥accumulation_stepså‰ï¼‰
                real_loss_val = loss.item()

                # ä¸ºæ¢¯åº¦ç´¯ç§¯åšé™¤æ³•
                loss = loss / self.config.gradient_accumulation_steps

            t1 = time.time()
            fw_ms = (t1 - t0) * 1000

            # --- â±ï¸ è®¡æ—¶ï¼šåå‘ä¼ æ’­ ---
            t2 = time.time()

            # Backward
            if self.scaler:
                self.scaler.scale(loss).backward()
            else:
                loss.backward()

            # æ¢¯åº¦ç´¯ç§¯æ›´æ–°
            if (batch_idx + 1) % self.config.gradient_accumulation_steps == 0:
                if self.scaler:
                    self.scaler.unscale_(self.optimizer)

                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(),
                    self.config.max_grad_norm
                )

                if self.scaler:
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    self.optimizer.step()

                self.optimizer.zero_grad()

            t3 = time.time()
            bw_ms = (t3 - t2) * 1000

            # --- ğŸ“Š è®¡ç®—æŒ‡æ ‡ï¼ˆAcc & PPLï¼‰---
            with torch.no_grad():
                # PPL (Perplexity) = exp(Loss)
                try:
                    ppl_val = math.exp(min(real_loss_val, 20))  # é™åˆ¶æœ€å¤§å€¼é˜²æ­¢æº¢å‡º
                except OverflowError:
                    ppl_val = float('inf')

                # Accuracy (å‡†ç¡®ç‡)
                preds = logits.argmax(dim=-1)
                mask = labels != -100
                correct = (preds == labels) & mask
                accuracy = correct.sum().float() / mask.sum().float() if mask.sum() > 0 else torch.tensor(0.0)
                acc_val = accuracy.item() * 100

            total_loss += real_loss_val
            num_batches += 1

            # ä¿å­˜batch lossç”¨äºå¯è§†åŒ–
            self.batch_losses.append({
                'epoch': epoch + 1,
                'batch': batch_idx,
                'loss': real_loss_val,
                'ppl': ppl_val,
                'acc': acc_val
            })

            # --- ğŸš€ å®æ—¶æ›´æ–°è¿›åº¦æ¡ ---
            current_lr = self.scheduler.get_last_lr()[0]
            pbar.set_postfix({
                "Loss": f"{real_loss_val:.4f}",
                "PPL": f"{ppl_val:.1f}",
                "Acc": f"{acc_val:.1f}%",
                "LR": f"{current_lr:.6f}",
                "FW": f"{fw_ms:.0f}ms",
                "BW": f"{bw_ms:.0f}ms"
            })

            # --- ğŸ“¡ å®æ—¶æ›´æ–°å¯è§†åŒ–ï¼ˆæ¯10ä¸ªbatchï¼‰---
            if batch_idx % 10 == 0:
                self._save_batch_progress()

        # Epochç»“æŸæ›´æ–°å­¦ä¹ ç‡
        self.scheduler.step()

        avg_loss = total_loss / num_batches
        epoch_time = time.time() - epoch_start
        self.losses.append(avg_loss)

        return avg_loss, epoch_time

    def _save_batch_progress(self):
        """ä¿å­˜batchçº§åˆ«çš„è¿›åº¦ï¼ˆclusterå­˜å‚¨ï¼Œä¸æ¯ç§’ä¸€ä¸ªæ–‡ä»¶ï¼‰"""
        report_path = Path(self.save_dir) / "experiment_report.json"

        # ç¡®ä¿ç›®å½•å­˜åœ¨
        report_path.parent.mkdir(parents=True, exist_ok=True)

        try:
            # Clusterå­˜å‚¨ï¼šæŒ‰epochèšåˆï¼Œæ¯ä¸ªepochæœ€å¤šä¿ç•™100ä¸ªç‚¹
            epoch_clusters = {}
            for item in self.batch_losses:
                epoch_num = item['epoch']
                if epoch_num not in epoch_clusters:
                    epoch_clusters[epoch_num] = []
                epoch_clusters[epoch_num].append(item['loss'])

            # å‹ç¼©ï¼šæ¯ä¸ªepochå‡åŒ€é‡‡æ ·æœ€å¤š100ä¸ªç‚¹
            clustered_losses = []
            for epoch_num in sorted(epoch_clusters.keys()):
                losses = epoch_clusters[epoch_num]
                if len(losses) <= 100:
                    clustered_losses.extend(losses)
                else:
                    # å‡åŒ€é‡‡æ ·
                    step = len(losses) / 100
                    sampled = [losses[int(i * step)] for i in range(100)]
                    clustered_losses.extend(sampled)

            report = {
                'control_losses': self.losses,  # Epochå¹³å‡loss
                'batch_losses': clustered_losses,  # Batchå®æ—¶lossï¼ˆclusterå‹ç¼©ï¼‰
                'current_epoch': len(self.losses),
                'total_batches': len(self.batch_losses),
                'dataset_stats': self.dataset_stats,
                'timestamp': time.time()
            }

            with open(report_path, 'w', encoding='utf-8') as f:
                json.dump(report, f, ensure_ascii=False, indent=2)
        except Exception as e:
            # é™é»˜å¤±è´¥ï¼Œä¸ä¸­æ–­è®­ç»ƒ
            pass

    def save_checkpoint(self, save_path: str, epoch: int):
        """ä¿å­˜checkpoint"""
        checkpoint = {
            'epoch': epoch,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'tokenizer_char_to_id': self.tokenizer.char_to_id,
            'tokenizer_id_to_char': self.tokenizer.id_to_char,
            'tokenizer_next_id': self.tokenizer.next_id,
            'tokenizer_vocab_size': self.tokenizer.vocab_size,
            'config': self.config.__dict__,
            'losses': self.losses,
            'dataset_stats': self.dataset_stats
        }

        if self.scaler:
            checkpoint['scaler_state_dict'] = self.scaler.state_dict()

        torch.save(checkpoint, save_path)
        print(f"âœ… Checkpointå·²ä¿å­˜: {save_path}")

        # å¦‚æœæ˜¯å¤šæ•°æ®é›†è®­ç»ƒï¼Œæ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        if len(self.dataset_stats) > 1:
            print(f"   æ•°æ®é›†æ¥æº:")
            for name, count in self.dataset_stats.items():
                print(f"     - {name}: {count} æ ·æœ¬")


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description='HLBD Playgroundè®­ç»ƒ - æ”¯æŒå•æ•°æ®é›†å’Œå¤šæ•°æ®é›†æ¨¡å—åŒ–è®­ç»ƒ',
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  # å•æ•°æ®é›†è®­ç»ƒï¼ˆå‘åå…¼å®¹ï¼‰
  python train_hlbd_playground.py --dataset data/HLBD_Hardcore_Full_V2.json

  # å¤šæ•°æ®é›†æ¨¡å—åŒ–è®­ç»ƒ
  python train_hlbd_playground.py --datasets data/HLBD_Full_V2.json data/HLBD_Hardcore_Full_V2.json

  # è‡ªå®šä¹‰è®­ç»ƒå‚æ•°
  python train_hlbd_playground.py --datasets data/*.json --epochs 50 --save-dir models/hlbd_combined
        """
    )

    parser.add_argument('--dataset', type=str, default=None,
                       help='å•ä¸ªHLBDæ•°æ®é›†è·¯å¾„ï¼ˆå‘åå…¼å®¹ï¼‰')
    parser.add_argument('--datasets', nargs='+', default=None,
                       help='å¤šä¸ªHLBDæ•°æ®é›†è·¯å¾„ï¼ˆæ¨¡å—åŒ–è®­ç»ƒï¼‰')
    parser.add_argument('--epochs', type=int, default=100,
                       help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--save-dir', type=str, default='hlbd_playground',
                       help='ä¿å­˜ç›®å½•')
    parser.add_argument('--save-interval', type=int, default=25,
                       help='ä¿å­˜é—´éš”')

    args = parser.parse_args()

    # å¤„ç†æ•°æ®é›†å‚æ•°
    if args.datasets:
        dataset_paths = args.datasets
    elif args.dataset:
        dataset_paths = args.dataset
    else:
        dataset_paths = str(PROJECT_ROOT / 'data' / 'HLBD_Hardcore_Full.json')
        print(f"âš ï¸  æœªæŒ‡å®šæ•°æ®é›†ï¼Œä½¿ç”¨é»˜è®¤: {dataset_paths}\n")

    # è®¾å¤‡
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")

    # é…ç½®
    config = PlaygroundConfig()
    config.epochs = args.epochs  # æ›´æ–°epochs

    # Tokenizer
    print(f"\nğŸ”¤ åˆå§‹åŒ–Tokenizerï¼ˆæ”¯æŒåŠ¨æ€æ ‡ç­¾ï¼‰...")
    tokenizer = DynamicTagTokenizer(vocab_size=5000)

    # æ•°æ®é›†
    dataset = HLBDPlaygroundDataset(dataset_paths, tokenizer)

    # DataLoader
    train_loader = DataLoader(
        dataset,
        batch_size=config.batch_size,
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=0
    )

    # æ¨¡å‹é…ç½®ï¼ˆâœ… ä¿®å¤ï¼šç»Ÿä¸€ä½¿ç”¨num_headsï¼‰
    print(f"\nğŸ—ï¸  æ„å»ºAPTæ¨¡å‹...")
    model_config = APTModelConfiguration(
        vocab_size=tokenizer.vocab_size,
        d_model=config.d_model,
        num_heads=config.num_heads,  # âœ… å·¦è¾¹å¿…é¡»æ˜¯num_heads
        num_encoder_layers=config.n_layers,
        num_decoder_layers=config.n_layers,
        d_ff=config.d_ff,
        max_seq_len=config.max_seq_len,
        dropout=config.dropout,
        use_dbc_dac=config.use_dbc_dac
    )
    model = APTModel(model_config)

    total_params = sum(p.numel() for p in model.parameters())
    print(f"   æ€»å‚æ•°: {total_params:,}")
    print(f"   d_model={config.d_model}, num_heads={config.num_heads} (æ¯å¤´ç»´åº¦={config.d_model//config.num_heads})")

    # è®­ç»ƒå™¨
    trainer = HLBDPlaygroundTrainer(
        config=config,
        model=model,
        train_loader=train_loader,
        tokenizer=tokenizer,
        dataset_stats=dataset.dataset_stats,
        save_dir=args.save_dir,
        device=device
    )

    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = Path(args.save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)

    # è®­ç»ƒå¾ªç¯
    print("\n" + "=" * 60)
    print("ğŸ® HLBD Playgroundè®­ç»ƒå¼€å§‹")
    print("=" * 60)

    for epoch in range(args.epochs):
        # è®­ç»ƒ
        loss, epoch_time = trainer.train_epoch(epoch)

        print(f"\nğŸ“Š Epoch {epoch + 1}/{args.epochs} å®Œæˆ:")
        print(f"   å¹³å‡Loss: {loss:.4f}")
        print(f"   PPL: {math.exp(min(loss, 20)):.2f}")
        print(f"   ç”¨æ—¶: {epoch_time:.2f}s")

        # å®šæœŸä¿å­˜
        if (epoch + 1) % args.save_interval == 0:
            save_path = save_dir / f"checkpoint_epoch_{epoch + 1}.pt"
            trainer.save_checkpoint(str(save_path), epoch + 1)

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_path = save_dir / "final_model.pt"
    trainer.save_checkpoint(str(final_path), args.epochs)

    print("\n" + "=" * 60)
    print("âœ¨ è®­ç»ƒå®Œæˆï¼")
    print("=" * 60)
    print(f"\nğŸ“ æ¨¡å‹ä¿å­˜ä½ç½®: {save_dir}")
    print(f"ğŸ“ˆ å¯è§†åŒ–æ•°æ®: {save_dir / 'experiment_report.json'}")


if __name__ == "__main__":
    main()
