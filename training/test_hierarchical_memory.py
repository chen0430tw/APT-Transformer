#!/usr/bin/env python3
"""
æµ‹è¯•åˆ†å±‚è®°å¿†ç³»ç»Ÿå’ŒåŠ¨æ€ç¼–ç å™¨

æµ‹è¯•å†…å®¹ï¼š
1. A/B/Cæ¡£è®°å¿†å­˜å‚¨ä¸æ£€ç´¢
2. é”šç‚¹æŒ‡ä»¤è§£æï¼ˆã€å°å­˜Â·åŸæ–‡ã€‘ç­‰ï¼‰
3. Keyæ§æ£€ç´¢ + ç‰ˆæœ¬åŒ–
4. é˜²æ¼‚ç§»éªŒè¯
5. åŠ¨æ€ç¼–ç å™¨æ‰©å……
6. ç»Ÿä¸€ä¸Šä¸‹æ–‡ç»„åˆ
"""

import os
import sys
import json
import tempfile
from pathlib import Path

# Add parent directory to path
sys.path.insert(0, str(Path(__file__).parent.parent))

import logging
logging.basicConfig(level=logging.INFO)


# ==================== æµ‹è¯•åˆ†å±‚è®°å¿†ç³»ç»Ÿ ====================

def test_abc_tier_storage():
    """æµ‹è¯• A/B/C æ¡£å­˜å‚¨"""
    print("\n" + "="*70)
    print("æµ‹è¯• 1: A/B/C æ¡£åˆ†å±‚å­˜å‚¨")
    print("="*70)

    from apt_model.memory.hierarchical_memory import create_hierarchical_memory

    memory = create_hierarchical_memory()

    # Aæ¡£ï¼šå¿…é¡»åŸæ ·ä¿ç•™
    memory.detail_store.add_verbatim(
        key="DEF:Apeiron:v1",
        content="Apeironï¼ˆá¼„Ï€ÎµÎ¹ÏÎ¿Î½ï¼‰æ˜¯æ— é™ã€æœªåˆ†åŒ–çš„åŸå§‹å­˜åœ¨ï¼Œé˜¿é‚£å…‹è¥¿æ›¼å¾·æå‡ºçš„å®‡å®™æœ¬åŸã€‚",
        version="v1",
        category="definition",
        importance=1.0
    )

    # Bæ¡£ï¼šç»“æ„åŒ–å­—æ®µ
    memory.detail_store.add_structured(
        key="PARAM:HyperParams:v1",
        fields={
            "learning_rate": 0.001,
            "batch_size": 32,
            "epochs": 100,
            "optimizer": "AdamW"
        },
        version="v1",
        category="parameter",
        importance=0.8
    )

    # Cæ¡£ï¼šå…è®¸æ‘˜è¦
    memory.detail_store.add_narrative(
        key="NARR:PhilosophyBackground:v1",
        summary="å¤å¸Œè…Šå“²å­¦å®¶æ¢ç´¢å®‡å®™æœ¬åŸæ—¶ï¼Œæå‡ºäº†å„ç§æ¦‚å¿µï¼šæ°´ã€ç«ã€æ°”ã€åŸå­ç­‰ã€‚",
        original_ref="å¯å›æº¯åˆ°èµ«æ‹‰å…‹åˆ©ç‰¹ã€æ³°å‹’æ–¯ç­‰äººçš„è‘—ä½œ",
        version="v1",
        category="background",
        importance=0.5
    )

    print(f"âœ… Aæ¡£ï¼ˆVerbatimï¼‰: {len(memory.detail_store.verbatim)} æ¡")
    print(f"âœ… Bæ¡£ï¼ˆStructuredï¼‰: {len(memory.detail_store.structured)} æ¡")
    print(f"âœ… Cæ¡£ï¼ˆNarrativeï¼‰: {len(memory.detail_store.narrative)} æ¡")

    # Keyæ£€ç´¢æµ‹è¯•
    entry_a = memory.detail_store.get_by_key("DEF:Apeiron:v1")
    print(f"\nğŸ” Keyæ£€ç´¢ Aæ¡£: {entry_a.content[:50]}...")

    entry_b = memory.detail_store.get_by_key("PARAM:HyperParams:v1")
    print(f"ğŸ” Keyæ£€ç´¢ Bæ¡£: {entry_b.fields}")

    return memory


def test_anchor_directives():
    """æµ‹è¯•é”šç‚¹æŒ‡ä»¤è§£æ"""
    print("\n" + "="*70)
    print("æµ‹è¯• 2: é”šç‚¹æŒ‡ä»¤è§£æï¼ˆã€å°å­˜Â·åŸæ–‡ã€‘ç­‰ï¼‰")
    print("="*70)

    from apt_model.memory.hierarchical_memory import create_hierarchical_memory

    memory = create_hierarchical_memory()

    # æ¨¡æ‹Ÿç”¨æˆ·è¾“å…¥ï¼ˆåŒ…å«é”šç‚¹æŒ‡ä»¤ï¼‰
    user_text = """
æˆ‘è¦å®šä¹‰ä¸€äº›æ ¸å¿ƒæ¦‚å¿µï¼š

ã€å°å­˜Â·åŸæ–‡ã€‘DEF:LeftSpinSmooth:v1: å·¦æ—‹å¹³æ»‘ï¼ˆLeft-Spin Smoothï¼‰æ˜¯ä¸€ç§å•å‘ç¼“å†²é—¨æ§æœºåˆ¶ï¼Œé€šè¿‡å°–ç‚¹å¼ºåº¦Ï†æ§åˆ¶æ­¥é•¿ï¼Œé¿å…è®­ç»ƒä¸­çš„æ•°å€¼ä¸ç¨³å®šã€‚

ã€å°å­˜Â·å­—æ®µã€‘PARAM:SmoothConfig:v1: {
    "alpha": 0.5,
    "tau": 0.3,
    "beta": 0.7,
    "spike_threshold": 0.3
}

ã€å°å­˜Â·æ‘˜è¦ã€‘NARR:Motivation:v1: è¿™ä¸ªè®¾è®¡çµæ„Ÿæ¥è‡ªäºè§‚å¯Ÿåˆ°æ·±åº¦å­¦ä¹ è®­ç»ƒä¸­æ¢¯åº¦å°–ç‚¹å¯¼è‡´çš„NaNé—®é¢˜ï¼Œä¼ ç»Ÿæ–¹æ³•ï¼ˆæ¢¯åº¦è£å‰ªã€æ³°å‹’å±•å¼€ï¼‰éƒ½å­˜åœ¨å±€é™æ€§ã€‚
"""

    # è‡ªåŠ¨è§£æå¹¶å­˜å‚¨
    memory.process_anchor_directives(user_text, default_version="v1")

    print(f"âœ… è§£æåˆ° Aæ¡£: {len(memory.detail_store.verbatim)} æ¡")
    print(f"âœ… è§£æåˆ° Bæ¡£: {len(memory.detail_store.structured)} æ¡")
    print(f"âœ… è§£æåˆ° Cæ¡£: {len(memory.detail_store.narrative)} æ¡")

    # æŸ¥çœ‹éª¨æ¶å¡ç´¢å¼•
    print(f"\nğŸ“‹ éª¨æ¶å¡ç´¢å¼•æ›´æ–°:")
    for key, oneliner in list(memory.skeleton.index.items())[:5]:
        print(f"  â€¢ {key}: {oneliner}")

    return memory


def test_versioning_and_retrieval():
    """æµ‹è¯•ç‰ˆæœ¬åŒ–å’Œæ£€ç´¢"""
    print("\n" + "="*70)
    print("æµ‹è¯• 3: ç‰ˆæœ¬åŒ– + Keyæ§æ£€ç´¢")
    print("="*70)

    from apt_model.memory.hierarchical_memory import create_hierarchical_memory

    memory = create_hierarchical_memory()

    # æ·»åŠ å¤šä¸ªç‰ˆæœ¬
    memory.detail_store.add_verbatim(
        key="DEF:RoPE:v1",
        content="RoPE (Rotary Position Embedding) æ˜¯æ—‹è½¬ä½ç½®ç¼–ç ã€‚",
        version="v1"
    )

    memory.detail_store.add_verbatim(
        key="DEF:RoPE:v2",
        content="RoPE (Rotary Position Embedding) æ˜¯æ—‹è½¬ä½ç½®ç¼–ç ï¼Œé€šè¿‡å¤æ•°æ—‹è½¬å®ç°ä½ç½®ä¿¡æ¯æ³¨å…¥ã€‚",
        version="v2"
    )

    memory.detail_store.add_verbatim(
        key="DEF:RoPE:v3",
        content="RoPE (Rotary Position Embedding) æ˜¯æ—‹è½¬ä½ç½®ç¼–ç ï¼Œé€šè¿‡å¤æ•°æ—‹è½¬å®ç°ä½ç½®ä¿¡æ¯æ³¨å…¥ï¼Œæ”¯æŒå¤–æ¨åˆ°æ›´é•¿åºåˆ—ã€‚",
        version="v3"
    )

    print("âœ… æ·»åŠ äº† 3 ä¸ªç‰ˆæœ¬")

    # Keyæ£€ç´¢ç‰¹å®šç‰ˆæœ¬
    v1 = memory.detail_store.get_by_key("DEF:RoPE:v1")
    v3 = memory.detail_store.get_by_key("DEF:RoPE:v3")

    print(f"\nğŸ” v1: {v1.content}")
    print(f"ğŸ” v3: {v3.content}")

    # éªŒè¯å®Œæ•´æ€§
    assert v1.verify_integrity(), "v1 å®Œæ•´æ€§éªŒè¯å¤±è´¥"
    assert v3.verify_integrity(), "v3 å®Œæ•´æ€§éªŒè¯å¤±è´¥"
    print(f"\nâœ… å®Œæ•´æ€§éªŒè¯é€šè¿‡ï¼ˆå“ˆå¸Œæ ¡éªŒï¼‰")

    return memory


def test_anti_drift_validation():
    """æµ‹è¯•é˜²æ¼‚ç§»éªŒè¯"""
    print("\n" + "="*70)
    print("æµ‹è¯• 4: é˜²æ¼‚ç§»éªŒè¯ï¼ˆä¸€è‡´æ€§æ£€æŸ¥ï¼‰")
    print("="*70)

    from apt_model.memory.hierarchical_memory import create_hierarchical_memory

    memory = create_hierarchical_memory()

    # æ·»åŠ å®šä¹‰
    memory.detail_store.add_verbatim(
        key="DEF:iRoPE:v1",
        content="iRoPE (Interleaved RoPE) æ˜¯äº¤é”™æ—‹è½¬ä½ç½®ç¼–ç ï¼ŒLlama 4 ä½¿ç”¨çš„æŠ€æœ¯ã€‚",
        version="v1"
    )

    memory.detail_store.add_structured(
        key="SYM:Notation:v1",
        fields={
            "Ï†": "å°–ç‚¹å¼ºåº¦",
            "Î±": "ç¼“å†²ç³»æ•°",
            "Ï„": "æ—¶é—´å¸¸æ•°"
        },
        version="v1"
    )

    # æµ‹è¯•æ–‡æœ¬ï¼ˆæ­£ç¡®ä½¿ç”¨ï¼‰
    good_text = """
æˆ‘ä»¬ä½¿ç”¨ iRoPE (Interleaved RoPE) æ˜¯äº¤é”™æ—‹è½¬ä½ç½®ç¼–ç ï¼Œè¿™æ˜¯ Llama 4 ä½¿ç”¨çš„æŠ€æœ¯ã€‚
å…¶ä¸­å°–ç‚¹å¼ºåº¦ Ï† ç”±ç¼“å†²ç³»æ•° Î± å’Œæ—¶é—´å¸¸æ•° Ï„ å…±åŒå†³å®šã€‚
"""

    # æµ‹è¯•æ–‡æœ¬ï¼ˆé”™è¯¯ä½¿ç”¨ - æœªåŸæ ·å¼•ç”¨ï¼‰
    bad_text = """
æˆ‘ä»¬ä½¿ç”¨ iRoPEï¼Œè¿™æ˜¯ä¸€ç§æ”¹è¿›çš„ä½ç½®ç¼–ç æ–¹æ³•ã€‚ï¼ˆâŒ æœªåŸæ ·å¼•ç”¨Aæ¡£ï¼‰
"""

    # éªŒè¯
    validation_good = memory.validator.validate_usage(
        good_text,
        referenced_keys=["DEF:iRoPE:v1", "SYM:Notation:v1"]
    )

    validation_bad = memory.validator.validate_usage(
        bad_text,
        referenced_keys=["DEF:iRoPE:v1"]
    )

    print(f"\nâœ… æ­£ç¡®ä½¿ç”¨éªŒè¯: valid={validation_good['valid']}")
    print(f"   Warnings: {len(validation_good['warnings'])}")

    print(f"\nâš ï¸  é”™è¯¯ä½¿ç”¨éªŒè¯: valid={validation_bad['valid']}")
    print(f"   Warnings: {len(validation_bad['warnings'])}")
    for warning in validation_bad['warnings']:
        print(f"     â€¢ {warning}")

    return memory


def test_skeleton_and_detail_layers():
    """æµ‹è¯•ä¸¤å±‚å­˜å‚¨ï¼ˆéª¨æ¶å¡ + ç»†èŠ‚ä»“ï¼‰"""
    print("\n" + "="*70)
    print("æµ‹è¯• 5: ä¸¤å±‚å­˜å‚¨ï¼ˆéª¨æ¶å¡ + ç»†èŠ‚ä»“ï¼‰")
    print("="*70)

    from apt_model.memory.hierarchical_memory import create_hierarchical_memory

    memory = create_hierarchical_memory()

    # æ·»åŠ å¤šæ¡è®°å¿†
    for i in range(10):
        memory.detail_store.add_verbatim(
            key=f"DEF:Concept{i}:v1",
            content=f"è¿™æ˜¯æ¦‚å¿µ {i} çš„è¯¦ç»†å®šä¹‰ï¼ŒåŒ…å«å¤§é‡ç»†èŠ‚...",
            version="v1"
        )
        # æ›´æ–°éª¨æ¶ç´¢å¼•ï¼ˆåªå­˜ one-linerï¼‰
        memory.skeleton.add_index(f"DEF:Concept{i}:v1", f"æ¦‚å¿µ {i}")

    # æ·»åŠ é”šç‚¹å’Œç¦æ­¢åç¦»ç‚¹
    memory.skeleton.add_anchor("æ ¸å¿ƒåŸåˆ™1ï¼šä¿æŒç®€æ´")
    memory.skeleton.add_anchor("æ ¸å¿ƒåŸåˆ™2ï¼šé¿å…è¿‡åº¦æŠ½è±¡")
    memory.skeleton.add_no_drift_point("ç¦æ­¢æ”¹å˜ RoPE çš„åŸºæœ¬å®šä¹‰")
    memory.skeleton.set_goal("å®ç° 10M tokens é•¿ä¸Šä¸‹æ–‡æ”¯æŒ")

    # ç¼–è¯‘éª¨æ¶å¡ï¼ˆ200-400 tokensï¼‰
    skeleton_text = memory.skeleton.compile()

    print(f"âœ… ç»†èŠ‚ä»“å­˜å‚¨: {len(memory.detail_store.verbatim)} æ¡")
    print(f"âœ… éª¨æ¶å¡ç´¢å¼•: {len(memory.skeleton.index)} æ¡")
    print(f"âœ… æ ¸å¿ƒé”šç‚¹: {len(memory.skeleton.anchors)} æ¡")
    print(f"âœ… ç¦æ­¢åç¦»ç‚¹: {len(memory.skeleton.no_drift_points)} æ¡")

    print(f"\nğŸ“‹ éª¨æ¶å¡ç¼–è¯‘ç»“æœï¼ˆå‰ 300 å­—ç¬¦ï¼‰:\n{skeleton_text[:300]}...")

    return memory


def test_context_composition():
    """æµ‹è¯•ä¸Šä¸‹æ–‡ç»„åˆï¼ˆè®°å¿†æ³¨å…¥ï¼‰"""
    print("\n" + "="*70)
    print("æµ‹è¯• 6: ä¸Šä¸‹æ–‡ç»„åˆï¼ˆéª¨æ¶å¡ + ç»†èŠ‚æ£€ç´¢ï¼‰")
    print("="*70)

    from apt_model.memory.hierarchical_memory import create_hierarchical_memory

    memory = create_hierarchical_memory()

    # è®¾ç½®å®Œæ•´åœºæ™¯
    text = """
ã€å°å­˜Â·åŸæ–‡ã€‘DEF:YaRN:v1: YaRN (Yet another RoPE extensioN) æ˜¯åˆ†ç»´åº¦ç¼©æ”¾çš„ RoPE å˜ä½“ï¼Œæ”¯æŒ 128K ä¸Šä¸‹æ–‡ã€‚

ã€å°å­˜Â·å­—æ®µã€‘PARAM:YaRN:v1: {
    "scale_factor": 4.0,
    "beta_fast": 32,
    "beta_slow": 1,
    "max_position_embeddings": 128000
}

ã€å°å­˜Â·æ‘˜è¦ã€‘NARR:YaRNUsage:v1: YaRN è¢« Qwenã€DeepSeekã€GPT-OSS ç­‰ä¸»æµæ¨¡å‹é‡‡ç”¨ã€‚
"""

    memory.process_anchor_directives(text)
    memory.skeleton.add_anchor("ä½¿ç”¨ YaRN è¿›è¡Œä¸Šä¸‹æ–‡æ‰©å±•")
    memory.skeleton.set_goal("é›†æˆ YaRN åˆ° APT-Transformer")

    # ç»„åˆä¸Šä¸‹æ–‡
    context = memory.compose_context(
        current_message="å¦‚ä½•åœ¨æ¨¡å‹ä¸­ä½¿ç”¨ YaRNï¼Ÿ",
        include_skeleton=True,
        retrieve_details=True,
        validate_consistency=True
    )

    print(f"âœ… ä¸Šä¸‹æ–‡ç»„åˆå®Œæˆ")
    print(f"ğŸ“¦ éª¨æ¶å¡é•¿åº¦: {len(context['skeleton_card'])} å­—ç¬¦")
    print(f"ğŸ” æ£€ç´¢åˆ°ç»†èŠ‚: {len(context['detail_entries'])} æ¡")
    print(f"âœ”ï¸  éªŒè¯ç»“æœ: valid={context['validation']['valid']}")

    print(f"\nğŸ’¬ å®Œæ•´ä¸Šä¸‹æ–‡ï¼ˆå‰ 500 å­—ç¬¦ï¼‰:\n{context['full_context'][:500]}...")

    return memory, context


def test_unified_composer():
    """æµ‹è¯•ç»Ÿä¸€ç»„åˆå™¨ï¼ˆä¸¤ç§ç³»ç»Ÿé›†æˆï¼‰"""
    print("\n" + "="*70)
    print("æµ‹è¯• 7: ç»Ÿä¸€ç»„åˆå™¨ï¼ˆåŸºç¡€ + åˆ†å±‚ï¼‰")
    print("="*70)

    from apt_model.memory.context_composer import create_hierarchical_composer

    composer = create_hierarchical_composer()

    # 1. ä½¿ç”¨åŸºç¡€ç³»ç»Ÿï¼ˆChatGPT-styleï¼‰
    composer.basic.save_memory("ç”¨æˆ·æ˜¯ AI ç ”ç©¶å‘˜", category="general", importance=0.9)
    composer.basic.add_message("user", "å¸®æˆ‘å®ç° YaRN")

    # 2. ä½¿ç”¨åˆ†å±‚ç³»ç»Ÿï¼ˆé”šç‚¹æŒ‡ä»¤ï¼‰
    text = """
ã€å°å­˜Â·åŸæ–‡ã€‘DEF:iRoPE:v1: iRoPE æ˜¯äº¤é”™æ—‹è½¬ä½ç½®ç¼–ç ï¼Œæ”¯æŒ 10M tokensã€‚
"""
    composer.hierarchical.process_anchor_directives(text)

    # 3. ç»Ÿä¸€ç»„åˆ
    unified_context = composer.compose_unified_context(
        current_message="ç°åœ¨æŠŠ iRoPE é›†æˆåˆ°æ¨¡å‹ä¸­",
        use_basic=True,
        use_hierarchical=True,
        validate=True
    )

    print(f"âœ… ç»Ÿä¸€ä¸Šä¸‹æ–‡ç»„åˆå®Œæˆ")
    print(f"ğŸ“¦ åŸºç¡€è®°å¿†: {len(composer.basic.saved_memories)} æ¡")
    print(f"ğŸ“¦ åˆ†å±‚è®°å¿†ï¼ˆAæ¡£ï¼‰: {len(composer.hierarchical.detail_store.verbatim)} æ¡")

    print(f"\nğŸ’¬ ç»Ÿä¸€ä¸Šä¸‹æ–‡ï¼ˆå‰ 600 å­—ç¬¦ï¼‰:\n{unified_context['full_context'][:600]}...")

    return composer


def test_persistence():
    """æµ‹è¯•æŒä¹…åŒ–ï¼ˆä¿å­˜/åŠ è½½ï¼‰"""
    print("\n" + "="*70)
    print("æµ‹è¯• 8: æŒä¹…åŒ–ï¼ˆJSON ä¿å­˜/åŠ è½½ï¼‰")
    print("="*70)

    from apt_model.memory.hierarchical_memory import create_hierarchical_memory

    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
    with tempfile.NamedTemporaryFile(mode='w', suffix='.json', delete=False) as f:
        temp_path = f.name

    try:
        # åˆ›å»ºå¹¶ä¿å­˜
        memory1 = create_hierarchical_memory()
        memory1.detail_store.add_verbatim(
            key="DEF:Test:v1",
            content="æµ‹è¯•å†…å®¹",
            version="v1"
        )
        memory1.skeleton.add_anchor("æµ‹è¯•é”šç‚¹")

        memory1.save_to_file(temp_path)
        print(f"âœ… ä¿å­˜åˆ°: {temp_path}")

        file_size = os.path.getsize(temp_path)
        print(f"ğŸ“ æ–‡ä»¶å¤§å°: {file_size} bytes")

        # åŠ è½½å¹¶éªŒè¯
        memory2 = create_hierarchical_memory()
        memory2.load_from_file(temp_path)

        assert len(memory2.detail_store.verbatim) == 1
        assert len(memory2.skeleton.anchors) == 1
        print(f"âœ… åŠ è½½éªŒè¯é€šè¿‡")

    finally:
        if os.path.exists(temp_path):
            os.remove(temp_path)


# ==================== æµ‹è¯•åŠ¨æ€ç¼–ç å™¨ ====================

def test_dynamic_embeddings():
    """æµ‹è¯•åŠ¨æ€ç¼–ç å™¨æ‰©å……"""
    print("\n" + "="*70)
    print("æµ‹è¯• 9: åŠ¨æ€ç¼–ç å™¨æ‰©å……")
    print("="*70)

    try:
        import torch
        from apt_model.modeling.embeddings import TokenEmbedding, ImageEmbedding

        # 1. æµ‹è¯• TokenEmbedding åŠ¨æ€æ‰©å……
        print("\n[TokenEmbedding] åŠ¨æ€è¯è¡¨æ‰©å……æµ‹è¯•:")
        token_embed = TokenEmbedding(vocab_size=1000, embedding_dim=128, enable_dynamic_expansion=True)

        print(f"  åˆå§‹è¯è¡¨å¤§å°: {token_embed.current_vocab_size}")

        # æ¨¡æ‹Ÿé‡åˆ° OOV token
        tokens = torch.tensor([[1, 2, 3, 1500]])  # 1500 è¶…å‡ºåˆå§‹è¯è¡¨
        output = token_embed(tokens)

        print(f"  æ‰©å……åè¯è¡¨å¤§å°: {token_embed.current_vocab_size}")
        print(f"  è¾“å‡ºå½¢çŠ¶: {output.shape}")
        assert token_embed.current_vocab_size >= 1500, "è¯è¡¨æœªæ‰©å……"
        print(f"  âœ… è¯è¡¨åŠ¨æ€æ‰©å……æˆåŠŸ")

        # 2. æµ‹è¯• ImageEmbedding åŠ¨æ€åˆ†è¾¨ç‡
        print("\n[ImageEmbedding] åŠ¨æ€åˆ†è¾¨ç‡æµ‹è¯•:")
        image_embed = ImageEmbedding(d_model=128, image_size=224, patch_size=16, enable_dynamic_resolution=True)

        print(f"  åˆå§‹ patch æ•°é‡: {image_embed.num_patches}")

        # æ¨¡æ‹Ÿä¸åŒåˆ†è¾¨ç‡å›¾åƒ
        images_224 = torch.randn(2, 3, 224, 224)
        images_448 = torch.randn(2, 3, 448, 448)  # 2å€åˆ†è¾¨ç‡

        output_224 = image_embed(images_224)
        output_448 = image_embed(images_448)

        print(f"  224x224 è¾“å‡º: {output_224.shape}")
        print(f"  448x448 è¾“å‡º: {output_448.shape}")
        assert output_224.shape[1] != output_448.shape[1], "æœªå¤„ç†ä¸åŒåˆ†è¾¨ç‡"
        print(f"  âœ… åŠ¨æ€åˆ†è¾¨ç‡æ”¯æŒæˆåŠŸ")

    except ImportError:
        print("âš ï¸  PyTorch æœªå®‰è£…ï¼Œè·³è¿‡ç¼–ç å™¨æµ‹è¯•")


# ==================== ä¸»æµ‹è¯•æµç¨‹ ====================

def run_all_tests():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("\n" + "="*70)
    print("ğŸš€ åˆ†å±‚è®°å¿†ç³»ç»Ÿ + åŠ¨æ€ç¼–ç å™¨æµ‹è¯•")
    print("="*70)
    print("åŸºäºæœ€ä½³å®è·µ 2026:")
    print("  â€¢ ç»†èŠ‚ä¸é æ‘˜è¦ä¿å­˜ï¼Œè€Œæ˜¯é æ£€ç´¢å–åŸæ–‡")
    print("  â€¢ åˆ†å±‚è®°å¿†ï¼šAæ¡£ï¼ˆåŸæ–‡ï¼‰ã€Bæ¡£ï¼ˆå­—æ®µï¼‰ã€Cæ¡£ï¼ˆæ‘˜è¦ï¼‰")
    print("  â€¢ é”šç‚¹æŒ‡ä»¤ï¼šã€å°å­˜Â·åŸæ–‡ã€‘ã€ã€å°å­˜Â·å­—æ®µã€‘ã€ã€å°å­˜Â·æ‘˜è¦ã€‘")
    print("  â€¢ ä¸¤å±‚å­˜å‚¨ï¼šéª¨æ¶å¡ï¼ˆéšæ—¶æ³¨å…¥ï¼‰+ ç»†èŠ‚ä»“ï¼ˆæŒ‰éœ€æ£€ç´¢ï¼‰")
    print("  â€¢ é˜²æ¼‚ç§»ï¼šç‰ˆæœ¬åŒ– + ä¸€è‡´æ€§æ ¡éªŒ")
    print("="*70)

    try:
        # åˆ†å±‚è®°å¿†æµ‹è¯•
        test_abc_tier_storage()
        test_anchor_directives()
        test_versioning_and_retrieval()
        test_anti_drift_validation()
        test_skeleton_and_detail_layers()
        test_context_composition()
        test_unified_composer()
        test_persistence()

        # åŠ¨æ€ç¼–ç å™¨æµ‹è¯•
        test_dynamic_embeddings()

        # æ€»ç»“
        print("\n" + "="*70)
        print("âœ… æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
        print("="*70)
        print("\nğŸ“Š ç³»ç»Ÿç‰¹æ€§:")
        print("  âœ… A/B/C æ¡£åˆ†å±‚å­˜å‚¨")
        print("  âœ… é”šç‚¹æŒ‡ä»¤è§£æï¼ˆã€å°å­˜Â·åŸæ–‡ã€‘ç­‰ï¼‰")
        print("  âœ… Key æ§æ£€ç´¢ + ç‰ˆæœ¬åŒ–")
        print("  âœ… é˜²æ¼‚ç§»éªŒè¯ï¼ˆå“ˆå¸Œ + ä¸€è‡´æ€§ï¼‰")
        print("  âœ… ä¸¤å±‚å­˜å‚¨ï¼ˆéª¨æ¶å¡ + ç»†èŠ‚ä»“ï¼‰")
        print("  âœ… ç»Ÿä¸€ç»„åˆå™¨ï¼ˆåŸºç¡€ + åˆ†å±‚ï¼‰")
        print("  âœ… åŠ¨æ€ç¼–ç å™¨ï¼ˆè¯è¡¨ + åˆ†è¾¨ç‡ï¼‰")
        print("\nğŸ¯ æ ¸å¿ƒä¼˜åŠ¿:")
        print("  â€¢ ç»†èŠ‚æ°¸ä¸ä¸¢å¤±ï¼ˆåŸæ–‡æ£€ç´¢ï¼‰")
        print("  â€¢ ä¸Šä¸‹æ–‡é«˜æ•ˆï¼ˆéª¨æ¶å¡ 200-400 tokensï¼‰")
        print("  â€¢ é˜²æ­¢æ¼‚ç§»ï¼ˆç‰ˆæœ¬åŒ–æ ¡éªŒï¼‰")
        print("  â€¢ åŠ¨æ€æ‰©å……ï¼ˆç¼–ç å™¨è‡ªé€‚åº”ï¼‰")
        print("\nğŸ“š å‚è€ƒæ–‡æ¡£:")
        print("  â€¢ docs/MEMORY_SYSTEM_GUIDE.md")
        print("  â€¢ apt_model/memory/hierarchical_memory.py")
        print("  â€¢ apt_model/modeling/embeddings.py")
        print("="*70)

        return True

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


if __name__ == "__main__":
    success = run_all_tests()
    sys.exit(0 if success else 1)
