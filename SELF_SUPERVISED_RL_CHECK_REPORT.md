# APT-Transformer è‡ªç›‘ç£å­¦ä¹ ä¸å¼ºåŒ–å­¦ä¹ æ£€æŸ¥æŠ¥å‘Š

## æ‰§è¡Œæ—¶é—´
2025-12-02

## æ£€æŸ¥èŒƒå›´
- è‡ªç›‘ç£å­¦ä¹  (Self-Supervised Learning)
- å¼ºåŒ–å­¦ä¹  (Reinforcement Learning)
- é¢„è®­ç»ƒæ–¹æ³• (Pretraining Methods)

---

## ğŸ” æ£€æŸ¥ç»“æœæ€»ç»“

### âœ… å‘ç°çš„å†…å®¹

#### 1. **å¼ºåŒ–å­¦ä¹  (Reinforcement Learning)**

**å‘ç°ä½ç½®**: `apt_model/console/plugins/grpo_plugin.py`

**å†…å®¹**: GRPO (Group Relative Policy Optimization) æ’ä»¶

**è¯¦ç»†ä¿¡æ¯**:
- **ç®—æ³•**: Group Relative Policy Optimization
- **åŠŸèƒ½**:
  - è®¡ç®—ç»„å†…ç›¸å¯¹ä¼˜åŠ¿ (group-relative advantages)
  - åŸºäºç»„æ¯”è¾ƒæ›´æ–°ç­–ç•¥ (policy updates based on group comparisons)
  - è¿½è¸ªGRPOç‰¹å®šæŒ‡æ ‡ (group variance, relative rewards)
- **å®ç°ç»†èŠ‚**:
  - é»˜è®¤ç»„å¤§å°: 4
  - ä¼˜åŠ¿ç¼“å†²åŒº (advantage buffer)
  - ç­–ç•¥æ›´æ–°è®¡æ•°å™¨
  - ç»„å†…æ–¹å·®è®¡ç®—
  - ç›¸å¯¹å¥–åŠ±å‡å€¼
- **é›†æˆæ–¹å¼**:
  - æ’ä»¶ç³»ç»Ÿé›†æˆ
  - äº‹ä»¶é©±åŠ¨: on_batch_end, on_step_end, on_epoch_end
  - ä¼˜å…ˆçº§: 380 (Training tier)
  - èƒ½åŠ›: write_metrics, read_state, write_state
- **å†²çªæ£€æµ‹**:
  - ä¸ RLHF æ’ä»¶å†²çª
  - ä¸ DPO æ’ä»¶å†²çª
- **èµ„æºä½¿ç”¨**:
  - CPU: 15ms per call
  - GPU: 5ms per call
  - Memory: 0.5MB

**ä»£ç ç‰‡æ®µ**:
```python
class GRPOPlugin(PluginBase):
    """
    GRPO Plugin
    Implements Group Relative Policy Optimization for RL-based training.
    """

    def on_batch_end(self, context: Dict[str, Any]):
        # è·å– batch å¥–åŠ±
        batch_rewards = data.get('rewards', [])

        # è®¡ç®—ç»„å†…ç›¸å¯¹ä¼˜åŠ¿
        if len(batch_rewards) >= self.group_size:
            group_rewards = batch_rewards[-self.group_size:]
            mean_reward = sum(group_rewards) / len(group_rewards)
            advantages = [r - mean_reward for r in group_rewards]

            # è®¡ç®—ç»„å†…æ–¹å·®
            variance = sum((r - mean_reward) ** 2 for r in group_rewards) / len(group_rewards)
```

**çŠ¶æ€**: âœ… **å·²å®ç°ä¸”å¯ç”¨**

---

#### 2. **é¢„è®­ç»ƒç›¸å…³ (Pretraining Related)**

**å‘ç°ä½ç½®**:
- `apt_model/modeling/apt_model.py`
- `apt_model/modeling/chinese_tokenizer_integration.py`
- `apt_model/data/hlbd/hlbd_adapter.py`

**å†…å®¹**: é¢„è®­ç»ƒæ¨¡å‹åŠ è½½å’Œä¿å­˜æ–¹æ³•

**å®ç°æ–¹æ³•**:
```python
# APTConfig ç±»ä¸­
def save_pretrained(self, save_directory):
    """ä¿å­˜é…ç½®åˆ°æŒ‡å®šç›®å½•"""

@classmethod
def from_pretrained(cls, model_path):
    """ä»é¢„è®­ç»ƒç›®å½•åŠ è½½é…ç½®"""
```

**åŠŸèƒ½**:
- ä¿å­˜æ¨¡å‹é…ç½®
- ä»é¢„è®­ç»ƒç›®å½•åŠ è½½é…ç½®
- å…¼å®¹ HuggingFace çš„ pretrained æ¥å£

**çŠ¶æ€**: âœ… **å·²å®ç° (åŸºç¡€è®¾æ–½)**

---

#### 3. **Masked Language Model ç›¸å…³æœç´¢**

**å‘ç°ä½ç½®**:
- `apt_model/runtime/decoder/routing.py`
- `apt_model/runtime/decoder/halting.py`
- `apt_model/runtime/decoder/reasoning_controller.py`
- `apt_model/modeling/apt_model.py`
- `apt_model/training/trainer.py`

**å†…å®¹**: è¿™äº›æ–‡ä»¶ä¸­æåˆ°äº† "masked" å…³é”®è¯ï¼Œä½†ä¸»è¦ç”¨äº:
- æ³¨æ„åŠ›æ©ç  (attention masking)
- åºåˆ—æ©ç  (sequence masking)
- **ä¸æ˜¯**ä¼ ç»Ÿçš„ Masked Language Modeling (MLM) é¢„è®­ç»ƒ

**çŠ¶æ€**: âš ï¸ **æœªå®ç°ä¸“é—¨çš„MLMé¢„è®­ç»ƒ**

---

### âŒ æœªå‘ç°çš„å†…å®¹

#### 1. **è‡ªç›‘ç£å­¦ä¹ ä¸“é—¨å®ç°**
- âŒ æ— å¯¹æ¯”å­¦ä¹  (Contrastive Learning) å®ç°
- âŒ æ—  SimCLR, MoCo, BYOL ç­‰æ–¹æ³•
- âŒ æ— ä¸“é—¨çš„è‡ªç›‘ç£é¢„è®­ç»ƒè„šæœ¬

#### 2. **ä¼ ç»Ÿé¢„è®­ç»ƒæ–¹æ³•**
- âŒ æ—  Masked Language Modeling (MLM)
- âŒ æ—  Next Sentence Prediction (NSP)
- âŒ æ—  Causal Language Modeling (CLM) ä¸“é—¨å®ç°

#### 3. **å…¶ä»–å¼ºåŒ–å­¦ä¹ æ–¹æ³•**
- âŒ æ—  RLHF (Reinforcement Learning from Human Feedback) å®ç°
  - è™½ç„¶åœ¨ GRPO æ’ä»¶ä¸­è¢«æåˆ°ä¸ºå†²çªé¡¹
  - ä½†å®é™…æ–‡ä»¶ä¸å­˜åœ¨
- âŒ æ—  DPO (Direct Preference Optimization) å®ç°
- âŒ æ—  PPO (Proximal Policy Optimization)
- âŒ æ—  Q-Learning / DQN
- âŒ æ—  Actor-Critic æ–¹æ³•

---

## ğŸ“Š è¯¦ç»†åˆ†æ

### å¼ºåŒ–å­¦ä¹ å®ç°è¯„ä¼°

**GRPO æ’ä»¶åˆ†æ**:

**ä¼˜ç‚¹**:
- âœ… æ’ä»¶åŒ–è®¾è®¡ï¼Œæ˜“äºé›†æˆå’Œç§»é™¤
- âœ… å®Œæ•´çš„äº‹ä»¶é’©å­ç³»ç»Ÿ
- âœ… èµ„æºä½¿ç”¨è¿½è¸ª
- âœ… å†²çªæ£€æµ‹æœºåˆ¶
- âœ… ç»„ç›¸å¯¹ä¼˜åŒ–ï¼Œé€‚åˆå¤šæ ·æœ¬å¯¹æ¯”

**å±€é™æ€§**:
- âš ï¸ å®ç°ç›¸å¯¹ç®€å•ï¼Œä¸»è¦æ˜¯æ¡†æ¶æ€§ä»£ç 
- âš ï¸ ç¼ºå°‘å®Œæ•´çš„å¥–åŠ±å‡½æ•°å®šä¹‰
- âš ï¸ ç­–ç•¥æ›´æ–°æ˜¯"æ¨¡æ‹Ÿ"çš„ (commented as "æ¨¡æ‹Ÿç­–ç•¥æ›´æ–°")
- âš ï¸ æœªå®é™…å®ç°ç­–ç•¥æ¢¯åº¦è®¡ç®—

**ä»£ç è¯æ®**:
```python
# line 151-152 in grpo_plugin.py
# æ¨¡æ‹Ÿç­–ç•¥æ›´æ–°
self.metrics['policy_updates'] += 1
```

**ç»“è®º**: è¿™æ˜¯ä¸€ä¸ª**æ’ä»¶æ¡†æ¶**è€Œéå®Œæ•´çš„RLå®ç°ï¼Œéœ€è¦è¿›ä¸€æ­¥å¼€å‘æ‰èƒ½å®é™…åº”ç”¨ã€‚

---

### é¢„è®­ç»ƒåŸºç¡€è®¾æ–½è¯„ä¼°

**å‘ç°çš„é¢„è®­ç»ƒç›¸å…³åŠŸèƒ½**:

1. **é…ç½®ä¿å­˜/åŠ è½½** (APTConfig)
   - `save_pretrained()`
   - `from_pretrained()`

2. **æ¨¡å‹ä¿å­˜/åŠ è½½** (Trainer)
   - checkpoint ç³»ç»Ÿ
   - æ¨¡å‹çŠ¶æ€ä¿å­˜

**ç¼ºå¤±çš„é¢„è®­ç»ƒåŠŸèƒ½**:
- âŒ æ— å¤§è§„æ¨¡é¢„è®­ç»ƒè„šæœ¬
- âŒ æ— é¢„è®­ç»ƒä»»åŠ¡å®šä¹‰ (MLM/CLM)
- âŒ æ— é¢„è®­ç»ƒæ•°æ®å¤„ç†æµç¨‹
- âŒ æ— é¢„è®­ç»ƒè¯„ä¼°æŒ‡æ ‡

---

## ğŸ¯ é¡¹ç›®ç°çŠ¶æ€»ç»“

### å·²æœ‰åŠŸèƒ½

| ç±»åˆ« | åŠŸèƒ½ | å®ç°çŠ¶æ€ | å®Œæ•´åº¦ |
|------|------|---------|--------|
| **å¼ºåŒ–å­¦ä¹ ** | GRPOæ’ä»¶ | âœ… æ¡†æ¶å®ç° | ğŸŸ¡ 30% |
| **é¢„è®­ç»ƒ** | é…ç½®åŠ è½½/ä¿å­˜ | âœ… å·²å®ç° | ğŸŸ¢ 80% |
| **é¢„è®­ç»ƒ** | æ¨¡å‹checkpoint | âœ… å·²å®ç° | ğŸŸ¢ 90% |
| **è‡ªç›‘ç£** | - | âŒ æœªå®ç° | ğŸ”´ 0% |

### åŠŸèƒ½ç¼ºå£

**é«˜ä¼˜å…ˆçº§ç¼ºå¤±**:
1. âŒ å®Œæ•´çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒå¾ªç¯
2. âŒ å¥–åŠ±æ¨¡å‹ (Reward Model)
3. âŒ ç­–ç•¥æ¢¯åº¦å®ç°
4. âŒ è‡ªç›‘ç£é¢„è®­ç»ƒæ–¹æ³•

**ä¸­ä¼˜å…ˆçº§ç¼ºå¤±**:
1. âŒ RLHF å®Œæ•´å®ç°
2. âŒ DPO å®ç°
3. âŒ å¯¹æ¯”å­¦ä¹ æ–¹æ³•
4. âŒ MLMé¢„è®­ç»ƒ

---

## ğŸ’¡ å»ºè®®

### å¦‚æœéœ€è¦å®ç°è‡ªç›‘ç£å­¦ä¹ 

**å»ºè®®1: å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒ**
```python
# å¯ä»¥åˆ›å»º: apt_model/training/contrastive_pretrain.py
class ContrastivePretrainer:
    def __init__(self, model, temperature=0.07):
        self.model = model
        self.temperature = temperature

    def contrastive_loss(self, z_i, z_j):
        # SimCLR é£æ ¼çš„å¯¹æ¯”æŸå¤±
        pass
```

**å»ºè®®2: Masked Language Modeling**
```python
# å¯ä»¥åˆ›å»º: apt_model/training/mlm_pretrain.py
class MLMPretrainer:
    def __init__(self, model, mask_ratio=0.15):
        self.model = model
        self.mask_ratio = mask_ratio

    def mask_tokens(self, input_ids):
        # BERT é£æ ¼çš„token masking
        pass
```

### å¦‚æœéœ€è¦å®Œå–„å¼ºåŒ–å­¦ä¹ 

**å»ºè®®1: å®Œå–„GRPOå®ç°**
```python
# åœ¨ grpo_plugin.py ä¸­æ·»åŠ :
class GRPOPlugin(PluginBase):
    def compute_policy_gradient(self, advantages, log_probs):
        """å®é™…çš„ç­–ç•¥æ¢¯åº¦è®¡ç®—"""
        policy_loss = -(log_probs * advantages).mean()
        return policy_loss

    def update_policy(self, policy_loss):
        """æ‰§è¡Œç­–ç•¥æ›´æ–°"""
        policy_loss.backward()
        self.optimizer.step()
```

**å»ºè®®2: å®ç°å¥–åŠ±æ¨¡å‹**
```python
# åˆ›å»º: apt_model/rl/reward_model.py
class RewardModel(nn.Module):
    def __init__(self, base_model):
        super().__init__()
        self.base_model = base_model
        self.value_head = nn.Linear(hidden_size, 1)

    def forward(self, input_ids, attention_mask):
        outputs = self.base_model(input_ids, attention_mask)
        rewards = self.value_head(outputs.last_hidden_state)
        return rewards
```

**å»ºè®®3: å®ç°RLHF**
```python
# åˆ›å»º: apt_model/rl/rlhf_trainer.py
class RLHFTrainer:
    def __init__(self, policy_model, reward_model):
        self.policy = policy_model
        self.reward_model = reward_model

    def compute_rewards(self, responses):
        """ä½¿ç”¨reward modelè®¡ç®—å¥–åŠ±"""
        pass

    def ppo_update(self, states, actions, rewards):
        """PPOé£æ ¼çš„ç­–ç•¥æ›´æ–°"""
        pass
```

---

## ğŸ“ å»ºè®®çš„æ–‡ä»¶ç»“æ„

å¦‚æœè¦å®Œå–„è¿™äº›åŠŸèƒ½ï¼Œå»ºè®®æ·»åŠ :

```
apt_model/
â”œâ”€â”€ rl/                          # æ–°å¢: å¼ºåŒ–å­¦ä¹ æ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ reward_model.py          # å¥–åŠ±æ¨¡å‹
â”‚   â”œâ”€â”€ rlhf_trainer.py          # RLHFè®­ç»ƒå™¨
â”‚   â”œâ”€â”€ dpo_trainer.py           # DPOè®­ç»ƒå™¨
â”‚   â”œâ”€â”€ ppo_trainer.py           # PPOè®­ç»ƒå™¨
â”‚   â””â”€â”€ grpo_trainer.py          # GRPOå®Œæ•´å®ç°
â”‚
â”œâ”€â”€ pretraining/                 # æ–°å¢: é¢„è®­ç»ƒæ¨¡å—
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ mlm_pretrain.py          # MLMé¢„è®­ç»ƒ
â”‚   â”œâ”€â”€ clm_pretrain.py          # CLMé¢„è®­ç»ƒ
â”‚   â”œâ”€â”€ contrastive_pretrain.py  # å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒ
â”‚   â””â”€â”€ pretrain_data.py         # é¢„è®­ç»ƒæ•°æ®å¤„ç†
â”‚
â””â”€â”€ console/plugins/
    â”œâ”€â”€ grpo_plugin.py           # ç°æœ‰ (éœ€è¦å®Œå–„)
    â”œâ”€â”€ rlhf_plugin.py           # å»ºè®®æ–°å¢
    â””â”€â”€ dpo_plugin.py            # å»ºè®®æ–°å¢
```

---

## ğŸ”— ç›¸å…³æ–‡ä»¶æ¸…å•

### å·²å­˜åœ¨çš„ç›¸å…³æ–‡ä»¶:
1. `apt_model/console/plugins/grpo_plugin.py` - GRPOæ’ä»¶ (184è¡Œ)
2. `apt_model/modeling/apt_model.py` - ä¸»æ¨¡å‹ (save/load pretrained)
3. `apt_model/training/trainer.py` - è®­ç»ƒå™¨
4. `apt_model/training/finetuner.py` - å¾®è°ƒå™¨
5. `apt_model/training/train_reasoning.py` - æ¨ç†è®­ç»ƒ

### éœ€è¦åˆ›å»ºçš„æ–‡ä»¶ (å»ºè®®):
1. `apt_model/rl/reward_model.py`
2. `apt_model/rl/rlhf_trainer.py`
3. `apt_model/pretraining/mlm_pretrain.py`
4. `apt_model/pretraining/contrastive_pretrain.py`

---

## âœ… æœ€ç»ˆç»“è®º

**å½“å‰çŠ¶æ€**:
- âœ… æœ‰å¼ºåŒ–å­¦ä¹ çš„**æ’ä»¶æ¡†æ¶** (GRPO)
- âœ… æœ‰é¢„è®­ç»ƒçš„**åŸºç¡€è®¾æ–½** (save/load)
- âŒ æ— å®Œæ•´çš„**å¼ºåŒ–å­¦ä¹ è®­ç»ƒå®ç°**
- âŒ æ— ä¸“é—¨çš„**è‡ªç›‘ç£å­¦ä¹ å®ç°**

**å»ºè®®ä¼˜å…ˆçº§**:
1. ğŸ”´ **é«˜ä¼˜å…ˆçº§**: å®Œå–„GRPOæ’ä»¶ï¼Œå®ç°å®é™…çš„ç­–ç•¥æ¢¯åº¦å’Œç­–ç•¥æ›´æ–°
2. ğŸŸ¡ **ä¸­ä¼˜å…ˆçº§**: å®ç°å¥–åŠ±æ¨¡å‹å’ŒRLHFæ¡†æ¶
3. ğŸŸ¢ **ä½ä¼˜å…ˆçº§**: æ·»åŠ MLM/å¯¹æ¯”å­¦ä¹ ç­‰è‡ªç›‘ç£é¢„è®­ç»ƒæ–¹æ³•

**ç°æœ‰GRPOæ’ä»¶å¯ä»¥ä½œä¸ºèµ·ç‚¹**ï¼Œä½†éœ€è¦å¤§é‡å¼€å‘æ‰èƒ½ç”¨äºå®é™…çš„å¼ºåŒ–å­¦ä¹ è®­ç»ƒã€‚

---

**æŠ¥å‘Šç”Ÿæˆæ—¶é—´**: 2025-12-02
**æ£€æŸ¥è€…**: Claude (APT-Transformer Module Integration)
