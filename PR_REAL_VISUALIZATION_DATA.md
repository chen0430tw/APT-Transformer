# Pull Request: Replace Simulated Visualization Data with Real Training Metrics

## é—®é¢˜

å¯è§†åŒ–å·¥å…·æ˜¾ç¤ºçš„æ¢¯åº¦èŒƒæ•°å’Œå­¦ä¹ ç‡æ˜¯**è™šç©ºæ•°æ®**ï¼ˆå…¬å¼ç¼–é€ çš„ï¼‰ï¼š
- æ¢¯åº¦èŒƒæ•°ï¼š`5.0 / (epoch + 1) + random()`
- å­¦ä¹ ç‡ï¼šä½™å¼¦é€€ç«å…¬å¼
- è®­ç»ƒè„šæœ¬ä»æœªä¿å­˜è¿™äº›çœŸå®æŒ‡æ ‡åˆ° `experiment_report.json`

## ä¿®å¤å†…å®¹

### è®­ç»ƒè„šæœ¬ (train_hlbd_playground.py)

âœ… **è®°å½•çœŸå®æ¢¯åº¦èŒƒæ•°**
- åœ¨ `clip_grad_norm_()` æ—¶è®°å½•è¿”å›å€¼
- æ¯ä¸ªepochå¹³å‡åä¿å­˜åˆ° `self.grad_norms`

âœ… **è®°å½•çœŸå®å­¦ä¹ ç‡**
- åœ¨epochç»“æŸæ—¶è¯»å– `scheduler.get_last_lr()[0]`
- ä¿å­˜åˆ° `self.learning_rates`

âœ… **ä¿å­˜åˆ°JSON**
```python
report = {
    'control_losses': self.losses,
    'grad_norms': self.grad_norms,        # æ–°å¢ï¼šçœŸå®æ¢¯åº¦èŒƒæ•°
    'learning_rates': self.learning_rates, # æ–°å¢ï¼šçœŸå®å­¦ä¹ ç‡
    'current_epoch': len(self.losses),
    'total_batches': len(self.batch_losses),
    'dataset_stats': self.dataset_stats,
    'timestamp': time.time()
}
```

âœ… **Checkpointå®Œæ•´æ”¯æŒ**
- `save_checkpoint()` ä¿å­˜ grad_norms å’Œ learning_rates
- æ–°å¢ `load_checkpoint()` æ–¹æ³•æ¢å¤æ‰€æœ‰è®­ç»ƒçŠ¶æ€
- æ”¯æŒ `--resume checkpoint.pt` å‘½ä»¤è¡Œå‚æ•°
- æ¢å¤è®­ç»ƒæ—¶å†å²æ•°æ®å®Œæ•´ä¿ç•™

### å¯è§†åŒ–å·¥å…· (visualize_training.py)

âŒ **åˆ é™¤è™šç©ºæ•°æ®ç”Ÿæˆä»£ç **ï¼ˆ19è¡Œï¼‰
```python
# åˆ é™¤è¿™äº›å…¬å¼ç¼–é€ çš„ä»£ç 
# æ¨¡æ‹Ÿæ¢¯åº¦èŒƒæ•°æ•°æ®ï¼ˆå¦‚æœæ²¡æœ‰çš„è¯ï¼‰
if len(self.grad_norms) < len(self.epochs):
    for i in range(len(self.grad_norms), len(self.epochs)):
        grad_norm = max(0.1, 5.0 / (i + 1) + np.random.rand() * 0.5)  # âŒ è™šç©º
        self.grad_norms.append(grad_norm)

# æ¨¡æ‹Ÿå­¦ä¹ ç‡æ•°æ®ï¼ˆCosineAnnealingï¼‰
if len(self.learning_rates) < len(self.epochs):
    base_lr = 3e-4
    min_lr = 1e-5
    T_0 = 10
    for i in range(len(self.learning_rates), len(self.epochs)):
        epoch = i + 1
        cycle = epoch // T_0
        epoch_in_cycle = epoch % T_0
        lr = min_lr + (base_lr - min_lr) * (1 + np.cos(np.pi * epoch_in_cycle / T_0)) / 2  # âŒ è™šç©º
        self.learning_rates.append(lr)
```

âœ… **æ”¹ä¸ºè¯»å–çœŸå®æ•°æ®**
```python
# è¯»å–çœŸå®æ¢¯åº¦èŒƒæ•°æ•°æ®
if 'grad_norms' in data:
    for i, grad_norm in enumerate(data['grad_norms']):
        if i >= len(self.grad_norms):
            self.grad_norms.append(grad_norm)  # âœ… çœŸå®

# è¯»å–çœŸå®å­¦ä¹ ç‡æ•°æ®
if 'learning_rates' in data:
    for i, lr in enumerate(data['learning_rates']):
        if i >= len(self.learning_rates):
            self.learning_rates.append(lr)  # âœ… çœŸå®
```

## Before vs After

### Beforeï¼ˆè™šç©ºæ•°æ®ï¼‰

| æŒ‡æ ‡ | æ•°æ®æ¥æº | å¯ç”¨æ€§ |
|-----|---------|--------|
| Lossæ›²çº¿ | âœ… çœŸå®è®­ç»ƒæ•°æ® | å¯ç”¨äºè°ƒè¯• |
| æ¢¯åº¦èŒƒæ•° | âŒ å…¬å¼ç¼–é€  `5.0/(epoch+1)+random()` | **æ— æ³•ç”¨äºè°ƒè¯•** |
| å­¦ä¹ ç‡ | âŒ å…¬å¼ç¼–é€ ï¼ˆä½™å¼¦é€€ç«å‡è®¾ï¼‰ | **æ— æ³•éªŒè¯è°ƒåº¦å™¨** |

### Afterï¼ˆçœŸå®æ•°æ®ï¼‰

| æŒ‡æ ‡ | æ•°æ®æ¥æº | å¯ç”¨æ€§ |
|-----|---------|--------|
| Lossæ›²çº¿ | âœ… çœŸå®è®­ç»ƒæ•°æ® | å¯ç”¨äºè°ƒè¯• |
| æ¢¯åº¦èŒƒæ•° | âœ… `clip_grad_norm_()` è¿”å›å€¼ | **å¯æ£€æµ‹æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±** |
| å­¦ä¹ ç‡ | âœ… `scheduler.get_last_lr()[0]` | **å¯éªŒè¯è°ƒåº¦å™¨è¡Œä¸º** |

## å®é™…åº”ç”¨åœºæ™¯

### 1. è°ƒè¯•æ¢¯åº¦çˆ†ç‚¸
```python
# ç°åœ¨å¯ä»¥ä»å¯è§†åŒ–ä¸­çœ‹åˆ°çœŸå®æ¢¯åº¦èŒƒæ•°
if max(grad_norms) > 10.0:
    print("âš ï¸  æ¢¯åº¦çˆ†ç‚¸ï¼")
```

### 2. è°ƒè¯•æ¢¯åº¦æ¶ˆå¤±
```python
if min(grad_norms) < 0.01:
    print("âš ï¸  æ¢¯åº¦æ¶ˆå¤±ï¼")
```

### 3. éªŒè¯å­¦ä¹ ç‡è°ƒåº¦å™¨
```python
# ç°åœ¨å¯ä»¥éªŒè¯CosineAnnealingWarmRestartsæ˜¯å¦æŒ‰é¢„æœŸå·¥ä½œ
# åº”è¯¥çœ‹åˆ°å‘¨æœŸæ€§çš„è¡°å‡å’Œé‡å¯
```

### 4. Checkpointæ¢å¤å®Œæ•´æ€§
```bash
# æ–°åŠŸèƒ½ï¼šä»checkpointæ¢å¤æ—¶ä¿ç•™æ‰€æœ‰å†å²
python training/train_hlbd_playground.py --resume hlbd_playground/checkpoint_epoch_10.pt

# å¯è§†åŒ–ä¼šæ˜¾ç¤ºå®Œæ•´çš„å†å²æ›²çº¿ï¼ˆåŒ…æ‹¬epoch 1-10çš„çœŸå®æ•°æ®ï¼‰
```

## æµ‹è¯•æ–¹æ³•

### æµ‹è¯•1ï¼šæ–°è®­ç»ƒ
```bash
# å¯åŠ¨æ–°è®­ç»ƒ
python training/train_hlbd_playground.py --dataset data/HLBD_Hardcore_Full.json --epochs 5

# å¯åŠ¨å¯è§†åŒ–
python tools/visualize_training.py

# é¢„æœŸï¼šçœ‹åˆ°çœŸå®çš„æ¢¯åº¦èŒƒæ•°å’Œå­¦ä¹ ç‡æ›²çº¿
```

### æµ‹è¯•2ï¼šCheckpointæ¢å¤
```bash
# è®­ç»ƒ10ä¸ªepoch
python training/train_hlbd_playground.py --dataset data/HLBD_Hardcore_Full.json --epochs 10

# ä»epoch 10æ¢å¤ç»§ç»­è®­ç»ƒ
python training/train_hlbd_playground.py --resume hlbd_playground/checkpoint_epoch_10.pt --epochs 20

# é¢„æœŸï¼šgrad_normså’Œlearning_rateså†å²å®Œæ•´ï¼ˆepoch 1-20ï¼‰
```

### æµ‹è¯•3ï¼šéªŒè¯æ•°æ®çœŸå®æ€§
```bash
# æ£€æŸ¥experiment_report.json
cat hlbd_playground/experiment_report.json | jq '.grad_norms, .learning_rates'

# é¢„æœŸï¼šçœ‹åˆ°æ•°ç»„ï¼Œè€Œä¸æ˜¯ç©º
```

## ä»£ç å˜æ›´ç»Ÿè®¡

```
training/train_hlbd_playground.py  | 72 insertions(+), 4 deletions(-)
tools/visualize_training.py        | 19 insertions(+), 18 deletions(-)
```

### å…³é”®å˜æ›´ç‚¹

**train_hlbd_playground.py:417**
```python
+ self.grad_norms = []  # æ¯ä¸ªepochçš„æ¢¯åº¦èŒƒæ•°
+ self.learning_rates = []  # æ¯ä¸ªepochçš„å­¦ä¹ ç‡
```

**train_hlbd_playground.py:477**
```python
  # è®°å½•æ¢¯åº¦èŒƒæ•°ï¼ˆåœ¨è£å‰ªä¹‹å‰ï¼‰
- torch.nn.utils.clip_grad_norm_(...)
+ grad_norm = torch.nn.utils.clip_grad_norm_(...)
+ epoch_grad_norms.append(grad_norm.item())
```

**train_hlbd_playground.py:534**
```python
  # Epochç»“æŸï¼šè®°å½•å¹³å‡æ¢¯åº¦èŒƒæ•°å’Œå­¦ä¹ ç‡
+ avg_grad_norm = sum(epoch_grad_norms) / len(epoch_grad_norms)
+ current_lr = self.scheduler.get_last_lr()[0]
+ self.grad_norms.append(avg_grad_norm)
+ self.learning_rates.append(current_lr)
```

**train_hlbd_playground.py:571**
```python
  report = {
      'control_losses': self.losses,
      'batch_losses': clustered_losses,
+     'grad_norms': self.grad_norms,
+     'learning_rates': self.learning_rates,
      ...
  }
```

**train_hlbd_playground.py:596** (æ–°å¢æ–¹æ³•)
```python
+ def load_checkpoint(self, checkpoint_path: str):
+     """ä»checkpointæ¢å¤è®­ç»ƒ"""
+     checkpoint = torch.load(checkpoint_path, map_location=self.device)
+     self.model.load_state_dict(checkpoint['model_state_dict'])
+     self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
+     ...
+     self.grad_norms = checkpoint.get('grad_norms', [])
+     self.learning_rates = checkpoint.get('learning_rates', [])
+     return checkpoint.get('epoch', 0)
```

**visualize_training.py:509-527** (åˆ é™¤è™šç©ºä»£ç )
```python
- # æ¨¡æ‹Ÿæ¢¯åº¦èŒƒæ•°æ•°æ®ï¼ˆå¦‚æœæ²¡æœ‰çš„è¯ï¼‰
- if len(self.grad_norms) < len(self.epochs):
-     for i in range(len(self.grad_norms), len(self.epochs)):
-         grad_norm = max(0.1, 5.0 / (i + 1) + np.random.rand() * 0.5)
-         self.grad_norms.append(grad_norm)
-
- # æ¨¡æ‹Ÿå­¦ä¹ ç‡æ•°æ®ï¼ˆCosineAnnealingï¼‰
- if len(self.learning_rates) < len(self.epochs):
-     ...

+ # è¯»å–çœŸå®æ¢¯åº¦èŒƒæ•°æ•°æ®
+ if 'grad_norms' in data:
+     for i, grad_norm in enumerate(data['grad_norms']):
+         if i >= len(self.grad_norms):
+             self.grad_norms.append(grad_norm)
+
+ # è¯»å–çœŸå®å­¦ä¹ ç‡æ•°æ®
+ if 'learning_rates' in data:
+     for i, lr in enumerate(data['learning_rates']):
+         if i >= len(self.learning_rates):
+             self.learning_rates.append(lr)
```

## Commits

```
commit 710a13c
Author: Claude Code
Date: 2026-01-20

Replace simulated visualization data with real training metrics

- Training script now records real gradient norms during clip_grad_norm_()
- Training script now records real learning rates from scheduler
- Save grad_norms and learning_rates to experiment_report.json
- Add load_checkpoint() method to restore training history
- Update save_checkpoint() to include grad_norms and learning_rates
- Add --resume argument for checkpoint restoration
- Remove 19 lines of simulated data generation code from visualization
- Visualization now displays 100% authentic training metrics
```

## PRåˆ›å»ºé“¾æ¥

**è¯·è®¿é—®ä»¥ä¸‹é“¾æ¥åˆ›å»ºPRå¹¶åˆå¹¶åˆ°mainï¼š**

ğŸ”— https://github.com/chen0430tw/APT-Transformer/pull/new/claude/review-codebase-6PYRx

## PRæ ‡é¢˜å’Œæè¿°ï¼ˆå¤åˆ¶ç²˜è´´ï¼‰

**Title:**
```
Replace simulated visualization data with real training metrics
```

**Description:**
```
ä¿®å¤å¯è§†åŒ–å·¥å…·æ˜¾ç¤ºè™šç©ºæ•°æ®çš„é—®é¢˜ï¼Œæ”¹ä¸ºæ˜¾ç¤ºçœŸå®çš„è®­ç»ƒæŒ‡æ ‡ã€‚

## é—®é¢˜
- æ¢¯åº¦èŒƒæ•°ï¼šå…¬å¼ç¼–é€  `5.0/(epoch+1)+random()`
- å­¦ä¹ ç‡ï¼šå…¬å¼ç¼–é€ ï¼ˆä½™å¼¦é€€ç«å‡è®¾ï¼‰

## ä¿®å¤
- è®­ç»ƒè„šæœ¬è®°å½•çœŸå®æ¢¯åº¦èŒƒæ•°å’Œå­¦ä¹ ç‡
- ä¿å­˜åˆ°experiment_report.json
- å¯è§†åŒ–è¯»å–çœŸå®æ•°æ®
- æ”¯æŒcheckpointæ¢å¤å†å²

## æ•ˆæœ
âœ… å¯ç”¨äºè°ƒè¯•æ¢¯åº¦çˆ†ç‚¸/æ¶ˆå¤±
âœ… å¯éªŒè¯å­¦ä¹ ç‡è°ƒåº¦å™¨
âœ… Checkpointæ¢å¤å®Œæ•´

Fixes #è™šç©ºæ•°æ®é—®é¢˜
```

---

**Masterè¦æ±‚ï¼šä¿®å¤å¯è§†åŒ–è™šç©ºæ•°æ® + æäº¤PR âœ…**
