#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æµ‹è¯• tokenizer å¯¹ emoji çš„å¤„ç†
"""

import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

# ç›´æ¥å¯¼å…¥ ChineseTokenizerï¼Œé¿å…è§¦å‘ __init__.py ä¸­çš„ torch å¯¼å…¥
import importlib.util
spec = importlib.util.spec_from_file_location(
    "chinese_tokenizer",
    os.path.join(os.path.dirname(__file__), "apt_model/modeling/chinese_tokenizer.py")
)
chinese_tokenizer_module = importlib.util.module_from_spec(spec)
spec.loader.exec_module(chinese_tokenizer_module)
ChineseTokenizer = chinese_tokenizer_module.ChineseTokenizer

def test_emoji_tokenization():
    """æµ‹è¯• emoji æ˜¯å¦è¢«è¯†åˆ«ä¸º [UNK]"""

    print("="*70)
    print("æµ‹è¯• ChineseTokenizer å¯¹ emoji çš„å¤„ç†")
    print("="*70)

    # åˆ›å»º tokenizer å®ä¾‹
    tokenizer = ChineseTokenizer(mode="char")

    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        "ğŸŒ§ï¸",           # é›¨æ»´ emoji
        "ä½ å¥½ğŸŒ§ï¸ä¸–ç•Œ",   # ä¸­æ–‡ + emoji
        "HelloğŸŒ§ï¸World", # è‹±æ–‡ + emoji
        "ğŸ˜€",           # ç¬‘è„¸ emoji
        "ğŸ‰ğŸŠğŸˆ",       # å¤šä¸ª emoji
        "æ™®é€šæ–‡æœ¬",      # çº¯ä¸­æ–‡
        "Hello",        # çº¯è‹±æ–‡
    ]

    print("\næµ‹è¯•ç»“æœ:")
    print("-"*70)

    for text in test_cases:
        # ç¼–ç 
        ids = tokenizer.encode(text)

        # è§£ç 
        decoded = tokenizer.decode(ids)

        # æ£€æŸ¥è¯æ±‡è¡¨ä¸­çš„å­—ç¬¦
        chars_in_vocab = [c for c in text if c in tokenizer.encoder]
        chars_not_in_vocab = [c for c in text if c not in tokenizer.encoder]

        print(f"\nåŸæ–‡æœ¬: {text!r}")
        print(f"  ç¼–ç  ID: {ids}")
        print(f"  ID é•¿åº¦: {len(ids)}")
        print(f"  è§£ç ç»“æœ: {decoded!r}")
        print(f"  åœ¨è¯æ±‡è¡¨ä¸­: {chars_in_vocab}")
        print(f"  ä¸åœ¨è¯æ±‡è¡¨: {chars_not_in_vocab}")

        # æ£€æŸ¥æ˜¯å¦æœ‰æŸå¤±
        if text != decoded:
            print(f"  âš ï¸  ç¼–ç /è§£ç æœ‰æŸå¤±ï¼")
        else:
            print(f"  âœ“  ç¼–ç /è§£ç æ— æŸ")

    # æ£€æŸ¥æ˜¯å¦æœ‰ UNK token
    print("\n" + "="*70)
    print("æ£€æŸ¥è¯æ±‡è¡¨ä¸­çš„ç‰¹æ®Š token:")
    print("-"*70)

    special_tokens = ["<|pad|>", "<|endoftext|>", "<unk>", "[UNK]", "<UNK>"]
    for token in special_tokens:
        if token in tokenizer.encoder:
            print(f"  {token}: ID = {tokenizer.encoder[token]}")
        else:
            print(f"  {token}: ä¸å­˜åœ¨")

    print("\n" + "="*70)
    print("ç»“è®º:")
    print("-"*70)

    # æµ‹è¯• emoji ç¼–ç 
    emoji_ids = tokenizer.encode("ğŸŒ§ï¸")

    if len(emoji_ids) == 0:
        print("  emoji 'ğŸŒ§ï¸' è¢«ç¼–ç ä¸ºç©ºåˆ—è¡¨ï¼ˆè¢«è·³è¿‡ï¼‰")
        print("  âŒ æ²¡æœ‰ä½¿ç”¨ [UNK] token")
    else:
        print(f"  emoji 'ğŸŒ§ï¸' è¢«ç¼–ç ä¸º: {emoji_ids}")
        decoded_emoji = tokenizer.decode(emoji_ids)
        if decoded_emoji == "ğŸŒ§ï¸":
            print("  âœ“ emoji å¯ä»¥æ­£ç¡®ç¼–ç /è§£ç ")
        else:
            print(f"  è§£ç ç»“æœ: {decoded_emoji!r}")
            if "[UNK]" in decoded_emoji or "<unk>" in decoded_emoji.lower():
                print("  âœ“ ä½¿ç”¨äº† [UNK] token")
            else:
                print("  âš ï¸  æœªçŸ¥å­—ç¬¦å¤„ç†æ–¹å¼ä¸æ˜ç¡®")

    print("="*70)

if __name__ == "__main__":
    test_emoji_tokenization()
