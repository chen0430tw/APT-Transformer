#!/usr/bin/env python3
"""
HLBDæ¨¡å‹éªŒè¯è„šæœ¬
ä¸“é—¨ç”¨äºæµ‹è¯•HLBDè®­ç»ƒåçš„æ¨¡å‹æ˜¯å¦"å¬è¯"

åŠŸèƒ½:
- åŠ è½½è®­ç»ƒå¥½çš„HLBDæ¨¡å‹
- ä½¿ç”¨HLBD Hardcoreæ•°æ®é›†æµ‹è¯•
- ç”Ÿæˆè¯¦ç»†çš„å‡†ç¡®ç‡æŠ¥å‘Š
- è¯Šæ–­æ¨¡å‹æ˜¯å¦å­˜åœ¨"å·æ‡’"é—®é¢˜
"""

import sys
import os
import json
import argparse
from pathlib import Path

import torch
import torch.nn as nn

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from apt.core.modeling.apt_model import APTModel, APTModelConfiguration
from apt_model.tokenization.char_tokenizer import CharacterTokenizer


class HLBDVerifier:
    """HLBDæ¨¡å‹éªŒè¯å™¨"""

    def __init__(self, model_path: str, dataset_path: str, device: str = 'cuda'):
        """
        Args:
            model_path: æ¨¡å‹checkpointè·¯å¾„
            dataset_path: HLBDæ•°æ®é›†è·¯å¾„
            device: è®¾å¤‡
        """
        self.device = device if torch.cuda.is_available() else 'cpu'
        self.model_path = Path(model_path)
        self.dataset_path = Path(dataset_path)

        # åŠ è½½æ¨¡å‹å’Œæ•°æ®é›†
        self.model, self.tokenizer = self.load_model()
        self.dataset = self.load_dataset()

    def load_model(self):
        """åŠ è½½HLBDæ¨¡å‹"""
        print(f"ğŸ“¦ åŠ è½½æ¨¡å‹: {self.model_path}")

        if not self.model_path.exists():
            raise FileNotFoundError(f"æ¨¡å‹æ–‡ä»¶ä¸å­˜åœ¨: {self.model_path}")

        checkpoint = torch.load(self.model_path, map_location=self.device)

        # æ¢å¤é…ç½®
        config_dict = checkpoint.get('config', {})
        config = APTModelConfiguration(**config_dict)

        # åˆ›å»ºæ¨¡å‹
        model = APTModel(config).to(self.device)
        model.load_state_dict(checkpoint['model_state_dict'])
        model.eval()

        # æ¢å¤tokenizer
        tokenizer = CharacterTokenizer(vocab_size=config.vocab_size)
        tokenizer.char_to_id = checkpoint['tokenizer_char_to_id']
        tokenizer.id_to_char = checkpoint['tokenizer_id_to_char']
        tokenizer.next_id = checkpoint['tokenizer_next_id']
        tokenizer.vocab_size = checkpoint['tokenizer_vocab_size']

        # è®­ç»ƒä¿¡æ¯
        training_info = checkpoint.get('training_info', {})
        print(f"   âœ“ æ¨¡å‹å·²åŠ è½½")
        print(f"   è®­ç»ƒè½®æ•°: {training_info.get('num_epochs', 'N/A')}")
        print(f"   æœ€ç»ˆLoss: {training_info.get('final_loss', 'N/A')}")
        print(f"   è¯æ±‡è¡¨å¤§å°: {len(tokenizer.char_to_id)}")

        return model, tokenizer

    def load_dataset(self):
        """åŠ è½½HLBDæ•°æ®é›†"""
        print(f"\nğŸ“‚ åŠ è½½æ•°æ®é›†: {self.dataset_path}")

        if not self.dataset_path.exists():
            raise FileNotFoundError(f"æ•°æ®é›†æ–‡ä»¶ä¸å­˜åœ¨: {self.dataset_path}")

        with open(self.dataset_path) as f:
            data = json.load(f)

        total = sum(len(items) for items in data['data'].values())
        print(f"   âœ“ æ•°æ®é›†å·²åŠ è½½")
        print(f"   æ¨¡å—æ•°: {len(data['data'])}")
        print(f"   æ€»æ ·æœ¬æ•°: {total}")

        return data

    def generate_text(self, input_text: str, max_length: int = 50):
        """ç”Ÿæˆæ–‡æœ¬"""
        # Encodeè¾“å…¥
        input_ids = self.tokenizer.encode(input_text)
        input_tensor = torch.tensor([input_ids], dtype=torch.long).to(self.device)

        # ç”Ÿæˆ
        with torch.no_grad():
            output = self.model(input_tensor)
            predicted_ids = output.argmax(dim=-1)[0].tolist()

        # Decode
        generated_text = self.tokenizer.decode(predicted_ids)

        return generated_text

    def test_module(self, module_name: str, module_data: list, sample_size: int = None):
        """æµ‹è¯•å•ä¸ªæ¨¡å—"""
        print(f"\n{'='*60}")
        print(f"æµ‹è¯•æ¨¡å—: {module_name}")
        print(f"{'='*60}")

        # é‡‡æ ·ï¼ˆå¦‚æœæ•°æ®å¤ªå¤šï¼‰
        test_data = module_data
        if sample_size and len(module_data) > sample_size:
            import random
            test_data = random.sample(module_data, sample_size)

        correct = 0
        total = len(test_data)
        failures = []

        for item in test_data:
            input_text = item['input']
            expected = item['output']

            # ç”Ÿæˆ
            generated = self.generate_text(input_text)

            # æ£€æŸ¥æ˜¯å¦æ­£ç¡®ï¼ˆåŒ…å«expectedå³å¯ï¼‰
            is_correct = expected in generated

            if is_correct:
                correct += 1
            else:
                failures.append({
                    'input': input_text,
                    'expected': expected,
                    'generated': generated
                })

        accuracy = correct / total * 100 if total > 0 else 0

        # æ‰“å°ç»“æœ
        print(f"\nå‡†ç¡®ç‡: {correct}/{total} ({accuracy:.1f}%)")

        # æ˜¾ç¤ºå¤±è´¥æ¡ˆä¾‹
        if failures:
            print(f"\nâŒ å¤±è´¥æ¡ˆä¾‹ (å‰5ä¸ª):")
            for i, failure in enumerate(failures[:5], 1):
                print(f"\n{i}. è¾“å…¥: {failure['input']}")
                print(f"   æœŸæœ›: {failure['expected']}")
                print(f"   ç”Ÿæˆ: {failure['generated']}")

        return {
            'module': module_name,
            'total': total,
            'correct': correct,
            'accuracy': accuracy,
            'failures': failures
        }

    def test_all(self, sample_size: int = None):
        """æµ‹è¯•æ‰€æœ‰æ¨¡å—"""
        print("\n" + "="*60)
        print("ğŸ§ª å¼€å§‹HLBDæ¨¡å‹éªŒè¯")
        print("="*60)

        results = []

        for module_name, module_data in self.dataset['data'].items():
            result = self.test_module(module_name, module_data, sample_size)
            results.append(result)

        return results

    def generate_report(self, results: list):
        """ç”ŸæˆéªŒè¯æŠ¥å‘Š"""
        print("\n" + "="*60)
        print("ğŸ“Š éªŒè¯æŠ¥å‘Š")
        print("="*60)

        # æ€»ä½“ç»Ÿè®¡
        total_correct = sum(r['correct'] for r in results)
        total_samples = sum(r['total'] for r in results)
        overall_accuracy = total_correct / total_samples * 100 if total_samples > 0 else 0

        print(f"\næ€»ä½“å‡†ç¡®ç‡: {total_correct}/{total_samples} ({overall_accuracy:.1f}%)\n")

        # æŒ‰æ¨¡å—ç»Ÿè®¡
        print("å„æ¨¡å—å‡†ç¡®ç‡:")
        print("-" * 60)
        for result in sorted(results, key=lambda x: x['accuracy'], reverse=True):
            status = "âœ…" if result['accuracy'] >= 80 else "âš ï¸ " if result['accuracy'] >= 50 else "âŒ"
            print(f"{status} {result['module']:20} {result['correct']:4}/{result['total']:4} ({result['accuracy']:5.1f}%)")

        # è¯Šæ–­
        print("\n" + "="*60)
        print("ğŸ” æ¨¡å‹è¯Šæ–­")
        print("="*60)

        if overall_accuracy >= 90:
            print("\nâœ… æ¨¡å‹è¡¨ç°ä¼˜ç§€ï¼")
            print("   æ¨¡å‹å·²ç»å­¦ä¼šäº†HLBDæ•°æ®é›†çš„æ˜ å°„å…³ç³»")
        elif overall_accuracy >= 70:
            print("\nâš ï¸  æ¨¡å‹è¡¨ç°è‰¯å¥½ï¼Œä½†ä»æœ‰æ”¹è¿›ç©ºé—´")
            print("   å»ºè®®: ç»§ç»­è®­ç»ƒæˆ–è°ƒæ•´å­¦ä¹ ç‡")
        elif overall_accuracy >= 50:
            print("\nâš ï¸  æ¨¡å‹è¡¨ç°ä¸€èˆ¬")
            print("   å¯èƒ½é—®é¢˜: è®­ç»ƒä¸å……åˆ†æˆ–å­¦ä¹ ç‡è¿‡å¤§")
        else:
            print("\nâŒ æ¨¡å‹è¡¨ç°ä¸ä½³")
            print("   å¯èƒ½é—®é¢˜:")
            print("   1. æ¨¡å‹å¯èƒ½åœ¨ã€Œå·æ‡’ã€ï¼ˆè¾“å‡ºé€šç”¨ç­”æ¡ˆï¼‰")
            print("   2. è®­ç»ƒè½®æ•°ä¸è¶³")
            print("   3. å­¦ä¹ ç‡è®¾ç½®ä¸å½“")
            print("   4. æ•°æ®é›†è´¨é‡é—®é¢˜")

        # æ£€æŸ¥å·æ‡’æ¨¡å¼
        print("\næ£€æŸ¥ã€Œå·æ‡’ã€æ¨¡å¼:")
        lazy_outputs = {}
        for result in results:
            for failure in result['failures']:
                output = failure['generated']
                lazy_outputs[output] = lazy_outputs.get(output, 0) + 1

        # æ‰¾å‡ºé‡å¤æœ€å¤šçš„è¾“å‡º
        if lazy_outputs:
            most_common = max(lazy_outputs.items(), key=lambda x: x[1])
            if most_common[1] >= 5:
                print(f"   âš ï¸  å‘ç°é«˜é¢‘è¾“å‡º (å‡ºç°{most_common[1]}æ¬¡):")
                print(f"      \"{most_common[0][:50]}...\"")
                print(f"   â†’ æ¨¡å‹å¯èƒ½åœ¨ã€Œå·æ‡’ã€ï¼Œè¾“å‡ºå›ºå®šæ¨¡æ¿")
            else:
                print(f"   âœ“ æœªå‘ç°æ˜æ˜¾çš„å·æ‡’æ¨¡å¼")
        else:
            print(f"   âœ“ æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡ï¼Œæ— å¤±è´¥æ¡ˆä¾‹")

        # ä¿å­˜æŠ¥å‘Š
        report_path = Path('hlbd_verification_report.json')
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump({
                'model_path': str(self.model_path),
                'dataset_path': str(self.dataset_path),
                'overall_accuracy': overall_accuracy,
                'total_correct': total_correct,
                'total_samples': total_samples,
                'results': results
            }, f, ensure_ascii=False, indent=2)

        print(f"\nğŸ“„ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜: {report_path}")


def main():
    parser = argparse.ArgumentParser(description='HLBDæ¨¡å‹éªŒè¯è„šæœ¬')

    parser.add_argument('--model', type=str, required=True,
                       help='æ¨¡å‹checkpointè·¯å¾„')
    parser.add_argument('--dataset', type=str, default='../data/HLBD_Hardcore_Full.json',
                       help='HLBDæ•°æ®é›†è·¯å¾„')
    parser.add_argument('--sample', type=int, default=None,
                       help='æ¯ä¸ªæ¨¡å—é‡‡æ ·æ•°é‡ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰')
    parser.add_argument('--device', type=str, default='cuda',
                       help='è®¾å¤‡ (cuda/cpu)')

    args = parser.parse_args()

    # åˆ›å»ºéªŒè¯å™¨
    verifier = HLBDVerifier(
        model_path=args.model,
        dataset_path=args.dataset,
        device=args.device
    )

    # è¿è¡Œæµ‹è¯•
    results = verifier.test_all(sample_size=args.sample)

    # ç”ŸæˆæŠ¥å‘Š
    verifier.generate_report(results)

    print("\n" + "="*60)
    print("âœ¨ éªŒè¯å®Œæˆï¼")
    print("="*60)


if __name__ == "__main__":
    main()
