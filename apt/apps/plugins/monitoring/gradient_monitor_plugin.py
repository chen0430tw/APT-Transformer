#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ¢¯åº¦ç›‘æ§å·¥å…·

åŠŸèƒ½ï¼š
1. æ£€æµ‹æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸
2. è®°å½•æ¢¯åº¦èŒƒæ•°å†å²
3. å¯è§†åŒ–æ¢¯åº¦æµ
4. æ¢¯åº¦å¼‚å¸¸æ£€æµ‹ï¼ˆNaN, Infï¼‰
5. ğŸ”® å¯¼å‡ºJSONæ•°æ®ï¼ˆä¾›WebUI/APIä½¿ç”¨ï¼‰
6. ğŸ”® æ”¯æŒåˆ†å¸ƒå¼è®­ç»ƒçš„æ¢¯åº¦åŒæ­¥
"""

import torch
import numpy as np
import json
from collections import defaultdict
from pathlib import Path
import logging
from typing import Dict, List, Tuple, Optional
import matplotlib
matplotlib.use('Agg')  # éGUI backend
import matplotlib.pyplot as plt


class GradientMonitor:
    """æ¢¯åº¦ç›‘æ§å·¥å…·"""

    def __init__(self, model, logger: Optional[logging.Logger] = None,
                 export_dir: Optional[str] = None):
        """
        åˆå§‹åŒ–æ¢¯åº¦ç›‘æ§å™¨

        Args:
            model: è¦ç›‘æ§çš„æ¨¡å‹
            logger: æ—¥å¿—è®°å½•å™¨
            export_dir: å¯¼å‡ºç›®å½•ï¼ˆç”¨äºWebUIæ•°æ®ï¼‰
        """
        self.model = model
        self.logger = logger or logging.getLogger(__name__)
        self.export_dir = Path(export_dir) if export_dir else Path(".cache/gradient_monitor")
        self.export_dir.mkdir(parents=True, exist_ok=True)

        # æ¢¯åº¦å†å²
        self.gradient_history = defaultdict(list)  # {layer_name: [norm1, norm2, ...]}
        self.gradient_norms = []  # [(step, total_norm), ...]

        # å¼‚å¸¸ç»Ÿè®¡
        self.anomaly_counts = {
            'nan': 0,
            'inf': 0,
            'vanishing': 0,  # < 1e-7
            'exploding': 0   # > 1e3
        }

        # ğŸ”® åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
        self.distributed = torch.distributed.is_initialized() if hasattr(torch.distributed, 'is_initialized') else False
        self.rank = torch.distributed.get_rank() if self.distributed else 0

    def check_gradient_flow(self) -> Tuple[Dict[str, float], List[str]]:
        """
        æ£€æŸ¥æ¢¯åº¦æµï¼Œè¯†åˆ«æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸

        Returns:
            gradients: {layer_name: grad_norm}
            issues: é—®é¢˜åˆ—è¡¨
        """
        gradients = {}
        issues = []

        for name, param in self.model.named_parameters():
            if param.grad is not None:
                grad_norm = param.grad.norm().item()
                gradients[name] = grad_norm
                self.gradient_history[name].append(grad_norm)

                # æ£€æµ‹å¼‚å¸¸
                if grad_norm < 1e-7:
                    issues.append(f"âš ï¸  æ¢¯åº¦æ¶ˆå¤±: {name} (norm={grad_norm:.2e})")
                    self.anomaly_counts['vanishing'] += 1

                elif grad_norm > 1e3:
                    issues.append(f"âš ï¸  æ¢¯åº¦çˆ†ç‚¸: {name} (norm={grad_norm:.2e})")
                    self.anomaly_counts['exploding'] += 1

                elif torch.isnan(torch.tensor(grad_norm)):
                    issues.append(f"âŒ NaNæ¢¯åº¦: {name}")
                    self.anomaly_counts['nan'] += 1

                elif torch.isinf(torch.tensor(grad_norm)):
                    issues.append(f"âŒ Infæ¢¯åº¦: {name}")
                    self.anomaly_counts['inf'] += 1

        # è®°å½•é—®é¢˜
        if issues and self.logger:
            for issue in issues:
                self.logger.warning(issue)

        return gradients, issues

    def log_gradient_norms(self, step: int) -> float:
        """
        è®°å½•æ¢¯åº¦èŒƒæ•°

        Args:
            step: å½“å‰æ­¥æ•°

        Returns:
            total_norm: æ€»æ¢¯åº¦èŒƒæ•°
        """
        total_norm = 0.0
        param_count = 0

        for param in self.model.parameters():
            if param.grad is not None:
                param_norm = param.grad.data.norm(2).item()
                total_norm += param_norm ** 2
                param_count += 1

        if param_count > 0:
            total_norm = total_norm ** 0.5
        else:
            total_norm = 0.0

        self.gradient_norms.append((step, total_norm))

        if self.logger:
            self.logger.debug(f"Step {step}: Total gradient norm = {total_norm:.4f}")

        return total_norm

    def detect_gradient_anomalies(self) -> List[str]:
        """
        æ£€æµ‹æ¢¯åº¦å¼‚å¸¸ï¼ˆNaN, Infç­‰ï¼‰

        Returns:
            anomalies: å¼‚å¸¸åˆ—è¡¨
        """
        anomalies = []

        for name, param in self.model.named_parameters():
            if param.grad is not None:
                if torch.isnan(param.grad).any():
                    anomalies.append(f"NaN in {name}")

                if torch.isinf(param.grad).any():
                    anomalies.append(f"Inf in {name}")

        return anomalies

    def plot_gradient_flow(self, save_path: Optional[str] = None) -> str:
        """
        å¯è§†åŒ–æ¢¯åº¦æµ

        Args:
            save_path: ä¿å­˜è·¯å¾„

        Returns:
            å®é™…ä¿å­˜è·¯å¾„
        """
        if save_path is None:
            save_path = self.export_dir / "gradient_flow.png"
        else:
            save_path = Path(save_path)

        fig, ax = plt.subplots(figsize=(15, 6))

        # æå–æ•°æ®
        layers = []
        avg_grads = []

        for name, grad_list in self.gradient_history.items():
            if len(grad_list) > 0:
                layers.append(name.replace('.', '\n'))  # æ¢è¡Œæ˜¾ç¤º
                avg_grads.append(np.mean(grad_list))

        # ç»˜åˆ¶æ¡å½¢å›¾
        colors = ['green' if g > 1e-7 and g < 1e3 else 'red' for g in avg_grads]

        ax.bar(range(len(layers)), avg_grads, alpha=0.7, color=colors)
        ax.set_xticks(range(len(layers)))
        ax.set_xticklabels(layers, rotation=90, ha='right', fontsize=8)
        ax.set_ylabel('Average Gradient Norm')
        ax.set_title('Gradient Flow Across Layers')
        ax.set_yscale('log')
        ax.axhline(y=1e-7, color='orange', linestyle='--', label='Vanishing threshold')
        ax.axhline(y=1e3, color='red', linestyle='--', label='Exploding threshold')
        ax.legend()
        ax.grid(True, alpha=0.3)

        plt.tight_layout()
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close(fig)

        self.logger.info(f"æ¢¯åº¦æµå›¾å·²ä¿å­˜: {save_path}")

        return str(save_path)

    def plot_gradient_norms_timeline(self, save_path: Optional[str] = None) -> str:
        """
        ç»˜åˆ¶æ¢¯åº¦èŒƒæ•°æ—¶é—´çº¿

        Args:
            save_path: ä¿å­˜è·¯å¾„

        Returns:
            å®é™…ä¿å­˜è·¯å¾„
        """
        if save_path is None:
            save_path = self.export_dir / "gradient_norms_timeline.png"
        else:
            save_path = Path(save_path)

        if len(self.gradient_norms) == 0:
            self.logger.warning("No gradient norms recorded")
            return ""

        fig, ax = plt.subplots(figsize=(12, 6))

        steps, norms = zip(*self.gradient_norms)

        ax.plot(steps, norms, linewidth=2, color='blue', alpha=0.7)
        ax.set_xlabel('Training Step')
        ax.set_ylabel('Total Gradient Norm')
        ax.set_title('Gradient Norm Over Time')
        ax.grid(True, alpha=0.3)
        ax.set_yscale('log')

        plt.tight_layout()
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close(fig)

        self.logger.info(f"æ¢¯åº¦èŒƒæ•°æ—¶é—´çº¿å·²ä¿å­˜: {save_path}")

        return str(save_path)

    def get_gradient_stats(self) -> Dict[str, Dict[str, float]]:
        """
        è·å–æ¢¯åº¦ç»Ÿè®¡ä¿¡æ¯

        Returns:
            stats: {layer_name: {mean, std, min, max}}
        """
        stats = {}

        for name, grad_list in self.gradient_history.items():
            if len(grad_list) > 0:
                stats[name] = {
                    'mean': float(np.mean(grad_list)),
                    'std': float(np.std(grad_list)),
                    'min': float(np.min(grad_list)),
                    'max': float(np.max(grad_list)),
                    'count': len(grad_list)
                }

        return stats

    # =========================================================================
    # ğŸ”® WebUI/APIæ•°æ®å¯¼å‡ºæ¥å£
    # =========================================================================

    def export_for_webui(self) -> Dict:
        """
        å¯¼å‡ºæ•°æ®ä¾›WebUI/APIä½¿ç”¨

        Returns:
            data: {
                'gradient_stats': {...},
                'gradient_timeline': [...],
                'anomaly_counts': {...},
                'latest_issues': [...]
            }

        WebUIå¯ä»¥é€šè¿‡APIè·å–ï¼š
        GET /api/training/gradients
        """
        # å¯¼å‡ºæ¢¯åº¦ç»Ÿè®¡
        gradient_stats = self.get_gradient_stats()

        # å¯¼å‡ºæ¢¯åº¦æ—¶é—´çº¿
        gradient_timeline = [
            {'step': int(step), 'norm': float(norm)}
            for step, norm in self.gradient_norms
        ]

        # å¯¼å‡ºå¼‚å¸¸è®¡æ•°
        anomaly_counts = self.anomaly_counts.copy()

        # æœ€è¿‘çš„é—®é¢˜
        _, latest_issues = self.check_gradient_flow()

        data = {
            'gradient_stats': gradient_stats,
            'gradient_timeline': gradient_timeline,
            'anomaly_counts': anomaly_counts,
            'latest_issues': latest_issues,
            'total_monitored_layers': len(self.gradient_history),
            'total_steps': len(self.gradient_norms)
        }

        # ä¿å­˜ä¸ºJSONï¼ˆä¾›WebUI/APIè¯»å–ï¼‰
        json_path = self.export_dir / "gradient_data.json"
        with open(json_path, 'w') as f:
            json.dump(data, f, indent=2)

        self.logger.info(f"æ¢¯åº¦æ•°æ®å·²å¯¼å‡º: {json_path}")

        return data

    def export_summary_report(self, save_path: Optional[str] = None) -> str:
        """
        å¯¼å‡ºæ±‡æ€»æŠ¥å‘Š

        Args:
            save_path: ä¿å­˜è·¯å¾„

        Returns:
            æŠ¥å‘Šè·¯å¾„
        """
        if save_path is None:
            save_path = self.export_dir / "gradient_summary.md"
        else:
            save_path = Path(save_path)

        stats = self.get_gradient_stats()

        # ç”ŸæˆMarkdownæŠ¥å‘Š
        report = "# æ¢¯åº¦ç›‘æ§æŠ¥å‘Š\n\n"

        # å¼‚å¸¸ç»Ÿè®¡
        report += "## å¼‚å¸¸ç»Ÿè®¡\n\n"
        report += f"- NaNæ¢¯åº¦: {self.anomaly_counts['nan']}\n"
        report += f"- Infæ¢¯åº¦: {self.anomaly_counts['inf']}\n"
        report += f"- æ¢¯åº¦æ¶ˆå¤±: {self.anomaly_counts['vanishing']}\n"
        report += f"- æ¢¯åº¦çˆ†ç‚¸: {self.anomaly_counts['exploding']}\n\n"

        # å±‚çº§ç»Ÿè®¡
        report += "## å„å±‚æ¢¯åº¦ç»Ÿè®¡\n\n"
        report += "| å±‚å | å¹³å‡å€¼ | æ ‡å‡†å·® | æœ€å°å€¼ | æœ€å¤§å€¼ |\n"
        report += "|------|--------|--------|--------|--------|\n"

        for name, stat in sorted(stats.items()):
            report += f"| {name} | {stat['mean']:.2e} | {stat['std']:.2e} | {stat['min']:.2e} | {stat['max']:.2e} |\n"

        # ä¿å­˜æŠ¥å‘Š
        with open(save_path, 'w', encoding='utf-8') as f:
            f.write(report)

        self.logger.info(f"æ±‡æ€»æŠ¥å‘Šå·²ä¿å­˜: {save_path}")

        return str(save_path)

    # =========================================================================
    # ğŸ”® åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
    # =========================================================================

    def sync_gradients_distributed(self):
        """
        åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­åŒæ­¥æ¢¯åº¦ä¿¡æ¯

        æœªæ¥DDPè®­ç»ƒæ—¶è°ƒç”¨ï¼Œç¡®ä¿æ‰€æœ‰rankçš„æ¢¯åº¦ç›‘æ§ä¸€è‡´
        """
        if not self.distributed:
            return

        # ğŸ”® åˆ†å¸ƒå¼ä¼ç¬”ï¼šåŒæ­¥æ¢¯åº¦èŒƒæ•°
        # åœ¨DDPè®­ç»ƒä¸­ï¼Œæ¯ä¸ªrankçš„æ¢¯åº¦å¯èƒ½ç•¥æœ‰ä¸åŒ
        # è¿™é‡Œå¯ä»¥all_reduceæ¢¯åº¦èŒƒæ•°ï¼Œè·å–å…¨å±€å¹³å‡å€¼

        if len(self.gradient_norms) > 0:
            last_step, last_norm = self.gradient_norms[-1]

            # å°†æ ‡é‡è½¬ä¸ºtensor
            norm_tensor = torch.tensor([last_norm], dtype=torch.float32)

            # æœªæ¥å¯ä»¥æ·»åŠ ï¼š
            # torch.distributed.all_reduce(norm_tensor, op=torch.distributed.ReduceOp.AVG)

            # æ›´æ–°ä¸ºå…¨å±€å¹³å‡å€¼
            # self.gradient_norms[-1] = (last_step, norm_tensor.item())

            if self.rank == 0:
                self.logger.debug(f"[Rank {self.rank}] æ¢¯åº¦èŒƒæ•°å·²åŒæ­¥")

    def aggregate_anomalies_distributed(self):
        """
        åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­èšåˆå¼‚å¸¸ç»Ÿè®¡

        æœªæ¥DDPè®­ç»ƒæ—¶è°ƒç”¨ï¼Œæ±‡æ€»æ‰€æœ‰rankçš„å¼‚å¸¸
        """
        if not self.distributed:
            return

        # ğŸ”® åˆ†å¸ƒå¼ä¼ç¬”ï¼šèšåˆå¼‚å¸¸è®¡æ•°
        # å°†æ‰€æœ‰rankçš„å¼‚å¸¸è®¡æ•°åŠ æ€»

        # æœªæ¥å¯ä»¥æ·»åŠ ï¼š
        # for key in self.anomaly_counts:
        #     count_tensor = torch.tensor([self.anomaly_counts[key]], dtype=torch.int64)
        #     torch.distributed.all_reduce(count_tensor, op=torch.distributed.ReduceOp.SUM)
        #     self.anomaly_counts[key] = count_tensor.item()

        if self.rank == 0:
            self.logger.info(f"[Rank {self.rank}] å¼‚å¸¸ç»Ÿè®¡å·²èšåˆ")

    # =========================================================================
    # ä¾¿æ·æ–¹æ³•
    # =========================================================================

    def generate_all_reports(self):
        """
        ç”Ÿæˆæ‰€æœ‰æŠ¥å‘Šï¼ˆå›¾è¡¨ + JSON + Markdownï¼‰

        ä¾›è®­ç»ƒç»“æŸåè°ƒç”¨
        """
        self.logger.info("æ­£åœ¨ç”Ÿæˆæ¢¯åº¦ç›‘æ§æŠ¥å‘Š...")

        # ç”Ÿæˆå›¾è¡¨
        self.plot_gradient_flow()
        self.plot_gradient_norms_timeline()

        # å¯¼å‡ºæ•°æ®
        self.export_for_webui()
        self.export_summary_report()

        self.logger.info(f"æ‰€æœ‰æŠ¥å‘Šå·²ç”Ÿæˆåœ¨: {self.export_dir}")

        return {
            'export_dir': str(self.export_dir),
            'gradient_flow_plot': str(self.export_dir / "gradient_flow.png"),
            'gradient_timeline_plot': str(self.export_dir / "gradient_norms_timeline.png"),
            'json_data': str(self.export_dir / "gradient_data.json"),
            'summary_report': str(self.export_dir / "gradient_summary.md")
        }


# ============================================================================
# ä½¿ç”¨ç¤ºä¾‹
# ============================================================================

if __name__ == "__main__":
    # ç¤ºä¾‹ï¼šå¦‚ä½•åœ¨è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨
    import torch.nn as nn

    # åˆ›å»ºç¤ºä¾‹æ¨¡å‹
    model = nn.Sequential(
        nn.Linear(10, 20),
        nn.ReLU(),
        nn.Linear(20, 10)
    )

    # åˆå§‹åŒ–ç›‘æ§å™¨
    monitor = GradientMonitor(model, export_dir=".cache/gradient_monitor")

    # æ¨¡æ‹Ÿè®­ç»ƒ
    optimizer = torch.optim.Adam(model.parameters())

    for step in range(100):
        # å‰å‘ä¼ æ’­
        x = torch.randn(5, 10)
        output = model(x)
        loss = output.sum()

        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()

        # æ¢¯åº¦ç›‘æ§
        gradients, issues = monitor.check_gradient_flow()
        total_norm = monitor.log_gradient_norms(step)

        # æ£€æµ‹å¼‚å¸¸
        anomalies = monitor.detect_gradient_anomalies()
        if anomalies:
            print(f"Step {step}: æ£€æµ‹åˆ°å¼‚å¸¸: {anomalies}")

        optimizer.step()

    # ç”ŸæˆæŠ¥å‘Š
    reports = monitor.generate_all_reports()
    print(f"æŠ¥å‘Šå·²ç”Ÿæˆ: {reports}")

    # å¯¼å‡ºWebUIæ•°æ®
    webui_data = monitor.export_for_webui()
    print(f"WebUIæ•°æ®: {webui_data['total_steps']} æ­¥")
