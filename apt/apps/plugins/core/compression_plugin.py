#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
APTæ¨¡å‹å‹ç¼©æ’ä»¶

é›†æˆå¤šç§å‹ç¼©æŠ€æœ¯:
1. æ¨¡å‹å‰ªæ (Pruning) - ç§»é™¤ä¸é‡è¦çš„æƒé‡
2. æ¨¡å‹é‡åŒ– (Quantization) - é™ä½æƒé‡ç²¾åº¦
3. çŸ¥è¯†è’¸é¦ (Distillation) - å°†çŸ¥è¯†è½¬ç§»åˆ°å°æ¨¡å‹
4. DBCåŠ é€Ÿè®­ç»ƒ (Dimension-Balanced Compression) - è®­ç»ƒåŠ é€Ÿ
5. ä½ç§©åˆ†è§£ (Low-Rank Decomposition) - æƒé‡çŸ©é˜µåˆ†è§£
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.nn.utils.prune as prune
from typing import Dict, Any, List, Tuple, Optional
from pathlib import Path
import json
import numpy as np
from datetime import datetime


class CompressionPlugin:
    """
    ç»Ÿä¸€çš„æ¨¡å‹å‹ç¼©æ’ä»¶

    æä¾›å¤šç§å‹ç¼©æŠ€æœ¯çš„ç»Ÿä¸€æ¥å£
    """

    def __init__(self, config: Dict[str, Any] = None):
        self.name = "apt-compression"
        self.version = "1.0.0"
        self.config = config or {}

        # å‹ç¼©é…ç½®
        self.compression_methods = self.config.get('methods', ['pruning'])
        self.target_compression_ratio = self.config.get('compression_ratio', 0.5)

        # å„æ–¹æ³•çš„é…ç½®
        self.pruning_config = self.config.get('pruning', {
            'ratio': 0.3,
            'type': 'magnitude',
            'structured': False
        })

        self.quantization_config = self.config.get('quantization', {
            'bits': 8,
            'type': 'dynamic',  # static/dynamic/qat
            'backend': 'fbgemm'
        })

        self.distillation_config = self.config.get('distillation', {
            'temperature': 4.0,
            'alpha': 0.7,
            'beta': 0.3
        })

        self.dbc_config = self.config.get('dbc', {
            'enabled': True,
            'rank_ratio': 0.1,
            'apply_to_gradients': True
        })

    # ========================================================================
    # 1. æ¨¡å‹å‰ªæ (Pruning)
    # ========================================================================

    def prune_model(
        self,
        model: nn.Module,
        prune_ratio: float = None,
        prune_type: str = 'magnitude',
        structured: bool = False
    ) -> nn.Module:
        """
        å‰ªææ¨¡å‹ä»¥å‡å°‘å‚æ•°é‡

        Args:
            model: å¾…å‰ªææ¨¡å‹
            prune_ratio: å‰ªææ¯”ä¾‹ (0-1)
            prune_type: å‰ªæç±»å‹ ('magnitude', 'random', 'l1')
            structured: æ˜¯å¦ç»“æ„åŒ–å‰ªæ

        Returns:
            å‰ªæåçš„æ¨¡å‹
        """
        if prune_ratio is None:
            prune_ratio = self.pruning_config['ratio']

        print(f"âœ‚ï¸  å¼€å§‹æ¨¡å‹å‰ªæ (æ¯”ä¾‹: {prune_ratio*100:.1f}%, ç±»å‹: {prune_type})")

        # æ”¶é›†éœ€è¦å‰ªæçš„å‚æ•°
        parameters_to_prune = []
        for name, module in model.named_modules():
            if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d)):
                parameters_to_prune.append((module, 'weight'))

        # åº”ç”¨å‰ªæ
        if structured:
            # ç»“æ„åŒ–å‰ªæ (å‰ªé™¤æ•´ä¸ªé€šé“/ç¥ç»å…ƒ)
            for module, param_name in parameters_to_prune:
                prune.ln_structured(
                    module, name=param_name,
                    amount=prune_ratio, n=2, dim=0
                )
        else:
            # éç»“æ„åŒ–å‰ªæ
            if prune_type == 'magnitude':
                prune.global_unstructured(
                    parameters_to_prune,
                    pruning_method=prune.L1Unstructured,
                    amount=prune_ratio,
                )
            elif prune_type == 'random':
                prune.global_unstructured(
                    parameters_to_prune,
                    pruning_method=prune.RandomUnstructured,
                    amount=prune_ratio,
                )

        # ç»Ÿè®¡å‰ªææ•ˆæœ
        total_params = sum(p.numel() for p in model.parameters())
        pruned_params = self._count_pruned_params(model)
        actual_ratio = pruned_params / total_params

        print(f"âœ… å‰ªæå®Œæˆ! å‰ªé™¤å‚æ•°: {pruned_params}/{total_params} ({actual_ratio*100:.2f}%)")

        return model

    def make_pruning_permanent(self, model: nn.Module) -> nn.Module:
        """æ°¸ä¹…åº”ç”¨å‰ªæ (ç§»é™¤æ©ç )"""
        print("ğŸ”§ æ°¸ä¹…åº”ç”¨å‰ªæ...")

        for module in model.modules():
            if isinstance(module, (nn.Linear, nn.Conv1d, nn.Conv2d)):
                try:
                    prune.remove(module, 'weight')
                except:
                    pass

        print("âœ… å‰ªæå·²æ°¸ä¹…åº”ç”¨!")
        return model

    def _count_pruned_params(self, model: nn.Module) -> int:
        """è®¡ç®—è¢«å‰ªæçš„å‚æ•°æ•°é‡"""
        pruned = 0

        for module in model.modules():
            if hasattr(module, 'weight'):
                weight = module.weight
                pruned += (weight == 0).sum().item()

        return pruned

    # ========================================================================
    # 2. æ¨¡å‹é‡åŒ– (Quantization)
    # ========================================================================

    def quantize_model(
        self,
        model: nn.Module,
        quantization_type: str = None,
        bits: int = None
    ) -> nn.Module:
        """
        é‡åŒ–æ¨¡å‹ä»¥é™ä½ç²¾åº¦å’Œå†…å­˜å ç”¨

        Args:
            model: å¾…é‡åŒ–æ¨¡å‹
            quantization_type: é‡åŒ–ç±»å‹ ('dynamic', 'static', 'qat')
            bits: é‡åŒ–ä½æ•° (4, 8, 16)

        Returns:
            é‡åŒ–åçš„æ¨¡å‹
        """
        if quantization_type is None:
            quantization_type = self.quantization_config['type']
        if bits is None:
            bits = self.quantization_config['bits']

        print(f"ğŸ”¢ å¼€å§‹æ¨¡å‹é‡åŒ– (ç±»å‹: {quantization_type}, ä½æ•°: {bits}bits)")

        model.eval()

        if quantization_type == 'dynamic':
            # åŠ¨æ€é‡åŒ– (è¿è¡Œæ—¶é‡åŒ–)
            quantized_model = torch.quantization.quantize_dynamic(
                model,
                {nn.Linear, nn.Conv1d, nn.LSTM, nn.GRU},
                dtype=torch.qint8
            )

        elif quantization_type == 'static':
            # é™æ€é‡åŒ– (éœ€è¦æ ¡å‡†æ•°æ®)
            model.qconfig = torch.quantization.get_default_qconfig(
                self.quantization_config.get('backend', 'fbgemm')
            )
            torch.quantization.prepare(model, inplace=True)
            # è¿™é‡Œéœ€è¦ç”¨æˆ·æä¾›æ ¡å‡†æ•°æ®è¿›è¡Œå‰å‘ä¼ æ’­
            # calibrate(model, calibration_data)
            quantized_model = torch.quantization.convert(model, inplace=False)

        elif quantization_type == 'qat':
            # é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ (Quantization-Aware Training)
            model.qconfig = torch.quantization.get_default_qat_qconfig(
                self.quantization_config.get('backend', 'fbgemm')
            )
            quantized_model = torch.quantization.prepare_qat(model, inplace=False)
            # éœ€è¦ç»§ç»­è®­ç»ƒ: train(quantized_model, ...)
            # æœ€åè½¬æ¢: quantized_model = torch.quantization.convert(quantized_model)

        else:
            raise ValueError(f"æœªçŸ¥çš„é‡åŒ–ç±»å‹: {quantization_type}")

        # è®¡ç®—æ¨¡å‹å¤§å°å‡å°‘
        original_size = self._get_model_size(model)
        quantized_size = self._get_model_size(quantized_model)
        compression_ratio = quantized_size / original_size

        print(f"âœ… é‡åŒ–å®Œæˆ! æ¨¡å‹å¤§å°: {original_size:.2f}MB â†’ {quantized_size:.2f}MB "
              f"(å‹ç¼©æ¯”: {compression_ratio:.2%})")

        return quantized_model

    def quantize_to_int8(self, model: nn.Module) -> nn.Module:
        """å¿«æ·æ–¹æ³•: INT8é‡åŒ–"""
        return self.quantize_model(model, quantization_type='dynamic', bits=8)

    def _get_model_size(self, model: nn.Module) -> float:
        """è®¡ç®—æ¨¡å‹å¤§å° (MB)"""
        param_size = 0
        for param in model.parameters():
            param_size += param.nelement() * param.element_size()

        buffer_size = 0
        for buffer in model.buffers():
            buffer_size += buffer.nelement() * buffer.element_size()

        size_mb = (param_size + buffer_size) / 1024 / 1024
        return size_mb

    # ========================================================================
    # 3. çŸ¥è¯†è’¸é¦ (Distillation)
    # ========================================================================

    def distillation_loss(
        self,
        student_logits: torch.Tensor,
        teacher_logits: torch.Tensor,
        labels: Optional[torch.Tensor] = None,
        temperature: float = None
    ) -> torch.Tensor:
        """
        çŸ¥è¯†è’¸é¦æŸå¤±

        Args:
            student_logits: å­¦ç”Ÿæ¨¡å‹è¾“å‡º [batch, seq_len, vocab]
            teacher_logits: æ•™å¸ˆæ¨¡å‹è¾“å‡º [batch, seq_len, vocab]
            labels: çœŸå®æ ‡ç­¾ (å¯é€‰)
            temperature: è’¸é¦æ¸©åº¦

        Returns:
            è’¸é¦æŸå¤±
        """
        if temperature is None:
            temperature = self.distillation_config['temperature']

        alpha = self.distillation_config['alpha']
        beta = self.distillation_config['beta']

        # æ¸©åº¦è½¯åŒ–
        student_log_probs = F.log_softmax(student_logits / temperature, dim=-1)
        teacher_probs = F.softmax(teacher_logits / temperature, dim=-1)

        # KLæ•£åº¦æŸå¤±
        distill_loss = F.kl_div(
            student_log_probs,
            teacher_probs,
            reduction='batchmean'
        ) * (temperature ** 2)

        # å¦‚æœæœ‰çœŸå®æ ‡ç­¾,ç»“åˆäº¤å‰ç†µæŸå¤±
        if labels is not None:
            ce_loss = F.cross_entropy(
                student_logits.view(-1, student_logits.size(-1)),
                labels.view(-1),
                ignore_index=-100
            )
            total_loss = alpha * distill_loss + beta * ce_loss
            return total_loss

        return distill_loss

    def train_with_distillation(
        self,
        student_model: nn.Module,
        teacher_model: nn.Module,
        dataloader: torch.utils.data.DataLoader,
        optimizer: torch.optim.Optimizer,
        num_epochs: int = 3,
        device: str = 'cuda'
    ):
        """
        ä½¿ç”¨çŸ¥è¯†è’¸é¦è®­ç»ƒå­¦ç”Ÿæ¨¡å‹

        Args:
            student_model: å­¦ç”Ÿæ¨¡å‹
            teacher_model: æ•™å¸ˆæ¨¡å‹
            dataloader: è®­ç»ƒæ•°æ®
            optimizer: ä¼˜åŒ–å™¨
            num_epochs: è®­ç»ƒè½®æ•°
            device: è®¾å¤‡
        """
        print(f"ğŸ“ å¼€å§‹çŸ¥è¯†è’¸é¦è®­ç»ƒ ({num_epochs} epochs)...")

        student_model.to(device)
        teacher_model.to(device)
        teacher_model.eval()

        for epoch in range(num_epochs):
            student_model.train()
            epoch_loss = 0

            for batch_idx, batch in enumerate(dataloader):
                # å‡†å¤‡æ•°æ®
                if isinstance(batch, dict):
                    batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v
                            for k, v in batch.items()}
                    input_ids = batch.get('input_ids')
                    labels = batch.get('labels', input_ids)
                elif isinstance(batch, (list, tuple)):
                    input_ids = batch[0].to(device)
                    labels = batch[1].to(device) if len(batch) > 1 else input_ids
                else:
                    input_ids = batch.to(device)
                    labels = input_ids

                # æ•™å¸ˆæ¨¡å‹æ¨ç†
                with torch.no_grad():
                    teacher_outputs = teacher_model(input_ids)
                    teacher_logits = teacher_outputs if isinstance(teacher_outputs, torch.Tensor) else teacher_outputs[0]

                # å­¦ç”Ÿæ¨¡å‹è®­ç»ƒ
                student_outputs = student_model(input_ids)
                student_logits = student_outputs if isinstance(student_outputs, torch.Tensor) else student_outputs[0]

                # è®¡ç®—è’¸é¦æŸå¤±
                loss = self.distillation_loss(student_logits, teacher_logits, labels)

                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                epoch_loss += loss.item()

                if batch_idx % 50 == 0:
                    print(f"  Epoch {epoch+1}/{num_epochs} | "
                          f"Batch {batch_idx}/{len(dataloader)} | "
                          f"Loss: {loss.item():.4f}")

            avg_loss = epoch_loss / len(dataloader)
            print(f"ğŸ“Š Epoch {epoch+1} å¹³å‡æŸå¤±: {avg_loss:.4f}")

        print("âœ… çŸ¥è¯†è’¸é¦å®Œæˆ!")

    # ========================================================================
    # 4. DBCåŠ é€Ÿè®­ç»ƒ (Dimension-Balanced Compression)
    # ========================================================================

    def enable_dbc_training(
        self,
        model: nn.Module,
        rank_ratio: float = None,
        apply_to_gradients: bool = True
    ) -> Tuple[nn.Module, Any]:
        """
        å¯ç”¨DBCåŠ é€Ÿè®­ç»ƒ

        DBCé€šè¿‡ç»´åº¦å¹³è¡¡å‹ç¼©æ¥ç¨³å®šè®­ç»ƒå’ŒåŠ é€Ÿæ”¶æ•›

        Args:
            model: APTæ¨¡å‹
            rank_ratio: ä½ç§©æ¯”ä¾‹
            apply_to_gradients: æ˜¯å¦åº”ç”¨äºæ¢¯åº¦ç¨³å®š

        Returns:
            (æ¨¡å‹, DBCä¼˜åŒ–å™¨)
        """
        if rank_ratio is None:
            rank_ratio = self.dbc_config['rank_ratio']

        print(f"ğŸš€ å¯ç”¨DBCåŠ é€Ÿè®­ç»ƒ (rank_ratio={rank_ratio})")

        # å¯¼å…¥DBCä¼˜åŒ–å™¨
        from apt_model.modeling.apt_model import DBCDAC_Optimizer, add_gradient_hooks_to_model

        # åˆ›å»ºDBCä¼˜åŒ–å™¨
        dbc_optimizer = DBCDAC_Optimizer(
            rank_ratio_proj=rank_ratio,
            rank_ratio_res=rank_ratio * 0.5,
            threshold=1e-6,
            iterations=1,
            use_quantization=False,
            quant_bits=8,
            apply_to_gradients=apply_to_gradients
        )

        # ä¸ºæ¨¡å‹æ·»åŠ æ¢¯åº¦ç¨³å®šé’©å­
        if apply_to_gradients:
            hooks = add_gradient_hooks_to_model(model, dbc_optimizer)
            print(f"âœ… DBCæ¢¯åº¦ç¨³å®šé’©å­å·²æ·»åŠ  ({len(hooks)} ä¸ªå‚æ•°)")

        print("âœ… DBCåŠ é€Ÿè®­ç»ƒå·²å¯ç”¨!")

        return model, dbc_optimizer

    # ========================================================================
    # 5. ä½ç§©åˆ†è§£ (Low-Rank Decomposition)
    # ========================================================================

    def low_rank_decomposition(
        self,
        model: nn.Module,
        rank_ratio: float = 0.5,
        layer_types: tuple = (nn.Linear,)
    ) -> nn.Module:
        """
        å¯¹æ¨¡å‹æƒé‡è¿›è¡Œä½ç§©åˆ†è§£

        å°† W (mÃ—n) åˆ†è§£ä¸º U (mÃ—r) @ V (rÃ—n)ï¼Œå…¶ä¸­ r << min(m,n)

        Args:
            model: å¾…åˆ†è§£æ¨¡å‹
            rank_ratio: ç§©æ¯”ä¾‹ (0-1)
            layer_types: è¦åˆ†è§£çš„å±‚ç±»å‹

        Returns:
            åˆ†è§£åçš„æ¨¡å‹
        """
        print(f"ğŸ“Š å¼€å§‹ä½ç§©åˆ†è§£ (rank_ratio={rank_ratio})")

        decomposed_layers = 0
        original_params = sum(p.numel() for p in model.parameters())

        for name, module in model.named_modules():
            if isinstance(module, layer_types):
                if hasattr(module, 'weight'):
                    weight = module.weight.data
                    m, n = weight.shape

                    # è®¡ç®—ç›®æ ‡ç§©
                    rank = max(1, int(min(m, n) * rank_ratio))

                    # SVDåˆ†è§£
                    U, S, Vh = torch.linalg.svd(weight, full_matrices=False)

                    # ä¿ç•™å‰rä¸ªå¥‡å¼‚å€¼
                    U_r = U[:, :rank]
                    S_r = S[:rank]
                    Vh_r = Vh[:rank, :]

                    # é‡æ„æƒé‡
                    weight_approx = U_r @ torch.diag(S_r) @ Vh_r
                    module.weight.data = weight_approx

                    decomposed_layers += 1

        new_params = sum(p.numel() for p in model.parameters())
        compression_ratio = new_params / original_params

        print(f"âœ… ä½ç§©åˆ†è§£å®Œæˆ! åˆ†è§£å±‚æ•°: {decomposed_layers}")
        print(f"   å‚æ•°é‡: {original_params} â†’ {new_params} (å‹ç¼©æ¯”: {compression_ratio:.2%})")

        return model

    # ========================================================================
    # 6. ç»¼åˆå‹ç¼©æµç¨‹
    # ========================================================================

    def compress_model(
        self,
        model: nn.Module,
        methods: List[str] = None,
        target_ratio: float = None
    ) -> Dict[str, Any]:
        """
        ç»¼åˆå‹ç¼©æ¨¡å‹

        Args:
            model: å¾…å‹ç¼©æ¨¡å‹
            methods: å‹ç¼©æ–¹æ³•åˆ—è¡¨ ['pruning', 'quantization', 'low_rank']
            target_ratio: ç›®æ ‡å‹ç¼©æ¯”

        Returns:
            å‹ç¼©ç»“æœå­—å…¸
        """
        if methods is None:
            methods = self.compression_methods
        if target_ratio is None:
            target_ratio = self.target_compression_ratio

        print("="*60)
        print("ğŸ—œï¸  å¼€å§‹ç»¼åˆæ¨¡å‹å‹ç¼©")
        print("="*60)

        # è®°å½•åŸå§‹æ¨¡å‹ä¿¡æ¯
        original_size = self._get_model_size(model)
        original_params = sum(p.numel() for p in model.parameters())

        results = {
            'original_size_mb': original_size,
            'original_params': original_params,
            'methods_applied': [],
            'compression_stages': []
        }

        # åº”ç”¨å„ç§å‹ç¼©æ–¹æ³•
        if 'pruning' in methods:
            print("\nğŸ“ æ­¥éª¤ 1: æ¨¡å‹å‰ªæ")
            model = self.prune_model(model)
            model = self.make_pruning_permanent(model)

            current_params = sum(p.numel() for p in model.parameters())
            results['methods_applied'].append('pruning')
            results['compression_stages'].append({
                'method': 'pruning',
                'params': current_params,
                'compression_ratio': current_params / original_params
            })

        if 'low_rank' in methods:
            print("\nğŸ“ æ­¥éª¤ 2: ä½ç§©åˆ†è§£")
            model = self.low_rank_decomposition(model)

            current_params = sum(p.numel() for p in model.parameters())
            results['methods_applied'].append('low_rank')
            results['compression_stages'].append({
                'method': 'low_rank',
                'params': current_params,
                'compression_ratio': current_params / original_params
            })

        if 'quantization' in methods:
            print("\nğŸ“ æ­¥éª¤ 3: æ¨¡å‹é‡åŒ–")
            model = self.quantize_model(model)

            results['methods_applied'].append('quantization')
            results['compression_stages'].append({
                'method': 'quantization',
                'size_mb': self._get_model_size(model)
            })

        # æœ€ç»ˆç»Ÿè®¡
        final_size = self._get_model_size(model)
        final_params = sum(p.numel() for p in model.parameters() if not p.dtype.is_floating_point or p.dtype == torch.float32)

        results['final_size_mb'] = final_size
        results['final_params'] = final_params
        results['size_compression_ratio'] = final_size / original_size
        results['param_compression_ratio'] = final_params / original_params

        print("\n" + "="*60)
        print("ğŸ“Š å‹ç¼©ç»“æœæ€»ç»“")
        print("="*60)
        print(f"åŸå§‹æ¨¡å‹: {original_size:.2f}MB, {original_params:,} å‚æ•°")
        print(f"å‹ç¼©å:   {final_size:.2f}MB, {final_params:,} å‚æ•°")
        print(f"å‹ç¼©æ¯”:   {results['size_compression_ratio']:.2%} (å¤§å°), "
              f"{results['param_compression_ratio']:.2%} (å‚æ•°)")
        print(f"åº”ç”¨æ–¹æ³•: {', '.join(results['methods_applied'])}")
        print("="*60)

        return results

    # ========================================================================
    # 7. å‹ç¼©è¯„ä¼°
    # ========================================================================

    def evaluate_compression(
        self,
        original_model: nn.Module,
        compressed_model: nn.Module,
        test_dataloader: torch.utils.data.DataLoader = None,
        device: str = 'cuda'
    ) -> Dict[str, Any]:
        """
        è¯„ä¼°å‹ç¼©æ•ˆæœ

        Args:
            original_model: åŸå§‹æ¨¡å‹
            compressed_model: å‹ç¼©åæ¨¡å‹
            test_dataloader: æµ‹è¯•æ•°æ®
            device: è®¾å¤‡

        Returns:
            è¯„ä¼°ç»“æœ
        """
        print("ğŸ“Š è¯„ä¼°å‹ç¼©æ•ˆæœ...")

        results = {}

        # 1. æ¨¡å‹å¤§å°å¯¹æ¯”
        original_size = self._get_model_size(original_model)
        compressed_size = self._get_model_size(compressed_model)

        results['size_reduction'] = {
            'original_mb': original_size,
            'compressed_mb': compressed_size,
            'reduction_ratio': compressed_size / original_size,
            'saved_mb': original_size - compressed_size
        }

        # 2. å‚æ•°é‡å¯¹æ¯”
        original_params = sum(p.numel() for p in original_model.parameters())
        compressed_params = sum(p.numel() for p in compressed_model.parameters())

        results['param_reduction'] = {
            'original': original_params,
            'compressed': compressed_params,
            'reduction_ratio': compressed_params / original_params,
            'saved': original_params - compressed_params
        }

        # 3. æ¨ç†é€Ÿåº¦å¯¹æ¯” (å¦‚æœæä¾›äº†æµ‹è¯•æ•°æ®)
        if test_dataloader is not None:
            import time

            original_model.to(device)
            compressed_model.to(device)

            # é¢„çƒ­
            with torch.no_grad():
                for i, batch in enumerate(test_dataloader):
                    if i >= 2:
                        break
                    if isinstance(batch, dict):
                        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v
                                for k, v in batch.items()}
                        original_model(**batch)
                        compressed_model(**batch)
                    else:
                        inputs = batch[0].to(device) if isinstance(batch, (list, tuple)) else batch.to(device)
                        original_model(inputs)
                        compressed_model(inputs)

            # æµ‹è¯•åŸå§‹æ¨¡å‹é€Ÿåº¦
            start_time = time.time()
            with torch.no_grad():
                for i, batch in enumerate(test_dataloader):
                    if i >= 10:
                        break
                    if isinstance(batch, dict):
                        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v
                                for k, v in batch.items()}
                        original_model(**batch)
                    else:
                        inputs = batch[0].to(device) if isinstance(batch, (list, tuple)) else batch.to(device)
                        original_model(inputs)
            original_time = time.time() - start_time

            # æµ‹è¯•å‹ç¼©æ¨¡å‹é€Ÿåº¦
            start_time = time.time()
            with torch.no_grad():
                for i, batch in enumerate(test_dataloader):
                    if i >= 10:
                        break
                    if isinstance(batch, dict):
                        batch = {k: v.to(device) if isinstance(v, torch.Tensor) else v
                                for k, v in batch.items()}
                        compressed_model(**batch)
                    else:
                        inputs = batch[0].to(device) if isinstance(batch, (list, tuple)) else batch.to(device)
                        compressed_model(inputs)
            compressed_time = time.time() - start_time

            results['inference_speed'] = {
                'original_time_sec': original_time,
                'compressed_time_sec': compressed_time,
                'speedup': original_time / compressed_time
            }

        # æ‰“å°ç»“æœ
        print("\nå‹ç¼©è¯„ä¼°ç»“æœ:")
        print(f"  å¤§å°: {original_size:.2f}MB â†’ {compressed_size:.2f}MB "
              f"({results['size_reduction']['reduction_ratio']:.2%})")
        print(f"  å‚æ•°: {original_params:,} â†’ {compressed_params:,} "
              f"({results['param_reduction']['reduction_ratio']:.2%})")

        if 'inference_speed' in results:
            print(f"  æ¨ç†é€Ÿåº¦: {results['inference_speed']['speedup']:.2f}x åŠ é€Ÿ")

        return results

    # ========================================================================
    # 8. ğŸ”® WebUI/APIå¯¼å‡ºæ¥å£
    # ========================================================================

    def export_for_webui(self, export_path: str = None) -> Dict[str, Any]:
        """
        å¯¼å‡ºå‹ç¼©æ•°æ®ä¾›WebUI/APIä½¿ç”¨

        æœªæ¥APIç«¯ç‚¹:
        - POST /api/compress/prune - å‰ªææ¨¡å‹
        - POST /api/compress/quantize - é‡åŒ–æ¨¡å‹
        - POST /api/compress/distill - çŸ¥è¯†è’¸é¦
        - POST /api/compress/full - ç»¼åˆå‹ç¼©
        - GET /api/compress/evaluate - è¯„ä¼°å‹ç¼©æ•ˆæœ

        Args:
            export_path: JSONæ–‡ä»¶å¯¼å‡ºè·¯å¾„ï¼ˆå¯é€‰ï¼‰

        Returns:
            å‹ç¼©é…ç½®å’Œç»Ÿè®¡æ•°æ®
        """
        data = {
            'plugin_info': {
                'name': self.name,
                'version': self.version
            },
            'compression_config': {
                'methods': self.compression_methods,
                'target_ratio': self.target_compression_ratio,
                'pruning': self.pruning_config,
                'quantization': self.quantization_config,
                'distillation': self.distillation_config,
                'dbc': self.dbc_config
            },
            'available_methods': [
                {
                    'name': 'pruning',
                    'description': 'æ¨¡å‹å‰ªæ - ç§»é™¤ä¸é‡è¦çš„æƒé‡',
                    'typical_compression': '30-50%',
                    'speed_impact': 'è½»å¾®æå‡'
                },
                {
                    'name': 'quantization',
                    'description': 'æ¨¡å‹é‡åŒ– - é™ä½æƒé‡ç²¾åº¦',
                    'typical_compression': '75% (INT8)',
                    'speed_impact': '2-4xåŠ é€Ÿ'
                },
                {
                    'name': 'distillation',
                    'description': 'çŸ¥è¯†è’¸é¦ - è½¬ç§»åˆ°å°æ¨¡å‹',
                    'typical_compression': '50-90%',
                    'speed_impact': 'è§†æ¨¡å‹å¤§å°'
                },
                {
                    'name': 'dbc',
                    'description': 'DBCåŠ é€Ÿè®­ç»ƒ - ç»´åº¦å¹³è¡¡å‹ç¼©',
                    'typical_compression': 'è®­ç»ƒåŠ é€Ÿ',
                    'speed_impact': 'è®­ç»ƒåŠ é€Ÿ20-30%'
                },
                {
                    'name': 'low_rank',
                    'description': 'ä½ç§©åˆ†è§£ - æƒé‡çŸ©é˜µåˆ†è§£',
                    'typical_compression': '40-60%',
                    'speed_impact': 'ä¸­ç­‰æå‡'
                }
            ],
            'generated_at': datetime.now().isoformat()
        }

        # å¯¼å‡ºåˆ°JSONæ–‡ä»¶
        if export_path:
            export_file = Path(export_path)
            export_file.parent.mkdir(parents=True, exist_ok=True)
            with open(export_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)

        return data

    def generate_compression_report(self, results: Dict[str, Any], output_path: str = None) -> str:
        """
        ç”Ÿæˆå‹ç¼©æŠ¥å‘Š (Markdownæ ¼å¼)

        Args:
            results: å‹ç¼©ç»“æœ
            output_path: æŠ¥å‘Šä¿å­˜è·¯å¾„

        Returns:
            Markdownæ ¼å¼æŠ¥å‘Š
        """
        report = []
        report.append("# æ¨¡å‹å‹ç¼©æŠ¥å‘Š")
        report.append(f"\nç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

        # å‹ç¼©æ–¹æ³•
        report.append("## åº”ç”¨çš„å‹ç¼©æ–¹æ³•\n")
        for method in results.get('methods_applied', []):
            report.append(f"- âœ… {method}")

        # å‹ç¼©æ•ˆæœ
        report.append("\n## å‹ç¼©æ•ˆæœ\n")
        report.append(f"- **åŸå§‹æ¨¡å‹å¤§å°**: {results['original_size_mb']:.2f} MB")
        report.append(f"- **å‹ç¼©åå¤§å°**: {results['final_size_mb']:.2f} MB")
        report.append(f"- **å¤§å°å‹ç¼©æ¯”**: {results['size_compression_ratio']:.2%}")
        report.append(f"- **åŸå§‹å‚æ•°é‡**: {results['original_params']:,}")
        report.append(f"- **å‹ç¼©åå‚æ•°é‡**: {results['final_params']:,}")
        report.append(f"- **å‚æ•°å‹ç¼©æ¯”**: {results['param_compression_ratio']:.2%}")

        # å„é˜¶æ®µè¯¦æƒ…
        if 'compression_stages' in results:
            report.append("\n## å‹ç¼©é˜¶æ®µè¯¦æƒ…\n")
            for stage in results['compression_stages']:
                method = stage['method']
                if 'compression_ratio' in stage:
                    report.append(f"- **{method}**: å‹ç¼©æ¯” {stage['compression_ratio']:.2%}")

        report_text = '\n'.join(report)

        # ä¿å­˜æŠ¥å‘Š
        if output_path:
            report_file = Path(output_path)
            report_file.parent.mkdir(parents=True, exist_ok=True)
            with open(report_file, 'w', encoding='utf-8') as f:
                f.write(report_text)

        return report_text


# ============================================================================
# ä½¿ç”¨ç¤ºä¾‹
# ============================================================================

if __name__ == "__main__":
    print("APTæ¨¡å‹å‹ç¼©æ’ä»¶ç¤ºä¾‹\n")

    # é…ç½®
    config = {
        'methods': ['pruning', 'quantization'],
        'compression_ratio': 0.5,
        'pruning': {
            'ratio': 0.3,
            'type': 'magnitude'
        },
        'quantization': {
            'bits': 8,
            'type': 'dynamic'
        }
    }

    plugin = CompressionPlugin(config)

    # åˆ›å»ºæµ‹è¯•æ¨¡å‹
    class SimpleModel(nn.Module):
        def __init__(self):
            super().__init__()
            self.fc1 = nn.Linear(512, 256)
            self.fc2 = nn.Linear(256, 128)
            self.fc3 = nn.Linear(128, 64)

        def forward(self, x):
            x = F.relu(self.fc1(x))
            x = F.relu(self.fc2(x))
            return self.fc3(x)

    model = SimpleModel()

    print(f"åŸå§‹æ¨¡å‹å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    print(f"åŸå§‹æ¨¡å‹å¤§å°: {plugin._get_model_size(model):.2f} MB\n")

    # ç»¼åˆå‹ç¼©
    results = plugin.compress_model(model, methods=['pruning', 'low_rank'])

    # ç”ŸæˆæŠ¥å‘Š
    report = plugin.generate_compression_report(results)
    print("\n" + report)

    # å¯¼å‡ºWebUIæ•°æ®
    webui_data = plugin.export_for_webui()
    print(f"\nğŸ”® WebUIæ•°æ®å·²å¯¼å‡º: {len(webui_data['available_methods'])} ç§å‹ç¼©æ–¹æ³•å¯ç”¨")

    print("\nâœ… æ¨¡å‹å‹ç¼©æ’ä»¶ç¤ºä¾‹å®Œæˆ!")
