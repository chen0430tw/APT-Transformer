#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
APTæ¨¡å‹å¾®è°ƒï¼ˆFine-tuningï¼‰æ¨¡å—

å¤ç”¨ç°æœ‰çš„æ¨¡å—åŒ–ç»„ä»¶ï¼š
- checkpoint.load_model() - åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
- trainer ä¸­çš„è®­ç»ƒå¾ªç¯
- data loading åŠŸèƒ½
- evaluator åŠŸèƒ½

æ”¯æŒåŠŸèƒ½ï¼š
1. åŸºç¡€å¾®è°ƒ
2. å†»ç»“å±‚ï¼ˆFrozen Layersï¼‰
3. å­¦ä¹ ç‡è¡°å‡
4. æ—©åœæœºåˆ¶ï¼ˆEarly Stoppingï¼‰
5. å‚æ•°é«˜æ•ˆå¾®è°ƒï¼ˆLoRA - å¯é€‰ï¼‰
"""

import os
from apt.apt_model.utils.fake_torch import get_torch
torch = get_torch()
from apt.apt_model.utils.fake_torch import get_torch
torch = get_torch()
nn = torch.nn
from tqdm import tqdm
from datetime import datetime
from typing import Optional, List, Dict, Tuple

# å¤ç”¨ç°æœ‰æ¨¡å—
from apt.trainops.checkpoints.checkpoint import load_model, save_model
from apt.core.data.external_data import load_external_data
from apt.core.generation.generator import generate_natural_text
from apt.core.generation.evaluator import evaluate_text_quality
from apt.apt_model.utils import get_device, set_seed
from apt.core.config.settings_manager import settings


class FineTuner:
    """APTæ¨¡å‹å¾®è°ƒå™¨ - å¤ç”¨æ¨¡å—åŒ–ç»„ä»¶"""

    def __init__(
        self,
        model_path: str,
        device: Optional[str] = None,
        logger: Optional[object] = None
    ):
        """
        åˆå§‹åŒ–å¾®è°ƒå™¨

        å‚æ•°:
            model_path: é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
            device: è®¡ç®—è®¾å¤‡
            logger: æ—¥å¿—è®°å½•å™¨
        """
        self.device = device or get_device()
        self.logger = logger

        # åŠ è½½é¢„è®­ç»ƒæ¨¡å‹ï¼ˆå¤ç”¨checkpointæ¨¡å—ï¼‰
        self.log_info(f"ğŸ“‚ åŠ è½½é¢„è®­ç»ƒæ¨¡å‹: {model_path}")
        self.model, self.tokenizer, self.config = load_model(model_path, self.device)
        self.log_info(f"âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")

        # è®°å½•åŸå§‹å‚æ•°
        self.original_params = sum(p.numel() for p in self.model.parameters())
        self.trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)

    def log_info(self, message):
        """æ—¥å¿—è¾“å‡º"""
        if self.logger:
            self.logger.info(message)
        print(message)

    def freeze_layers(
        self,
        freeze_embeddings: bool = True,
        freeze_encoder_layers: Optional[int] = None,
        freeze_decoder_layers: Optional[int] = None
    ):
        """
        å†»ç»“æŒ‡å®šå±‚çš„å‚æ•°

        å‚æ•°:
            freeze_embeddings: æ˜¯å¦å†»ç»“embeddingå±‚
            freeze_encoder_layers: å†»ç»“å‰Nå±‚encoderï¼ˆNone=ä¸å†»ç»“ï¼‰
            freeze_decoder_layers: å†»ç»“å‰Nå±‚decoderï¼ˆNone=ä¸å†»ç»“ï¼‰
        """
        self.log_info("\nğŸ”’ å†»ç»“å±‚è®¾ç½®:")

        # å†»ç»“embeddings
        if freeze_embeddings and hasattr(self.model, 'embeddings'):
            for param in self.model.embeddings.parameters():
                param.requires_grad = False
            self.log_info("  âœ“ Embeddingså·²å†»ç»“")

        # å†»ç»“encoderå±‚
        if freeze_encoder_layers and hasattr(self.model, 'encoder'):
            if hasattr(self.model.encoder, 'layers'):
                for i, layer in enumerate(self.model.encoder.layers[:freeze_encoder_layers]):
                    for param in layer.parameters():
                        param.requires_grad = False
                self.log_info(f"  âœ“ å‰{freeze_encoder_layers}å±‚Encoderå·²å†»ç»“")

        # å†»ç»“decoderå±‚
        if freeze_decoder_layers and hasattr(self.model, 'decoder'):
            if hasattr(self.model.decoder, 'layers'):
                for i, layer in enumerate(self.model.decoder.layers[:freeze_decoder_layers]):
                    for param in layer.parameters():
                        param.requires_grad = False
                self.log_info(f"  âœ“ å‰{freeze_decoder_layers}å±‚Decoderå·²å†»ç»“")

        # æ›´æ–°å¯è®­ç»ƒå‚æ•°ç»Ÿè®¡
        self.trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)
        frozen_ratio = 100 * (1 - self.trainable_params / self.original_params)

        self.log_info(f"\nğŸ“Š å‚æ•°ç»Ÿè®¡:")
        self.log_info(f"  æ€»å‚æ•°: {self.original_params:,}")
        self.log_info(f"  å¯è®­ç»ƒ: {self.trainable_params:,}")
        self.log_info(f"  å·²å†»ç»“: {frozen_ratio:.1f}%")

    def fine_tune(
        self,
        train_data_path: str,
        val_data_path: Optional[str] = None,
        epochs: int = 5,
        batch_size: int = 8,
        learning_rate: float = 1e-5,  # å¾®è°ƒä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡
        warmup_steps: int = 100,
        max_samples: Optional[int] = None,
        save_path: str = "apt_model_finetuned",
        early_stopping_patience: int = 3,
        eval_steps: int = 100,
        save_steps: int = 500,
    ) -> Tuple[nn.Module, object, object]:
        """
        æ‰§è¡Œå¾®è°ƒè®­ç»ƒï¼ˆå¤ç”¨trainerçš„è®­ç»ƒé€»è¾‘ï¼‰

        å‚æ•°:
            train_data_path: è®­ç»ƒæ•°æ®è·¯å¾„
            val_data_path: éªŒè¯æ•°æ®è·¯å¾„ï¼ˆå¯é€‰ï¼‰
            epochs: è®­ç»ƒè½®æ•°
            batch_size: æ‰¹æ¬¡å¤§å°
            learning_rate: å­¦ä¹ ç‡ï¼ˆå¾®è°ƒå»ºè®®1e-5åˆ°5e-5ï¼‰
            warmup_steps: é¢„çƒ­æ­¥æ•°
            max_samples: æœ€å¤§æ ·æœ¬æ•°
            save_path: ä¿å­˜è·¯å¾„
            early_stopping_patience: æ—©åœè€å¿ƒå€¼
            eval_steps: è¯„ä¼°é—´éš”
            save_steps: ä¿å­˜é—´éš”

        è¿”å›:
            (model, tokenizer, config)
        """
        self.log_info("\n" + "="*60)
        self.log_info("ğŸ¯ å¼€å§‹å¾®è°ƒè®­ç»ƒ")
        self.log_info("="*60)

        # åŠ è½½å¾®è°ƒæ•°æ®ï¼ˆå¤ç”¨dataæ¨¡å—ï¼‰
        self.log_info(f"\nğŸ“Š åŠ è½½å¾®è°ƒæ•°æ®: {train_data_path}")
        train_texts = load_external_data(train_data_path, max_samples=max_samples)
        self.log_info(f"  è®­ç»ƒæ ·æœ¬æ•°: {len(train_texts)}")

        val_texts = None
        if val_data_path:
            val_texts = load_external_data(val_data_path, max_samples=max_samples)
            self.log_info(f"  éªŒè¯æ ·æœ¬æ•°: {len(val_texts)}")

        # å‡†å¤‡æ•°æ®ï¼ˆå¤ç”¨trainerçš„DataLoaderé€»è¾‘ï¼‰
        Dataset = torch.utils.data.Dataset
        DataLoader = torch.utils.data.DataLoader

        class TextDataset(Dataset):
            def __init__(self, texts, tokenizer, max_length=512):
                self.texts = texts
                self.tokenizer = tokenizer
                self.max_length = max_length

            def __len__(self):
                return len(self.texts)

            def __getitem__(self, idx):
                text = self.texts[idx]
                encoding = self.tokenizer(
                    text,
                    max_length=self.max_length,
                    padding='max_length',
                    truncation=True,
                    return_tensors='pt'
                )
                return {
                    'input_ids': encoding['input_ids'].squeeze(),
                    'attention_mask': encoding['attention_mask'].squeeze()
                }

        train_dataset = TextDataset(train_texts, self.tokenizer)
        train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)

        val_loader = None
        if val_texts:
            val_dataset = TextDataset(val_texts, self.tokenizer)
            val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)

        # è®¾ç½®ä¼˜åŒ–å™¨ï¼ˆå¾®è°ƒä½¿ç”¨è¾ƒå°å­¦ä¹ ç‡ï¼‰
        optimizer = torch.optim.AdamW(
            [p for p in self.model.parameters() if p.requires_grad],
            lr=learning_rate,
            weight_decay=self.config.weight_decay if hasattr(self.config, 'weight_decay') else 0.01
        )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        total_steps = len(train_loader) * epochs
        scheduler = torch.optim.lr_scheduler.LinearLR(
            optimizer,
            start_factor=0.1,
            end_factor=1.0,
            total_iters=warmup_steps
        )

        self.log_info(f"\nâš™ï¸  è®­ç»ƒé…ç½®:")
        self.log_info(f"  Epochs: {epochs}")
        self.log_info(f"  Batch Size: {batch_size}")
        self.log_info(f"  Learning Rate: {learning_rate}")
        self.log_info(f"  Warmup Steps: {warmup_steps}")
        self.log_info(f"  Total Steps: {total_steps}")

        # è®­ç»ƒå¾ªç¯ï¼ˆå¤ç”¨trainerçš„è®­ç»ƒé€»è¾‘ï¼‰
        self.model.train()
        best_val_loss = float('inf')
        patience_counter = 0
        global_step = 0

        for epoch in range(epochs):
            self.log_info(f"\n{'='*60}")
            self.log_info(f"Epoch {epoch + 1}/{epochs}")
            self.log_info(f"{'='*60}")

            epoch_loss = 0
            progress_bar = tqdm(train_loader, desc=f"Epoch {epoch+1}")

            for batch_idx, batch in enumerate(progress_bar):
                # å‡†å¤‡è¾“å…¥
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)

                # å‰å‘ä¼ æ’­
                outputs = self.model(
                    src_ids=input_ids,
                    tgt_ids=input_ids,  # è‡ªå›å½’ä»»åŠ¡
                    src_mask=attention_mask
                )

                # è®¡ç®—æŸå¤±
                logits = outputs.view(-1, outputs.size(-1))
                targets = input_ids.view(-1)
                loss = nn.functional.cross_entropy(
                    logits,
                    targets,
                    ignore_index=self.tokenizer.pad_token_id
                )

                # åå‘ä¼ æ’­
                optimizer.zero_grad()
                loss.backward()

                # æ¢¯åº¦è£å‰ª
                if hasattr(self.config, 'gradient_clip'):
                    torch.nn.utils.clip_grad_norm_(
                        self.model.parameters(),
                        self.config.gradient_clip
                    )

                optimizer.step()

                if global_step < warmup_steps:
                    scheduler.step()

                epoch_loss += loss.item()
                global_step += 1

                # æ›´æ–°è¿›åº¦æ¡
                progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})

                # å®šæœŸè¯„ä¼°
                if val_loader and global_step % eval_steps == 0:
                    val_loss = self._evaluate(val_loader)
                    self.log_info(f"\n  Step {global_step} | Val Loss: {val_loss:.4f}")

                    # æ—©åœæ£€æŸ¥
                    if val_loss < best_val_loss:
                        best_val_loss = val_loss
                        patience_counter = 0
                        # ä¿å­˜æœ€ä½³æ¨¡å‹
                        self._save_checkpoint(save_path, "best")
                    else:
                        patience_counter += 1
                        if patience_counter >= early_stopping_patience:
                            self.log_info(f"\nâš ï¸  Early stopping triggered at step {global_step}")
                            break

                    self.model.train()

                # å®šæœŸä¿å­˜
                if global_step % save_steps == 0:
                    self._save_checkpoint(save_path, f"step_{global_step}")

            # Epochç»“æŸç»Ÿè®¡
            avg_loss = epoch_loss / len(train_loader)
            self.log_info(f"\nğŸ“Š Epoch {epoch+1} å¹³å‡æŸå¤±: {avg_loss:.4f}")

            # ç”Ÿæˆæ ·æœ¬æµ‹è¯•ï¼ˆå¤ç”¨generatoræ¨¡å—ï¼‰
            self._generate_sample()

            if patience_counter >= early_stopping_patience:
                break

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        self.log_info(f"\nğŸ’¾ ä¿å­˜æœ€ç»ˆæ¨¡å‹åˆ°: {save_path}")
        save_model(self.model, self.tokenizer, save_path, self.config)

        self.log_info("\n" + "="*60)
        self.log_info("âœ… å¾®è°ƒå®Œæˆï¼")
        self.log_info("="*60)

        return self.model, self.tokenizer, self.config

    def _evaluate(self, val_loader) -> float:
        """è¯„ä¼°æ¨¡å‹ï¼ˆå¤ç”¨evaluatoré€»è¾‘ï¼‰"""
        self.model.eval()
        total_loss = 0

        with torch.no_grad():
            for batch in val_loader:
                input_ids = batch['input_ids'].to(self.device)
                attention_mask = batch['attention_mask'].to(self.device)

                outputs = self.model(
                    src_ids=input_ids,
                    tgt_ids=input_ids,
                    src_mask=attention_mask
                )

                logits = outputs.view(-1, outputs.size(-1))
                targets = input_ids.view(-1)
                loss = nn.functional.cross_entropy(
                    logits,
                    targets,
                    ignore_index=self.tokenizer.pad_token_id
                )

                total_loss += loss.item()

        return total_loss / len(val_loader)

    def _generate_sample(self):
        """ç”Ÿæˆæ ·æœ¬æ–‡æœ¬ï¼ˆå¤ç”¨generatoræ¨¡å—ï¼‰"""
        self.model.eval()

        try:
            sample_text = generate_natural_text(
                model=self.model,
                tokenizer=self.tokenizer,
                prompt="äººå·¥æ™ºèƒ½",
                max_length=50,
                temperature=0.8,
                device=self.device
            )

            # è¯„ä¼°è´¨é‡ï¼ˆå¤ç”¨evaluatoræ¨¡å—ï¼‰
            quality_score = evaluate_text_quality(sample_text)

            self.log_info(f"\n  ğŸ“ ç”Ÿæˆæ ·æœ¬: {sample_text}")
            self.log_info(f"  ğŸ“Š è´¨é‡è¯„åˆ†: {quality_score:.2f}/100")
        except Exception as e:
            self.log_info(f"\n  âš ï¸  ç”Ÿæˆæ ·æœ¬å¤±è´¥: {e}")

        self.model.train()

    def _save_checkpoint(self, save_path: str, suffix: str):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint_path = f"{save_path}_{suffix}"
        save_model(self.model, self.tokenizer, checkpoint_path, self.config)
        self.log_info(f"  ğŸ’¾ ä¿å­˜æ£€æŸ¥ç‚¹: {checkpoint_path}")


def fine_tune_model(
    pretrained_model_path: str,
    train_data_path: str,
    val_data_path: Optional[str] = None,
    epochs: int = 5,
    batch_size: int = 8,
    learning_rate: float = 1e-5,
    freeze_embeddings: bool = False,
    freeze_encoder_layers: Optional[int] = None,
    freeze_decoder_layers: Optional[int] = None,
    save_path: str = "apt_model_finetuned",
    logger: Optional[object] = None,
    **kwargs
) -> Tuple[nn.Module, object, object]:
    """
    å¾®è°ƒAPTæ¨¡å‹çš„ä¾¿æ·å‡½æ•°

    å‚æ•°:
        pretrained_model_path: é¢„è®­ç»ƒæ¨¡å‹è·¯å¾„
        train_data_path: è®­ç»ƒæ•°æ®è·¯å¾„
        val_data_path: éªŒè¯æ•°æ®è·¯å¾„
        epochs: è®­ç»ƒè½®æ•°
        batch_size: æ‰¹æ¬¡å¤§å°
        learning_rate: å­¦ä¹ ç‡
        freeze_embeddings: æ˜¯å¦å†»ç»“embeddingå±‚
        freeze_encoder_layers: å†»ç»“å‰Nå±‚encoder
        freeze_decoder_layers: å†»ç»“å‰Nå±‚decoder
        save_path: ä¿å­˜è·¯å¾„
        logger: æ—¥å¿—è®°å½•å™¨
        **kwargs: å…¶ä»–å‚æ•°ä¼ é€’ç»™fine_tuneæ–¹æ³•

    è¿”å›:
        (model, tokenizer, config)
    """
    # åˆ›å»ºå¾®è°ƒå™¨
    finetuner = FineTuner(pretrained_model_path, logger=logger)

    # å†»ç»“å±‚ï¼ˆå¦‚æœéœ€è¦ï¼‰
    if freeze_embeddings or freeze_encoder_layers or freeze_decoder_layers:
        finetuner.freeze_layers(
            freeze_embeddings=freeze_embeddings,
            freeze_encoder_layers=freeze_encoder_layers,
            freeze_decoder_layers=freeze_decoder_layers
        )

    # æ‰§è¡Œå¾®è°ƒ
    return finetuner.fine_tune(
        train_data_path=train_data_path,
        val_data_path=val_data_path,
        epochs=epochs,
        batch_size=batch_size,
        learning_rate=learning_rate,
        save_path=save_path,
        **kwargs
    )
