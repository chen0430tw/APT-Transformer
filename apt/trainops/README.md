# APT TrainOps Domain

è®­ç»ƒè¿è¥åŸŸ - è®­ç»ƒç¼–æ’å’Œç”Ÿå‘½å‘¨æœŸç®¡ç†

## æ¦‚è¿°

`apt.trainops` æ˜¯APT 2.0æ¶æ„çš„æ ¸å¿ƒåŸŸä¹‹ä¸€ï¼Œè´Ÿè´£è®­ç»ƒç¼–æ’ã€åˆ†å¸ƒå¼è®­ç»ƒã€æ•°æ®ç®¡ç†å’Œè®­ç»ƒç”Ÿå‘½å‘¨æœŸã€‚

## ç›®å½•ç»“æ„

```
apt/trainops/
â”œâ”€â”€ engine/         # è®­ç»ƒå¼•æ“
â”œâ”€â”€ distributed/    # åˆ†å¸ƒå¼è®­ç»ƒ
â”œâ”€â”€ data/          # æ•°æ®åŠ è½½
â”œâ”€â”€ checkpoints/   # æ£€æŸ¥ç‚¹ç®¡ç†
â”œâ”€â”€ eval/          # è¯„ä¼°å’ŒéªŒè¯
â””â”€â”€ artifacts/     # è®­ç»ƒäº§ç‰©ç®¡ç†
```

## æ¨¡å—è¯´æ˜

### 1. engine/

è®­ç»ƒå¼•æ“å®ç°ï¼š

```python
from apt.trainops.engine import Trainer, Finetuner

# åˆ›å»ºè®­ç»ƒå™¨
trainer = Trainer(
    model=model,
    train_dataset=train_data,
    eval_dataset=eval_data,
    config=config
)

# å¼€å§‹è®­ç»ƒ
trainer.train()
```

åŒ…å«çš„è®­ç»ƒå™¨ï¼š
- Trainer - ä¸»è®­ç»ƒå™¨
- Finetuner - å¾®è°ƒå™¨
- PreTrainer - é¢„è®­ç»ƒå™¨
- ClaudeTrainer - Claudeé£æ ¼è®­ç»ƒ
- GPT5Trainer - GPT-5é£æ ¼è®­ç»ƒ
- O1Trainer - O1é£æ ¼è®­ç»ƒ

### 2. distributed/

åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒï¼š

```python
from apt.trainops.distributed import setup_distributed, DDPWrapper

# è®¾ç½®åˆ†å¸ƒå¼ç¯å¢ƒ
setup_distributed(backend='nccl')

# åŒ…è£…æ¨¡å‹
model = DDPWrapper(model)

# åˆ†å¸ƒå¼è®­ç»ƒ
trainer = Trainer(model=model, distributed=True)
trainer.train()
```

æ”¯æŒçš„å¹¶è¡Œç­–ç•¥ï¼š
- **DDP** - DistributedDataParallelï¼ˆæ•°æ®å¹¶è¡Œï¼‰
- **FSDP** - Fully Sharded Data Parallelï¼ˆå…¨åˆ†ç‰‡æ•°æ®å¹¶è¡Œï¼‰
- **Pipeline Parallel** - æµæ°´çº¿å¹¶è¡Œ
- **Tensor Parallel** - å¼ é‡å¹¶è¡Œ
- **Expert Parallel** - ä¸“å®¶å¹¶è¡Œï¼ˆfor MoEï¼‰
- **Sequence Parallel** - åºåˆ—å¹¶è¡Œ

### 3. data/

æ•°æ®åŠ è½½å’Œé¢„å¤„ç†ï¼š

```python
from apt.trainops.data import APTDataLoader, create_dataloader

# åˆ›å»ºæ•°æ®åŠ è½½å™¨
dataloader = create_dataloader(
    dataset=dataset,
    batch_size=32,
    num_workers=4,
    shuffle=True
)

# è¿­ä»£æ•°æ®
for batch in dataloader:
    # è®­ç»ƒé€»è¾‘...
```

åŠŸèƒ½ï¼š
- æ•°æ®åŠ è½½å™¨
- æ•°æ®é¢„å¤„ç†
- æ•°æ®å¢å¼º
- æ•°æ®é›†å®ç°

### 4. checkpoints/

æ£€æŸ¥ç‚¹ç®¡ç†ï¼š

```python
from apt.trainops.checkpoints import CheckpointManager

# åˆ›å»ºæ£€æŸ¥ç‚¹ç®¡ç†å™¨
checkpoint_manager = CheckpointManager(
    save_dir='checkpoints/',
    save_interval=500,
    keep_last_n=5
)

# ä¿å­˜æ£€æŸ¥ç‚¹
checkpoint_manager.save(
    model=model,
    optimizer=optimizer,
    step=1000
)

# åŠ è½½æ£€æŸ¥ç‚¹
state = checkpoint_manager.load('checkpoint-1000')
```

åŠŸèƒ½ï¼š
- æ£€æŸ¥ç‚¹ä¿å­˜/åŠ è½½
- æ–­ç‚¹ç»­è®­
- æ£€æŸ¥ç‚¹ç‰ˆæœ¬ç®¡ç†
- åˆ†å¸ƒå¼æ£€æŸ¥ç‚¹
- å¼‚æ­¥ä¿å­˜

### 5. eval/

è¯„ä¼°å’ŒéªŒè¯ï¼š

```python
from apt.trainops.eval import Evaluator, compute_metrics

# åˆ›å»ºè¯„ä¼°å™¨
evaluator = Evaluator(
    model=model,
    eval_dataset=eval_data,
    metrics=['accuracy', 'perplexity']
)

# è¿è¡Œè¯„ä¼°
results = evaluator.evaluate()
print(results)  # {'accuracy': 0.95, 'perplexity': 12.3}
```

åŠŸèƒ½ï¼š
- è¯„ä¼°å¾ªç¯
- æŒ‡æ ‡è®¡ç®—
- åŸºå‡†æµ‹è¯•
- æ€§èƒ½ç›‘æ§

### 6. artifacts/

è®­ç»ƒäº§ç‰©ç®¡ç†ï¼š

```python
from apt.trainops.artifacts import ArtifactManager

# åˆ›å»ºäº§ç‰©ç®¡ç†å™¨
artifact_manager = ArtifactManager(
    output_dir='outputs/',
    experiment_name='my_experiment'
)

# ä¿å­˜äº§ç‰©
artifact_manager.save_model(model)
artifact_manager.save_metrics(metrics)
artifact_manager.save_logs(logs)
```

ç®¡ç†çš„äº§ç‰©ï¼š
- è®­ç»ƒæ¨¡å‹
- è®­ç»ƒæ—¥å¿—
- æŒ‡æ ‡æ•°æ®
- å®éªŒé…ç½®
- ä¸­é—´ç»“æœ

## ä½¿ç”¨ç¤ºä¾‹

### åŸºç¡€è®­ç»ƒ

```python
from apt.model.architectures import APTLargeModel
from apt.trainops.engine import Trainer
from apt.trainops.data import create_dataloader

# åˆ›å»ºæ¨¡å‹
model = APTLargeModel()

# å‡†å¤‡æ•°æ®
train_loader = create_dataloader(train_dataset, batch_size=32)
eval_loader = create_dataloader(eval_dataset, batch_size=32)

# åˆ›å»ºè®­ç»ƒå™¨
trainer = Trainer(
    model=model,
    train_dataloader=train_loader,
    eval_dataloader=eval_loader,
    max_steps=10000,
    learning_rate=3e-5
)

# å¼€å§‹è®­ç»ƒ
trainer.train()
```

### åˆ†å¸ƒå¼è®­ç»ƒ

```python
from apt.model.architectures import APTLargeModel
from apt.trainops.engine import Trainer
from apt.trainops.distributed import setup_distributed

# è®¾ç½®åˆ†å¸ƒå¼ç¯å¢ƒ
setup_distributed(
    backend='nccl',
    world_size=8,
    rank=int(os.environ['RANK'])
)

# åˆ›å»ºæ¨¡å‹
model = APTLargeModel()

# åˆ†å¸ƒå¼è®­ç»ƒ
trainer = Trainer(
    model=model,
    train_dataloader=train_loader,
    distributed_config={
        'strategy': 'fsdp',
        'world_size': 8
    }
)

trainer.train()
```

### ä½¿ç”¨é…ç½®æ–‡ä»¶

```python
from apt.core.config import load_profile
from apt.trainops.engine import Trainer

# åŠ è½½é…ç½®
config = load_profile('standard')

# ä»é…ç½®åˆ›å»ºè®­ç»ƒå™¨
trainer = Trainer.from_config(config)

# è®­ç»ƒ
trainer.train()
```

### å¾®è°ƒ

```python
from apt.model.architectures import APTLargeModel
from apt.trainops.engine import Finetuner

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model = APTLargeModel.from_pretrained('apt-base')

# åˆ›å»ºå¾®è°ƒå™¨
finetuner = Finetuner(
    model=model,
    train_dataset=finetune_data,
    learning_rate=1e-5,
    num_epochs=3
)

# å¾®è°ƒ
finetuner.finetune()

# ä¿å­˜å¾®è°ƒåçš„æ¨¡å‹
finetuner.save_model('my_finetuned_model')
```

### æ–­ç‚¹ç»­è®­

```python
from apt.trainops.engine import Trainer
from apt.trainops.checkpoints import CheckpointManager

# åŠ è½½æ£€æŸ¥ç‚¹
checkpoint_manager = CheckpointManager('checkpoints/')
state = checkpoint_manager.load_latest()

# æ¢å¤è®­ç»ƒ
trainer = Trainer(
    model=model,
    resume_from_checkpoint=state
)

trainer.train()
```

## ä¸modelçš„å…³ç³»

- **apt.model** - å®šä¹‰"what"ï¼ˆæ¨¡å‹æ˜¯ä»€ä¹ˆï¼‰
- **apt.trainops** - å®šä¹‰"how"ï¼ˆå¦‚ä½•è®­ç»ƒæ¨¡å‹ï¼‰

æ¸…æ™°çš„èŒè´£åˆ†ç¦»ï¼š

```python
# model: å®šä¹‰æ¶æ„
from apt.model.architectures import APTLargeModel
model = APTLargeModel(hidden_size=2048, num_layers=32)

# trainops: è®­ç»ƒæ¨¡å‹
from apt.trainops.engine import Trainer
trainer = Trainer(model=model)
trainer.train()
```

## é…ç½®é©±åŠ¨è®­ç»ƒ

ä½¿ç”¨profileé…ç½®æ•´ä¸ªè®­ç»ƒæµç¨‹ï¼š

```yaml
# profiles/my_training.yaml
training:
  batch_size: 32
  learning_rate: 3e-5
  max_steps: 10000

  distributed:
    enabled: true
    strategy: fsdp
    world_size: 8

  checkpoints:
    save_interval: 500
    keep_last_n: 5
```

```python
from apt.core.config import load_profile
from apt.trainops.engine import Trainer

config = load_profile('my_training')
trainer = Trainer.from_config(config)
trainer.train()
```

## åˆ†å¸ƒå¼è®­ç»ƒç­–ç•¥é€‰æ‹©

æ ¹æ®æ¨¡å‹å¤§å°å’Œèµ„æºé€‰æ‹©åˆé€‚çš„ç­–ç•¥ï¼š

| æ¨¡å‹å¤§å° | GPUæ•°é‡ | æ¨èç­–ç•¥ |
|---------|--------|---------|
| < 1B | 1-4 | DDP |
| 1B-7B | 4-16 | FSDP |
| 7B-30B | 16-64 | FSDP + Pipeline |
| 30B-100B | 64-256 | FSDP + Pipeline + Tensor |
| > 100B | 256+ | å…¨å¹¶è¡Œï¼ˆFSDP+PP+TP+EPï¼‰ |

ç¤ºä¾‹é…ç½®ï¼š

```python
# å°æ¨¡å‹ (< 1B)
trainer = Trainer(
    model=model,
    distributed_config={'strategy': 'ddp'}
)

# ä¸­æ¨¡å‹ (1B-7B)
trainer = Trainer(
    model=model,
    distributed_config={'strategy': 'fsdp'}
)

# å¤§æ¨¡å‹ (> 30B)
trainer = Trainer(
    model=model,
    distributed_config={
        'strategy': 'hybrid',
        'pipeline_parallel': 4,
        'tensor_parallel': 4,
        'data_parallel': 4
    }
)
```

## è¿ç§»çŠ¶æ€

ğŸš§ **å½“å‰çŠ¶æ€**: Skeletonå·²åˆ›å»ºï¼Œå†…å®¹å°†åœ¨PR-3ä¸­è¿ç§»

è¿ç§»è®¡åˆ’ï¼š
- [ ] PR-3: ä»apt.apt_model.trainingè¿ç§»æ‰€æœ‰è®­ç»ƒå™¨
- [ ] PR-3: ä»apt.core.dataè¿ç§»æ•°æ®å¤„ç†
- [ ] PR-3: æ•´åˆåˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
- [ ] PR-5: å®Œå–„compatå±‚é‡å¯¼å‡º

## è®­ç»ƒç”Ÿå‘½å‘¨æœŸ

å®Œæ•´çš„è®­ç»ƒç”Ÿå‘½å‘¨æœŸï¼š

```
1. åˆå§‹åŒ–
   â”œâ”€â”€ åŠ è½½é…ç½®
   â”œâ”€â”€ åˆ›å»ºæ¨¡å‹
   â”œâ”€â”€ å‡†å¤‡æ•°æ®
   â””â”€â”€ è®¾ç½®åˆ†å¸ƒå¼ç¯å¢ƒ

2. è®­ç»ƒå¾ªç¯
   â”œâ”€â”€ å‰å‘ä¼ æ’­
   â”œâ”€â”€ è®¡ç®—æŸå¤±
   â”œâ”€â”€ åå‘ä¼ æ’­
   â”œâ”€â”€ æ›´æ–°å‚æ•°
   â”œâ”€â”€ è¯„ä¼°ï¼ˆå¯é€‰ï¼‰
   â””â”€â”€ ä¿å­˜æ£€æŸ¥ç‚¹

3. ç»“æŸ
   â”œâ”€â”€ æœ€ç»ˆè¯„ä¼°
   â”œâ”€â”€ ä¿å­˜æ¨¡å‹
   â””â”€â”€ æ¸…ç†èµ„æº
```

ç”±TrainOpsç»Ÿä¸€ç®¡ç†æ•´ä¸ªç”Ÿå‘½å‘¨æœŸã€‚

## ç›‘æ§å’Œæ—¥å¿—

é›†æˆå¤šç§ç›‘æ§å·¥å…·ï¼š

```python
from apt.trainops.engine import Trainer

trainer = Trainer(
    model=model,
    monitoring={
        'tensorboard': True,
        'wandb': True,
        'mlflow': True
    }
)

trainer.train()  # è‡ªåŠ¨è®°å½•åˆ°æ‰€æœ‰ç›‘æ§ç³»ç»Ÿ
```

## æœ€ä½³å®è·µ

1. **ä½¿ç”¨é…ç½®æ–‡ä»¶** - ä¸è¦ç¡¬ç¼–ç å‚æ•°
2. **å¯ç”¨æ£€æŸ¥ç‚¹** - å®šæœŸä¿å­˜ï¼Œé¿å…è®­ç»ƒä¸­æ–­æŸå¤±
3. **ç›‘æ§æŒ‡æ ‡** - ä½¿ç”¨TensorBoard/W&Bè¿½è¸ªè®­ç»ƒ
4. **æ¢¯åº¦è£å‰ª** - é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸
5. **æ··åˆç²¾åº¦** - ä½¿ç”¨bf16/fp16åŠ é€Ÿè®­ç»ƒ
6. **åˆ†å¸ƒå¼è®­ç»ƒ** - å¤§æ¨¡å‹å¿…é¡»ç”¨FSDPæˆ–æ›´é«˜çº§ç­–ç•¥

## æ•…éšœæ¢å¤

TrainOpsè‡ªåŠ¨å¤„ç†æ•…éšœæ¢å¤ï¼š

```python
trainer = Trainer(
    model=model,
    auto_resume=True,  # è‡ªåŠ¨ä»æœ€æ–°æ£€æŸ¥ç‚¹æ¢å¤
    checkpoint_dir='checkpoints/'
)

# è®­ç»ƒä¸­æ–­åé‡æ–°è¿è¡Œï¼Œè‡ªåŠ¨æ¢å¤
trainer.train()
```

## APIæ–‡æ¡£

è¯¦ç»†APIæ–‡æ¡£ï¼šhttps://apt-transformer.readthedocs.io/trainops/

## æµ‹è¯•

```bash
# æµ‹è¯•è®­ç»ƒæ¨¡å—
pytest apt/trainops/tests/

# æµ‹è¯•åˆ†å¸ƒå¼è®­ç»ƒï¼ˆéœ€è¦å¤šGPUï¼‰
torchrun --nproc_per_node=4 apt/trainops/tests/test_distributed.py
```

## ç›¸å…³é“¾æ¥

- [Model Domain](../model/README.md) - æ¨¡å‹åŸŸ
- [vGPU Domain](../vgpu/README.md) - è™šæ‹ŸGPUåŸŸ
- [Configuration Profiles](../../profiles/README.md)
- [Distributed Training Guide](../../docs/guides/distributed_training.md)

---

**Version**: 2.0.0-alpha
**Status**: Skeleton (å†…å®¹è¿ç§»ä¸­)
**Last Updated**: 2026-01-22
