#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CompAct éšæœºæŠ•å½±æ ¸ - ä½¿ç”¨ç¤ºä¾‹
=============================

æ¼”ç¤ºå¦‚ä½•åœ¨ Virtual Blackwell è®­ç»ƒä¸­é›†æˆéšæœºæŠ•å½±æ ¸

ä½œè€…ï¼šGPT-5.2 R2
ç‰ˆæœ¬ï¼š1.0.0
"""

import sys
import io

# Windows æ§åˆ¶å°ç¼–ç ä¿®å¤
if sys.platform == 'win32':
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

import torch
import torch.nn as nn
from random_projection_kernel import (
    RandomProjectionKernel,
    ProjectionKernelConfig,
    CompActLinear,
    compact_act_forward,
    estimate_memory,
)


def demo_memory_estimation():
    """æ¼”ç¤ºå†…å­˜ä¼°ç®—"""
    print("=" * 70)
    print("ğŸ’¾ éšæœºæŠ•å½±æ ¸å†…å­˜ä¼°ç®—")
    print("=" * 70)
    print()

    models = [
        ("LLaMA-7B", 32, 4096),
        ("LLaMA-13B", 40, 5120),
        ("LLaMA-30B", 60, 6656),
        ("LLaMA-65B", 80, 8192),
    ]

    ranks = [128, 256, 512, 1024]

    print(f"{'æ¨¡å‹':<15} {'å±‚æ•°':<6} {'hidden':<8} {'rank=128':<12} {'rank=256':<12} {'rank=512':<12}")
    print("-" * 70)

    for name, layers, hidden in models:
        row = f"{name:<15} {layers:<6} {hidden:<8}"
        for rank in ranks:
            mem = estimate_memory(layers, hidden, rank)
            row += f"{mem['total_gb']:>6.2f} GB  "
        print(row)

    print()

    # å¯¹æ¯”ï¼šåŸ CompAct vs æˆ‘ä»¬çš„ä¼˜åŒ–
    print("å¯¹æ¯”ï¼šåŸ CompAct vs éšæœºæŠ•å½±æ ¸ä¼˜åŒ–")
    print()
    print("  åŸ CompAct:")
    print("    - ä¸å­˜å‚¨ P")
    print("    - æ¯æ¬¡åå‘ä¼ æ’­é‡æ–°ç”Ÿæˆï¼ˆæ…¢ï¼‰")
    print("    - å†…å­˜: 0 MBï¼ˆä½†è®¡ç®—å¼€é”€å¤§ï¼‰")
    print()
    print("  éšæœºæŠ•å½±æ ¸ï¼ˆæœ¬å®ç°ï¼‰:")
    print("    - å­˜å‚¨ P^T")
    print("    - åå‘ä¼ æ’­ç›´æ¥ä½¿ç”¨ï¼ˆå¿«ï¼‰")
    print(f"    - å†…å­˜: LLaMA-65B (rank=512) â†’ {estimate_memory(80, 8192, 512)['total_gb']:.2f} GB")
    print()


def demo_basic_usage():
    """æ¼”ç¤ºåŸºæœ¬ä½¿ç”¨"""
    print("=" * 70)
    print("ğŸš€ åŸºæœ¬ä½¿ç”¨ç¤ºä¾‹")
    print("=" * 70)
    print()

    # åˆ›å»ºé…ç½®
    config = ProjectionKernelConfig(
        rank=512,
        distribution="gaussian",
        seed_mode="per_layer",
        device="cuda" if torch.cuda.is_available() else "cpu",
    )

    # åˆ›å»ºæ ¸ç®¡ç†å™¨
    kernel_mgr = RandomProjectionKernel(config, global_seed=42)

    # æ³¨å†Œå±‚
    hidden_dim = 4096
    for i in range(32):  # LLaMA-7B æœ‰ 32 å±‚
        layer_id = f"transformer.h.{i}"
        P = kernel_mgr.register_layer(layer_id, n=hidden_dim)

        if i == 0:
            print(f"æ³¨å†Œå±‚ {layer_id}:")
            print(f"  æŠ•å½±çŸ©é˜µ P å½¢çŠ¶: {P.shape}")
            print(f"  ä¼´éšçŸ©é˜µ P^T å½¢çŠ¶: {kernel_mgr.get_adjoint(layer_id).shape}")
            print()

    # å‰å‘ä¼ æ’­
    B, T = 2, 128
    x = torch.randn(B, T, hidden_dim, device=config.device)

    print(f"è¾“å…¥æ¿€æ´»: {x.shape}")
    z = compact_act_forward(x, "transformer.h.0", kernel_mgr)
    print(f"å‹ç¼©æ¿€æ´»: {z.shape}")
    print(f"å‹ç¼©æ¯”: {hidden_dim / config.rank:.1f}x")
    print()


def demo_global_kernel():
    """æ¼”ç¤ºå…¨å±€æ ¸ä¼˜åŒ–"""
    print("=" * 70)
    print("ğŸŒ å…¨å±€æ ¸ä¼˜åŒ–ï¼ˆæè‡´èŠ‚çœå†…å­˜ï¼‰")
    print("=" * 70)
    print()

    config = ProjectionKernelConfig(
        rank=512,
        seed_mode="global",
        global_transform="roll",  # å¾ªç¯ç§»ä½å˜æ¢
        device="cuda" if torch.cuda.is_available() else "cpu",
    )

    kernel_mgr = RandomProjectionKernel(config, global_seed=42)

    # åˆå§‹åŒ–å…¨å±€æ ¸
    hidden_dim = 8192
    rank = 512
    kernel_mgr.init_global_kernel(n=hidden_dim, r=rank)

    print(f"å…¨å±€æ ¸å½¢çŠ¶: {kernel_mgr.global_kernel.shape}")
    print(f"å…¨å±€æ ¸å†…å­˜: {rank * hidden_dim * 4 / (1024**2):.2f} MB")
    print()

    # æ³¨å†Œå¤šå±‚ï¼ˆéƒ½ä»å…¨å±€æ ¸æ´¾ç”Ÿï¼‰
    num_layers = 80
    for i in range(num_layers):
        layer_id = f"layer.{i}"
        kernel_mgr.register_layer(layer_id, n=hidden_dim)

    print(f"æ³¨å†Œ {num_layers} å±‚å:")
    print(f"  æ€»å†…å­˜: {kernel_mgr.memory_usage_mb():.2f} MB")
    print(f"  å¯¹æ¯”ï¼ˆç‹¬ç«‹æ ¸ï¼‰: {num_layers * rank * hidden_dim * 4 / (1024**2):.2f} MB")
    print(f"  èŠ‚çœ: {100 * (1 - kernel_mgr.memory_usage_mb() / (num_layers * rank * hidden_dim * 4 / (1024**2))):.1f}%")
    print()


def demo_linear_layer():
    """æ¼”ç¤ºé›†æˆåˆ° nn.Linear"""
    print("=" * 70)
    print("ğŸ”§ é›†æˆåˆ° nn.Linear")
    print("=" * 70)
    print()

    config = ProjectionKernelConfig(
        rank=256,
        device="cuda" if torch.cuda.is_available() else "cpu",
    )
    kernel_mgr = RandomProjectionKernel(config)

    # åˆ›å»º CompAct çº¿æ€§å±‚
    layer = CompActLinear(
        in_features=4096,
        out_features=4096,
        kernel_manager=kernel_mgr,
        layer_id="test_layer",
        device=config.device,
    )

    print(f"CompAct çº¿æ€§å±‚:")
    print(f"  è¾“å…¥ç»´åº¦: {layer.in_features}")
    print(f"  è¾“å‡ºç»´åº¦: {layer.out_features}")
    print(f"  å‹ç¼©ç§©: {config.rank}")
    print()

    # å‰å‘ä¼ æ’­
    x = torch.randn(2, 128, 4096, device=config.device)
    layer.train()
    y = layer(x)

    print(f"è¾“å…¥: {x.shape}")
    print(f"è¾“å‡º: {y.shape}")
    print()


def demo_gradient_flow():
    """æ¼”ç¤ºæ¢¯åº¦æµ"""
    print("=" * 70)
    print("ğŸ“Š æ¢¯åº¦æµéªŒè¯")
    print("=" * 70)
    print()

    config = ProjectionKernelConfig(
        rank=128,
        device="cuda" if torch.cuda.is_available() else "cpu",
    )
    kernel_mgr = RandomProjectionKernel(config)

    # æ³¨å†Œå±‚
    layer_id = "test_layer"
    hidden_dim = 1024
    kernel_mgr.register_layer(layer_id, n=hidden_dim)

    # å‰å‘ä¼ æ’­
    x = torch.randn(1, 16, hidden_dim, device=config.device, requires_grad=True)
    z = compact_act_forward(x, layer_id, kernel_mgr)

    # æŸå¤±
    loss = z.pow(2).mean()

    # åå‘ä¼ æ’­
    loss.backward()

    print(f"æ¢¯åº¦æ£€æŸ¥:")
    print(f"  è¾“å…¥æ¢¯åº¦å½¢çŠ¶: {x.grad.shape}")
    print(f"  è¾“å…¥æ¢¯åº¦èŒƒæ•°: {x.grad.norm():.6f}")
    print(f"  æ¢¯åº¦æ˜¯å¦ä¸º NaN: {torch.isnan(x.grad).any()}")
    print(f"  æ¢¯åº¦æ˜¯å¦ä¸º Inf: {torch.isinf(x.grad).any()}")
    print()

    # éªŒè¯ä¼´éšçŸ©é˜µçš„ä½¿ç”¨
    P = kernel_mgr.get_projection(layer_id)
    P_adjoint = kernel_mgr.get_adjoint(layer_id)

    print(f"æŠ•å½±æ ¸éªŒè¯:")
    print(f"  P å½¢çŠ¶: {P.shape}")
    print(f"  P^T å½¢çŠ¶: {P_adjoint.shape}")
    print(f"  P^T æ˜¯å¦ç­‰äº P.T: {torch.allclose(P_adjoint, P.T)}")
    print()


def demo_comparison_with_compact():
    """å¯¹æ¯”åŸ CompAct æ–¹æ³•"""
    print("=" * 70)
    print("âš¡ æ€§èƒ½å¯¹æ¯”ï¼šåŸ CompAct vs éšæœºæŠ•å½±æ ¸")
    print("=" * 70)
    print()

    import time

    config = ProjectionKernelConfig(
        rank=512,
        device="cuda" if torch.cuda.is_available() else "cpu",
    )
    kernel_mgr = RandomProjectionKernel(config)
    hidden_dim = 8192
    layer_id = "benchmark"

    # æ³¨å†Œå±‚
    kernel_mgr.register_layer(layer_id, n=hidden_dim)

    # æµ‹è¯•æ•°æ®
    B, T = 4, 256
    x = torch.randn(B, T, hidden_dim, device=config.device)

    # æš–èº«
    for _ in range(10):
        z = compact_act_forward(x, layer_id, kernel_mgr)

    # æµ‹è¯•å‰å‘ä¼ æ’­
    torch.cuda.synchronize() if torch.cuda.is_available() else None
    start = time.time()
    for _ in range(100):
        z = compact_act_forward(x, layer_id, kernel_mgr)
    torch.cuda.synchronize() if torch.cuda.is_available() else None
    forward_time = time.time() - start

    # æµ‹è¯•åå‘ä¼ æ’­
    grad_output = torch.randn_like(z)
    torch.cuda.synchronize() if torch.cuda.is_available() else None
    start = time.time()
    for _ in range(100):
        grad_input = torch.autograd.grad(z, x, grad_output, retain_graph=True)[0]
    torch.cuda.synchronize() if torch.cuda.is_available() else None
    backward_time = time.time() - start

    print(f"100 æ¬¡è¿­ä»£æ—¶é—´:")
    print(f"  å‰å‘ä¼ æ’­: {forward_time*1000:.2f} ms")
    print(f"  åå‘ä¼ æ’­: {backward_time*1000:.2f} ms")
    print(f"  æ€»è®¡: {(forward_time+backward_time)*1000:.2f} ms")
    print()

    print("ä¼˜åŠ¿åˆ†æ:")
    print("  âœ“ åå‘ä¼ æ’­ä¸éœ€è¦é‡æ–°ç”ŸæˆéšæœºçŸ©é˜µ")
    print("  âœ“ ç›´æ¥ä½¿ç”¨å­˜å‚¨çš„ä¼´éšçŸ©é˜µ P^T")
    print("  âœ“ é¿å…äº†éšæœºæ•°ç”Ÿæˆå¼€é”€")
    print()


def main():
    print("""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘            ğŸ¯ éšæœºæŠ•å½±æ ¸ (Random Projection Kernel) ğŸ¯          â•‘
â•‘                     CompAct ä¼˜åŒ–å®ç° v1.0                       â•‘
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£
â•‘  æ ¸å¿ƒï¼šå­˜å‚¨ä¼´éšçŸ©é˜µ P^Tï¼Œåå‘ä¼ æ’­ç›´æ¥ä½¿ç”¨                       â•‘
â•‘  ä¼˜åŠ¿ï¼šé¿å…é‡æ–°ç”ŸæˆéšæœºçŸ©é˜µï¼Œæå‡è®­ç»ƒé€Ÿåº¦                      â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    """)

    demos = [
        ("å†…å­˜ä¼°ç®—", demo_memory_estimation),
        ("åŸºæœ¬ä½¿ç”¨", demo_basic_usage),
        ("å…¨å±€æ ¸ä¼˜åŒ–", demo_global_kernel),
        ("é›†æˆ nn.Linear", demo_linear_layer),
        ("æ¢¯åº¦æµéªŒè¯", demo_gradient_flow),
        ("æ€§èƒ½å¯¹æ¯”", demo_comparison_with_compact),
    ]

    while True:
        print("\nğŸ¯ é€‰æ‹©æ¼”ç¤º:")
        for i, (name, _) in enumerate(demos, 1):
            print(f"  {i}. {name}")
        print("  0. é€€å‡º")
        print()

        choice = input("è¯·é€‰æ‹© (0-6): ").strip()

        if choice == "0":
            print("\nğŸ‘‹ æ­å–œï¼ä½ å·²æŒæ¡éšæœºæŠ•å½±æ ¸ä¼˜åŒ–ï¼")
            break
        elif choice.isdigit() and 1 <= int(choice) <= len(demos):
            _, func = demos[int(choice) - 1]
            func()
        else:
            print("âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡è¯•")


if __name__ == "__main__":
    main()
