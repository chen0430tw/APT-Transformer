"""
è™šæ‹ŸBlackwellå…¨å±€å¯ç”¨å™¨ï¼ˆå¢å¼ºç‰ˆï¼‰

ä¸€è¡Œä»£ç å¯ç”¨è™šæ‹ŸBlackwellä¼˜åŒ–ï¼ˆæ”¯æŒGPU/NPU/CPU + 100K GPUé›†ç¾¤ï¼‰ï¼š
    import apt.perf.optimization.vb_global as vb
    vb.enable()

æ–°ç‰¹æ€§ (2026-01-21):
âœ¨ MXFP4 é‡åŒ–: 4xæ¨ç†åŠ é€Ÿ + 4xæ˜¾å­˜èŠ‚çœ
âœ¨ GPUä¼˜åŒ–MoE: Token Dispatch + è´Ÿè½½å‡è¡¡
âœ¨ 100K GPUæ”¯æŒ: 3D Parallelism + DeepSpeed ZeRO + Megatron-LM
âœ¨ NVLink 5 + GB200 NVL72: 72 GPUs per rack

æ‰€æœ‰åç»­åˆ›å»ºçš„APTæ¨¡å‹éƒ½ä¼šè‡ªåŠ¨åº”ç”¨VGPUä¼˜åŒ–ã€‚
æ”¯æŒè®¾å¤‡ï¼šNVIDIA CUDA GPUã€åä¸ºæ˜‡è…¾NPUã€CPU
"""

import torch
import torch.nn as nn
from typing import Optional, Dict
import os
import logging

logger = logging.getLogger(__name__)

# è™šæ‹ŸBlackwellç»„ä»¶
from apt.vgpu.runtime.vgpu_stack import VGPUStack, create_vgpu_stack
from apt.vgpu.scheduler.vgpu_estimator import VGPUResourceEstimator, ModelConfig
from apt.perf.optimization.npu_backend import (
    get_accelerator_type,
    is_cuda_available,
    is_npu_available,
    is_hpu_available,
    is_xpu_available
)

# æ–°å¢ï¼šMXFP4 é‡åŒ–
try:
    from apt.perf.optimization.mxfp4_quantization import (
        MXFP4Quantizer,
        MXFP4Config,
        convert_model_to_mxfp4
    )
    _mxfp4_available = True
except ImportError as e:
    logger.warning(f"MXFP4 é‡åŒ–æ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    _mxfp4_available = False

# æ–°å¢ï¼šGPUä¼˜åŒ–MoE
try:
    from apt.model.layers.moe_optimized import (
        MoELayerOptimized,
        MoELayerFast,
        MoEConfig
    )
    _moe_optimized_available = True
except ImportError as e:
    logger.warning(f"GPUä¼˜åŒ–MoEæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    _moe_optimized_available = False

# æ–°å¢ï¼šè¶…å¤§è§„æ¨¡è®­ç»ƒ
try:
    from apt.perf.optimization.extreme_scale_training import (
        ExtremeScaleConfig,
        ExtremeScaleTrainer,
        setup_extreme_scale_training
    )
    _extreme_scale_available = True
except ImportError as e:
    logger.warning(f"è¶…å¤§è§„æ¨¡è®­ç»ƒæ¨¡å—å¯¼å…¥å¤±è´¥: {e}")
    _extreme_scale_available = False

# å…¨å±€çŠ¶æ€
_vb_enabled = False
_vb_stack = None
_vb_config = {
    # åŸæœ‰é…ç½®
    'use_fp4': False,
    'use_flash_attn': False,
    'mixed_precision': False,
    'gradient_checkpointing': False,
    'auto_estimate': True,
    'verbose': True,

    # æ–°å¢é…ç½®
    'use_mxfp4': False,  # MXFP4 é‡åŒ–ï¼ˆä¼˜å…ˆçº§é«˜äºFP4ï¼‰
    'use_moe_optimized': False,  # GPUä¼˜åŒ–MoE
    'enable_extreme_scale': False,  # 100K GPUæ”¯æŒ

    # è¶…å¤§è§„æ¨¡è®­ç»ƒé…ç½®
    'extreme_scale_config': None,  # ExtremeScaleConfigå®ä¾‹
}

# è¶…å¤§è§„æ¨¡è®­ç»ƒå™¨å®ä¾‹
_extreme_scale_trainer = None


def enable(use_fp4: bool = False,
          use_flash_attn: bool = False,
          mixed_precision: bool = False,
          gradient_checkpointing: bool = False,
          auto_estimate: bool = True,
          vgpu_config: Optional[Dict] = None,
          verbose: bool = True,
          # æ–°å¢å‚æ•°
          use_mxfp4: bool = False,
          mxfp4_block_size: int = 32,
          use_moe_optimized: bool = False,
          moe_num_experts: int = 8,
          moe_top_k: int = 2,
          enable_extreme_scale: bool = False,
          extreme_scale_total_gpus: int = 100000):
    """
    å…¨å±€å¯ç”¨è™šæ‹ŸBlackwellä¼˜åŒ–ï¼ˆå¢å¼ºç‰ˆï¼‰

    Args:
        use_fp4: å¯ç”¨FP4é‡åŒ–
        use_flash_attn: å¯ç”¨Flash Attention
        mixed_precision: å¯ç”¨æ··åˆç²¾åº¦
        gradient_checkpointing: å¯ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
        auto_estimate: è‡ªåŠ¨ä¼°ç®—èµ„æºéœ€æ±‚
        vgpu_config: è‡ªå®šä¹‰VGPUé…ç½®ï¼ˆNone=ä½¿ç”¨é»˜è®¤ï¼‰
        verbose: æ‰“å°è¯¦ç»†ä¿¡æ¯

        # æ–°å¢å‚æ•°
        use_mxfp4: å¯ç”¨MXFP4é‡åŒ–ï¼ˆ4-bitï¼Œä¼˜å…ˆçº§é«˜äºFP4ï¼‰
        mxfp4_block_size: MXFP4å—å¤§å°ï¼ˆé»˜è®¤32ï¼‰
        use_moe_optimized: å¯ç”¨GPUä¼˜åŒ–MoEå±‚
        moe_num_experts: MoEä¸“å®¶æ•°é‡ï¼ˆé»˜è®¤8ï¼‰
        moe_top_k: MoEæ¿€æ´»ä¸“å®¶æ•°ï¼ˆé»˜è®¤2ï¼‰
        enable_extreme_scale: å¯ç”¨100K GPUå¤§è§„æ¨¡è®­ç»ƒæ”¯æŒ
        extreme_scale_total_gpus: é›†ç¾¤æ€»GPUæ•°ï¼ˆé»˜è®¤100,000ï¼‰

    Example:
        >>> import apt.perf.optimization.vb_global as vb
        >>> # åŸºç¡€ä½¿ç”¨
        >>> vb.enable(use_fp4=True, use_flash_attn=True)
        >>>
        >>> # å¯ç”¨MXFP4ï¼ˆæ›´å¿«ï¼‰
        >>> vb.enable(use_mxfp4=True)
        >>>
        >>> # å¯ç”¨100K GPUæ”¯æŒ
        >>> vb.enable_extreme_scale_mode(total_gpus=100000)
        >>>
        >>> # ä¹‹åæ‰€æœ‰APTæ¨¡å‹éƒ½ä¼šè‡ªåŠ¨ä¼˜åŒ–
        >>> from apt.model.architectures.apt_model import APTLargeModel
        >>> model = APTLargeModel(config)  # è‡ªåŠ¨åº”ç”¨VGPUä¼˜åŒ–
    """
    global _vb_enabled, _vb_stack, _vb_config, _extreme_scale_trainer

    _vb_enabled = True

    # ä¼˜å…ˆçº§ï¼šMXFP4 > FP4
    if use_mxfp4 and _mxfp4_available:
        use_fp4 = False  # ç¦ç”¨æ—§ç‰ˆFP4
        logger.info("[VB] ä½¿ç”¨MXFP4é‡åŒ–ï¼ˆä¼˜å…ˆçº§é«˜äºFP4ï¼‰")

    _vb_config.update({
        # åŸæœ‰é…ç½®
        'use_fp4': use_fp4,
        'use_flash_attn': use_flash_attn,
        'mixed_precision': mixed_precision,
        'gradient_checkpointing': gradient_checkpointing,
        'auto_estimate': auto_estimate,
        'verbose': verbose,

        # æ–°å¢é…ç½®
        'use_mxfp4': use_mxfp4,
        'mxfp4_block_size': mxfp4_block_size,
        'use_moe_optimized': use_moe_optimized,
        'moe_num_experts': moe_num_experts,
        'moe_top_k': moe_top_k,
        'enable_extreme_scale': enable_extreme_scale,
    })

    # åˆ›å»ºVGPU Stack
    if vgpu_config:
        from apt.vgpu.runtime.vgpu_stack import VGPUStack
        _vb_stack = VGPUStack(vgpu_config)
    else:
        _vb_stack = create_vgpu_stack()

    # åˆ›å»ºè¶…å¤§è§„æ¨¡è®­ç»ƒé…ç½®
    if enable_extreme_scale and _extreme_scale_available:
        extreme_config = ExtremeScaleConfig(
            total_gpus=extreme_scale_total_gpus,
            use_mixed_precision=mixed_precision,
            use_mxfp4=use_mxfp4,
            use_gradient_checkpointing=gradient_checkpointing
        )
        _vb_config['extreme_scale_config'] = extreme_config
        logger.info(
            f"[VB] è¶…å¤§è§„æ¨¡è®­ç»ƒé…ç½®å·²åˆ›å»º "
            f"({extreme_scale_total_gpus:,} GPUs)"
        )

    if verbose:
        # æ£€æµ‹è®¾å¤‡ç±»å‹
        device_type = get_accelerator_type()
        device_emoji = {
            'cuda': 'ğŸŸ¢ NVIDIA GPU',
            'hpu': 'ğŸŸ£ Intel Habana Gaudi HPU',
            'npu': 'ğŸŸ¡ Huawei Ascend NPU',
            'xpu': 'ğŸ”µ Intel XPU',
            'cpu': 'âšª CPU'
        }.get(device_type, 'âš« æœªçŸ¥è®¾å¤‡')

        print("\n" + "="*70)
        print("[>>] è™šæ‹ŸBlackwellå·²å…¨å±€å¯ç”¨ï¼ˆå¢å¼ºç‰ˆï¼‰")
        print("="*70)
        print(f"åŠ é€Ÿè®¾å¤‡:        {device_emoji}")

        # é‡åŒ–é€‰é¡¹
        if use_mxfp4:
            print(f"MXFP4é‡åŒ–:       [OK] å¯ç”¨ (4-bit, block_size={mxfp4_block_size})")
        elif use_fp4:
            print(f"FP4é‡åŒ–:         [OK] å¯ç”¨")
        else:
            print(f"é‡åŒ–:            [X] ç¦ç”¨")

        # å…¶ä»–ä¼˜åŒ–
        print(f"Flash Attention: {'[OK] å¯ç”¨' if use_flash_attn else '[X] ç¦ç”¨'}")
        print(f"æ··åˆç²¾åº¦:        {'[OK] å¯ç”¨' if mixed_precision else '[X] ç¦ç”¨'}")
        print(f"æ¢¯åº¦æ£€æŸ¥ç‚¹:      {'[OK] å¯ç”¨' if gradient_checkpointing else '[X] ç¦ç”¨'}")

        # æ–°å¢ç‰¹æ€§
        if use_moe_optimized:
            print(f"GPUä¼˜åŒ–MoE:      [OK] å¯ç”¨ ({moe_num_experts}ä¸“å®¶, top-{moe_top_k})")
        else:
            print(f"GPUä¼˜åŒ–MoE:      [X] ç¦ç”¨")

        if enable_extreme_scale:
            print(f"100K GPUè®­ç»ƒ:    [OK] å¯ç”¨ ({extreme_scale_total_gpus:,} GPUs)")
            print(f"  â”œâ”€ 3Då¹¶è¡Œ:     [OK]")
            print(f"  â”œâ”€ DeepSpeed:  [OK]")
            print(f"  â”œâ”€ NVLink 5:   [OK] 1.8TB/s per GPU")
            print(f"  â””â”€ GB200æ”¯æŒ:  [OK] 72 GPUs per rack")
        else:
            print(f"100K GPUè®­ç»ƒ:    [X] ç¦ç”¨")

        print(f"è‡ªåŠ¨ä¼°ç®—:        {'[OK] å¯ç”¨' if auto_estimate else '[X] ç¦ç”¨'}")
        print("="*70 + "\n")


def disable():
    """ç¦ç”¨è™šæ‹ŸBlackwellä¼˜åŒ–"""
    global _vb_enabled, _vb_stack
    _vb_enabled = False
    _vb_stack = None
    print("è™šæ‹ŸBlackwellå·²ç¦ç”¨")


def is_enabled() -> bool:
    """æ£€æŸ¥è™šæ‹ŸBlackwellæ˜¯å¦å·²å¯ç”¨"""
    return _vb_enabled


def get_stack() -> Optional[VGPUStack]:
    """è·å–å…¨å±€VGPU Stack"""
    return _vb_stack


def get_config() -> Dict:
    """è·å–å½“å‰é…ç½®"""
    return _vb_config.copy()


def optimize_model(model: nn.Module, model_name: str = "model") -> nn.Module:
    """
    ä¼˜åŒ–å•ä¸ªæ¨¡å‹

    Args:
        model: PyTorchæ¨¡å‹
        model_name: æ¨¡å‹åç§°ï¼ˆç”¨äºæ—¥å¿—ï¼‰

    Returns:
        ä¼˜åŒ–åçš„æ¨¡å‹
    """
    if not _vb_enabled:
        return model

    if _vb_stack is None:
        raise RuntimeError("è¯·å…ˆè°ƒç”¨vb.enable()å¯ç”¨è™šæ‹ŸBlackwell")

    verbose = _vb_config['verbose']

    if verbose:
        print(f"\nä¼˜åŒ–æ¨¡å‹: {model_name}")

    # å¯¼å…¥ä¼˜åŒ–åŒ…è£…å™¨
    from training.test_vb_apt_integration import VBOptimizedAPTModel

    # è·å–æ¨¡å‹é…ç½®
    if hasattr(model, 'config'):
        apt_config = model.config
    else:
        raise ValueError("æ¨¡å‹å¿…é¡»æœ‰configå±æ€§")

    # åˆ›å»ºä¼˜åŒ–æ¨¡å‹
    optimized_model = VBOptimizedAPTModel(
        apt_config,
        _vb_stack,
        use_fp4=_vb_config['use_fp4'],
        use_flash_attn=_vb_config['use_flash_attn']
    )

    # å¤åˆ¶åŸå§‹æƒé‡
    optimized_model.base_model.load_state_dict(model.state_dict())

    if verbose:
        print(f"[OK] å·²ä¼˜åŒ– {len(optimized_model.optimized_layers)} ä¸ªçº¿æ€§å±‚")

    return optimized_model


def estimate_model_resources(model_config, batch_size: int = 8):
    """
    ä¼°ç®—æ¨¡å‹èµ„æºéœ€æ±‚

    Args:
        model_config: APTæ¨¡å‹é…ç½®
        batch_size: æ‰¹æ¬¡å¤§å°
    """
    if not _vb_config['auto_estimate']:
        return

    # è½¬æ¢ä¸ºè¯„ä¼°å™¨é…ç½®
    estimator_config = ModelConfig(
        vocab_size=getattr(model_config, 'vocab_size', 50000),
        hidden_size=getattr(model_config, 'hidden_size', 768),
        num_layers=getattr(model_config, 'num_layers', 12),
        num_heads=getattr(model_config, 'num_heads', 12),
        seq_length=getattr(model_config, 'max_position_embeddings', 2048),
        batch_size=batch_size,
        mixed_precision=_vb_config['mixed_precision'],
        gradient_checkpointing=_vb_config['gradient_checkpointing']
    )

    # è¯„ä¼°
    estimator = VGPUResourceEstimator()
    estimator.estimate_transformer(estimator_config)

    # ç”ŸæˆVGPUé…ç½®
    available_gpus = []
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            props = torch.cuda.get_device_properties(i)
            available_gpus.append({
                'device': f'cuda:{i}',
                'vram_gb': props.total_memory / (1024**3),
                'speed_gbps': 900
            })

    if available_gpus:
        estimator.generate_vgpu_config(available_gpus)
        estimator.print_report()


def get_stats() -> Dict:
    """è·å–VGPUç»Ÿè®¡ä¿¡æ¯"""
    stats = {}

    if _vb_stack is not None:
        stats['vgpu'] = _vb_stack.get_stats()

    if _extreme_scale_trainer is not None:
        stats['extreme_scale'] = {
            'enabled': True,
            'total_gpus': _vb_config.get('extreme_scale_config').total_gpus
        }

    return stats


def print_stats():
    """æ‰“å°VGPUç»Ÿè®¡ä¿¡æ¯"""
    if _vb_stack is None:
        print("è™šæ‹ŸBlackwellæœªå¯ç”¨")
        return
    _vb_stack.print_stats()


def apply_mxfp4_to_model(model: nn.Module) -> nn.Module:
    """
    å°†MXFP4é‡åŒ–åº”ç”¨åˆ°æ¨¡å‹

    Args:
        model: PyTorchæ¨¡å‹

    Returns:
        é‡åŒ–åçš„æ¨¡å‹

    Example:
        >>> model = MyModel()
        >>> quantized_model = vb.apply_mxfp4_to_model(model)
    """
    if not _mxfp4_available:
        logger.warning("[VB] MXFP4ä¸å¯ç”¨ï¼Œè·³è¿‡é‡åŒ–")
        return model

    if not _vb_config.get('use_mxfp4'):
        logger.warning("[VB] MXFP4æœªå¯ç”¨ï¼Œè¯·å…ˆè°ƒç”¨vb.enable(use_mxfp4=True)")
        return model

    config = MXFP4Config(block_size=_vb_config.get('mxfp4_block_size', 32))
    quantized_model = convert_model_to_mxfp4(model, config)

    logger.info("[VB] MXFP4é‡åŒ–å·²åº”ç”¨")
    return quantized_model


def setup_extreme_scale_training_for_model(
    model: nn.Module,
    optimizer: torch.optim.Optimizer
) -> 'ExtremeScaleTrainer':
    """
    ä¸ºæ¨¡å‹è®¾ç½®è¶…å¤§è§„æ¨¡è®­ç»ƒ

    Args:
        model: PyTorchæ¨¡å‹
        optimizer: ä¼˜åŒ–å™¨

    Returns:
        ExtremeScaleTrainerå®ä¾‹

    Example:
        >>> model = MyModel()
        >>> optimizer = torch.optim.Adam(model.parameters())
        >>> trainer = vb.setup_extreme_scale_training_for_model(model, optimizer)
        >>> for batch in dataloader:
        ...     stats = trainer.train_step(batch)
    """
    global _extreme_scale_trainer

    if not _extreme_scale_available:
        raise ImportError("è¶…å¤§è§„æ¨¡è®­ç»ƒæ¨¡å—ä¸å¯ç”¨")

    if not _vb_config.get('enable_extreme_scale'):
        raise RuntimeError(
            "è¯·å…ˆå¯ç”¨è¶…å¤§è§„æ¨¡è®­ç»ƒ: "
            "vb.enable_extreme_scale_mode(total_gpus=100000)"
        )

    config = _vb_config.get('extreme_scale_config')
    if config is None:
        raise RuntimeError("è¶…å¤§è§„æ¨¡è®­ç»ƒé…ç½®æœªåˆå§‹åŒ–")

    _extreme_scale_trainer = ExtremeScaleTrainer(model, optimizer, config)

    logger.info(
        f"[VB] è¶…å¤§è§„æ¨¡è®­ç»ƒå™¨å·²è®¾ç½® ({config.total_gpus:,} GPUs)"
    )

    return _extreme_scale_trainer


def get_extreme_scale_trainer() -> Optional['ExtremeScaleTrainer']:
    """è·å–è¶…å¤§è§„æ¨¡è®­ç»ƒå™¨å®ä¾‹"""
    return _extreme_scale_trainer


# ä¾¿æ·é¢„è®¾
def enable_full_optimization():
    """å¯ç”¨æ‰€æœ‰ä¼˜åŒ–ï¼ˆæœ€å¤§æ˜¾å­˜èŠ‚çœ + MXFP4ï¼‰"""
    enable(
        use_mxfp4=True,  # å‡çº§åˆ°MXFP4
        use_flash_attn=True,
        mixed_precision=True,
        gradient_checkpointing=True,
        auto_estimate=True
    )


def enable_speed_mode():
    """å¯ç”¨é€Ÿåº¦æ¨¡å¼ï¼ˆMXFP4é‡åŒ–ï¼Œ4xåŠ é€Ÿï¼‰"""
    enable(
        use_mxfp4=True,  # å‡çº§åˆ°MXFP4
        use_flash_attn=False,
        mixed_precision=False,
        gradient_checkpointing=False,
        auto_estimate=True
    )


def enable_memory_mode():
    """å¯ç”¨æ˜¾å­˜æ¨¡å¼ï¼ˆFlash Attention + æ¢¯åº¦æ£€æŸ¥ç‚¹ï¼‰"""
    enable(
        use_fp4=False,
        use_flash_attn=True,
        mixed_precision=True,
        gradient_checkpointing=True,
        auto_estimate=True
    )


def enable_balanced_mode():
    """å¯ç”¨å¹³è¡¡æ¨¡å¼ï¼ˆæ¨èï¼‰"""
    enable(
        use_fp4=False,
        use_flash_attn=True,
        mixed_precision=True,
        gradient_checkpointing=False,
        auto_estimate=True
    )


def enable_moe_mode(num_experts: int = 8, top_k: int = 2):
    """
    å¯ç”¨MoEæ¨¡å¼ï¼ˆGPUä¼˜åŒ–MoE + MXFP4ï¼‰

    Args:
        num_experts: ä¸“å®¶æ•°é‡ï¼ˆé»˜è®¤8ï¼‰
        top_k: æ¿€æ´»ä¸“å®¶æ•°ï¼ˆé»˜è®¤2ï¼‰

    Example:
        >>> vb.enable_moe_mode(num_experts=16, top_k=2)
    """
    enable(
        use_mxfp4=True,
        use_flash_attn=True,
        mixed_precision=True,
        use_moe_optimized=True,
        moe_num_experts=num_experts,
        moe_top_k=top_k,
        auto_estimate=True
    )


def enable_extreme_scale_mode(
    total_gpus: int = 100000,
    data_parallel: int = 64,
    tensor_parallel: int = 8,
    pipeline_parallel: int = 8
):
    """
    å¯ç”¨è¶…å¤§è§„æ¨¡è®­ç»ƒæ¨¡å¼ï¼ˆ100K+ GPUsï¼‰

    Args:
        total_gpus: æ€»GPUæ•°ï¼ˆé»˜è®¤100,000ï¼‰
        data_parallel: æ•°æ®å¹¶è¡Œåº¦
        tensor_parallel: å¼ é‡å¹¶è¡Œåº¦
        pipeline_parallel: æµæ°´çº¿å¹¶è¡Œåº¦

    Example:
        >>> # Meta Llama 4 è§„æ¨¡ (350K GPUs)
        >>> vb.enable_extreme_scale_mode(total_gpus=350000)
        >>>
        >>> # OpenAI GPT-5 è§„æ¨¡ (500K+ GPUs)
        >>> vb.enable_extreme_scale_mode(total_gpus=500000)
    """
    enable(
        use_mxfp4=True,
        use_flash_attn=True,
        mixed_precision=True,
        gradient_checkpointing=True,
        use_moe_optimized=True,
        enable_extreme_scale=True,
        extreme_scale_total_gpus=total_gpus,
        auto_estimate=True
    )

    # é¢å¤–è®¾ç½®3Då¹¶è¡Œé…ç½®
    if _extreme_scale_available and _vb_config.get('extreme_scale_config'):
        config = _vb_config['extreme_scale_config']
        config.data_parallel_size = data_parallel
        config.tensor_parallel_size = tensor_parallel
        config.pipeline_parallel_size = pipeline_parallel

        logger.info(
            f"[VB] 3Då¹¶è¡Œé…ç½®: "
            f"DP={data_parallel}, TP={tensor_parallel}, PP={pipeline_parallel}"
        )


# ç¯å¢ƒå˜é‡æ§åˆ¶ï¼ˆå¯é€šè¿‡ç¯å¢ƒå˜é‡è‡ªåŠ¨å¯ç”¨ï¼‰
if os.getenv('ENABLE_VIRTUAL_BLACKWELL', '').lower() in ('1', 'true', 'yes'):
    mode = os.getenv('VB_MODE', 'balanced').lower()

    if mode == 'full':
        enable_full_optimization()
    elif mode == 'speed':
        enable_speed_mode()
    elif mode == 'memory':
        enable_memory_mode()
    else:
        enable_balanced_mode()

    print(f"[OK] é€šè¿‡ç¯å¢ƒå˜é‡è‡ªåŠ¨å¯ç”¨è™šæ‹ŸBlackwell ({mode}æ¨¡å¼)")


if __name__ == "__main__":
    # æµ‹è¯•
    print("è™šæ‹ŸBlackwellå…¨å±€å¯ç”¨å™¨")
    print("\nä½¿ç”¨ç¤ºä¾‹:")
    print("```python")
    print("import apt.perf.optimization.vb_global as vb")
    print("")
    print("# å¯ç”¨è™šæ‹ŸBlackwell")
    print("vb.enable(use_fp4=True, use_flash_attn=True)")
    print("")
    print("# ä¹‹åæ‰€æœ‰APTæ¨¡å‹éƒ½ä¼šè‡ªåŠ¨ä¼˜åŒ–")
    print("from apt.model.architectures.apt_model import APTLargeModel")
    print("model = APTLargeModel(config)  # è‡ªåŠ¨åº”ç”¨VGPUä¼˜åŒ–")
    print("")
    print("# æŸ¥çœ‹ç»Ÿè®¡")
    print("vb.print_stats()")
    print("```")
