# virtual_vram.py
# -*- coding: utf-8 -*-
"""
Virtual VRAM v1.0 (Pinned-Memory Async Activation Offload)

æ¶æ„ï¼š
  Pack:  pinned CPU tensor + non_blocking D2H on ä¸“ç”¨ copy_stream
         â†’ ä¸ forward compute é‡å ï¼Œä¸é˜»å¡é»˜è®¤æµ
  Unpack: ç­‰å¾… D2H event â†’ H2D æ¢å¤ï¼ˆåŒæ­¥ï¼Œbackward ç«‹å³éœ€è¦æ•°æ®ï¼‰

Key è®¾è®¡ï¼ˆé‡‡çº³ Opus v1.0ï¼‰ï¼š
  tensor_key = (storage_data_ptr, shape, stride, offset)
  æ¯ä¸ªå”¯ä¸€ view å­˜ä¸€ä»½ CPU tensorï¼Œä¸åš contiguous()ï¼Œä¿ç•™åŸå§‹ strideã€‚
  æ¯” v0.2 çš„ as_strided æ–¹æ¡ˆæ›´ç®€å•ã€æ›´é²æ£’ã€‚

Usage:
    from apt.vgpu.runtime.virtual_vram import VirtualVRAMConfig, virtual_vram

    cfg = VirtualVRAMConfig(enabled=True, min_tensor_bytes=1<<20)
    with virtual_vram(cfg):
        loss = model(x).sum()
        loss.backward()
"""

from __future__ import annotations

import math
from dataclasses import dataclass
from contextlib import contextmanager
from typing import Any, Dict, Optional, Tuple

import torch

# è‡ªç„¶å¹³è¡¡å¸¸æ•°ï¼š4/e â‰ˆ 1.4715ï¼Œç”¨äº LECaC é‡åŒ–è¡¥å¿
NATURAL_EQUILIBRIUM_CONSTANT: float = 4.0 / math.e


@dataclass
class VirtualVRAMConfig:
    enabled: bool = True
    min_tensor_bytes: int = 1 << 20   # 1MB â€” åª offload >= æ­¤å¤§å°çš„ tensor
    # å·²åºŸå¼ƒï¼Œä¿ç•™å‘åå…¼å®¹
    min_storage_bytes: int = 0
    verbose: bool = False


class _Packed:
    """pack_hook è¿”å›å€¼ï¼šCPU tensor + æ¢å¤ç”¨å…ƒæ•°æ® + D2H å®Œæˆäº‹ä»¶"""
    __slots__ = ('cpu_tensor', 'device', 'dtype', 'requires_grad', 'event')

    def __init__(self, cpu_tensor: torch.Tensor,
                 device: torch.device, dtype: torch.dtype,
                 requires_grad: bool,
                 event: Optional[torch.cuda.Event]):
        self.cpu_tensor = cpu_tensor
        self.device = device
        self.dtype = dtype
        self.requires_grad = requires_grad
        self.event = event     # D2H å®Œæˆä¿¡å·ï¼ˆé CUDA ç¯å¢ƒæ—¶ä¸º Noneï¼‰


@contextmanager
def virtual_vram(cfg: VirtualVRAMConfig):
    """
    Pinned-memory async activation offload.
    """
    if not cfg.enabled:
        yield
        return

    min_bytes = cfg.min_tensor_bytes or cfg.min_storage_bytes or (1 << 20)

    # æŸ¥è¡¨ï¼šé¿å…å¯¹åŒä¸€ view é‡å¤ offload
    # key = (storage_data_ptr, shape, stride, offset) â†’ _Packed
    cache: Dict[Tuple, _Packed] = {}

    # D2H ä¸“ç”¨æµï¼šä¸é»˜è®¤ compute stream å¹¶è¡Œæ¬è¿
    copy_stream: Optional[torch.cuda.Stream] = None
    if torch.cuda.is_available():
        try:
            copy_stream = torch.cuda.Stream()
        except Exception:
            pass

    def _make_key(t: torch.Tensor) -> Tuple:
        """ä¸ºä¸€ä¸ª tensor view ç”Ÿæˆå”¯ä¸€æ ‡è¯†"""
        return (
            t.untyped_storage().data_ptr(),
            tuple(t.shape),
            tuple(t.stride()),
            t.storage_offset(),
        )

    def pack_hook(t: torch.Tensor) -> Any:
        # â”€â”€ å¿«é€Ÿç­›é€‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if not torch.is_tensor(t) or not t.is_cuda:
            return t

        nbytes = t.numel() * t.element_size()
        if nbytes < min_bytes:
            return t

        # â”€â”€ æŸ¥è¡¨ï¼šåŒä¸€ view åªæ¬ä¸€æ¬¡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        key = _make_key(t)
        if key in cache:
            if cfg.verbose:
                print(f"[VirtualVRAM] ğŸ”— cache hit {tuple(t.shape)}")
            return cache[key]

        # â”€â”€ D2Hï¼špinned memory + async copy â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        device = t.device
        dtype = t.dtype
        requires_grad = bool(t.requires_grad)

        try:
            # åˆ†é… pinned CPU tensorï¼ˆDMA ç›´è¾¾ï¼Œæ¯” pageable å¿« 2-3 å€ï¼‰
            # ä¸åš contiguous()ï¼ä¿ç•™åŸå§‹ strideï¼Œè¿™æ˜¯ v0.1 å´©çš„æ ¹å› ä¿®å¤
            cpu_tensor = torch.empty(
                t.shape, dtype=dtype, device='cpu', pin_memory=True,
            )

            if copy_stream is not None:
                # å…³é”®ï¼šè®© copy_stream ç­‰å¾… default stream å½“å‰é˜Ÿåˆ—ä¸­çš„æ‰€æœ‰å·¥ä½œ
                # PyTorch åˆ‡æ¢æµæ—¶ä¸è‡ªåŠ¨æ’å…¥è·¨æµä¾èµ–ï¼ä¸åŠ è¿™è¡Œï¼Œcopy_stream
                # å¯èƒ½åœ¨ compute kernel è¿˜æœªå†™å®Œ t æ—¶å°±å¼€å§‹ D2H â†’ è¯»åˆ° NaN
                copy_stream.wait_stream(torch.cuda.current_stream())
                with torch.cuda.stream(copy_stream):
                    cpu_tensor.copy_(t, non_blocking=True)
                event = copy_stream.record_event()
            else:
                # æ—  CUDA æˆ– stream åˆ›å»ºå¤±è´¥ï¼Œé€€åŒ–åˆ°åŒæ­¥
                cpu_tensor.copy_(t)
                event = None

        except Exception as e:
            if cfg.verbose:
                print(f"[VirtualVRAM] âŒ D2H å¤±è´¥: {e}")
            return t

        packed = _Packed(cpu_tensor, device, dtype, requires_grad, event)
        cache[key] = packed

        if cfg.verbose:
            mb = nbytes / 1024 / 1024
            print(f"[VirtualVRAM] âœ… D2H {mb:.2f}MB {tuple(t.shape)} "
                  f"stride={tuple(t.stride())} async={'yes' if event else 'no'}")

        return packed

    def unpack_hook(packed: Any) -> torch.Tensor:
        if not isinstance(packed, _Packed):
            return packed

        # â”€â”€ ç­‰å¾… D2H å®Œæˆ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if packed.event is not None:
            packed.event.synchronize()

        # â”€â”€ H2Dï¼špinned â†’ CUDAï¼ˆåŒæ­¥ï¼Œbackward ç«‹å³éœ€è¦æ•°æ®ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        try:
            with torch.no_grad():
                restored = packed.cpu_tensor.to(
                    device=packed.device,
                    non_blocking=False,
                )
            restored.requires_grad_(packed.requires_grad)
        except Exception as e:
            raise RuntimeError(f"Failed to restore tensor from CPU: {e}")

        if cfg.verbose:
            mb = restored.numel() * restored.element_size() / 1024 / 1024
            print(f"[VirtualVRAM] â†©ï¸  H2D {mb:.2f}MB {tuple(restored.shape)}")

        return restored

    with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):
        yield

    # context é€€å‡ºï¼šæ¸…ç† cacheï¼ˆé‡Šæ”¾ pinned CPU å†…å­˜ï¼‰
    cache.clear()
