# virtual_vram.py
# -*- coding: utf-8 -*-
"""
Virtual VRAM v1.1 (Stable Activation Offload)

å†å²æ•™è®­ï¼ˆv1.0 çš„å‘ï¼‰ï¼š
  pinned memory + async D2H ç†è®ºä¸Šæ›´å¿«ï¼Œä½†å®æµ‹è¸©äº†ä¸‰ä¸ªå‘ï¼š
  1. copy_stream ç¼ºå°‘è·¨æµä¾èµ– â†’ D2H åœ¨ compute kernel å†™å®Œå‰å°±å¼€å§‹ â†’ NaN
  2. bool mask (dropout) è¢« offload â†’ æ¢å¤æ—¶ dtype é”™è¯¯ â†’ Mask should be Bool
  3. å³ä½¿åŠ äº† wait_stream + event.synchronize()ï¼ŒæŸäº› tensor ä»æ•°æ®æŸå
  å®æµ‹: pinned+async 28s/stepï¼Œ.detach().cpu() 6.44s/stepï¼ˆå¿« 4.5xï¼‰

v1.1 è®¾è®¡ï¼š
  Pack:   bool ç±»å‹ç›´æ¥è·³è¿‡ï¼›å…¶ä½™ .detach().cpu()ï¼ˆPyTorch å†…éƒ¨ä¼˜åŒ–ï¼Œç¨³å®šå¯é ï¼‰
  Unpack: .to(device, non_blocking=False)ï¼ˆåŒæ­¥ H2Dï¼Œbackward ç«‹å³éœ€è¦æ•°æ®ï¼‰
  Cache:  per-view key å»é‡ï¼ˆstorage_ptr, shape, stride, offsetï¼‰ï¼Œé¿å…åŒä¸€ view é‡å¤æ¬è¿

Usage:
    from apt.vgpu.runtime.virtual_vram import VirtualVRAMConfig, virtual_vram

    cfg = VirtualVRAMConfig(enabled=True, min_tensor_bytes=1<<20)
    with virtual_vram(cfg):
        loss = model(x).sum()
        loss.backward()
"""

from __future__ import annotations

import math
from dataclasses import dataclass
from contextlib import contextmanager
from typing import Any, Dict, Tuple

import torch

# è‡ªç„¶å¹³è¡¡å¸¸æ•°ï¼š4/e â‰ˆ 1.4715ï¼Œç”¨äº LECaC é‡åŒ–è¡¥å¿
NATURAL_EQUILIBRIUM_CONSTANT: float = 4.0 / math.e


@dataclass
class VirtualVRAMConfig:
    enabled: bool = True
    min_tensor_bytes: int = 1 << 20   # 1MB â€” åª offload >= æ­¤å¤§å°çš„ tensor
    # å·²åºŸå¼ƒï¼Œä¿ç•™å‘åå…¼å®¹
    min_storage_bytes: int = 0
    verbose: bool = False


class _Packed:
    """pack_hook è¿”å›å€¼ï¼šCPU tensor + æ¢å¤ç”¨å…ƒæ•°æ®"""
    __slots__ = ('cpu_tensor', 'device', 'requires_grad')

    def __init__(self, cpu_tensor: torch.Tensor,
                 device: torch.device,
                 requires_grad: bool):
        self.cpu_tensor = cpu_tensor
        self.device = device
        self.requires_grad = requires_grad


@contextmanager
def virtual_vram(cfg: VirtualVRAMConfig):
    """
    ç¨³å®šçš„æ¿€æ´»å€¼ CPU offload context managerã€‚
    """
    if not cfg.enabled:
        yield
        return

    min_bytes = cfg.min_tensor_bytes or cfg.min_storage_bytes or (1 << 20)

    # æŸ¥è¡¨ï¼šé¿å…å¯¹åŒä¸€ view é‡å¤ offload
    # key = (storage_data_ptr, shape, stride, offset) â†’ _Packed
    cache: Dict[Tuple, _Packed] = {}

    def _make_key(t: torch.Tensor) -> Tuple:
        return (
            t.untyped_storage().data_ptr(),
            tuple(t.shape),
            tuple(t.stride()),
            t.storage_offset(),
        )

    def pack_hook(t: torch.Tensor) -> Any:
        # â”€â”€ å¿«é€Ÿç­›é€‰ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        if not torch.is_tensor(t) or not t.is_cuda:
            return t

        # bool ç±»å‹ï¼ˆå¦‚ dropout maskï¼‰å¿…é¡»è·³è¿‡ï¼š
        # offload åæ¢å¤æ—¶ .to(device) ä¸ä¿è¯ä¿ç•™ bool dtypeï¼Œ
        # ä¼šå¯¼è‡´ "Mask should be Bool" ç±»å‹é”™è¯¯
        if t.dtype == torch.bool:
            return t

        nbytes = t.numel() * t.element_size()
        if nbytes < min_bytes:
            return t

        # â”€â”€ æŸ¥è¡¨ï¼šåŒä¸€ view åªæ¬ä¸€æ¬¡ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        key = _make_key(t)
        if key in cache:
            if cfg.verbose:
                print(f"[VirtualVRAM] ğŸ”— cache hit {tuple(t.shape)}")
            return cache[key]

        # â”€â”€ D2Hï¼šdetach().cpu()ï¼ˆåŒæ­¥ï¼Œç¨³å®šå¯é ï¼‰â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # æ•™è®­ï¼špinned memory + async copy åœ¨å¤šæµç¯å¢ƒä¸‹æ•°æ®æŸåé—®é¢˜æéš¾è¿½æŸ¥ã€‚
        # torch.Tensor.cpu() å†…éƒ¨èµ° PyTorch æ ‡å‡† D2H è·¯å¾„ï¼Œ
        # ä¼šç­‰å¾… tensor æ‰€åœ¨æµå®Œæˆå†™å…¥ï¼Œä¸ä¼šäº§ç”Ÿ NaNã€‚
        # å®æµ‹æ¯” pinned+async å¿« 4.5xï¼ˆ6.44s vs 28s per stepï¼‰ã€‚
        try:
            cpu_tensor = t.detach().cpu()
        except Exception as e:
            if cfg.verbose:
                print(f"[VirtualVRAM] âŒ D2H å¤±è´¥: {e}")
            return t

        packed = _Packed(cpu_tensor, t.device, bool(t.requires_grad))
        cache[key] = packed

        if cfg.verbose:
            mb = nbytes / 1024 / 1024
            print(f"[VirtualVRAM] âœ… D2H {mb:.2f}MB {tuple(t.shape)} "
                  f"dtype={t.dtype} stride={tuple(t.stride())}")

        return packed

    def unpack_hook(packed: Any) -> torch.Tensor:
        if not isinstance(packed, _Packed):
            return packed

        # H2Dï¼šåŒæ­¥æ¢å¤ï¼ˆbackward ç«‹å³éœ€è¦æ•°æ®ï¼‰
        try:
            restored = packed.cpu_tensor.to(
                device=packed.device,
                non_blocking=False,
            )
            restored.requires_grad_(packed.requires_grad)
        except Exception as e:
            raise RuntimeError(f"Failed to restore tensor from CPU: {e}")

        if cfg.verbose:
            mb = restored.numel() * restored.element_size() / 1024 / 1024
            print(f"[VirtualVRAM] â†©ï¸  H2D {mb:.2f}MB {tuple(restored.shape)}")

        return restored

    with torch.autograd.graph.saved_tensors_hooks(pack_hook, unpack_hook):
        yield

    # context é€€å‡ºï¼šæ¸…ç† cacheï¼ˆé‡Šæ”¾ CPU å†…å­˜ï¼‰
    cache.clear()
