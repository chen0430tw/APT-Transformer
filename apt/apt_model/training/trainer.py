#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ä¿®æ”¹APTæ¨¡å‹è®­ç»ƒå™¨ä»¥æ”¯æŒä¸­æ–‡åˆ†è¯
"""

import os
from apt.apt_model.utils.fake_torch import get_torch
torch = get_torch()
from apt.apt_model.utils.fake_torch import get_torch
torch = get_torch()
F = torch.nn.functional
import traceback
from tqdm import tqdm
from datetime import datetime
Dataset = torch.utils.data.Dataset
DataLoader = torch.utils.data.DataLoader

from apt.apt_model.utils import set_seed
from apt.apt_model.utils import get_device, device
from apt.core.config.apt_config import APTConfig
from apt.apt_model.modeling.apt_model import APTModel, APTLargeModel
from apt.core.generation.generator import generate_natural_text
from apt.core.generation.evaluator import evaluate_text_quality
from apt.core.config.settings_manager import settings
from apt.apt_model.training.training_guard import TrainingGuard, EarlyStopping

# å¯¼å…¥ä¸­æ–‡åˆ†è¯å™¨ç›¸å…³å‡½æ•°
from apt.apt_model.modeling.chinese_tokenizer_integration import (
    get_appropriate_tokenizer,
    save_tokenizer,
    is_chinese_text
)

# ===== Debugæ¨¡å¼æ§åˆ¶è¾“å‡º =====
def debug_print(*args, **kwargs):
    """ä»…åœ¨Debugæ¨¡å¼ä¸‹æ‰“å°ä¿¡æ¯"""
    if settings.get_debug_enabled():
        print(*args, **kwargs)

def info_print(*args, **kwargs):
    """å§‹ç»ˆæ‰“å°çš„å…³é”®ä¿¡æ¯ï¼ˆéDebugæ¨¡å¼ä¹Ÿæ˜¾ç¤ºï¼‰"""
    print(*args, **kwargs)

def get_training_texts():
    """
    è·å–è®­ç»ƒæ–‡æœ¬æ•°æ®ã€‚å¦‚æœåœ¨é¡¹ç›®æ ¹ç›®å½•ä¸‹å­˜åœ¨ "train.txt" æ–‡ä»¶ï¼Œåˆ™è¯»å–æ–‡ä»¶ï¼Œå¦åˆ™è¿”å›å†…ç½®é¢„è®¾æ•°æ®ã€‚
    """
    import os
    # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆå‡è®¾ä»£ç æ–‡ä»¶åœ¨é¡¹ç›®å­ç›®å½•ä¸­ï¼‰
    script_dir = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
    train_file = os.path.join(script_dir, "train.txt")
    
    debug_print("æ£€æŸ¥è®­ç»ƒæ•°æ®æ–‡ä»¶è·¯å¾„ï¼š", train_file)

    if os.path.exists(train_file):
        with open(train_file, "r", encoding="utf-8") as f:
            texts = [line.strip() for line in f if line.strip()]
        if texts:
            return texts
        else:
            debug_print("è®­ç»ƒæ•°æ®æ–‡ä»¶ 'train.txt' ä¸ºç©ºï¼Œä½¿ç”¨é¢„è®¾è®­ç»ƒæ•°æ®ã€‚")
    else:
        debug_print("æœªæ‰¾åˆ°è®­ç»ƒæ•°æ®æ–‡ä»¶ 'train.txt'ï¼Œä½¿ç”¨é¢„è®¾è®­ç»ƒæ•°æ®ã€‚")
    
    # é¢„è®¾è®­ç»ƒæ•°æ®é›†
    return [
        # åŸºæœ¬å¯¹è¯
        "Hello, how are you?",
        "I'm doing well, thank you for asking. How about you?",
        "Good morning! How did you sleep last night?",
        "Good morning! I slept very well, thank you for asking.",
        "What's your name?",
        "My name is Claude. It's nice to meet you.",
        "How's the weather today?",
        "The weather is lovely today. It's sunny and warm.",
        "Can you help me with a question?",
        "Of course, I'd be happy to help you with any questions you have.",
        "What time is it?",
        "I'm sorry, I don't have access to real-time information like the current time.",
        
        # åŸç¥ç›¸å…³å†…å®¹
        "å®‰æŸï¼šä¸€èµ·æ¥è®­ç»ƒå§ï¼",
        "å®‰æŸæ˜¯è’™å¾·åŸçš„ä¾¦å¯Ÿéª‘å£«ï¼Œæ“…é•¿å¼“ç®­å’Œä¾¦å¯Ÿã€‚",
        "æ—…è¡Œè€…ï¼Œæ¬¢è¿æ¥åˆ°æç“¦ç‰¹å¤§é™†ã€‚",
        "åŸç¥æ˜¯ä¸€æ¬¾å¼€æ”¾ä¸–ç•Œå†’é™©æ¸¸æˆã€‚",
        "é£èµ·ä¸‡å±±æ‘‡ï¼Œæš—æ½®å¯„ä½™ç”Ÿã€‚",
        "ç’ƒæœˆæ¸¯æ˜¯ä¸ƒå›½ä¹‹ä¸€ç’ƒæœˆçš„ä¸»è¦æ¸¯å£åŸå¸‚ã€‚",
        "å…ƒç´ åŠ›é‡æ˜¯æç“¦ç‰¹å¤§é™†ä¸Šçš„åŸºç¡€èƒ½åŠ›ä½“ç³»ã€‚",
        "å®‰æŸï¼šè®­ç»ƒ...è¿˜ä¸å¤Ÿ...",
        "æ´¾è’™æ˜¯æ—…è¡Œè€…çš„åŒä¼´ï¼Œè¢«ç§°ä¸ºåº”æ€¥é£Ÿå“ã€‚",
        "ä¸ƒç¥ç»Ÿæ²»ç€æç“¦ç‰¹å¤§é™†çš„ä¸ƒä¸ªå›½å®¶ã€‚",
        "éª‘å£«å›¢è´Ÿè´£å®ˆæŠ¤è’™å¾·åŸçš„å’Œå¹³ä¸å®‰å…¨ã€‚",
        "å†’é™©å®¶åä¼šä¸ºæ—…è¡Œè€…æä¾›å„ç§ä»»åŠ¡å’Œæƒ…æŠ¥ã€‚",
        "æç“¦ç‰¹å¤§é™†æœ‰é£ã€å²©ã€é›·ã€æ°´ã€ç«ã€è‰ã€å†°ä¸ƒç§å…ƒç´ ã€‚",
        
        # åŸºæœ¬è§£é‡Š
        "What is artificial intelligence?",
        "Artificial intelligence refers to computer systems designed to perform tasks that typically require human intelligence.",
        "Can you explain what a neural network is?",
        "A neural network is a computational model inspired by the human brain that consists of layers of interconnected nodes.",
        "What is machine learning?",
        "Machine learning is a subset of artificial intelligence that focuses on developing algorithms that allow computers to learn from data.",
        "What is deep learning?",
        "Deep learning is a subset of machine learning that uses artificial neural networks with multiple layers to extract features from data.",
        
        # å®Œæ•´å¥å­
        "This is a test sentence for APT model training.",
        "Deep learning models require a lot of data.",
        "Transformers are widely used in NLP tasks.",
        "The quick brown fox jumps over the lazy dog.",
        "I enjoy reading books in my free time.",
        "Music has the power to change our mood.",
        "The Internet has revolutionized how we access information.",
        "Learning a new language can be challenging but rewarding.",
        "Yesterday I went to the store to buy some groceries.",
        "The mountains looked beautiful against the sunset sky.",
        "She opened the window to let in some fresh air.",
        "They decided to take a vacation to the beach this summer.",
        "The professor explained the complex theory to the students.",
        "My favorite season is autumn when the leaves change color.",
        "The company announced their new product at the conference.",
        "We should meet for coffee sometime next week.",
        "The children played happily in the park all afternoon.",
        "He finished writing his novel after five years of work.",
        
        # é—®ç­”å¯¹
        "What is the capital of France?",
        "The capital of France is Paris.",
        "Who wrote Romeo and Juliet?",
        "William Shakespeare wrote Romeo and Juliet.",
        "What is photosynthesis?",
        "Photosynthesis is the process by which green plants use sunlight to synthesize foods with carbon dioxide and water.",
        "How far is the Moon from Earth?",
        "The average distance between the Earth and the Moon is about 384,400 kilometers.",
        "What is the largest ocean on Earth?",
        "The Pacific Ocean is the largest and deepest ocean on Earth.",
        
        # å¸¸è§çŸ­è¯­
        "Thank you very much.",
        "You're welcome.",
        "How can I help you today?",
        "That's a great question.",
        "I'm not sure about that.",
        "Let me think about it.",
        "Could you please explain that again?",
        "I understand what you're saying.",
        "That's an interesting perspective.",
        "I agree with your point of view.",
        
        # æ›´å¤æ‚çš„å†…å®¹
        "In recent years, large language models have demonstrated impressive capabilities in understanding and generating human language.",
        "The development of self-driving cars represents a significant advancement in artificial intelligence.",
        "Climate change is one of the most pressing challenges facing our planet today.",
        "The human brain contains approximately 86 billion neurons.",
        "Quantum computing has the potential to solve certain problems exponentially faster than classical computers.",
        "The history of art spans thousands of years, reflecting human civilization.",
        "Telescopes allow us to observe distant galaxies.",
        "Blockchain technology provides a secure way to record transactions.",
        "Photosynthesis is essential for most life on Earth.",
        "Biodiversity is crucial for maintaining healthy ecosystems.",
        
        # è¿è´¯æ®µè½
        """
        The sun was setting behind the mountains, casting long shadows across the valley.
        Birds were returning to their nests, filling the air with their evening songs.
        A gentle breeze rustled the leaves, bringing the sweet scent of wildflowers.
        In the distance, the small town's lights twinkled like fallen stars.
        """,
        
        """
        Learning to code can be challenging but rewarding.
        It requires patience, logical thinking, and attention to detail.
        Many beginners start with simple languages like Python.
        The key to success is consistent practice and learning from mistakes.
        """,
        
        """
        The history of aviation began with the Wright brothers.
        Since then, aviation has transformed global travel.
        Modern aircraft can fly faster than the speed of sound.
        Air travel connects people across continents in just hours.
        """,
        
        # ä¸­æ–‡å†…å®¹
        "æœºå™¨å­¦ä¹ æ˜¯äººå·¥æ™ºèƒ½çš„ä¸€ä¸ªå­é¢†åŸŸï¼Œå®ƒä½¿ç”¨æ•°æ®å’Œç®—æ³•æ¥æ¨¡ä»¿äººç±»å­¦ä¹ çš„æ–¹å¼ã€‚",
        "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ç§ç‰¹æ®Šå½¢å¼ï¼Œå®ƒä½¿ç”¨å¤šå±‚ç¥ç»ç½‘ç»œå¤„ç†å¤æ‚çš„æ¨¡å¼ã€‚",
        "è‡ªç„¶è¯­è¨€å¤„ç†è®©è®¡ç®—æœºèƒ½å¤Ÿç†è§£ã€åˆ†æå’Œç”Ÿæˆäººç±»è¯­è¨€ã€‚",
        "è®¡ç®—æœºè§†è§‰ä½¿æœºå™¨èƒ½å¤Ÿä»å›¾åƒå’Œè§†é¢‘ä¸­è·å–æœ‰æ„ä¹‰çš„ä¿¡æ¯ã€‚",
        "å¼ºåŒ–å­¦ä¹ æ˜¯ä¸€ç§è®©æœºå™¨é€šè¿‡è¯•é”™æ¥å­¦ä¹ çš„æ–¹æ³•ï¼Œä»¥è·å¾—æœ€å¤§çš„å¥–åŠ±ã€‚",
        "å¤§å‹è¯­è¨€æ¨¡å‹å¦‚GPTèƒ½å¤Ÿç”Ÿæˆæµç•…çš„æ–‡æœ¬ï¼Œå¹¶å›ç­”å„ç§é—®é¢˜ã€‚",
        "äººå·¥æ™ºèƒ½ä¼¦ç†å…³æ³¨AIå‘å±•ä¸­çš„é“å¾·é—®é¢˜å’Œç¤¾ä¼šå½±å“ã€‚",
        "æ•°æ®ç§‘å­¦ç»“åˆäº†ç»Ÿè®¡å­¦ã€ç¼–ç¨‹å’Œé¢†åŸŸçŸ¥è¯†æ¥æå–æ•°æ®ä¸­çš„ä»·å€¼ã€‚",
        "æœºå™¨å­¦ä¹ ç®—æ³•å¯ä»¥åˆ†ä¸ºç›‘ç£å­¦ä¹ ã€æ— ç›‘ç£å­¦ä¹ å’Œå¼ºåŒ–å­¦ä¹ ã€‚",
        "ç¥ç»ç½‘ç»œæ˜¯å—äººè„‘ç»“æ„å¯å‘è€Œè®¾è®¡çš„ç®—æ³•ã€‚"
    ]

# =============================================================================
# ä¸»è®­ç»ƒå‡½æ•°
# =============================================================================
def train_model(epochs=20, batch_size=8, learning_rate=3e-5, save_path="apt_model",
                logger=None, resource_monitor=None, multimodal_config=None,
                tokenizer_type=None, language=None, texts=None, tokenizer=None,
                # Training guard parameters
                enable_guard=True, max_steps=None, max_time_hours=None,
                early_stopping_patience=None, guard_verbose=True):
    """è®­ç»ƒæ¨¡å‹çš„ä¸»å‡½æ•°ï¼ˆå¸¦è®­ç»ƒä¿æŠ¤ï¼‰"""
    # è®¾ç½®éšæœºç§å­
    set_seed(42)

    # æ˜¾ç¤º APT å…”å­å‰ç¥¥ç‰©ï¼ˆå½©è‰² ANSI è‰ºæœ¯ï¼Œç»“åˆå®‰æŸå½¢è±¡å¢å¼ºç”¨æˆ·ç²˜æ€§ï¼‰
    try:
        # ä½¿ç”¨ importlib ç›´æ¥å¯¼å…¥æ¨¡å—ï¼Œé¿å…è§¦å‘ utils.__init__ çš„é‡é‡çº§å¯¼å…¥
        import importlib.util
        import sys
        mascot_path = os.path.join(os.path.dirname(os.path.dirname(__file__)), 'utils', 'mascot_render.py')
        spec = importlib.util.spec_from_file_location("mascot_render", mascot_path)
        mascot_module = importlib.util.module_from_spec(spec)
        spec.loader.exec_module(mascot_module)
        # ä¼ é€’ info_print ä»¥ä¾¿åœ¨ logger ç¯å¢ƒä¸­æ­£ç¡®æ˜¾ç¤º
        # cols=35 ç»æµ‹è¯•æ•ˆæœæœ€ä½³ï¼Œé€‚åˆç»ˆç«¯æ˜¾ç¤º
        mascot_module.print_apt_mascot(cols=35, show_banner=True, print_func=info_print)
    except Exception as e:
        # å¦‚æœæ¸²æŸ“å¤±è´¥ï¼Œè‡³å°‘æ˜¾ç¤ºæ–‡å­—æ¨ªå¹…ï¼ˆé™é»˜å¤±è´¥ï¼Œä¸å½±å“è®­ç»ƒï¼‰
        info_print("\n" + "="*70)
        info_print("  APT - Autopoietic Transformer | è‡ªç”Ÿæˆå˜æ¢å™¨")
        info_print("="*70 + "\n")

    # æ˜¾ç¤ºå®‰æŸçš„æ¬¢è¿æ¶ˆæ¯ï¼ˆé»˜è®¤ä¸­æ–‡ï¼‰
    # åªæœ‰æ˜ç¡®æŒ‡å®šlanguage="en"æ—¶æ‰æ˜¾ç¤ºè‹±æ–‡
    if language == "en":
        info_print("Amber: Let's train together!\n")
    else:
        # é»˜è®¤ä¸­æ–‡æˆ–language="zh"æˆ–æ£€æµ‹åˆ°ä¸­æ–‡
        info_print("å®‰æŸï¼šä¸€èµ·æ¥è®­ç»ƒå§ï¼\n")

    if logger:
        logger.info("å¼€å§‹è®­ç»ƒæ¨¡å‹...")
    else:
        info_print("å¼€å§‹è®­ç»ƒæ¨¡å‹...\n")

    # è·å–è®­ç»ƒæ•°æ®
    if texts is None:
        train_texts = get_training_texts()
    else:
        train_texts = texts

    info_print(f"è®­ç»ƒæ•°æ®é›†å¤§å°: {len(train_texts)} æ¡æ–‡æœ¬")
    
    # å¦‚æœæ•°æ®ä¸ºç©ºï¼Œåˆ™æŠ¥é”™
    if len(train_texts) == 0:
        raise ValueError("è®­ç»ƒæ•°æ®ä¸ºç©ºï¼Œè¯·ç¡®ä¿æ•°æ®æ–‡ä»¶å­˜åœ¨æˆ–å†…ç½®æ•°æ®æ­£ç¡®åŠ è½½ã€‚")
    
    # è‡ªåŠ¨æ£€æµ‹è¯­è¨€å¹¶é€‰æ‹©åˆé€‚çš„åˆ†è¯å™¨
    tokenizer, detected_language = get_appropriate_tokenizer(
        train_texts, 
        tokenizer_type=tokenizer_type, 
        language=language
    )
    
    debug_print(f"ä½¿ç”¨{detected_language}è¯­è¨€åˆ†è¯å™¨: {type(tokenizer).__name__}")
    
    # è®¾ç½®æ•°æ®é›†å’Œæ•°æ®åŠ è½½å™¨
    class TextDataset(Dataset):
        def __init__(self, texts, tokenizer, max_length=128):
            self.texts = texts
            self.tokenizer = tokenizer
            self.max_length = max_length
        
        def __len__(self):
            return len(self.texts)
        
        def __getitem__(self, idx):
            text = self.texts[idx]
            encoding = self.tokenizer.encode(
                text, 
                return_tensors="pt", 
                max_length=self.max_length, 
                truncation=True
            ).squeeze(0)
            return encoding, encoding
    
    def collate_fn(batch):
        src_ids_list, tgt_ids_list = zip(*batch)
        src_ids = torch.nn.utils.rnn.pad_sequence(
            src_ids_list, 
            batch_first=True, 
            padding_value=tokenizer.pad_token_id
        )
        tgt_ids = torch.nn.utils.rnn.pad_sequence(
            tgt_ids_list, 
            batch_first=True, 
            padding_value=tokenizer.pad_token_id
        )
        return src_ids, tgt_ids
    
    debug_print("æ­£åœ¨å‡†å¤‡æ•°æ®é›†...")
    dataset = TextDataset(train_texts, tokenizer)
    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        collate_fn=collate_fn,
        pin_memory=True
    )

    debug_print("åˆ›å»ºæ¨¡å‹é…ç½®...")
    config = APTConfig(
        vocab_size=tokenizer.vocab_size,
        d_model=768,
        d_ff=2048,
        num_heads=12,
        num_encoder_layers=4,
        num_decoder_layers=4,
        max_seq_len=128,
        dropout=0.2,
        epsilon=2.0,
        alpha=0.001,
        beta=0.001,
        base_lr=learning_rate
    )
    
    debug_print("åˆå§‹åŒ–æ¨¡å‹...")
    model = APTLargeModel(config).to(device)
    model.train()
    
    import inspect
    #print("===== æ¨¡å‹ forward æ–¹æ³•å‚æ•° =====")
    #print(inspect.signature(model.forward))
    #print("================================")

    # ä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨è®¾ç½®
    from apt.apt_model.training.optimizer import create_optimizer_and_scheduler
    optimizer, scheduler = create_optimizer_and_scheduler(
        model, learning_rate, len(dataloader), epochs
    )
    
    # æ—©åœè®¾ç½®
    best_loss = float('inf')
    patience = 5
    patience_counter = 0

    # è®­ç»ƒä¿æŠ¤è®¾ç½®
    guard = None
    if enable_guard:
        early_stopping = None
        if early_stopping_patience is not None:
            early_stopping = EarlyStopping(
                patience=early_stopping_patience,
                mode='min',
                verbose=guard_verbose
            )

        guard = TrainingGuard(
            max_steps=max_steps,
            max_time_hours=max_time_hours,
            early_stopping=early_stopping,
            verbose=guard_verbose
        )
        guard.start()
        info_print("ğŸ›¡ï¸ è®­ç»ƒä¿æŠ¤å·²å¯ç”¨")
    
    # å°è¯•ä½¿ç”¨tensorboardè®°å½•è®­ç»ƒè¿‡ç¨‹
    try:
        SummaryWriter = torch.utils.tensorboard.SummaryWriter
        writer = SummaryWriter(log_dir=f"{save_path}_logs")
        use_tensorboard = True
    except:
        use_tensorboard = False
        debug_print("æœªå®‰è£…tensorboardï¼Œå°†ä¸ä½¿ç”¨tensorboardè®°å½•è®­ç»ƒè¿‡ç¨‹")
    
    # ä¿å­˜å‡½æ•°
    from apt.apt_model.training.checkpoint import save_model
    
    # ä¿å­˜è®­ç»ƒå‰çš„æ¨¡å‹ç”¨äºæ¯”è¾ƒ
    untrained_model = APTLargeModel(config).to(device)
    untrained_model.load_state_dict(model.state_dict())
    untrained_model.eval()
    
    global_step = 0
    train_losses = []
    best_quality_score = 0.0
    
    info_print(f"\nå¼€å§‹è®­ç»ƒï¼Œæ€»å…± {epochs} è½®...")
    
    autocast = torch.cuda.amp.autocast
    GradScaler = torch.cuda.amp.GradScaler
    
    # åœ¨è®­ç»ƒå¼€å§‹å‰åˆå§‹åŒ– GradScaler
    try:
        GradScaler = torch.cuda.amp.GradScaler
        scaler = GradScaler()
    except (ImportError, AttributeError):
        # åˆ›å»ºä¸€ä¸ªå‡çš„ GradScaler ä»¥ä¿æŒä»£ç å…¼å®¹æ€§
        class DummyScaler:
            def scale(self, loss):
                return loss
            def step(self, optimizer):
                optimizer.step()
            def update(self):
                pass
        scaler = DummyScaler()
        debug_print("è­¦å‘Š: æ··åˆç²¾åº¦è®­ç»ƒä¸å¯ç”¨ï¼Œä½¿ç”¨æ ‡å‡†ç²¾åº¦è®­ç»ƒ")

    # æ·»åŠ æ¢¯åº¦ç´¯ç§¯å‚æ•°
    accumulation_steps = 4  # å¯ä»¥æ ¹æ®éœ€æ±‚è°ƒæ•´
    
    # ä¸»è®­ç»ƒå¾ªç¯
    for epoch in range(epochs):
        total_loss = 0
        progress_bar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{epochs}")
        
        for i, batch in enumerate(progress_bar):
            try:
                if resource_monitor:
                    resource_monitor.check_resources()
                
                src_ids, tgt_ids = batch
                src_ids = src_ids.to(device)
                tgt_ids = tgt_ids.to(device)
                
                src_padding_mask = (src_ids == tokenizer.pad_token_id)  # å¡«å……æ©ç  [batch, src_len]
                tgt_mask = torch.triu(torch.ones(tgt_ids.size(1), tgt_ids.size(1), device=tgt_ids.device) * float('-inf'), diagonal=1)
                
                # åªåœ¨ç´¯ç§¯å‘¨æœŸå¼€å§‹æ—¶æ¸…é›¶æ¢¯åº¦
                if i % accumulation_steps == 0:
                    optimizer.zero_grad()
                
                # ä½¿ç”¨æ›´æ–°åçš„ autocast è¿›è¡Œæ··åˆç²¾åº¦å‰å‘è®¡ç®—å’ŒæŸå¤±è®¡ç®—
                amp_dtype = torch.bfloat16  # æˆ–ä½¿ç”¨torch.float32
                with torch.amp.autocast('cuda'):
                    try:
                        # åœ¨è¿™é‡Œæ·»åŠ æ‰“å°è¯­å¥
                        import inspect
                        #print(f"Model type: {type(model)}")
                        #print(f"Model forward signature: {inspect.signature(model.forward)}")
                        
                        logits = model(src_tokens=src_ids, tgt_tokens=src_ids, src_key_padding_mask=src_padding_mask, src_mask=None)
                    except Exception as e:
                        if logger:
                            logger.error(f"å‰å‘ä¼ æ’­å‡ºé”™: {e}")
                        debug_print(f"è­¦å‘Š: å‰å‘ä¼ æ’­å¤±è´¥: {e}ï¼Œè·³è¿‡å½“å‰æ‰¹æ¬¡")
                        continue

                    if torch.isnan(logits).any():
                        debug_print(f"è­¦å‘Š: ç¬¬{epoch+1}è½®ç¬¬{i+1}æ‰¹æ¬¡çš„logitsåŒ…å«NaNï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                        continue
                    
                    # è®¡ç®—æŸå¤±
                    shift_logits = logits[:, :-1, :].contiguous()
                    shift_labels = tgt_ids[:, 1:].contiguous()
                    
                    try:
                        loss = F.cross_entropy(
                            shift_logits.view(-1, shift_logits.size(-1)),
                            shift_labels.view(-1), 
                            ignore_index=tokenizer.pad_token_id,
                            label_smoothing=0.1
                        )
                        # æ ¹æ®ç´¯ç§¯æ­¥éª¤ç¼©æ”¾æŸå¤±
                        loss = loss / accumulation_steps
                    except Exception as e:
                        if logger:
                            logger.error(f"æŸå¤±è®¡ç®—å‡ºé”™: {e}")
                        debug_print(f"è­¦å‘Š: æŸå¤±è®¡ç®—å¤±è´¥: {e}ï¼Œè·³è¿‡å½“å‰æ‰¹æ¬¡")
                        continue

                    if torch.isnan(loss).any():
                        debug_print(f"è­¦å‘Š: ç¬¬{epoch+1}è½®ç¬¬{i+1}æ‰¹æ¬¡å‘ç°NaNæŸå¤±ï¼Œè·³è¿‡æ­¤æ‰¹æ¬¡")
                        continue
                
                # ä½¿ç”¨ GradScaler è¿›è¡Œåå‘ä¼ æ’­å’Œå‚æ•°æ›´æ–°
                try:
                    scaler.scale(loss).backward()
                except Exception as e:
                    if logger:
                        logger.error(f"åå‘ä¼ æ’­å‡ºé”™: {e}")
                    debug_print(f"è­¦å‘Š: åå‘ä¼ æ’­å¤±è´¥: {e}ï¼Œè·³è¿‡å½“å‰æ‰¹æ¬¡")
                    optimizer.zero_grad()
                    continue

                # æ¢¯åº¦è£å‰ªï¼ˆå¦‚æœéœ€è¦ï¼Œå¯ä»¥æ”¾åœ¨ scaler.step() å‰åï¼‰
                try:
                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)
                except Exception as e:
                    if logger:
                        logger.warning(f"æ¢¯åº¦è£å‰ªå‡ºé”™ï¼Œè·³è¿‡: {e}")
                    debug_print(f"è­¦å‘Š: æ¢¯åº¦è£å‰ªå¤±è´¥: {e}")
                
                # åªåœ¨ç´¯ç§¯å®Œæˆåæ›´æ–°å‚æ•°
                if (i + 1) % accumulation_steps == 0 or (i + 1) == len(dataloader):
                    # è¿›è¡Œå‚æ•°æ›´æ–°
                    scaler.step(optimizer)
                    scaler.update()
                    scheduler.step()
                
                    torch.cuda.empty_cache()  # æ¸…ç†GPUç¼“å­˜
                    
                    try:
                        current_lr = scheduler.get_last_lr()[0]
                        model.update_dynamic_taylor_parameters(current_lr)
                    except Exception as e:
                        if logger:
                            logger.warning(f"åŠ¨æ€å‚æ•°æ›´æ–°å‡ºé”™: {e}")
                        debug_print(f"è­¦å‘Š: åŠ¨æ€å‚æ•°æ›´æ–°å¤±è´¥: {e}")
                
                total_loss += loss.item() * accumulation_steps  # æ¢å¤å®é™…æŸå¤±
                train_losses.append(loss.item() * accumulation_steps)
                progress_bar.set_postfix({"loss": f"{loss.item() * accumulation_steps:.4f}", "lr": f"{scheduler.get_last_lr()[0]:.6f}"})
                
                if use_tensorboard:
                    writer.add_scalar('Loss/train', loss.item() * accumulation_steps, global_step)
                    writer.add_scalar('Learning_rate', scheduler.get_last_lr()[0], global_step)
                
                global_step += 1

                # è®­ç»ƒä¿æŠ¤æ£€æŸ¥
                if guard:
                    if not guard.step(loss=loss.item() * accumulation_steps, model=model):
                        info_print("ğŸ›‘ è®­ç»ƒä¿æŠ¤è§¦å‘åœæ­¢")
                        break

                # æ¯20ä¸ªbatchæ˜¾ç¤ºä¸€æ¬¡è®­ç»ƒæ–‡æœ¬ï¼ˆä¸é™åˆ¶debugæ¨¡å¼ï¼‰
                if (i + 1) % 20 == 0:
                    try:
                        # æ˜¾ç¤ºå½“å‰batchçš„ç¬¬ä¸€ä¸ªæ ·æœ¬
                        sample_ids = src_ids[0].cpu().tolist()
                        # ç§»é™¤padding token
                        sample_ids = [tid for tid in sample_ids if tid != tokenizer.pad_token_id]
                        sample_text = tokenizer.decode(sample_ids, skip_special_tokens=True)
                        info_print(f"\nğŸ“ è®­ç»ƒæ ·æœ¬ (Batch {i+1}): {sample_text[:100]}...")
                    except Exception as e:
                        pass  # é™é»˜å¿½ç•¥è§£ç é”™è¯¯
                    
            except Exception as e:
                if logger:
                    logger.error(f"å¤„ç†æ‰¹æ¬¡ {i} æ—¶å‡ºé”™: {e}")
                    logger.error(traceback.format_exc())
                debug_print(f"æ‰¹æ¬¡å¤„ç†é”™è¯¯: {e}ï¼Œè·³è¿‡å½“å‰æ‰¹æ¬¡")
                continue

        avg_loss = total_loss / max(1, len(dataloader))
        info_print(f"Epoch {epoch+1}/{epochs} å®Œæˆ, å¹³å‡æŸå¤±: {avg_loss:.4f}")

        # è®­ç»ƒä¿æŠ¤éªŒè¯æ£€æŸ¥
        should_stop_guard = False
        if guard:
            if not guard.validate(avg_loss):
                info_print("ğŸ›‘ è®­ç»ƒä¿æŠ¤ Early Stopping è§¦å‘")
                should_stop_guard = True

        if use_tensorboard:
            writer.add_scalar('Loss/epoch', avg_loss, epoch)

        try:
            if avg_loss < best_loss:
                best_loss = avg_loss
                save_model(model, tokenizer, path=save_path, config=config)
                info_print(f"âœ“ å‘ç°æ–°çš„æœ€ä½³æ¨¡å‹ï¼Œå·²ä¿å­˜åˆ° {save_path}")
                patience_counter = 0
            else:
                patience_counter += 1
                if patience_counter >= patience:
                    info_print(f"æ—©åœ: {patience} è½®æ²¡æœ‰æ”¹å–„ï¼Œåœæ­¢è®­ç»ƒ")
                    break

            if should_stop_guard:
                break

            _test_generation_after_epoch(model, tokenizer, logger, detected_language)
        except Exception as e:
            if logger:
                logger.error(f"è½®æ¬¡ç»“æŸå¤„ç†å‡ºé”™: {e}")
            debug_print(f"è­¦å‘Š: è½®æ¬¡ç»“æŸå¤„ç†å¤±è´¥: {e}")
    
    if use_tensorboard:
        writer.close()

    # æ‰“å°è®­ç»ƒä¿æŠ¤ç»Ÿè®¡
    if guard:
        stats = guard.get_stats()
        info_print(f"\n{'='*80}")
        info_print("è®­ç»ƒä¿æŠ¤ç»Ÿè®¡:")
        info_print(f"  æ€»æ­¥æ•°: {stats['total_steps']}")
        info_print(f"  è®­ç»ƒæ—¶é—´: {stats['elapsed_hours']:.2f} å°æ—¶")
        info_print(f"  NaN æŸå¤±: {stats['nan_losses']}")
        info_print(f"  Inf æŸå¤±: {stats['inf_losses']}")
        info_print(f"  æ¢¯åº¦çˆ†ç‚¸: {stats['gradient_explosions']}")
        info_print(f"  å†…å­˜è­¦å‘Š: {stats['memory_warnings']}")
        if stats['stopped']:
            info_print(f"  åœæ­¢åŸå› : {stats['stop_reason']}")
        info_print(f"{'='*80}\n")

    info_print("âœ“ è®­ç»ƒå®Œæˆï¼æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜ã€‚")

    try:
        _compare_model_outputs(untrained_model, model, tokenizer, detected_language)
    except Exception as e:
        if logger:
            logger.error(f"æ¨¡å‹æ¯”è¾ƒå‡ºé”™: {e}")
        debug_print(f"è­¦å‘Š: æ¨¡å‹æ¯”è¾ƒå¤±è´¥: {e}")
    
    return model, tokenizer, config

def _test_generation_after_epoch(model, tokenizer, logger=None, language="en"):
    """æµ‹è¯•æ¯ä¸ªè½®æ¬¡åçš„ç”Ÿæˆæ•ˆæœ"""
    # æ·»åŠ è¯Šæ–­æ‰“å°
    #print("\n===== å¼€å§‹è¯Šæ–­ _test_generation_after_epoch =====")
    #print(f"æ¨¡å‹ç±»å‹: {type(model)}")
    #print(f"æ¨¡å‹å±æ€§: {dir(model)}")
    #print("æ£€æŸ¥æ˜¯å¦æœ‰generateæ–¹æ³•:", hasattr(model, 'generate'))
    #print("æ£€æŸ¥æ˜¯å¦æ˜¯APTModelçš„å®ä¾‹:", isinstance(model, APTModel))
    
    #if hasattr(model, 'generate'):
        #print("generateæ–¹æ³•çš„ç­¾å:", model.generate.__code__.co_varnames)
    #print("===== è¯Šæ–­ç»“æŸ =====\n")
    # æ ¹æ®è¯­è¨€é€‰æ‹©æµ‹è¯•æç¤º
    if language == "zh":
        test_prompts = ["äººå·¥æ™ºèƒ½", "æ·±åº¦å­¦ä¹ ", "è‡ªç„¶è¯­è¨€", "å®‰æŸæ˜¯"]
    else:
        test_prompts = ["Hello", "What is", "The quick", "Artificial"]
        
    model.eval()
    if settings.get_debug_enabled():
        debug_print("\næœ¬è½®è®­ç»ƒåçš„æ–‡æœ¬ç”Ÿæˆç¤ºä¾‹:")
    gen_texts = []
    for prompt in test_prompts:
        with torch.no_grad():
            gen_text, _, _, _ = generate_natural_text(model, tokenizer, prompt, max_steps=15)
            debug_print(f"æç¤º: '{prompt}'")
            debug_print(f"ç”Ÿæˆ: '{gen_text}'")
            debug_print("-" * 30)
            gen_texts.append(gen_text)
    avg_quality = sum(evaluate_text_quality(text)[0] for text in gen_texts) / len(gen_texts)
    if settings.get_debug_enabled():
        debug_print(f"æœ¬è½®ç”Ÿæˆæ–‡æœ¬å¹³å‡è´¨é‡: {avg_quality:.2f}/100")
        if avg_quality < 40:
            debug_print("\nå®‰æŸï¼šè®­ç»ƒ...è¿˜ä¸å¤Ÿ...")
    model.train()
    return avg_quality

def _compare_model_outputs(untrained_model, trained_model, tokenizer, language="en"):
    """æ¯”è¾ƒè®­ç»ƒå‰åçš„æ¨¡å‹è¾“å‡º"""
    # æ ¹æ®è¯­è¨€é€‰æ‹©æµ‹è¯•æç¤º
    if language == "zh":
        test_prompts = [
            "äººå·¥æ™ºèƒ½æ˜¯",
            "æ·±åº¦å­¦ä¹ å¯ä»¥",
            "è‡ªç„¶è¯­è¨€å¤„ç†",
            "å®‰æŸæ˜¯",
            "æˆ‘è®¤ä¸º"
        ]
    else:
        test_prompts = [
            "Hello, how are you",
            "What is artificial intelligence",
            "The future of technology",
            "I think that",
            "The best way to"
        ]

    trained_model.eval()
    untrained_model.eval()
    untrained_scores = []
    trained_scores = []

    # Debugæ¨¡å¼ä¸‹æ˜¾ç¤ºè¯¦ç»†å¯¹æ¯”
    show_details = settings.get_debug_enabled()

    if show_details:
        debug_print("\n====================")
        debug_print("è®­ç»ƒå‰åæ•ˆæœå¯¹æ¯”")
        debug_print("====================")

    for prompt in test_prompts:
        if show_details:
            debug_print(f"\næç¤º: '{prompt}'")

        with torch.no_grad():
            untrained_text, _, _, _ = generate_natural_text(untrained_model, tokenizer, prompt, max_steps=20)
            untrained_score, untrained_feedback = evaluate_text_quality(untrained_text)
            untrained_scores.append(untrained_score)

        if show_details:
            debug_print(f"æœªè®­ç»ƒæ¨¡å‹: '{untrained_text}'")
            debug_print(f"è´¨é‡è¯„åˆ†: {untrained_score}/100 - {untrained_feedback}")

        with torch.no_grad():
            trained_text, _, _, _ = generate_natural_text(trained_model, tokenizer, prompt, max_steps=20)
            trained_score, trained_feedback = evaluate_text_quality(trained_text)
            trained_scores.append(trained_score)

        if show_details:
            debug_print(f"è®­ç»ƒåæ¨¡å‹: '{trained_text}'")
            debug_print(f"è´¨é‡è¯„åˆ†: {trained_score}/100 - {trained_feedback}")
            debug_print("-" * 50)

    avg_untrained = sum(untrained_scores) / len(untrained_scores)
    avg_trained = sum(trained_scores) / len(trained_scores)
    improvement = avg_trained - avg_untrained

    # ã€å§‹ç»ˆæ˜¾ç¤ºã€‘æœ€ç»ˆè¯„ä¼°å’Œå®‰æŸè¯„åˆ†
    info_print(f"\næ•´ä½“è¯„ä¼°:")
    info_print(f"æœªè®­ç»ƒæ¨¡å‹å¹³å‡è´¨é‡: {avg_untrained:.2f}/100")
    info_print(f"è®­ç»ƒåæ¨¡å‹å¹³å‡è´¨é‡: {avg_trained:.2f}/100")
    info_print(f"è´¨é‡æå‡: {improvement:.2f} åˆ†")

    # å®‰æŸçš„æœ€ç»ˆè¯„ä»·ï¼ˆå§‹ç»ˆæ˜¾ç¤ºï¼‰
    if improvement < -5:
        info_print("\nå®‰æŸï¼šå¥‡æ€ªâ€¦â€¦æ€ä¹ˆæ„Ÿè§‰å®ƒå˜ç¬¨äº†ï¼Ÿï¼ˆè´¨é‡ä¸‹é™ï¼Œå»ºè®®æ£€æŸ¥è¶…å‚æ•°ï¼‰")
    elif improvement < 0:
        info_print("\nå®‰æŸï¼šçœ‹èµ·æ¥æ•ˆæœå·®ä¸å¤šï¼Œä¹Ÿè®¸è¿˜éœ€è¦æ›´å¤šè®­ç»ƒæ•°æ®ï¼Ÿ")
    elif avg_trained < 50:
        info_print("\nå®‰æŸï¼šè™½ç„¶æœ‰è¿›æ­¥ï¼Œä½†è¿˜è¿œè¿œä¸å¤Ÿå“¦ï¼ç»§ç»­åŠ æ²¹ï¼")
    else:
        info_print("\nå®‰æŸï¼šè®­ç»ƒå®Œæˆå¾—ä¸é”™ï¼ä¾¦å¯Ÿéª‘å£«ä¸ºä½ ç‚¹èµï¼")