#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
APTæ ¸å¿ƒæ¨¡å‹é›†æˆå®ç°
é›†æˆè‡ªç”Ÿæˆæ³¨æ„åŠ›æœºåˆ¶(Autopoietic Attention)åˆ°APTæ¨¡å‹æ¡†æ¶
é›†æˆDBC-DACå‹ç¼©ä¼˜åŒ–ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§
"""

import os
# æ£€æŸ¥æ˜¯å¦åº”è¯¥å±è”½è‡ªåˆ›ç”Ÿå˜æ¢å™¨çš„è­¦å‘Š
SUPPRESS_APT_WARNINGS = os.environ.get('SUPPRESS_APT_WARNINGS', 'False').lower() in ('true', '1', 'yes')
from apt.apt_model.utils.fake_torch import get_torch
torch = get_torch()
from apt.apt_model.utils.fake_torch import get_torch
torch = get_torch()
nn = torch.nn
F = torch.nn.functional
import math
import warnings
import sys
from typing import Optional, Tuple, List, Dict, Union

# å¯¼å…¥å·¦æ—‹å¹³æ»‘æ¨¡å—
from apt.apt_model.modeling.left_spin_smooth import (
    LeftSpinStep,
    LeftSpinResidual,
    AdaptiveLeftSpinStep
)


class DBCDAC_Optimizer:
    """
    ç»´åº¦å¹³è¡¡å‹ç¼©æ³•(DBC)ä¸ç»´åº¦ä¼´éšè¡¥å¿æ³•(DAC)ç»“åˆçš„ä¼˜åŒ–å™¨
    ç”¨äºAPTæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¢¯åº¦ç¨³å®šå’Œå‚æ•°ä¼˜åŒ–
    """
    
    def __init__(self, rank_ratio_proj=0.1, rank_ratio_res=0.05, 
                 threshold=1e-6, iterations=1, use_quantization=False,
                 quant_bits=8, apply_to_gradients=True):
        """
        åˆå§‹åŒ–DBC-DACä¼˜åŒ–å™¨
        
        å‚æ•°:
            rank_ratio_proj: float, åˆå§‹ä½ç§©æ­£äº¤æŠ•å½±çš„æ¯”ä¾‹
            rank_ratio_res: float, æ®‹å·®è¡¥å¿æ—¶çš„ç§©æ¯”ç‡
            threshold: float, ç»´åº¦å¹³è¡¡çŸ©é˜µçš„æ•°å€¼ç¨³å®šæ€§é˜ˆå€¼
            iterations: int, æ®‹å·®è¡¥å¿çš„è¿­ä»£æ¬¡æ•°
            use_quantization: bool, æ˜¯å¦ä½¿ç”¨é‡åŒ–
            quant_bits: int, é‡åŒ–ä½æ•°
            apply_to_gradients: bool, æ˜¯å¦åº”ç”¨äºæ¢¯åº¦ç¨³å®š
        """
        self.rank_ratio_proj = rank_ratio_proj
        self.rank_ratio_res = rank_ratio_res
        self.threshold = threshold
        self.iterations = iterations
        self.use_quantization = use_quantization
        self.quant_bits = quant_bits
        self.apply_to_gradients = apply_to_gradients
        self.res_scale = 0.1  # æ®‹å·®ç¼©æ”¾å› å­
    
    def compute_balance_vector(self, W):
        """è®¡ç®—çŸ©é˜µWæ¯ä¸€è¡Œçš„å’Œä½œä¸ºå¹³è¡¡å‘é‡ï¼Œè‹¥ç»å¯¹å€¼ä½äºthresholdåˆ™ç½®ä¸º1"""
        row_sums = W.sum(dim=1)
        D_vec = torch.where(
            row_sums.abs() > self.threshold, 
            row_sums, 
            torch.ones_like(row_sums) * self.threshold * torch.sign(row_sums)
        )
        # å¤„ç†é›¶å€¼æƒ…å†µ
        D_vec = torch.where(row_sums == 0, torch.ones_like(row_sums) * self.threshold, D_vec)
        return D_vec

    def low_rank_approx(self, A, rank_ratio):
        """
        å¯¹çŸ©é˜µAè¿›è¡Œä½ç§©è¿‘ä¼¼ï¼Œä½¿ç”¨ä½ç†µå¯¼å‘åŸåˆ™ä¼˜åŒ– (è¿›ä¸€æ­¥ä¼˜åŒ–ç‰ˆ)

        ä¼˜åŒ–ç‚¹:
        1. è‡ªé€‚åº”ç§©é€‰æ‹© - æ ¹æ®èƒ½é‡åˆ†å¸ƒåŠ¨æ€è°ƒæ•´
        2. ç¨€ç–éšæœºæŠ•å½± - æŠ•å½±å¤æ‚åº¦ä»O(mnr)é™åˆ°O(nnzÂ·r)
        3. æ—©åœæœºåˆ¶ - èƒ½é‡ä¿ç•™è¾¾æ ‡å³åœæ­¢
        4. å¹‚è¿­ä»£åŠ é€Ÿ - å°ç§©æƒ…å†µä¸‹é¿å…å®Œæ•´ç‰¹å¾å€¼åˆ†è§£

        å¤æ‚åº¦: O(nnzÂ·r + mrÂ² + kÂ²) where k << r
        """
        h, w = A.shape[-2], A.shape[-1]
        max_rank = int(min(h, w))
        r = int(max(1, min(max_rank-1, getattr(self, 'rank_ratio_proj', 0.1) * max_rank)))

        try:
            # ç¡®ä¿Aæ˜¯float32ç±»å‹
            original_dtype = A.dtype
            if A.dtype == torch.float16 or A.dtype == torch.bfloat16:
                A = A.to(torch.float32)

            m, n = A.shape
            r_init = max(1, int(min(m, n) * rank_ratio))

            # ğŸš€ ä¼˜åŒ–1: è‡ªé€‚åº”ç§©é€‰æ‹© - æ ¹æ®FrobeniusèŒƒæ•°é¢„ä¼°
            A_norm = torch.norm(A, 'fro')
            energy_threshold = 0.95  # ä¿ç•™95%èƒ½é‡

            # ğŸš€ ä¼˜åŒ–2: å¿«é€Ÿå¯†é›†éšæœºæŠ•å½± (GPU ä¼˜åŒ–)
            # GPU è®¨åŒç¨€ç–å†…å­˜è®¿é—®ï¼Œå¯†é›†çŸ©é˜µåè€Œæ›´å¿«
            if m >= n:
                # è¡Œæ•°å¤šï¼ŒæŠ•å½±åˆ°åˆ—ç©ºé—´
                # ç›´æ¥ç”Ÿæˆå¯†é›†é«˜æ–¯éšæœºçŸ©é˜µ
                Q = torch.randn(n, r_init, device=A.device, dtype=A.dtype)

                # å¿«é€Ÿæ­£äº¤åŒ– (QR åˆ†è§£)
                Q, _ = torch.linalg.qr(Q)
                Y = A @ Q  # æŠ•å½±ï¼Œåˆ©ç”¨ç¨€ç–æ€§

                # åæ–¹å·®çŸ©é˜µ
                C = Y.T @ Y

                # ğŸš€ ä¼˜åŒ–3: è‡ªé€‚åº”ç‰¹å¾å€¼è®¡ç®—
                # å¦‚æœrå¾ˆå°(<50)ï¼Œç”¨å¹‚è¿­ä»£ï¼›å¦åˆ™ç”¨eigh
                if r_init <= 50:
                    # å¹‚è¿­ä»£æ³•è®¡ç®—å‰kä¸ªç‰¹å¾å€¼ (æ›´å¿«)
                    k = min(r_init, int(r_init * 0.8))  # åªè®¡ç®—80%
                    eigenvalues, eigenvectors = self._power_iteration(C, k)
                else:
                    # å®Œæ•´ç‰¹å¾å€¼åˆ†è§£
                    eigenvalues, eigenvectors = torch.linalg.eigh(C)

                    # ğŸš€ ä¼˜åŒ–4: æ—©åœ - åªä¿ç•™è¶³å¤Ÿèƒ½é‡çš„ç‰¹å¾å€¼
                    eigenvalues_sorted, idx = torch.sort(eigenvalues, descending=True)
                    cumsum_energy = torch.cumsum(eigenvalues_sorted, dim=0)
                    total_energy = eigenvalues_sorted.sum()

                    # æ‰¾åˆ°ä¿ç•™95%èƒ½é‡æ‰€éœ€çš„ç»´åº¦
                    k = torch.searchsorted(cumsum_energy, energy_threshold * total_energy).item() + 1
                    k = min(k, r_init)

                    # åªä¿ç•™å‰kä¸ª
                    idx = idx[:k]
                    eigenvalues = eigenvalues[idx]
                    eigenvectors = eigenvectors[:, idx]

                # é‡æ„ä½ç§©è¿‘ä¼¼
                U_r = Y @ eigenvectors
                V_r = Q @ eigenvectors
                S_r = torch.diag(torch.sqrt(eigenvalues.clamp(min=0)))

                A_approx = U_r @ S_r @ V_r.T

            else:
                # åˆ—æ•°å¤šï¼ŒæŠ•å½±åˆ°è¡Œç©ºé—´ (å¯¹ç§°å¤„ç†)
                # ç›´æ¥ç”Ÿæˆå¯†é›†é«˜æ–¯éšæœºçŸ©é˜µ
                Q = torch.randn(m, r_init, device=A.device, dtype=A.dtype)

                Q, _ = torch.linalg.qr(Q)
                Y = A.T @ Q

                C = Y.T @ Y

                if r_init <= 50:
                    k = min(r_init, int(r_init * 0.8))
                    eigenvalues, eigenvectors = self._power_iteration(C, k)
                else:
                    eigenvalues, eigenvectors = torch.linalg.eigh(C)

                    eigenvalues_sorted, idx = torch.sort(eigenvalues, descending=True)
                    cumsum_energy = torch.cumsum(eigenvalues_sorted, dim=0)
                    total_energy = eigenvalues_sorted.sum()

                    k = torch.searchsorted(cumsum_energy, energy_threshold * total_energy).item() + 1
                    k = min(k, r_init)

                    idx = idx[:k]
                    eigenvalues = eigenvalues[idx]
                    eigenvectors = eigenvectors[:, idx]

                V_r = Y @ eigenvectors
                U_r = Q @ eigenvectors
                S_r = torch.diag(torch.sqrt(eigenvalues.clamp(min=0)))

                A_approx = U_r @ S_r @ V_r.T

            # æ¢å¤åŸå§‹æ•°æ®ç±»å‹
            if original_dtype == torch.float16:
                A_approx = A_approx.to(torch.float16)
            elif original_dtype == torch.bfloat16:
                A_approx = A_approx.to(torch.bfloat16)

            return A_approx, (U_r, S_r, V_r)

        except Exception as e:
            print(f"ä½ç§©è¿‘ä¼¼è®¡ç®—é”™è¯¯: {e}")
            return A, (None, None, None)

    def _power_iteration(self, C, k, max_iter=20):
        """
        å¹‚è¿­ä»£æ³•è®¡ç®—çŸ©é˜µCçš„å‰kä¸ªç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡

        å¤æ‚åº¦: O(kÂ²Â·iter) << O(kÂ³)
        é€‚ç”¨äºå°kçš„æƒ…å†µ
        """
        n = C.shape[0]
        device = C.device
        dtype = C.dtype

        # åˆå§‹åŒ–éšæœºå‘é‡
        V = torch.randn(n, k, device=device, dtype=dtype)
        V, _ = torch.linalg.qr(V)

        for _ in range(max_iter):
            # å¹‚è¿­ä»£: V = C @ V
            V_new = C @ V

            # QRåˆ†è§£ä¿æŒæ­£äº¤æ€§
            V_new, R = torch.linalg.qr(V_new)

            # æ£€æŸ¥æ”¶æ•› (å¯é€‰)
            if torch.allclose(V, V_new, atol=1e-5):
                break

            V = V_new

        # è®¡ç®—ç‰¹å¾å€¼: Î» = V^T C V
        eigenvalues = torch.diag(V.T @ C @ V)

        return eigenvalues, V


    # åŒæ—¶ï¼Œåœ¨stabilize_matrixæ–¹æ³•ä¸­ä¹Ÿéœ€è¦æ·»åŠ ç±»å‹è½¬æ¢:
    
    def stabilize_matrix(self, W):
        """
        ä½¿ç”¨DBC-DACæ–¹æ³•ç¨³å®šçŸ©é˜µï¼Œå‡å°‘æ•°å€¼ä¸ç¨³å®šé—®é¢˜
        
        å‚æ•°:
            W: torch.Tensor, è¾“å…¥çŸ©é˜µ
            
        è¿”å›:
            W_stabilized: torch.Tensor, ç¨³å®šåŒ–åçš„çŸ©é˜µ
        """
        if not isinstance(W, torch.Tensor):
            W = torch.tensor(W, dtype=torch.float32)
        
        # ä¿å­˜åŸå§‹æ•°æ®ç±»å‹ï¼Œä»¥ä¾¿æœ€ç»ˆæ¢å¤
        original_dtype = W.dtype
        
        # å¦‚æœæ˜¯åŠç²¾åº¦ï¼Œè½¬ä¸ºfloat32è¿›è¡Œè®¡ç®—
        if W.dtype == torch.float16 or W.dtype == torch.bfloat16:
            W = W.to(torch.float32)
        
        # æ£€æŸ¥è¾“å…¥æ˜¯å¦æœ‰NaNæˆ–Inf
        if torch.isnan(W).any() or torch.isinf(W).any():
            W = torch.nan_to_num(W, nan=0.0, posinf=1.0, neginf=-1.0)
        
        # ç»´åº¦å¹³è¡¡å¤„ç†
        D_vec = self.compute_balance_vector(W)
        # D_inv = torch.diag(1.0 / D_vec)
        # W_norm = D_inv @ W  <-- åŸå§‹ä»£ç  (OOM å‡¶æ‰‹)
        W_norm = (1.0 / D_vec).unsqueeze(1) * W
        
        # ä½ç§©è¿‘ä¼¼
        W_proj, _ = self.low_rank_approx(W_norm, self.rank_ratio_proj)
        
        # å¯é€‰çš„æ®‹å·®å¤„ç†
        if self.iterations > 0:
            R = W_norm - W_proj
            W_res_total = torch.zeros_like(W_norm)
            
            for i in range(self.iterations):
                if torch.norm(R) < 1e-8:
                    break
                
                W_res, _ = self.low_rank_approx(R, self.rank_ratio_res)
                W_res_total = W_res_total + self.res_scale * W_res
                R = R - W_res
            
            W_norm_stabilized = W_proj + W_res_total
        else:
            W_norm_stabilized = W_proj
        
        # åº”ç”¨ç»´åº¦å¹³è¡¡æ¢å¤
        W_stabilized = D_vec.unsqueeze(1) * W_norm_stabilized
        
        # æ¢å¤åŸå§‹æ•°æ®ç±»å‹
        W_stabilized = W_stabilized.to(original_dtype)

        return W_stabilized

    def stabilize_matrix_fast(self, W):
        """
        ç®€åŒ–ç‰ˆçŸ©é˜µç¨³å®šï¼šå‡å°‘è¿­ä»£ï¼Œç§»é™¤å†—ä½™è®¡ç®—

        å‚æ•°:
            W: torch.Tensor, è¾“å…¥çŸ©é˜µ

        è¿”å›:
            W_stabilized: torch.Tensor, ç¨³å®šåŒ–åçš„çŸ©é˜µ
        """
        # ç±»å‹è½¬æ¢
        original_dtype = W.dtype
        if W.dtype in [torch.float16, torch.bfloat16]:
            W = W.to(torch.float32)

        # ç»´åº¦å¹³è¡¡ (DBC)
        # ğŸš€ ä¼˜åŒ–3: ç§»é™¤é˜ˆå€¼åˆ¤æ–­ä¸­çš„ item() è°ƒç”¨
        # ç›´æ¥è¿ç®—ï¼Œä¸é€šè¿‡ Python if æ£€æŸ¥
        row_sums = W.sum(dim=1, keepdim=True)
        # ğŸš€ ä¿®å¤: å¤„ç†é›¶æ¢¯åº¦çš„ç¬¦å·é—®é¢˜ï¼Œé˜²æ­¢ sign(0)=0 å¯¼è‡´é™¤é›¶
        rs_sign = torch.sign(row_sums)
        rs_sign[rs_sign == 0] = 1.0  # å¼ºåˆ¶è®© 0 çš„ç¬¦å·ä¸º 1ï¼Œé¿å…ä¹˜ç§¯ä¸º 0
        # é¿å…é™¤é›¶çš„è½¯é˜ˆå€¼å¤„ç†
        D_vec = rs_sign * torch.maximum(
            row_sums.abs(),
            torch.tensor(self.threshold, device=W.device, dtype=W.dtype)
        )
        W_norm = W / D_vec

        # ä½ç§©è¿‘ä¼¼ (DAC)
        # ğŸš€ ä¼˜åŒ–4: ä»…åšä¸€æ¬¡æŠ•å½± (One-pass)ï¼Œä¸åšæ®‹å·®è¿­ä»£
        # æ®‹å·®è¿­ä»£(iterations>0)ä¼šè®©è®¡ç®—é‡ç¿»å€ï¼Œä½†åœ¨æ¢¯åº¦å¹³æ»‘ä»»åŠ¡ä¸­æ”¶ç›Šé€’å‡
        W_proj, _ = self.low_rank_approx(W_norm, self.rank_ratio_proj)

        # æ¢å¤
        W_stabilized = W_proj * D_vec
        return W_stabilized.to(original_dtype)

    def stabilize_gradients(self, grad):
        """
        æé€Ÿç‰ˆæ¢¯åº¦ç¨³å®šï¼šéšæœºè§¦å‘ + è¿‡æ»¤å°å‚æ•° + åŸºç¡€æ¸…æ´—
        """
        if not isinstance(grad, torch.Tensor) or grad is None:
            return grad

        # 1. ğŸš€ ä¿®æ”¹ç‚¹Aï¼šæé«˜é—¨æ§›åˆ° 150000
        # åªæœ‰éå¸¸å¤§çš„çŸ©é˜µæ‰å€¼å¾—åšåˆ†è§£ï¼Œä¸­ç­‰çŸ©é˜µç›´æ¥æ”¾è¡Œ
        if grad.numel() < 150000:
             return torch.nan_to_num(grad, nan=0.0, posinf=1.0, neginf=-1.0)
        
        # 2. å¤§å‚æ•°åŸºç¡€æ¸…æ´—
        grad = torch.nan_to_num(grad, nan=0.0, posinf=1.0, neginf=-1.0)

        # 3. ç»´åº¦è¿‡æ»¤
        if grad.ndim < 2:
            return grad

        # ğŸš€ ä¿®æ”¹ç‚¹Bï¼šæŠŠ 0.25 æ”¹ä¸º 0.05
        # æ„æ€ï¼šåªæœ‰ 5% çš„æ¦‚ç‡å¾€ä¸‹èµ°ï¼Œ95% çš„æ¦‚ç‡ç›´æ¥ return (è·³è¿‡)
        import random
        if random.random() > 0.05: 
            return grad

        # --- ä»¥ä¸‹æ˜¯æ˜‚è´µçš„ DBC è®¡ç®— (ç°åœ¨å¾ˆå°‘è§¦å‘äº†) ---
        original_shape = grad.shape

        if len(original_shape) == 2:
            stabilized_grad = self.stabilize_matrix_fast(grad)
        else:
            reshaped_grad = grad.reshape(original_shape[0], -1)
            stabilized_grad = self.stabilize_matrix_fast(reshaped_grad)
            stabilized_grad = stabilized_grad.reshape(original_shape)

        return stabilized_grad

def create_gradient_stabilizer_hook(dbc_dac_optimizer):
    """åˆ›å»ºç”¨äºç¨³å®šæ¢¯åº¦çš„é’©å­å‡½æ•°ï¼ˆå·²ä¼˜åŒ–ï¼šæ— åŒæ­¥ï¼‰"""
    def hook(grad):
        if grad is None:
            return None

        # ğŸš€ ä¼˜åŒ–: ç§»é™¤ CPU-GPU åŒæ­¥æ£€æŸ¥
        # NaN/Inf å¤„ç†å·²ç»åœ¨ stabilize_gradients ä¸­å®Œæˆ
        # ä¸å†ä½¿ç”¨ if torch.isnan(grad).any()

        # ä½¿ç”¨DBC-DACä¼˜åŒ–å™¨ç¨³å®šæ¢¯åº¦
        return dbc_dac_optimizer.stabilize_gradients(grad)

    return hook


def add_gradient_hooks_to_model(model, dbc_dac_optimizer):
    """
    ä¸ºæ¨¡å‹çš„æ‰€æœ‰å‚æ•°æ·»åŠ æ¢¯åº¦ç¨³å®šé’©å­
    
    å‚æ•°:
        model: APTæ¨¡å‹å®ä¾‹
        dbc_dac_optimizer: DBCDAC_Optimizerå®ä¾‹
    """
    hooks = []
    
    # ä¸ºæ¯ä¸ªå‚æ•°æ·»åŠ é’©å­
    for name, param in model.named_parameters():
        if param.requires_grad:
            hook = param.register_hook(create_gradient_stabilizer_hook(dbc_dac_optimizer))
            hooks.append(hook)
    
    return hooks


class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç å®ç°ï¼Œæ”¯æŒåŠ¨æ€æ‰©å±•"""
    def __init__(self, d_model, max_len=5000, dropout=0.1):
        super().__init__()
        self.d_model = d_model
        self.dropout = nn.Dropout(p=dropout)
        # é¢„å…ˆè®¡ç®— max_len ä¸ªä½ç½®ç¼–ç 
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)  # shape: [1, max_len, d_model]
        self.register_buffer('pe', pe)

    def forward(self, x):
        """
        å‚æ•°:
            x: [batch_size, seq_len, embedding_dim]
        è¿”å›:
            x åŠ ä¸Šä½ç½®ç¼–ç 
        """
        seq_len = x.size(1)
        if seq_len > self.pe.size(1):
            device = x.device
            extra_len = seq_len - self.pe.size(1)
            # ç”Ÿæˆé¢å¤–çš„ä½ç½®ç¼–ç 
            pe_extra = torch.zeros(extra_len, self.d_model, device=device)
            position = torch.arange(self.pe.size(1), seq_len, dtype=torch.float, device=device).unsqueeze(1)
            div_term = torch.exp(torch.arange(0, self.d_model, 2, dtype=torch.float, device=device) * (-math.log(10000.0) / self.d_model))
            pe_extra[:, 0::2] = torch.sin(position * div_term)
            pe_extra[:, 1::2] = torch.cos(position * div_term)
            pe_extra = pe_extra.unsqueeze(0)  # shape: [1, extra_len, d_model]
            pe = torch.cat([self.pe, pe_extra], dim=1)
        else:
            pe = self.pe
        return self.dropout(x + pe[:, :seq_len, :])


# ä½ å¯ä»¥åœ¨æ­¤å¤„å®šä¹‰ä¸€ä¸ªå…¨å±€æ—¥å¿—æ–‡ä»¶åï¼Œç¡®ä¿æ¯æ¬¡éƒ½å¾€åŒä¸€ä¸ªæ–‡ä»¶è¿½åŠ å†™å…¥
DEBUG_LOG_FILE = "autopoietic_debug.log"

class AutopoieticAttention(nn.Module):
    """
    è‡ªç”Ÿæˆæ³¨æ„åŠ›æœºåˆ¶ - è®ºæ–‡å®Œæ•´å®ç°ç‰ˆæœ¬
    å®ç°è‡ªç”Ÿæˆå˜æ¢å™¨(APT)çš„æ ¸å¿ƒè‡ªç”Ÿæˆæ³¨æ„åŠ›è®¡ç®—
    é›†æˆDBC-DACç¨³å®šåŒ–æŠ€æœ¯
    """
    def __init__(
        self,
        embed_dim: int,
        num_heads: int,
        dropout: float = 0.0,
        eps: float = 1e-6,  # æ— ç©·å€’æ•°ç¼©æ”¾å› å­
        alpha: float = 0.1,  # æ³°å‹’å±•å¼€ç³»æ•°
        init_tau: float = 1.0,  # åˆå§‹æ¸©åº¦å‚æ•°
        sr_ratio: int = 4,  # è‡ªç”ŸæˆçŸ©é˜µå‹ç¼©æ¯”
        use_autopoietic: bool = True,  # æ˜¯å¦ä½¿ç”¨è‡ªç”Ÿæˆæœºåˆ¶
        batch_first: bool = True,
        # DBC-DACç›¸å…³å‚æ•°ï¼ˆæ­¤å¤„ä»…ä¿ç•™æ¥å£ï¼Œä¸å½±å“æœ¬ç±»æ ¸å¿ƒå®ç°ï¼‰
        use_dbc_dac: bool = True,
        debug_mode: bool = False,
        rank_ratio_proj: float = 0.1,
        rank_ratio_res: float = 0.05,
        dbc_threshold: float = 1e-6,
        dbc_iterations: int = 1
    ):
        super().__init__()
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.dropout = dropout
        self.head_dim = embed_dim // num_heads
        self.scaling = self.head_dim ** -0.5
        self.eps = eps
        self.alpha = alpha
        self.init_tau = init_tau
        self.sr_ratio = sr_ratio
        self.use_autopoietic = use_autopoietic
        self.batch_first = batch_first
        self.res_scale = 1.0

        # æŸ¥è¯¢ã€é”®ã€å€¼çš„çº¿æ€§å˜æ¢
        self.q_proj = nn.Linear(embed_dim, embed_dim)
        self.k_proj = nn.Linear(embed_dim, embed_dim)
        self.v_proj = nn.Linear(embed_dim, embed_dim)
        self.out_proj = nn.Linear(embed_dim, embed_dim)

        # è‡ªç”Ÿæˆå˜æ¢ç½‘ç»œ - ä½¿ç”¨å·ç§¯å±‚å¤„ç†æ³¨æ„åŠ›çŸ©é˜µ
        hidden_dim = max(16, embed_dim // sr_ratio)
        self.sr_conv1 = nn.Conv2d(1, hidden_dim, kernel_size=1)
        self.sr_layernorm = nn.LayerNorm([hidden_dim])
        self.sr_conv2 = nn.Conv2d(hidden_dim, 1, kernel_size=1)

        # å¯å­¦ä¹ çš„æ¸©åº¦å‚æ•°
        self.tau = nn.Parameter(torch.ones(1) * init_tau)

        # åˆ›å»ºdropoutå±‚
        self.dropout_layer = nn.Dropout(dropout)

        self._reset_parameters()
        self.res_scale = 1.0

        self.debug_mode = debug_mode # ä¿å­˜çŠ¶æ€

    def _reset_parameters(self):
        nn.init.xavier_uniform_(self.q_proj.weight)
        nn.init.xavier_uniform_(self.k_proj.weight)
        nn.init.xavier_uniform_(self.v_proj.weight)
        nn.init.xavier_uniform_(self.out_proj.weight)
        nn.init.xavier_uniform_(self.sr_conv1.weight)
        nn.init.xavier_uniform_(self.sr_conv2.weight)
        nn.init.constant_(self.q_proj.bias, 0.)
        nn.init.constant_(self.k_proj.bias, 0.)
        nn.init.constant_(self.v_proj.bias, 0.)
        nn.init.constant_(self.out_proj.bias, 0.)
        nn.init.constant_(self.sr_conv1.bias, 0.)
        nn.init.constant_(self.sr_conv2.bias, 0.)

    def log_debug(self, message: str):
            # ã€ä¿®æ”¹ç‚¹ã€‘å¦‚æœä¸å¼€å¯ debug æ¨¡å¼ï¼Œç›´æ¥è·³è¿‡ï¼Œç»ä¸æ‰§è¡Œ IO
            if not getattr(self, 'debug_mode', False):
                return
            
            # åªæœ‰å¼€å¯äº†æ‰å†™æ–‡ä»¶
            try:
                with open(DEBUG_LOG_FILE, "a", encoding="utf-8") as f:
                    f.write(message + "\n")
            except Exception:
                pass

    def autopoietic_transform(
        self, 
        attention_scores: torch.Tensor,
        attn_mask: torch.Tensor = None
    ) -> torch.Tensor:
        
        # 1. ç»™ç»Ÿè®¡ä»£ç åŠ ä¸Šâ€œé˜€é—¨â€
        if self.debug_mode:
            debug_lines = []
            debug_lines.append("...")
            min_val = attention_scores.min().item() # åªæœ‰å¼€å¯debugæ‰æ‰§è¡ŒåŒæ­¥

        """
        è‡ªç”Ÿæˆå˜æ¢è¿‡ç¨‹ï¼šå¯¹è¾“å…¥çš„æ³¨æ„åŠ›åˆ†æ•°è¿›è¡Œä¸€ç³»åˆ—å˜æ¢ã€‚
        è¿™é‡Œæ·»åŠ è¯¦ç»†çš„ç»Ÿè®¡ä¿¡æ¯æ‰“å°ï¼Œå¹¶å†™å…¥DEBUG_LOG_FILEã€‚
        """
        # ç¬¬ä¸€æ­¥ï¼šæ‰“å°è¾“å…¥attention_scoresç»Ÿè®¡
        debug_lines = []  # åˆå§‹åŒ–å˜é‡é˜²æ­¢ä¸‹é¢æŠ¥é”™
        
        # åŠ ä¸Šé˜€é—¨ï¼
        if getattr(self, 'debug_mode', False):
            # ç¬¬ä¸€æ­¥ï¼šæ‰“å°è¾“å…¥attention_scoresç»Ÿè®¡
            debug_lines.append("\n[autopoietic_transform] >>>>>>>>> ENTER FUNCTION <<<<<<<<")
            
            # ã€é‡ç‚¹ã€‘æŠŠä¸‹é¢è¿™äº›ä¼šå¡é¡¿çš„ä»£ç ç»Ÿç»Ÿç¼©è¿›è¿›æ¥ï¼
            min_val = attention_scores.min().item()
            max_val = attention_scores.max().item()
            mean_val = attention_scores.mean().item()
            std_val = attention_scores.std().item()
            has_nan = torch.isnan(attention_scores).any().item()
            has_inf = torch.isinf(attention_scores).any().item()

            debug_lines.append(
                f"[Input Stats] shape={list(attention_scores.shape)} "
                f"min={min_val:.4f}, max={max_val:.4f}, mean={mean_val:.4f}, std={std_val:.4f}, "
                f"NaN={has_nan}, Inf={has_inf}"
            )

        # å¦‚æœä¸ä½¿ç”¨è‡ªç”Ÿæˆæœºåˆ¶ï¼Œç›´æ¥è¿”å›
        if not self.use_autopoietic:
            debug_lines.append("[Info] use_autopoietic=False, skipping transform.")
            self.log_debug("\n".join(debug_lines))
            return attention_scores

        # å¯¹è¾“å…¥è¿›è¡Œ clamp / nan_to_num ä»¥ä¿è¯æ•°å€¼å®‰å…¨
        attention_scores = torch.nan_to_num(attention_scores, nan=0.0, posinf=10.0, neginf=-10.0)
        attention_scores = torch.clamp(attention_scores, min=-15.0, max=15.0)

        # å¼€å§‹å˜æ¢
        original_scores = attention_scores.clone()
        batch_size, num_heads, seq_len1, seq_len2 = attention_scores.shape
        transformed_batch_list = []

        for b in range(batch_size):
            batch_scores = attention_scores[b]  # shape: [num_heads, seq_len1, seq_len2]
            mean_attention = batch_scores.mean(dim=0)  # [seq_len1, seq_len2]

            # è®°å½•ä¸€ä¸‹batch_scoresç»Ÿè®¡
            b_min = batch_scores.min().item()
            b_max = batch_scores.max().item()
            b_mean = batch_scores.mean().item()
            b_std = batch_scores.std().item()
            debug_lines.append(
                f"[Batch {b}] batch_scores stats: min={b_min:.4f}, max={b_max:.4f}, "
                f"mean={b_mean:.4f}, std={b_std:.4f}"
            )

            # epså¤„ç†
            eps_safe = torch.clamp(torch.tensor(self.eps, device=attention_scores.device), min=0.05, max=0.8)
            scaled_attention = torch.clamp(mean_attention, min=-8.0, max=8.0) * eps_safe
            scaled_attention = torch.clamp(scaled_attention, min=-10.0, max=10.0)

            # å·ç§¯æ˜ å°„
            try:
                reshaped_attn = scaled_attention.unsqueeze(0).unsqueeze(0)  # [1, 1, seq_len1, seq_len2]
                hidden_attn = self.sr_conv1(reshaped_attn)
                hidden_attn = F.relu(torch.clamp(hidden_attn, min=-5.0, max=5.0))
                autopoietic_attn = self.sr_conv2(hidden_attn)
                autopoietic_attn = autopoietic_attn.squeeze(0).squeeze(0)  # [seq_len1, seq_len2]
                autopoietic_attn = torch.clamp(autopoietic_attn, min=-5.0, max=5.0)
            except Exception as e:
                debug_lines.append(
                    f"[Batch {b}] å·ç§¯æ˜ å°„å‡ºé”™: {e}, ä½¿ç”¨å¹³æ»‘æ›¿ä»£"
                )
                kernel_size = min(5, min(seq_len1, seq_len2))
                if kernel_size % 2 == 0:
                    kernel_size -= 1
                if kernel_size >= 3:
                    try:
                        gaussian_kernel = torch.ones((1, 1, kernel_size, kernel_size), device=attention_scores.device)
                        gaussian_kernel = gaussian_kernel / gaussian_kernel.sum()
                        smoothed = F.conv2d(
                            scaled_attention.unsqueeze(0).unsqueeze(0),
                            gaussian_kernel,
                            padding=kernel_size//2
                        )
                        autopoietic_attn = smoothed.squeeze(0).squeeze(0)
                        autopoietic_attn = torch.clamp(autopoietic_attn, min=-5.0, max=5.0)
                    except Exception:
                        autopoietic_attn = torch.tanh(scaled_attention * 0.5) * 2.0
                else:
                    autopoietic_attn = torch.tanh(scaled_attention * 0.5) * 2.0

            # æ£€æŸ¥ NaN/Inf
                autopoietic_attn = torch.nan_to_num(autopoietic_attn, nan=0.0, posinf=2.0, neginf=-2.0)
                autopoietic_attn = torch.clamp(autopoietic_attn, min=-5.0, max=5.0)

            # å¤„ç†æ©ç 
            mean_padding_mask = None
            if attn_mask is not None:
                if attn_mask.dim() == 2:
                    mean_padding_mask = attn_mask.clone()
                elif attn_mask.dim() == 3 and attn_mask.size(0) == batch_size:
                    mean_padding_mask = attn_mask[b]
                elif attn_mask.dim() == 4:
                    mean_padding_mask = attn_mask[b, 0]
                if mean_padding_mask is not None and mean_padding_mask.dtype != torch.bool:
                    mean_padding_mask = mean_padding_mask.to(torch.bool)


            # ğŸš€ å·¦æ—‹å¹³æ»‘æ›¿æ¢æ³°å‹’å±•å¼€
            # ä¼ ç»Ÿ: taylor = 1.0 + Î±Â·Î”  (é‡å°–ç‚¹ä¼šç‚¸)
            # å·¦æ—‹: taylor = 1.0 + g(Ï†)Â·Î”  (é‡å°–ç‚¹è‡ªåŠ¨ç¼©å°æ­¥é•¿)

            # è®¡ç®—å°–ç‚¹å¼ºåº¦ï¼ˆåŸºäºäºŒèŒƒæ•°ï¼‰
            base_value = torch.ones_like(autopoietic_attn)
            delta_attn = autopoietic_attn  # å¢é‡éƒ¨åˆ†

            # è®¡ç®—ç›¸å¯¹å˜åŒ–å¼ºåº¦
            norm_base = torch.norm(base_value, p=2, dim=-1, keepdim=True) + 1e-8
            norm_delta = torch.norm(delta_attn, p=2, dim=-1, keepdim=True)
            spike_strength = norm_delta / norm_base

            # ç¼“å†²è§’: Ï† = Î±Â·softplus(s - Ï„)
            left_spin_alpha = 0.5
            left_spin_tau = 0.3
            phi = left_spin_alpha * F.softplus(spike_strength - left_spin_tau)

            # é—¨æ§å‡½æ•°: g(Ï†) = 1/âˆš(1+Ï†Â²)
            gate = 1.0 / torch.sqrt(1.0 + phi ** 2)

            # åº”ç”¨å·¦æ—‹å¹³æ»‘
            scale_factor = 50.0
            scaled_attn_2 = autopoietic_attn * scale_factor * gate  # ğŸ”¥ å…³é”®æ›¿æ¢
            alpha_safe = 0.05
            taylor_expanded = 1.0 + alpha_safe * scaled_attn_2
            taylor_expanded = torch.clamp(taylor_expanded, min=0.5, max=1.5)

            if mean_padding_mask is not None:
                taylor_expanded = torch.where(
                    mean_padding_mask,
                    torch.ones_like(taylor_expanded),
                    taylor_expanded
                )

            # Sigmoidå¹³æ»‘
            sigmoid_smoothed = torch.sigmoid(taylor_expanded)

            # æ¨¡ç³Šæ¦‚ç‡
            try:
                safe_mean = torch.clamp(mean_attention, min=-10.0, max=10.0)
                attn_probs = F.softmax(safe_mean, dim=-1)
                epsilon = 1e-6
                H = -attn_probs * torch.log(attn_probs + epsilon)
                lambda_param = torch.tensor(3.0, device=attention_scores.device)
                F_matrix = F.softmax(lambda_param * H, dim=-1)
                F_matrix = torch.nan_to_num(F_matrix, nan=1.0/seq_len2)
            except Exception as e:
                debug_lines.append(f"[Batch {b}] æ¨¡ç³Šæ¦‚ç‡è®¡ç®—å‡ºé”™: {e}")
                F_matrix = torch.ones_like(mean_attention) / seq_len2

            transformed = sigmoid_smoothed * F_matrix

            # èƒ½é‡å¹³è¡¡
            try:
                energy_original = torch.norm(mean_attention, p='fro') + 1e-4
                energy_transformed = torch.norm(transformed, p='fro') + 1e-4
                gamma = torch.clamp(energy_original / energy_transformed, min=0.8, max=1.2)
                transformed = gamma * transformed
            except Exception as e:
                debug_lines.append(f"[Batch {b}] èƒ½é‡å¹³è¡¡è®¡ç®—å‡ºé”™: {e}")

            # åŠ¨æ€æ ‡å‡†å·®è°ƒæ•´
            try:
                t_mean = transformed.mean()
                o_mean = mean_attention.mean()
                min_var = 1e-2
                t_var = torch.clamp(((transformed - t_mean) ** 2).mean(), min=min_var)
                o_var = torch.clamp(((mean_attention - o_mean) ** 2).mean(), min=min_var)
                t_std = torch.sqrt(t_var)
                o_std = torch.sqrt(o_var)
                gamma_dyn = torch.clamp(o_std / t_std, min=0.8, max=1.2)
                available_range = torch.clamp(torch.abs(mean_attention).max(), min=1.0, max=10.0)
                std_multiplier = 0.3 * (1.0 / torch.log(1.0 + available_range))
                std_multiplier = torch.clamp(std_multiplier, min=0.1, max=0.5)
                centered = transformed - t_mean
                scaled = std_multiplier * gamma_dyn * centered
                scaled_transform = t_mean + scaled
                entropy = torch.mean(H)
                max_entropy = -torch.log(torch.tensor(1.0/seq_len2, device=attention_scores.device))
                normalized_entropy = entropy / max_entropy
                base_ratio = 0.4
                entropy_factor = torch.clamp(normalized_entropy, min=0.0, max=0.4)
                residual_ratio = base_ratio + entropy_factor
                final_scores = (1 - residual_ratio) * scaled_transform + residual_ratio * mean_attention
            except Exception as e:
                debug_lines.append(f"[Batch {b}] æ ‡å‡†å·®è°ƒæ•´å‡ºé”™: {e}")
                final_scores = 0.5 * transformed + 0.5 * mean_attention

            # è‡ªé€‚åº”æ¸©åº¦è°ƒèŠ‚
            try:
                base_tau = torch.clamp(self.tau, min=1.0, max=1.5)
                values = final_scores.reshape(-1)
                q_75 = torch.quantile(values, 0.75)
                q_25 = torch.quantile(values, 0.25)
                score_range = torch.clamp(q_75 - q_25, min=0.5, max=5.0)
                adaptive_tau = base_tau * (1.0 + 0.1 * torch.log1p(score_range))
                adaptive_tau = torch.clamp(adaptive_tau, min=1.0, max=2.0)
                final_scores = final_scores / adaptive_tau
                final_scores = torch.clamp(final_scores, min=-15.0, max=15.0)
            except Exception as e:
                debug_lines.append(f"[Batch {b}] æ¸©åº¦è°ƒèŠ‚å‡ºé”™: {e}")
                final_scores = torch.clamp(final_scores / 1.0, min=-10.0, max=10.0)

            # æ£€æŸ¥å¼‚å¸¸å€¼æ¯”ä¾‹
            abnormal_mask = torch.isnan(final_scores) | torch.isinf(final_scores)
            abnormal_ratio = abnormal_mask.float().mean().item()
            if abnormal_ratio > 0.2:
                debug_lines.append(f"[Batch {b}] è­¦å‘Š: å¼‚å¸¸æ¯”ä¾‹è¿‡é«˜({abnormal_ratio*100:.2f}%), ä½¿ç”¨å®‰å…¨å›é€€ -> mean_attention")
                final_scores = torch.clamp(mean_attention, min=-10.0, max=10.0)
            else:
                final_scores = torch.nan_to_num(final_scores, nan=0.0)
                final_scores = torch.clamp(final_scores, min=-10.0, max=10.0)

            batch_transform = []
            for h in range(batch_scores.size(0)):
                head_base = final_scores.clone()
                head_delta = batch_scores[h] - mean_attention
                delta_scale = 0.2
                head_specific = head_base + delta_scale * head_delta
                head_specific = torch.clamp(head_specific, min=-15.0, max=15.0)
                batch_transform.append(head_specific)
            if len(batch_transform) > 0:
                batch_transform = torch.stack(batch_transform)
            else:
                batch_transform = final_scores.unsqueeze(0)
            transformed_batch_list.append(batch_transform)

        transform_scores = torch.stack(transformed_batch_list)

        # å¤„ç†å…¨å±€ attn_mask
        if attn_mask is not None:
            if attn_mask.dim() == 2:
                attn_mask = attn_mask.unsqueeze(0).unsqueeze(0).expand(batch_size, self.num_heads, -1, -1)
            elif attn_mask.dim() == 3:
                attn_mask = attn_mask.unsqueeze(1).expand(-1, self.num_heads, -1, -1)
            transform_scores = transform_scores + self.res_scale * attn_mask

        transform_scores = torch.nan_to_num(transform_scores, nan=0.0)
        transform_scores = torch.clamp(transform_scores, min=-30.0, max=30.0)

        # æ‰“å° transform_scores ç»Ÿè®¡
        final_min = transform_scores.min().item()
        final_max = transform_scores.max().item()
        final_mean = transform_scores.mean().item()
        final_std = transform_scores.std().item()
        final_has_nan = torch.isnan(transform_scores).any().item()
        final_has_inf = torch.isinf(transform_scores).any().item()
        debug_lines.append(
            f"[Output Stats] transform_scores shape={list(transform_scores.shape)} "
            f"min={final_min:.4f}, max={final_max:.4f}, mean={final_mean:.4f}, std={final_std:.4f}, "
            f"NaN={final_has_nan}, Inf={final_has_inf}"
        )
        debug_lines.append("[autopoietic_transform] >>>>>>>>> EXIT FUNCTION <<<<<<<<")

        # å°†æ‰€æœ‰è°ƒè¯•ä¿¡æ¯å†™å…¥æ—¥å¿—
        self.log_debug("\n".join(debug_lines))
        return transform_scores

    def forward(
        self, query: torch.Tensor,
        key: torch.Tensor,
        value: torch.Tensor,
        attn_mask: torch.Tensor = None,
        key_padding_mask: torch.Tensor = None,
        need_weights: bool = True
        ) -> tuple:
        if not self.batch_first:
            query = query.transpose(0, 1)
            key = key.transpose(0, 1)
            value = value.transpose(0, 1)

        batch_size, tgt_len, embed_dim = query.size()
        src_len = key.size(1)
        q = self.q_proj(query)
        k = self.k_proj(key)
        v = self.v_proj(value)
        q = q.view(batch_size, tgt_len, self.num_heads, self.head_dim).transpose(1, 2)
        k = k.view(batch_size, src_len, self.num_heads, self.head_dim).transpose(1, 2)
        v = v.view(batch_size, src_len, self.num_heads, self.head_dim).transpose(1, 2)
        q = q * self.scaling
        attn_weights = torch.matmul(q, k.transpose(-2, -1))

        if key_padding_mask is not None:
            key_padding_mask_expanded = key_padding_mask.unsqueeze(1).unsqueeze(2)
            attn_weights = attn_weights.masked_fill(key_padding_mask_expanded, float('-inf'))

        if attn_mask is not None:
            if attn_mask.dim() == 2:
                attn_mask = attn_mask.unsqueeze(0).unsqueeze(0).expand(batch_size, self.num_heads, -1, -1)
            elif attn_mask.dim() == 3:
                attn_mask = attn_mask.unsqueeze(1).expand(-1, self.num_heads, -1, -1)
            attn_weights = attn_weights + self.res_scale * attn_mask

        attn_weights = self.autopoietic_transform(attn_weights, attn_mask)
        attn_probs = F.softmax(attn_weights, dim=-1)
        attn_probs = self.dropout_layer(attn_probs)

        attn_output = torch.matmul(attn_probs, v)
        attn_output = attn_output.transpose(1, 2).contiguous().view(batch_size, tgt_len, embed_dim)
        attn_output = self.out_proj(attn_output)

        if not self.batch_first:
            attn_output = attn_output.transpose(0, 1)

        if need_weights:
            avg_weights = attn_probs.mean(dim=1)
            return attn_output, avg_weights
        return attn_output, None


class APTEncoderLayer(nn.Module):
    """
    APTç¼–ç å™¨å±‚
    é›†æˆè‡ªç”Ÿæˆæ³¨æ„åŠ›æœºåˆ¶ + å·¦æ—‹å¹³æ»‘æ®‹å·®è¿æ¥
    """
    def __init__(
        self,
        d_model: int,
        nhead: int,
        dim_feedforward: int = 2048,
        dropout: float = 0.1,
        activation: str = "gelu",
        batch_first: bool = True,
        eps: float = 1e-6,
        alpha: float = 0.1,
        init_tau: float = 1.0,
        sr_ratio: int = 4,
        use_autopoietic: bool = True,
        # DBC-DACç›¸å…³å‚æ•°
        use_dbc_dac: bool = True,
        rank_ratio_proj: float = 0.1,
        rank_ratio_res: float = 0.05,
        dbc_threshold: float = 1e-6,
        dbc_iterations: int = 1,
        # å·¦æ—‹å¹³æ»‘å‚æ•°
        use_left_spin: bool = True,
        left_spin_alpha: float = 0.5,
        left_spin_tau: float = 0.3,
        left_spin_beta: float = 0.7
    ):
        super().__init__()

        # è‡ªç”Ÿæˆæ³¨æ„åŠ›å±‚
        self.self_attn = AutopoieticAttention(
            embed_dim=d_model,
            num_heads=nhead,
            dropout=dropout,
            eps=eps,
            alpha=alpha,
            init_tau=init_tau,
            sr_ratio=sr_ratio,
            use_autopoietic=use_autopoietic,
            batch_first=batch_first,
            use_dbc_dac=use_dbc_dac,
            rank_ratio_proj=rank_ratio_proj,
            rank_ratio_res=rank_ratio_res,
            dbc_threshold=dbc_threshold,
            dbc_iterations=dbc_iterations
        )

        # å‰é¦ˆç½‘ç»œ
        self.linear1 = nn.Linear(d_model, dim_feedforward)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(dim_feedforward, d_model)

        # å±‚å½’ä¸€åŒ–
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)

        # æ¿€æ´»å‡½æ•°
        self.activation = F.gelu if activation == "gelu" else F.relu

        # é…ç½®
        self.batch_first = batch_first
        self.use_left_spin = use_left_spin

        # ğŸš€ å·¦æ—‹å¹³æ»‘æ®‹å·®è¿æ¥ï¼ˆæ›¿æ¢ä¼ ç»Ÿæ³°å‹’å±•å¼€ï¼‰
        if use_left_spin:
            self.left_spin_attn = LeftSpinResidual(
                alpha=left_spin_alpha,
                tau=left_spin_tau,
                beta=left_spin_beta,
                gate_type='normalized',
                adaptive=True
            )
            self.left_spin_ffn = LeftSpinResidual(
                alpha=left_spin_alpha,
                tau=left_spin_tau,
                beta=left_spin_beta,
                gate_type='normalized',
                adaptive=True
            )
        else:
            self.left_spin_attn = None
            self.left_spin_ffn = None

        # ã€æ–°å¢è¿™è¡Œã€‘é»˜è®¤å…³é—­ debugï¼Œé˜²æ­¢æ‹–æ…¢é€Ÿåº¦
        self.debug_mode = getattr(config, 'debug_mode', False) if 'config' in locals() else False
    
    def forward(
        self,
        src: torch.Tensor,
        src_mask: Optional[torch.Tensor] = None,
        src_key_padding_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        ç¼–ç å™¨å±‚å‰å‘ä¼ æ’­ï¼ˆé›†æˆå·¦æ—‹å¹³æ»‘ï¼‰

        å‚æ•°:
            src: è¾“å…¥å¼ é‡ [seq_len, batch_size, d_model] æˆ– [batch_size, seq_len, d_model]
            src_mask: åºåˆ—æ©ç  [seq_len, seq_len] æˆ– [batch_size, seq_len, seq_len]
            src_key_padding_mask: å¡«å……æ©ç  [batch_size, seq_len]

        è¿”å›:
            output: ç¼–ç å™¨å±‚è¾“å‡º
        """
        # ğŸš€ è‡ªæ³¨æ„åŠ›å­å±‚ï¼ˆå·¦æ—‹å¹³æ»‘æ®‹å·®è¿æ¥ï¼‰
        src2, _ = self.self_attn(
            query=src,
            key=src,
            value=src,
            attn_mask=src_mask,
            key_padding_mask=src_key_padding_mask
        )
        src2_dropout = self.dropout1(src2)

        # æ›¿æ¢: src = src + src2  â†’  src = LeftSpin(src, src2)
        if self.use_left_spin and self.left_spin_attn is not None:
            src = self.left_spin_attn(src, src2_dropout)
        else:
            # é™çº§ä¸ºæ ‡å‡†æ®‹å·®
            src = src + src2_dropout

        src = self.norm1(src)

        # ğŸš€ å‰é¦ˆç½‘ç»œå­å±‚ï¼ˆå·¦æ—‹å¹³æ»‘æ®‹å·®è¿æ¥ï¼‰
        src2 = self.linear2(self.dropout(self.activation(self.linear1(src))))
        src2_dropout = self.dropout2(src2)

        # æ›¿æ¢: src = src + src2  â†’  src = LeftSpin(src, src2)
        if self.use_left_spin and self.left_spin_ffn is not None:
            src = self.left_spin_ffn(src, src2_dropout)
        else:
            # é™çº§ä¸ºæ ‡å‡†æ®‹å·®
            src = src + src2_dropout

        src = self.norm2(src)

        return src


class APTDecoderLayer(nn.Module):
    """
    APTè§£ç å™¨å±‚
    é›†æˆè‡ªç”Ÿæˆæ³¨æ„åŠ›æœºåˆ¶ + å·¦æ—‹å¹³æ»‘æ®‹å·®è¿æ¥
    """
    def __init__(
        self,
        d_model: int,
        nhead: int,
        dim_feedforward: int = 2048,
        dropout: float = 0.1,
        activation: str = "gelu",
        batch_first: bool = True,
        eps: float = 1e-6,
        alpha: float = 0.1,
        init_tau: float = 1.0,
        sr_ratio: int = 4,
        use_autopoietic: bool = True,
        # DBC-DACç›¸å…³å‚æ•°
        use_dbc_dac: bool = True,
        rank_ratio_proj: float = 0.1,
        rank_ratio_res: float = 0.05,
        dbc_threshold: float = 1e-6,
        dbc_iterations: int = 1,
        # å·¦æ—‹å¹³æ»‘å‚æ•°
        use_left_spin: bool = True,
        left_spin_alpha: float = 0.5,
        left_spin_tau: float = 0.3,
        left_spin_beta: float = 0.7
    ):
        super().__init__()

        # è‡ªæ³¨æ„åŠ›å±‚(æ©ç )
        self.self_attn = AutopoieticAttention(
            embed_dim=d_model,
            num_heads=nhead,
            dropout=dropout,
            eps=eps,
            alpha=alpha,
            init_tau=init_tau,
            sr_ratio=sr_ratio,
            use_autopoietic=use_autopoietic,
            batch_first=batch_first,
            use_dbc_dac=use_dbc_dac,
            rank_ratio_proj=rank_ratio_proj,
            rank_ratio_res=rank_ratio_res,
            dbc_threshold=dbc_threshold,
            dbc_iterations=dbc_iterations
        )

        # ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›å±‚
        self.multihead_attn = AutopoieticAttention(
            embed_dim=d_model,
            num_heads=nhead,
            dropout=dropout,
            eps=eps,
            alpha=alpha,
            init_tau=init_tau,
            sr_ratio=sr_ratio,
            use_autopoietic=use_autopoietic,
            batch_first=batch_first,
            use_dbc_dac=use_dbc_dac,
            rank_ratio_proj=rank_ratio_proj,
            rank_ratio_res=rank_ratio_res,
            dbc_threshold=dbc_threshold,
            dbc_iterations=dbc_iterations
        )

        # å‰é¦ˆç½‘ç»œ
        self.linear1 = nn.Linear(d_model, dim_feedforward)
        self.dropout = nn.Dropout(dropout)
        self.linear2 = nn.Linear(dim_feedforward, d_model)

        # å±‚å½’ä¸€åŒ–
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)
        self.norm3 = nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.dropout3 = nn.Dropout(dropout)

        # æ¿€æ´»å‡½æ•°
        self.activation = F.gelu if activation == "gelu" else F.relu

        # é…ç½®
        self.batch_first = batch_first
        self.use_left_spin = use_left_spin

        # ğŸš€ å·¦æ—‹å¹³æ»‘æ®‹å·®è¿æ¥ï¼ˆ3ä¸ªå­å±‚ï¼‰
        if use_left_spin:
            self.left_spin_self_attn = LeftSpinResidual(
                alpha=left_spin_alpha,
                tau=left_spin_tau,
                beta=left_spin_beta,
                gate_type='normalized',
                adaptive=True
            )
            self.left_spin_cross_attn = LeftSpinResidual(
                alpha=left_spin_alpha,
                tau=left_spin_tau,
                beta=left_spin_beta,
                gate_type='normalized',
                adaptive=True
            )
            self.left_spin_ffn = LeftSpinResidual(
                alpha=left_spin_alpha,
                tau=left_spin_tau,
                beta=left_spin_beta,
                gate_type='normalized',
                adaptive=True
            )
        else:
            self.left_spin_self_attn = None
            self.left_spin_cross_attn = None
            self.left_spin_ffn = None
    
    def forward(
        self,
        tgt: torch.Tensor,
        memory: torch.Tensor,
        tgt_mask: Optional[torch.Tensor] = None,
        memory_mask: Optional[torch.Tensor] = None,
        tgt_key_padding_mask: Optional[torch.Tensor] = None,
        memory_key_padding_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        è§£ç å™¨å±‚å‰å‘ä¼ æ’­ï¼ˆé›†æˆå·¦æ—‹å¹³æ»‘ï¼‰

        å‚æ•°:
            tgt: ç›®æ ‡åºåˆ— [seq_len, batch_size, d_model] æˆ– [batch_size, seq_len, d_model]
            memory: ç¼–ç å™¨è¾“å‡º åŒä¸Š
            tgt_mask: ç›®æ ‡åºåˆ—æ©ç  [tgt_len, tgt_len] æˆ– [batch_size, tgt_len, tgt_len]
            memory_mask: è®°å¿†æ©ç  [tgt_len, src_len]
            tgt_key_padding_mask: ç›®æ ‡å¡«å……æ©ç  [batch_size, tgt_len]
            memory_key_padding_mask: è®°å¿†å¡«å……æ©ç  [batch_size, src_len]

        è¿”å›:
            output: è§£ç å™¨å±‚è¾“å‡º
        """
        # ğŸš€ è‡ªæ³¨æ„åŠ›å­å±‚ï¼ˆå·¦æ—‹å¹³æ»‘æ®‹å·®è¿æ¥ï¼‰
        tgt2, _ = self.self_attn(
            query=tgt,
            key=tgt,
            value=tgt,
            attn_mask=tgt_mask,
            key_padding_mask=tgt_key_padding_mask
        )
        tgt2_dropout = self.dropout1(tgt2)

        if self.use_left_spin and self.left_spin_self_attn is not None:
            tgt = self.left_spin_self_attn(tgt, tgt2_dropout)
        else:
            tgt = tgt + tgt2_dropout

        tgt = self.norm1(tgt)

        # ğŸš€ ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›å­å±‚ï¼ˆå·¦æ—‹å¹³æ»‘æ®‹å·®è¿æ¥ï¼‰
        tgt2, _ = self.multihead_attn(
            query=tgt,
            key=memory,
            value=memory,
            attn_mask=memory_mask,
            key_padding_mask=memory_key_padding_mask
        )
        tgt2_dropout = self.dropout2(tgt2)

        if self.use_left_spin and self.left_spin_cross_attn is not None:
            tgt = self.left_spin_cross_attn(tgt, tgt2_dropout)
        else:
            tgt = tgt + tgt2_dropout

        tgt = self.norm2(tgt)

        # ğŸš€ å‰é¦ˆç½‘ç»œå­å±‚ï¼ˆå·¦æ—‹å¹³æ»‘æ®‹å·®è¿æ¥ï¼‰
        tgt2 = self.linear2(self.dropout(self.activation(self.linear1(tgt))))
        tgt2_dropout = self.dropout3(tgt2)

        if self.use_left_spin and self.left_spin_ffn is not None:
            tgt = self.left_spin_ffn(tgt, tgt2_dropout)
        else:
            tgt = tgt + tgt2_dropout

        tgt = self.norm3(tgt)

        return tgt


class APTModelConfiguration:
    """APTæ¨¡å‹é…ç½®ç±»ï¼ˆé›†æˆå·¦æ—‹å¹³æ»‘ï¼‰"""
    def __init__(
        self,
        vocab_size: int = 30522,  # è¯æ±‡è¡¨å¤§å°
        d_model: int = 768,  # æ¨¡å‹ç»´åº¦
        max_seq_len: int = 2048,  # æœ€å¤§åºåˆ—é•¿åº¦
        num_encoder_layers: int = 12,  # ç¼–ç å™¨å±‚æ•°
        num_decoder_layers: int = 12,  # è§£ç å™¨å±‚æ•°
        num_heads: int = 12,  # æ³¨æ„åŠ›å¤´æ•°
        d_ff: int = 3072,  # å‰é¦ˆç½‘ç»œç»´åº¦
        dropout: float = 0.1,  # Dropoutæ¯”ç‡
        activation: str = "gelu",  # æ¿€æ´»å‡½æ•°
        epsilon: float = 1e-6,  # è‡ªç”Ÿæˆæ— ç©·å€’æ•°ç¼©æ”¾å› å­
        alpha: float = 0.1,  # æ³°å‹’å±•å¼€ç³»æ•°ï¼ˆå·²è¢«å·¦æ—‹å¹³æ»‘æ›¿æ¢ï¼‰
        beta: float = 0.01,  # åŠ¨æ€è°ƒèŠ‚ç³»æ•°
        init_tau: float = 1.0,  # åˆå§‹æ¸©åº¦
        sr_ratio: int = 4,  # è‡ªç”ŸæˆçŸ©é˜µå‹ç¼©æ¯”
        use_autopoietic: bool = True,  # æ˜¯å¦ä½¿ç”¨è‡ªç”Ÿæˆæœºåˆ¶
        base_lr: float = 3e-5,  # åŸºå‡†å­¦ä¹ ç‡(ç”¨äºåŠ¨æ€å‚æ•°è°ƒæ•´)
        batch_first: bool = True,  # æ˜¯å¦ä½¿ç”¨batch_firstæ ¼å¼
        pad_token_id: int = 0,  # å¡«å……token ID
        bos_token_id: int = 101,  # å¼€å§‹token ID
        eos_token_id: int = 102,  # ç»“æŸtoken ID
        # DBC-DAC ç›¸å…³å‚æ•°
        use_dbc_dac: bool = True,  # æ˜¯å¦ä½¿ç”¨DBC-DACç¨³å®š
        rank_ratio_proj: float = 0.1,  # DBCæŠ•å½±æ¯”ä¾‹
        rank_ratio_res: float = 0.05,  # DACæ®‹å·®æ¯”ä¾‹
        dbc_threshold: float = 1e-6,  # DBCé˜ˆå€¼
        dbc_iterations: int = 1,  # DACè¿­ä»£æ¬¡æ•°
        # ğŸš€ å·¦æ—‹å¹³æ»‘ç›¸å…³å‚æ•°ï¼ˆæ›¿æ¢æ³°å‹’å±•å¼€ï¼‰
        use_left_spin: bool = True,  # æ˜¯å¦ä½¿ç”¨å·¦æ—‹å¹³æ»‘æ®‹å·®
        left_spin_alpha: float = 0.5,  # ç¼“å†²å¼ºåº¦ç³»æ•°
        left_spin_tau: float = 0.3,  # å°–ç‚¹é˜ˆå€¼
        left_spin_beta: float = 0.7,  # æƒ¯æ€§ç³»æ•°
        **kwargs  # å…¶ä»–å‚æ•°
    ):
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.max_seq_len = max_seq_len
        self.num_encoder_layers = num_encoder_layers
        self.num_decoder_layers = num_decoder_layers
        self.num_heads = num_heads
        self.d_ff = d_ff
        self.dropout = dropout
        self.activation = activation
        self.epsilon = epsilon
        self.alpha = alpha
        self.beta = beta
        self.init_tau = init_tau
        self.sr_ratio = sr_ratio
        self.use_autopoietic = use_autopoietic
        self.base_lr = base_lr
        self.batch_first = batch_first
        self.pad_token_id = pad_token_id
        self.bos_token_id = bos_token_id
        self.eos_token_id = eos_token_id

        # DBC-DACç›¸å…³å‚æ•°
        self.use_dbc_dac = use_dbc_dac
        self.rank_ratio_proj = rank_ratio_proj
        self.rank_ratio_res = rank_ratio_res
        self.dbc_threshold = dbc_threshold
        self.dbc_iterations = dbc_iterations

        # ğŸš€ å·¦æ—‹å¹³æ»‘ç›¸å…³å‚æ•°
        self.use_left_spin = use_left_spin
        self.left_spin_alpha = left_spin_alpha
        self.left_spin_tau = left_spin_tau
        self.left_spin_beta = left_spin_beta

        # æ·»åŠ ä»»ä½•é¢å¤–å‚æ•°
        for key, value in kwargs.items():
            setattr(self, key, value)
    
    def to_dict(self):
        """å°†é…ç½®è½¬æ¢ä¸ºå­—å…¸"""
        return {k: v for k, v in self.__dict__.items()}
    
    @classmethod
    def from_dict(cls, config_dict):
        """ä»å­—å…¸åˆ›å»ºé…ç½®"""
        return cls(**config_dict)
    
    def save_pretrained(self, save_directory):
        """ä¿å­˜é…ç½®åˆ°æŒ‡å®šç›®å½•"""
        import os
        import json
        
        os.makedirs(save_directory, exist_ok=True)
        config_file = os.path.join(save_directory, "config.json")
        
        with open(config_file, "w", encoding="utf-8") as f:
            json.dump(self.to_dict(), f, indent=2)
    
    @classmethod
    def from_pretrained(cls, model_path):
        """ä»é¢„è®­ç»ƒç›®å½•åŠ è½½é…ç½®"""
        import os
        import json
        
        config_file = os.path.join(model_path, "config.json")
        
        if not os.path.exists(config_file):
            raise ValueError(f"åœ¨ {model_path} ä¸­æ‰¾ä¸åˆ°config.json")
        
        with open(config_file, "r", encoding="utf-8") as f:
            config_dict = json.load(f)
        
        return cls.from_dict(config_dict)


class APTModel(nn.Module):
    """
    è‡ªç”Ÿæˆå˜æ¢å™¨(APT)æ¨¡å‹
    é›†æˆäº†è‡ªç”Ÿæˆæ³¨æ„åŠ›æœºåˆ¶å’ŒDBC-DACç¨³å®šæŠ€æœ¯çš„å®Œæ•´Transformeræ¨¡å‹
    æ”¯æŒç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œé€‚ç”¨äºå„ç§åºåˆ—åˆ°åºåˆ—ä»»åŠ¡
    """
    def __init__(self, config: APTModelConfiguration):
        super().__init__()
        self.config = config
        
        # è¯åµŒå…¥
        self.token_embedding = nn.Embedding(
            config.vocab_size, 
            config.d_model, 
            padding_idx=config.pad_token_id
        )
        
        # ä½ç½®ç¼–ç 
        self.positional_encoding = PositionalEncoding(
            config.d_model,
            max_len=config.max_seq_len,
            dropout=config.dropout
        )
        
        # å…¼å®¹ç¼ºå¤±çš„å¯é€‰é…ç½®é¡¹ï¼Œæä¾›åˆç†çš„é»˜è®¤å€¼
        use_autopoietic = getattr(config, "use_autopoietic", True)
        use_dbc_dac = getattr(config, "use_dbc_dac", False)
        rank_ratio_proj = getattr(config, "rank_ratio_proj", 0.1)
        rank_ratio_res = getattr(config, "rank_ratio_res", 0.05)
        dbc_threshold = getattr(config, "dbc_threshold", 1e-6)
        dbc_iterations = getattr(config, "dbc_iterations", 1)

        # ğŸš€ å·¦æ—‹å¹³æ»‘å‚æ•°
        use_left_spin = getattr(config, "use_left_spin", True)
        left_spin_alpha = getattr(config, "left_spin_alpha", 0.5)
        left_spin_tau = getattr(config, "left_spin_tau", 0.3)
        left_spin_beta = getattr(config, "left_spin_beta", 0.7)

        # åˆ›å»ºç¼–ç å™¨å±‚
        encoder_layers = []
        for _ in range(config.num_encoder_layers):
            encoder_layers.append(
                APTEncoderLayer(
                    d_model=config.d_model,
                    nhead=config.num_heads,
                    dim_feedforward=config.d_ff,
                    dropout=config.dropout,
                    activation=config.activation,
                    batch_first=config.batch_first,
                    eps=config.epsilon,
                    alpha=config.alpha,
                    init_tau=config.init_tau,
                    sr_ratio=config.sr_ratio,
                    use_autopoietic=use_autopoietic,
                    use_dbc_dac=use_dbc_dac,
                    rank_ratio_proj=rank_ratio_proj,
                    rank_ratio_res=rank_ratio_res,
                    dbc_threshold=dbc_threshold,
                    dbc_iterations=dbc_iterations,
                    # ğŸš€ å·¦æ—‹å¹³æ»‘å‚æ•°
                    use_left_spin=use_left_spin,
                    left_spin_alpha=left_spin_alpha,
                    left_spin_tau=left_spin_tau,
                    left_spin_beta=left_spin_beta
                )
            )

        # åˆ›å»ºè§£ç å™¨å±‚
        decoder_layers = []
        for _ in range(config.num_decoder_layers):
            decoder_layers.append(
                APTDecoderLayer(
                    d_model=config.d_model,
                    nhead=config.num_heads,
                    dim_feedforward=config.d_ff,
                    dropout=config.dropout,
                    activation=config.activation,
                    batch_first=config.batch_first,
                    eps=config.epsilon,
                    alpha=config.alpha,
                    init_tau=config.init_tau,
                    sr_ratio=config.sr_ratio,
                    use_autopoietic=use_autopoietic,
                    use_dbc_dac=use_dbc_dac,
                    rank_ratio_proj=rank_ratio_proj,
                    rank_ratio_res=rank_ratio_res,
                    dbc_threshold=dbc_threshold,
                    dbc_iterations=dbc_iterations,
                    # ğŸš€ å·¦æ—‹å¹³æ»‘å‚æ•°
                    use_left_spin=use_left_spin,
                    left_spin_alpha=left_spin_alpha,
                    left_spin_tau=left_spin_tau,
                    left_spin_beta=left_spin_beta
                )
            )
        
        # ç¼–ç å™¨å’Œè§£ç å™¨
        self.encoder_layers = nn.ModuleList(encoder_layers)
        self.decoder_layers = nn.ModuleList(decoder_layers)
        
        # æœ€ç»ˆå±‚å½’ä¸€åŒ–
        self.encoder_norm = nn.LayerNorm(config.d_model)
        self.decoder_norm = nn.LayerNorm(config.d_model)
        
        # è¾“å‡ºæŠ•å½±
        self.output_projection = nn.Linear(config.d_model, config.vocab_size, bias=False)
        
        # æƒé‡å…±äº«(å¯é€‰)
        self.output_projection.weight = self.token_embedding.weight
        
        # åˆå§‹åŒ–DBC-DACä¼˜åŒ–å™¨
        self.dbc_dac_optimizer = DBCDAC_Optimizer(
            rank_ratio_proj=rank_ratio_proj,
            rank_ratio_res=rank_ratio_res,
            threshold=dbc_threshold,
            iterations=dbc_iterations,
            apply_to_gradients=True
        ) if use_dbc_dac else None
        
        # æ·»åŠ æ¢¯åº¦ç¨³å®šé’©å­
        self.gradient_hooks = add_gradient_hooks_to_model(self, self.dbc_dac_optimizer) if self.dbc_dac_optimizer else []
            
        # åˆå§‹åŒ–å‚æ•°
        self._reset_parameters()
    
    def _reset_parameters(self):
        """åˆå§‹åŒ–æ¨¡å‹å‚æ•°"""
        # åˆå§‹åŒ–åµŒå…¥
        nn.init.normal_(self.token_embedding.weight, mean=0.0, std=0.02)
        
        # å¯¹äºpadding_idxï¼Œå°†åµŒå…¥å‘é‡ç½®é›¶
        if self.token_embedding.padding_idx is not None:
            with torch.no_grad():
                self.token_embedding.weight[self.token_embedding.padding_idx].fill_(0)

    def encode(
        self,
        src_tokens: torch.Tensor,
        src_mask: Optional[torch.Tensor] = None,
        src_key_padding_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        ç¼–ç å™¨å‰å‘ä¼ æ’­
    
        å‚æ•°:
            src_tokens: æºåºåˆ—token ID [batch_size, src_len]
            src_mask: æºåºåˆ—æ©ç 
            src_key_padding_mask: æºåºåˆ—å¡«å……æ©ç 
    
        è¿”å›:
            memory: ç¼–ç å™¨è¾“å‡º
        """
        # è·å–è¯åµŒå…¥
        src = self.token_embedding(src_tokens)
    
        # æ·»åŠ ä½ç½®ç¼–ç 
        src = self.positional_encoding(src)
    
        # é€šè¿‡ç¼–ç å™¨å±‚
        for layer in self.encoder_layers:
            src = layer(
                src=src,
                src_mask=src_mask,
                src_key_padding_mask=src_key_padding_mask
            )
    
        # æœ€ç»ˆå±‚å½’ä¸€åŒ–
        memory = self.encoder_norm(src)
    
        return memory
    
    def decode(
        self,
        tgt_tokens: torch.Tensor,
        memory: torch.Tensor,
        tgt_mask: Optional[torch.Tensor] = None,
        memory_mask: Optional[torch.Tensor] = None,
        tgt_key_padding_mask: Optional[torch.Tensor] = None,
        memory_key_padding_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        è§£ç å™¨å‰å‘ä¼ æ’­
    
        å‚æ•°:
            tgt_tokens: ç›®æ ‡åºåˆ—token ID [batch_size, tgt_len]
            memory: ç¼–ç å™¨è¾“å‡º
            tgt_mask: ç›®æ ‡åºåˆ—æ©ç 
            memory_mask: è®°å¿†æ©ç 
            tgt_key_padding_mask: ç›®æ ‡åºåˆ—å¡«å……æ©ç 
            memory_key_padding_mask: è®°å¿†å¡«å……æ©ç 
    
        è¿”å›:
            output: è§£ç å™¨è¾“å‡º
        """
        # è·å–è¯åµŒå…¥
        tgt = self.token_embedding(tgt_tokens)
    
        # æ·»åŠ ä½ç½®ç¼–ç 
        tgt = self.positional_encoding(tgt)
    
        # å¦‚æœæ²¡æœ‰æä¾›ç›®æ ‡æ©ç ï¼Œåˆ›å»ºè‡ªå›å½’æ©ç ï¼ˆä¸Šä¸‰è§’çŸ©é˜µï¼‰
        if tgt_mask is None and self.config.batch_first:
            tgt_len = tgt.size(1)
            device = tgt.device
            tgt_mask = torch.triu(
                torch.full((tgt_len, tgt_len), float('-inf'), device=device),
                diagonal=1
            )
    
        # é€šè¿‡è§£ç å™¨å±‚
        for layer in self.decoder_layers:
            tgt = layer(
                tgt=tgt,
                memory=memory,
                tgt_mask=tgt_mask,
                memory_mask=memory_mask,
                tgt_key_padding_mask=tgt_key_padding_mask,
                memory_key_padding_mask=memory_key_padding_mask
            )
    
        # æœ€ç»ˆå±‚å½’ä¸€åŒ–
        output = self.decoder_norm(tgt)
    
        return output
    
    def forward(
        self,
        src_tokens: torch.Tensor = None,
        tgt_tokens: Optional[torch.Tensor] = None,
        src_mask: Optional[torch.Tensor] = None,
        tgt_mask: Optional[torch.Tensor] = None,
        memory_mask: Optional[torch.Tensor] = None,
        src_key_padding_mask: Optional[torch.Tensor] = None,
        tgt_key_padding_mask: Optional[torch.Tensor] = None,
        memory_key_padding_mask: Optional[torch.Tensor] = None,
        return_dict: bool = False,
        # å…¼å®¹AutopoieticAttentioné£æ ¼å‚æ•°
        query: torch.Tensor = None,
        key: torch.Tensor = None,
        value: torch.Tensor = None,
        attn_mask: Optional[torch.Tensor] = None,
        key_padding_mask: Optional[torch.Tensor] = None,
        need_weights: bool = True,
        **kwargs
    ) -> Union[torch.Tensor, Dict[str, torch.Tensor]]:
        
        # å°†è‡ªæ³¨æ„åŠ›æ¥å£çš„å‚æ•°æ˜ å°„åˆ°Transformeræ¥å£
        if src_tokens is None and query is not None:
            src_tokens = query
        if tgt_tokens is None:
            if key is not None:
                tgt_tokens = key
            else:
                tgt_tokens = src_tokens
        if src_mask is None and attn_mask is not None:
            src_mask = attn_mask
        if src_key_padding_mask is None and key_padding_mask is not None:
            src_key_padding_mask = key_padding_mask
        
        # **ç¡®ä¿æ©ç æ˜¯boolç±»å‹**ï¼ˆè‹¥åŸæœ¬æ˜¯floatæˆ–longï¼Œåˆ™è½¬ä¸ºboolï¼‰
        if src_mask is not None and src_mask.dtype != torch.bool:
            src_mask = src_mask.to(torch.bool)
        if tgt_mask is not None and tgt_mask.dtype != torch.bool:
            tgt_mask = tgt_mask.to(torch.bool)
        if memory_mask is not None and memory_mask.dtype != torch.bool:
            memory_mask = memory_mask.to(torch.bool)
        if src_key_padding_mask is not None and src_key_padding_mask.dtype != torch.bool:
            src_key_padding_mask = src_key_padding_mask.to(torch.bool)
        if tgt_key_padding_mask is not None and tgt_key_padding_mask.dtype != torch.bool:
            tgt_key_padding_mask = tgt_key_padding_mask.to(torch.bool)
        if memory_key_padding_mask is not None and memory_key_padding_mask.dtype != torch.bool:
            memory_key_padding_mask = memory_key_padding_mask.to(torch.bool)
    
        memory = self.encode(
            src_tokens=src_tokens,
            src_mask=src_mask,
            src_key_padding_mask=src_key_padding_mask
        )
        decoder_output = self.decode(
            tgt_tokens=tgt_tokens,
            memory=memory,
            tgt_mask=tgt_mask,
            memory_mask=memory_mask,
            tgt_key_padding_mask=tgt_key_padding_mask,
            memory_key_padding_mask=memory_key_padding_mask if memory_key_padding_mask is not None else src_key_padding_mask
        )
        
        # ç”Ÿæˆlogits
        logits = self.output_projection(decoder_output)
        
        # æ ¹æ®return_dictå‚æ•°å†³å®šè¿”å›å½¢å¼
        if return_dict:
            return {
                "logits": logits,
                "encoder_output": memory,
                "decoder_output": decoder_output
            }
        else:
            # é»˜è®¤ç›´æ¥è¿”å›logits
            return logits

    def generate(
        self,
        input_ids,
        max_length=50,
        temperature=1.0,
        top_p=0.9,
        top_k=50,
        repetition_penalty=1.0,
        do_sample=True,
        num_beams=1,
        eos_token_id=None,
        pad_token_id=None,
    ):
        """
        â­ ä¿®å¤åçš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³• (Encoder-Decoder é€»è¾‘ä¿®æ­£ç‰ˆ)

        Args:
            input_ids: è¾“å…¥token IDs [batch_size, seq_len]
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            temperature: é‡‡æ ·æ¸©åº¦
            top_p: nucleusé‡‡æ ·å‚æ•°
            top_k: top-ké‡‡æ ·å‚æ•°
            repetition_penalty: é‡å¤æƒ©ç½š
            do_sample: æ˜¯å¦é‡‡æ ·(Falseåˆ™è´ªå¿ƒè§£ç )
            num_beams: beam searchçš„beamæ•°é‡
            eos_token_id: ç»“æŸæ ‡è®°ID
            pad_token_id: å¡«å……æ ‡è®°ID

        Returns:
            ç”Ÿæˆçš„token IDs [batch_size, generated_length]
        """
        del num_beams  # å½“å‰å®ç°ä¸æ”¯æŒbeam searchï¼Œé¿å…æœªä½¿ç”¨å‚æ•°è­¦å‘Š

        if input_ids is None:
            raise ValueError("input_ids ä¸èƒ½ä¸ºç©º")

        device = input_ids.device
        batch_size = input_ids.size(0)

        # 1. å‡†å¤‡ç‰¹æ®Š Token
        bos_token_id = getattr(self.config, "bos_token_id", 2)
        if eos_token_id is None:
            eos_token_id = getattr(self.config, "eos_token_id", 3)
        if pad_token_id is None:
            pad_token_id = getattr(self.config, "pad_token_id", 0)
        unk_token_id = getattr(self.config, "unk_token_id", None)

        # ------------------------------------------------------------------
        # ğŸš€ æ ¸å¿ƒé€»è¾‘ä¿®å¤ï¼šä» GPT æ¨¡å¼åˆ‡æ¢å› Encoder-Decoder æ¨¡å¼
        # ------------------------------------------------------------------
        
        # 2. ç¼–ç é˜¶æ®µ (Encoder)
        # ä¸€æ¬¡æ€§è¯»æ‡‚ Promptï¼Œè·å–è®°å¿†
        memory = self.encode(
            src_tokens=input_ids,
            src_key_padding_mask=(input_ids == pad_token_id)
        )

        # 3. è§£ç å‡†å¤‡ (Decoder)
        # ç»™è§£ç å™¨ä¸€å¼ ç™½çº¸ï¼Œåªå†™ä¸€ä¸ª [BOS] å¼€å¤´
        # ç»å¯¹ä¸èƒ½æŠŠ input_ids å–‚ç»™è§£ç å™¨ï¼Œå¦åˆ™å®ƒçœ‹åˆ° EOS å°±ä¼šåœæ­¢ï¼
        decoder_input = torch.full((batch_size, 1), bos_token_id, device=device, dtype=torch.long)
        
        # ç”¨äºä¿å­˜ç”Ÿæˆç»“æœ (ä¸åŒ…å« BOS)
        generated_ids = torch.empty((batch_size, 0), device=device, dtype=torch.long)

        was_training = self.training
        self.eval()

        try:
            with torch.no_grad():
                # å¾ªç¯ç”Ÿæˆ Response
                for step in range(max_length):
                    # å‰å‘è§£ç ï¼šä¼ å…¥ memory å’Œ å½“å‰å·²ç”Ÿæˆçš„ decoder_input
                    # æ³¨æ„ï¼šæˆ‘ä»¬ä½¿ç”¨ decode() æ–¹æ³•è€Œä¸æ˜¯ forward()
                    decoder_output = self.decode(
                        tgt_tokens=decoder_input,
                        memory=memory,
                        tgt_mask=None, # å†…éƒ¨ä¼šè‡ªåŠ¨ç”Ÿæˆå› æœæ©ç 
                        memory_mask=None,
                        tgt_key_padding_mask=None,
                        memory_key_padding_mask=(input_ids == pad_token_id)
                    )
                    
                    # æ˜ å°„åˆ°è¯è¡¨
                    logits = self.output_projection(decoder_output)
                    next_token_logits = logits[:, -1, :] # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥

                    # --- é‡å¤æƒ©ç½šé€»è¾‘ ---
                    if repetition_penalty != 1.0:
                        for i in range(batch_size):
                            # æ³¨æ„ï¼šæˆ‘ä»¬æ£€æŸ¥çš„æ˜¯å·²ç»ç”Ÿæˆçš„ generated_ids (ä¸å« prompt)
                            history = set(generated_ids[i].tolist())
                            if not history:
                                continue
                            
                            # å°† tensor è½¬ä¸º list ä»¥ä¾¿ç´¢å¼•ï¼Œæˆ–è€…ç›´æ¥ä½¿ç”¨ scatter/gather ä¼˜åŒ–
                            # è¿™é‡Œä¸ºäº†å…¼å®¹æ€§ä¿æŒå¾ªç¯å†™æ³•ï¼Œä½†åŠ å…¥äº† logits æ­£è´Ÿå€¼çš„æ­£ç¡®å¤„ç†
                            for token_id in history:
                                if next_token_logits[i, token_id] > 0:
                                    next_token_logits[i, token_id] /= repetition_penalty
                                else:
                                    next_token_logits[i, token_id] *= repetition_penalty

                    # æ¸©åº¦è°ƒèŠ‚
                    temperature = max(float(temperature), 1e-5)
                    next_token_logits = next_token_logits / temperature

                    # å±è”½ç‰¹æ®Šç¬¦å· (PAD, UNK)
                    if pad_token_id is not None and 0 <= pad_token_id < next_token_logits.size(-1):
                        next_token_logits[:, pad_token_id] = -float('inf')
                    if unk_token_id is not None and 0 <= unk_token_id < next_token_logits.size(-1):
                        next_token_logits[:, unk_token_id] = -float('inf')

                    # é‡‡æ ·
                    if do_sample:
                        # Top-K
                        if top_k > 0:
                            v, _ = torch.topk(next_token_logits, min(top_k, next_token_logits.size(-1)))
                            next_token_logits[next_token_logits < v[:, [-1]]] = -float('inf')
                        
                        # Top-P
                        if 0 < top_p < 1.0:
                            sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
                            sorted_probs = F.softmax(sorted_logits, dim=-1)
                            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
                            sorted_indices_to_remove = cumulative_probs > top_p
                            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                            sorted_indices_to_remove[..., 0] = 0
                            for i in range(batch_size):
                                indices_to_remove = sorted_indices[i][sorted_indices_to_remove[i]]
                                next_token_logits[i, indices_to_remove] = -float('inf')

                        probs = F.softmax(next_token_logits, dim=-1)
                        next_token = torch.multinomial(probs, num_samples=1)
                    else:
                        # è´ªå©ªæœç´¢
                        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)

                    # æ‹¼æ¥åˆ°ç»“æœä¸­
                    generated_ids = torch.cat([generated_ids, next_token], dim=1)
                    decoder_input = torch.cat([decoder_input, next_token], dim=1)

                    # æ£€æŸ¥ EOS
                    if (next_token == eos_token_id).all():
                        break
                        
        finally:
            if was_training:
                self.train()

        # è¿”å›ç”Ÿæˆç»“æœ (åªè¿”å›ç”Ÿæˆçš„å›å¤éƒ¨åˆ†)
        return generated_ids


class APTLargeModel(APTModel):
    """APTLargeModelæ˜¯APTModelçš„åˆ«åï¼Œç”¨äºå…¼å®¹æ€§ç›®çš„"""
    def __init__(self, config):
        super().__init__(config)
        # åˆå§‹åŒ–Taylorå‚æ•°ï¼Œç¡®ä¿å­˜åœ¨è¿™äº›å±æ€§
        self.register_parameter(
            'taylor_epsilon', 
            nn.Parameter(torch.tensor(config.epsilon, dtype=torch.float))
        )
        self.register_parameter(
            'taylor_alpha', 
            nn.Parameter(torch.tensor(config.alpha, dtype=torch.float))
        )
    
    def update_dynamic_taylor_parameters(self, learning_rate):
        """æ›´æ–°åŠ¨æ€Taylorå±•å¼€å‚æ•°"""
        try:
            # å¦‚æœä½¿ç”¨LRè°ƒåº¦å™¨ï¼Œéœ€è¦æ ¹æ®å½“å‰å­¦ä¹ ç‡è°ƒæ•´å‚æ•°
            lr_factor = float(learning_rate) / float(self.config.base_lr)
            
            # å®‰å…¨åœ°æ›´æ–°å‚æ•°
            if hasattr(self, 'taylor_epsilon'):
                self.taylor_epsilon.data = torch.clamp(
                    self.taylor_epsilon * (1.0 + self.config.alpha * lr_factor),
                    min=0.1, max=10.0
                )
            
            if hasattr(self, 'taylor_alpha'):
                self.taylor_alpha.data = torch.clamp(
                    self.taylor_alpha * (1.0 - self.config.beta * lr_factor),
                    min=0.001, max=0.1
                )
            
            # æ›´æ–°æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„å‚æ•°
            for name, module in self.named_modules():
                if hasattr(module, 'tau') and isinstance(module.tau, torch.nn.Parameter):
                    # è°ƒæ•´æ¸©åº¦å‚æ•°
                    module.tau.data = torch.clamp(
                        module.tau * (1.0 - 0.01 * lr_factor),
                        min=0.5, max=2.0
                    )
        except Exception as e:
            # å¦‚æœå‡ºç°ä»»ä½•å¼‚å¸¸ï¼Œè®°å½•ä½†ä¸ä¸­æ–­è®­ç»ƒ
            print(f"è­¦å‘Š: åŠ¨æ€å‚æ•°æ›´æ–°å¤±è´¥: {e}")
            # ç®€å•åœ°é€šè¿‡ï¼Œç¡®ä¿æ–¹æ³•ä¸ä¼šæŠ›å‡ºå¼‚å¸¸
            pass