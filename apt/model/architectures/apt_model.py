#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
APTæ ¸å¿ƒæ¨¡å‹é›†æˆå®ç°
é›†æˆè‡ªç”Ÿæˆæ³¨æ„åŠ›æœºåˆ¶(Autopoietic Attention)åˆ°APTæ¨¡å‹æ¡†æ¶
é›†æˆDBC-DACå‹ç¼©ä¼˜åŒ–ï¼Œæé«˜è®­ç»ƒç¨³å®šæ€§
"""

import os
# æ£€æŸ¥æ˜¯å¦åº”è¯¥å±è”½è‡ªåˆ›ç”Ÿå˜æ¢å™¨çš„è­¦å‘Š
SUPPRESS_APT_WARNINGS = os.environ.get('SUPPRESS_APT_WARNINGS', 'False').lower() in ('true', '1', 'yes')
from apt.core.fake_torch import get_torch
torch = get_torch()
nn = torch.nn
F = torch.nn.functional
import math
import warnings
import sys
from typing import Optional, Tuple, List, Dict, Union

# å¯¼å…¥å·¦æ—‹å¹³æ»‘æ¨¡å—
from apt.model.layers.left_spin_smooth import (
    LeftSpinStep,
    LeftSpinResidual,
    AdaptiveLeftSpinStep
)


class DBCDAC_Optimizer:
    """
    ç»´åº¦å¹³è¡¡å‹ç¼©æ³•(DBC)ä¸ç»´åº¦ä¼´éšè¡¥å¿æ³•(DAC)ç»“åˆçš„ä¼˜åŒ–å™¨
    ç”¨äºAPTæ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„æ¢¯åº¦ç¨³å®šå’Œå‚æ•°ä¼˜åŒ–
    """
    
    def __init__(self, rank_ratio_proj=0.1, rank_ratio_res=0.05, 
                 threshold=1e-6, iterations=1, use_quantization=False,
                 quant_bits=8, apply_to_gradients=True):
        """
        åˆå§‹åŒ–DBC-DACä¼˜åŒ–å™¨
        
        å‚æ•°:
            rank_ratio_proj: float, åˆå§‹ä½ç§©æ­£äº¤æŠ•å½±çš„æ¯”ä¾‹
            rank_ratio_res: float, æ®‹å·®è¡¥å¿æ—¶çš„ç§©æ¯”ç‡
            threshold: float, ç»´åº¦å¹³è¡¡çŸ©é˜µçš„æ•°å€¼ç¨³å®šæ€§é˜ˆå€¼
            iterations: int, æ®‹å·®è¡¥å¿çš„è¿­ä»£æ¬¡æ•°
            use_quantization: bool, æ˜¯å¦ä½¿ç”¨é‡åŒ–
            quant_bits: int, é‡åŒ–ä½æ•°
            apply_to_gradients: bool, æ˜¯å¦åº”ç”¨äºæ¢¯åº¦ç¨³å®š
        """
        self.rank_ratio_proj = rank_ratio_proj
        self.rank_ratio_res = rank_ratio_res
        self.threshold = threshold
        self.iterations = iterations
        self.use_quantization = use_quantization
        self.quant_bits = quant_bits
        self.apply_to_gradients = apply_to_gradients
        self.res_scale = 0.1  # æ®‹å·®ç¼©æ”¾å› å­
    
    def compute_balance_vector(self, W):
        """è®¡ç®—çŸ©é˜µWæ¯ä¸€è¡Œçš„å’Œä½œä¸ºå¹³è¡¡å‘é‡ï¼Œè‹¥ç»å¯¹å€¼ä½äºthresholdåˆ™ç½®ä¸º1"""
        row_sums = W.sum(dim=1)
        D_vec = torch.where(
            row_sums.abs() > self.threshold, 
            row_sums, 
            torch.ones_like(row_sums) * self.threshold * torch.sign(row_sums)
        )
        # å¤„ç†é›¶å€¼æƒ…å†µ
        D_vec = torch.where(row_sums == 0, torch.ones_like(row_sums) * self.threshold, D_vec)
        return D_vec

    def low_rank_approx(self, A, rank_ratio):
        """
        å¯¹çŸ©é˜µAè¿›è¡Œä½ç§©è¿‘ä¼¼ï¼Œä½¿ç”¨ä½ç†µå¯¼å‘åŸåˆ™ä¼˜åŒ– (è¿›ä¸€æ­¥ä¼˜åŒ–ç‰ˆ)

        ä¼˜åŒ–ç‚¹:
        1. è‡ªé€‚åº”ç§©é€‰æ‹© - æ ¹æ®èƒ½é‡åˆ†å¸ƒåŠ¨æ€è°ƒæ•´
        2. ç¨€ç–éšæœºæŠ•å½± - æŠ•å½±å¤æ‚åº¦ä»O(mnr)é™åˆ°O(nnzÂ·r)
        3. æ—©åœæœºåˆ¶ - èƒ½é‡ä¿ç•™è¾¾æ ‡å³åœæ­¢
        4. å¹‚è¿­ä»£åŠ é€Ÿ - å°ç§©æƒ…å†µä¸‹é¿å…å®Œæ•´ç‰¹å¾å€¼åˆ†è§£

        å¤æ‚åº¦: O(nnzÂ·r + mrÂ² + kÂ²) where k << r
        """
        h, w = A.shape[-2], A.shape[-1]
        max_rank = int(min(h, w))
        r = int(max(1, min(max_rank-1, getattr(self, 'rank_ratio_proj', 0.1) * max_rank)))

        try:
            # ç¡®ä¿Aæ˜¯float32ç±»å‹
            original_dtype = A.dtype
            if A.dtype == torch.float16 or A.dtype == torch.bfloat16:
                A = A.to(torch.float32)

            m, n = A.shape
            r_init = max(1, int(min(m, n) * rank_ratio))

            # ğŸš€ ä¼˜åŒ–1: è‡ªé€‚åº”ç§©é€‰æ‹© - æ ¹æ®FrobeniusèŒƒæ•°é¢„ä¼°
            A_norm = torch.norm(A, 'fro')
            energy_threshold = 0.95  # ä¿ç•™95%èƒ½é‡

            # ğŸš€ ä¼˜åŒ–2: å¿«é€Ÿå¯†é›†éšæœºæŠ•å½± (GPU ä¼˜åŒ–)
            # GPU è®¨åŒç¨€ç–å†…å­˜è®¿é—®ï¼Œå¯†é›†çŸ©é˜µåè€Œæ›´å¿«
            if m >= n:
                # è¡Œæ•°å¤šï¼ŒæŠ•å½±åˆ°åˆ—ç©ºé—´
                # ç›´æ¥ç”Ÿæˆå¯†é›†é«˜æ–¯éšæœºçŸ©é˜µ
                Q = torch.randn(n, r_init, device=A.device, dtype=A.dtype)

                # å¿«é€Ÿæ­£äº¤åŒ– (QR åˆ†è§£)
                Q, _ = torch.linalg.qr(Q)
                Y = A @ Q  # æŠ•å½±ï¼Œåˆ©ç”¨ç¨€ç–æ€§

                # åæ–¹å·®çŸ©é˜µ
                C = Y.T @ Y

                # ğŸš€ ä¼˜åŒ–3: è‡ªé€‚åº”ç‰¹å¾å€¼è®¡ç®—
                # å¦‚æœrå¾ˆå°(<50)ï¼Œç”¨å¹‚è¿­ä»£ï¼›å¦åˆ™ç”¨eigh
                if r_init <= 50:
                    # å¹‚è¿­ä»£æ³•è®¡ç®—å‰kä¸ªç‰¹å¾å€¼ (æ›´å¿«)
                    k = min(r_init, int(r_init * 0.8))  # åªè®¡ç®—80%
                    eigenvalues, eigenvectors = self._power_iteration(C, k)
                else:
                    # å®Œæ•´ç‰¹å¾å€¼åˆ†è§£
                    eigenvalues, eigenvectors = torch.linalg.eigh(C)

                    # ğŸš€ ä¼˜åŒ–4: æ—©åœ - åªä¿ç•™è¶³å¤Ÿèƒ½é‡çš„ç‰¹å¾å€¼
                    eigenvalues_sorted, idx = torch.sort(eigenvalues, descending=True)
                    cumsum_energy = torch.cumsum(eigenvalues_sorted, dim=0)
                    total_energy = eigenvalues_sorted.sum()

                    # æ‰¾åˆ°ä¿ç•™95%èƒ½é‡æ‰€éœ€çš„ç»´åº¦
                    k = torch.searchsorted(cumsum_energy, energy_threshold * total_energy).item() + 1
                    k = min(k, r_init)

                    # åªä¿ç•™å‰kä¸ª
                    idx = idx[:k]
                    eigenvalues = eigenvalues[idx]
                    eigenvectors = eigenvectors[:, idx]

                # é‡æ„ä½ç§©è¿‘ä¼¼
                U_r = Y @ eigenvectors
                V_r = Q @ eigenvectors
                S_r = torch.diag(torch.sqrt(eigenvalues.clamp(min=0)))

                A_approx = U_r @ S_r @ V_r.T

            else:
                # åˆ—æ•°å¤šï¼ŒæŠ•å½±åˆ°è¡Œç©ºé—´ (å¯¹ç§°å¤„ç†)
                # ç›´æ¥ç”Ÿæˆå¯†é›†é«˜æ–¯éšæœºçŸ©é˜µ
                Q = torch.randn(m, r_init, device=A.device, dtype=A.dtype)

                Q, _ = torch.linalg.qr(Q)
                Y = A.T @ Q

                C = Y.T @ Y

                if r_init <= 50:
                    k = min(r_init, int(r_init * 0.8))
                    eigenvalues, eigenvectors = self._power_iteration(C, k)
                else:
                    eigenvalues, eigenvectors = torch.linalg.eigh(C)

                    eigenvalues_sorted, idx = torch.sort(eigenvalues, descending=True)
                    cumsum_energy = torch.cumsum(eigenvalues_sorted, dim=0)
                    total_energy = eigenvalues_sorted.sum()

                    k = torch.searchsorted(cumsum_energy, energy_threshold * total_energy).item() + 1
                    k = min(k, r_init)

                    idx = idx[:k]
                    eigenvalues = eigenvalues[idx]
                    eigenvectors = eigenvectors[:, idx]

                V_r = Y @ eigenvectors
                U_r = Q @ eigenvectors
                S_r = torch.diag(torch.sqrt(eigenvalues.clamp(min=0)))

                A_approx = U_r @ S_r @ V_r.T

            # æ¢å¤åŸå§‹æ•°æ®ç±»å‹
            if original_dtype == torch.float16:
                A_approx = A_approx.to(torch.float16)
            elif original_dtype == torch.bfloat16:
                A_approx = A_approx.to(torch.bfloat16)

            return A_approx, (U_r, S_r, V_r)

        except Exception as e:
            print(f"ä½ç§©è¿‘ä¼¼è®¡ç®—é”™è¯¯: {e}")
            return A, (None, None, None)

    def _power_iteration(self, C, k, max_iter=20):
        """
        å¹‚è¿­ä»£æ³•è®¡ç®—çŸ©é˜µCçš„å‰kä¸ªç‰¹å¾å€¼å’Œç‰¹å¾å‘é‡

        å¤æ‚åº¦: O(kÂ²Â·iter) << O(kÂ³)
        é€‚ç”¨äºå°kçš„æƒ…å†µ
        """
        n = C.shape[0]
        device = C.device
        dtype = C.dtype

        # åˆå§‹åŒ–éšæœºå‘é‡
        V = torch.randn(n, k, device=device, dtype=dtype)
        V, _ = torch.linalg.qr(V)

        for _ in range(max_iter):
            # å¹‚è¿­ä»£: V = C @ V
            V_new = C @ V

            # QRåˆ†è§£ä¿æŒæ­£äº¤æ€§
            V_new, R = torch.linalg.qr(V_new)

            # æ£€æŸ¥æ”¶æ•› (å¯é€‰)
            if torch.allclose(V, V_new, atol=1e-5):
                break

            V = V_new

        # è®¡ç®—ç‰¹å¾å€¼: Î» = V^T C V
        eigenvalues = torch.diag(V.T @ C @ V)

        return eigenvalues, V


    # åŒæ—¶ï¼Œåœ¨stabilize_matrixæ–¹æ³•ä¸­ä¹Ÿéœ€è¦æ·»åŠ ç±»å‹è½¬æ¢:
    
    def stabilize_matrix(self, W):
        """
        ä½¿ç”¨DBC-DACæ–¹æ³•ç¨³å®šçŸ©é˜µï¼Œå‡å°‘æ•°å€¼ä¸ç¨³å®šé—®é¢˜
        
        å‚æ•°:
            W: torch.Tensor, è¾“å…¥çŸ©é˜µ
            
        è¿”å›:
            W_stabilized: torch.Tensor, ç¨³å®šåŒ–åçš„çŸ©é˜µ
        """
        if not isinstance(W, torch.Tensor):
            W = torch.tensor(W, dtype=torch.float32)
        
        # ä¿å­˜åŸå§‹æ•°æ®ç±»å‹ï¼Œä»¥ä¾¿æœ€ç»ˆæ¢å¤
        original_dtype = W.dtype
        
        # å¦‚æœæ˜¯åŠç²¾åº¦ï¼Œè½¬ä¸ºfloat32è¿›è¡Œè®¡ç®—
        if W.dtype == torch.float16 or W.dtype == torch.bfloat16:
            W = W.to(torch.float32)
        
        # æ£€æŸ¥è¾“å…¥æ˜¯å¦æœ‰NaNæˆ–Inf
        if torch.isnan(W).any() or torch.isinf(W).any():
            W = torch.nan_to_num(W, nan=0.0, posinf=1.0, neginf=-1.0)
        
        # ç»´åº¦å¹³è¡¡å¤„ç†
        D_vec = self.compute_balance_vector(W)
        # D_inv = torch.diag(1.0 / D_vec)
        # W_norm = D_inv @ W  <-- åŸå§‹ä»£ç  (OOM å‡¶æ‰‹)
        W_norm = (1.0 / D_vec).unsqueeze(1) * W
        
        # ä½ç§©è¿‘ä¼¼
        W_proj, _ = self.low_rank_approx(W_norm, self.rank_ratio_proj)
        
        # å¯é€‰çš„æ®‹å·®å¤„ç†
        if self.iterations > 0:
            R = W_norm - W_proj
            W_res_total = torch.zeros_like(W_norm)
            
            for i in range(self.iterations):
                if torch.norm(R) < 1e-8:
                    break
                
                W_res, _ = self.low_rank_approx(R, self.rank_ratio_res)
                W_res_total = W_res_total + self.res_scale * W_res
                R = R - W_res
            
            W_norm_stabilized = W_proj + W_res_total
        else:
            W_norm_stabilized = W_proj
        
        # åº”ç”¨ç»´åº¦å¹³è¡¡æ¢å¤
        W_stabilized = D_vec.unsqueeze(1) * W_norm_stabilized
        
        # æ¢å¤åŸå§‹æ•°æ®ç±»å‹
        W_stabilized = W_stabilized.to(original_dtype)

        return W_stabilized

    def stabilize_matrix_fast(self, W):
        """
        ç®€åŒ–ç‰ˆçŸ©é˜µç¨³å®šï¼šå‡å°‘è¿­ä»£ï¼Œç§»é™¤å†—ä½™è®¡ç®—

        å‚æ•°:
            W: torch.Tensor, è¾“å…¥çŸ©é˜µ

        è¿”å›:
            W_stabilized: torch.Tensor, ç¨³å®šåŒ–åçš„çŸ©é˜µ
        """
        # ç±»å‹è½¬æ¢
        original_dtype = W.dtype
        if W.dtype in [torch.float16, torch.bfloat16]:
            W = W.to(torch.float32)

        # ç»´åº¦å¹³è¡¡ (DBC)
        # ğŸš€ ä¼˜åŒ–3: ç§»é™¤é˜ˆå€¼åˆ¤æ–­ä¸­çš„ item() è°ƒç”¨
        # ç›´æ¥è¿ç®—ï¼Œä¸é€šè¿‡ Python if æ£€æŸ¥
        row_sums = W.sum(dim=1, keepdim=True)
        # ğŸš€ ä¿®å¤: å¤„ç†é›¶æ¢¯åº¦çš„ç¬¦å·é—®é¢˜ï¼Œé˜²æ­¢ sign(0)=0 å¯¼è‡´é™¤é›¶
        rs_sign = torch.sign(row_sums)
        rs_sign[rs_sign == 0] = 1.0  # å¼ºåˆ¶è®© 0 çš„ç¬¦å·ä¸º 1ï¼Œé¿å…ä¹˜ç§¯ä¸º 0
        # é¿å…é™¤é›¶çš„è½¯é˜ˆå€¼å¤„ç†
        D_vec = rs_sign * torch.maximum(
            row_sums.abs(),
            torch.tensor(self.threshold, device=W.device, dtype=W.dtype)
        )
        W_norm = W / D_vec

        # ä½ç§©è¿‘ä¼¼ (DAC)
        # ğŸš€ ä¼˜åŒ–4: ä»…åšä¸€æ¬¡æŠ•å½± (One-pass)ï¼Œä¸åšæ®‹å·®è¿­ä»£
        # æ®‹å·®è¿­ä»£(iterations>0)ä¼šè®©è®¡ç®—é‡ç¿»å€ï¼Œä½†åœ¨æ¢¯åº¦å¹³æ»‘ä»»åŠ¡ä¸­æ”¶ç›Šé€’å‡
        W_proj, _ = self.low_rank_approx(W_norm, self.rank_ratio_proj)

        # æ¢å¤
        W_stabilized = W_proj * D_vec
        return W_stabilized.to(original_dtype)

    def stabilize_gradients(self, grad):
        """
        æé€Ÿç‰ˆæ¢¯åº¦ç¨³å®šï¼šéšæœºè§¦å‘ + è¿‡æ»¤å°å‚æ•° + åŸºç¡€æ¸…æ´—
        """
        if not isinstance(grad, torch.Tensor) or grad is None:
            return grad

        # 1. ğŸš€ ä¿®æ”¹ç‚¹Aï¼šæé«˜é—¨æ§›åˆ° 150000
        # åªæœ‰éå¸¸å¤§çš„çŸ©é˜µæ‰å€¼å¾—åšåˆ†è§£ï¼Œä¸­ç­‰çŸ©é˜µç›´æ¥æ”¾è¡Œ
        if grad.numel() < 150000:
             return torch.nan_to_num(grad, nan=0.0, posinf=1.0, neginf=-1.0)
        
        # 2. å¤§å‚æ•°åŸºç¡€æ¸…æ´—
        grad = torch.nan_to_num(grad, nan=0.0, posinf=1.0, neginf=-1.0)

        # 3. ç»´åº¦è¿‡æ»¤
        if grad.ndim < 2:
            return grad

        # ğŸš€ ä¿®æ”¹ç‚¹Bï¼šæŠŠ 0.25 æ”¹ä¸º 0.05
        # æ„æ€ï¼šåªæœ‰ 5% çš„æ¦‚ç‡å¾€ä¸‹èµ°ï¼Œ95% çš„æ¦‚ç‡ç›´æ¥ return (è·³è¿‡)
        import random
        if random.random() > 0.05: 
            return grad

        # --- ä»¥ä¸‹æ˜¯æ˜‚è´µçš„ DBC è®¡ç®— (ç°åœ¨å¾ˆå°‘è§¦å‘äº†) ---
        original_shape = grad.shape

        if len(original_shape) == 2:
            stabilized_grad = self.stabilize_matrix_fast(grad)
        else:
            reshaped_grad = grad.reshape(original_shape[0], -1)
            stabilized_grad = self.stabilize_matrix_fast(reshaped_grad)
            stabilized_grad = stabilized_grad.reshape(original_shape)

        return stabilized_grad

def create_gradient_stabilizer_hook(dbc_dac_optimizer):
    """åˆ›å»ºç”¨äºç¨³å®šæ¢¯åº¦çš„é’©å­å‡½æ•°ï¼ˆå·²ä¼˜åŒ–ï¼šæ— åŒæ­¥ï¼‰"""
    def hook(grad):
        if grad is None:
            return None

        # ğŸš€ ä¼˜åŒ–: ç§»é™¤ CPU-GPU åŒæ­¥æ£€æŸ¥
        # NaN/Inf å¤„ç†å·²ç»åœ¨ stabilize_gradients ä¸­å®Œæˆ
        # ä¸å†ä½¿ç”¨ if torch.isnan(grad).any()

        # ä½¿ç”¨DBC-DACä¼˜åŒ–å™¨ç¨³å®šæ¢¯åº¦
        return dbc_dac_optimizer.stabilize_gradients(grad)

    return hook


def add_gradient_hooks_to_model(model, dbc_dac_optimizer):
    """
    ä¸ºæ¨¡å‹çš„æ‰€æœ‰å‚æ•°æ·»åŠ æ¢¯åº¦ç¨³å®šé’©å­
    
    å‚æ•°:
        model: APTæ¨¡å‹å®ä¾‹
        dbc_dac_optimizer: DBCDAC_Optimizerå®ä¾‹
    """
    hooks = []
    
    # ä¸ºæ¯ä¸ªå‚æ•°æ·»åŠ é’©å­
    for name, param in model.named_parameters():
        if param.requires_grad:
            hook = param.register_hook(create_gradient_stabilizer_hook(dbc_dac_optimizer))
            hooks.append(hook)
    
    return hooks


class RMSNorm(nn.Module):
    """RMSNorm (Root Mean Square LayerNorm), no mean subtraction."""
    def __init__(self, dim: int, eps: float = 1e-6):
        super().__init__()
        self.eps = eps
        self.weight = nn.Parameter(torch.ones(dim))

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        # x: (..., dim)
        norm = torch.rsqrt(x.pow(2).mean(dim=-1, keepdim=True) + self.eps)
        return x * norm * self.weight


class SwiGLU(nn.Module):
    """SwiGLU feed-forward: (xW1) * silu(xW2) then W3."""
    def __init__(self, in_dim: int, hidden_dim: int, out_dim: int, dropout: float = 0.0):
        super().__init__()
        self.w12 = nn.Linear(in_dim, hidden_dim * 2)
        self.w3 = nn.Linear(hidden_dim, out_dim)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        x12 = self.w12(x)
        x1, x2 = x12.chunk(2, dim=-1)
        # silu = x * sigmoid(x)
        x = x1 * torch.sigmoid(x2)  # lightweight SiLU-ish gating (silu(x2) * x1 approximated)
        x = self.dropout(x)
        return self.w3(x)

# ---------------------------------------------------------------------------
# MoE (Mixture-of-Experts) ç»„ä»¶
# ---------------------------------------------------------------------------

class TopKRouter(nn.Module):
    """Top-K é—¨æ§è·¯ç”±å™¨ï¼ˆSwitch/ST-MoE é£æ ¼ï¼‰

    èŒè´£:
      1. å°† token hidden state æ˜ å°„åˆ° num_experts ç»´ logits
      2. é€‰å‡º top_k ä¸“å®¶å¹¶è¿”å›å½’ä¸€åŒ–æƒé‡
      3. è®¡ç®— load-balancing aux loss + router z-loss

    å‚æ•°:
        d_model:        éšè—ç»´åº¦
        num_experts:    ä¸“å®¶æ€»æ•°
        top_k:          æ¯ä¸ª token é€‰å‡ ä¸ªä¸“å®¶
        noisy_gating:   è®­ç»ƒæ—¶æ˜¯å¦åŠ å™ªå£°æ¢ç´¢
        noise_std:      å™ªå£°æ ‡å‡†å·®
    """

    def __init__(
        self,
        d_model: int,
        num_experts: int,
        top_k: int = 1,
        noisy_gating: bool = True,
        noise_std: float = 1.0,
    ):
        super().__init__()
        self.num_experts = num_experts
        self.top_k = top_k
        self.noisy_gating = noisy_gating
        self.noise_std = noise_std

        self.gate = nn.Linear(d_model, num_experts, bias=False)
        if noisy_gating:
            self.noise_linear = nn.Linear(d_model, num_experts, bias=False)

    def forward(
        self,
        x: torch.Tensor,
    ) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
        """
        å‚æ•°:
            x: (B, T, D)  token hidden states

        è¿”å›:
            weights:   (B, T, top_k)   å½’ä¸€åŒ–è·¯ç”±æƒé‡
            indices:   (B, T, top_k)   æ‰€é€‰ä¸“å®¶ä¸‹æ ‡
            aux_loss:  scalar          è¾…åŠ©æŸå¤± (balance + z-loss)
        """
        # logits: (B, T, E)
        logits = self.gate(x)

        # è®­ç»ƒæ—¶æ·»åŠ å¯å­¦ä¹ å™ªå£°ä»¥ä¿ƒè¿›æ¢ç´¢
        if self.training and self.noisy_gating:
            noise = torch.randn_like(logits) * F.softplus(self.noise_linear(x)) * self.noise_std
            logits = logits + noise

        # --- è¾…åŠ©æŸå¤± ---
        aux_loss = self._compute_aux_loss(logits)

        # top-k é€‰æ‹©
        top_k_logits, top_k_indices = torch.topk(logits, self.top_k, dim=-1)  # (B,T,k)
        top_k_weights = torch.softmax(top_k_logits, dim=-1)                    # (B,T,k)

        return top_k_weights, top_k_indices, aux_loss

    def _compute_aux_loss(self, logits: torch.Tensor) -> torch.Tensor:
        """è®¡ç®— Switch-style load-balancing loss + router z-lossã€‚

        balance loss = N * sum_i(f_i * P_i)
        z-loss       = mean(logsumexp(logits)^2)
        """
        # logits: (B, T, E)
        num_experts = logits.size(-1)
        probs = torch.softmax(logits, dim=-1)  # (B,T,E)

        # f_i: æ¯ä¸ªä¸“å®¶è¢« top-k é€‰ä¸­çš„ token æ¯”ä¾‹
        _, top_indices = torch.topk(logits, self.top_k, dim=-1)          # (B,T,k)
        expert_mask = torch.zeros_like(probs)
        expert_mask.scatter_(-1, top_indices, 1.0)                        # (B,T,E)
        f = expert_mask.float().mean(dim=(0, 1))                          # (E,)

        # P_i: æ¯ä¸ªä¸“å®¶è·å¾—çš„å¹³å‡è·¯ç”±æ¦‚ç‡
        P = probs.mean(dim=(0, 1))                                        # (E,)

        # Switch Transformer balance loss
        balance_loss = num_experts * (f * P).sum()

        # Router z-loss: é˜²æ­¢ logits è¿‡å¤§å¯¼è‡´æ•°å€¼ä¸ç¨³
        z_loss = torch.logsumexp(logits, dim=-1).pow(2).mean()

        return balance_loss + z_loss


class MoEFFN(nn.Module):
    """MoE-Ready FFNï¼šæ”¯æŒ Dense / MoE å¯åˆ‡æ¢çš„å‰é¦ˆç½‘ç»œ

    å½“ use_moe=False æ—¶é€€åŒ–ä¸ºæ™®é€š Dense FFN (aux_loss=0)ã€‚
    å½“ use_moe=True æ—¶ï¼š
      - åˆ›å»º num_experts ä¸ª FFN ä¸“å®¶
      - ç”¨ TopKRouter é€‰ä¸“å®¶
      - å¯é€‰ shared_expertï¼ˆalways-on çš„ dense ä¸“å®¶å½“åº•ç›˜ï¼‰
      - forward è¿”å› (output, aux_loss)

    åç«¯ç­–ç•¥ï¼ˆå¯æ‰©å±•ï¼‰:
      - é»˜è®¤: çº¯ PyTorch loop dispatch (å…¼å®¹æ€§æœ€å¥½)
      - å¯é€‰: megablocks / scattermoe / tutel (å®‰è£…åè‡ªåŠ¨å¯ç”¨)
    """

    def __init__(
        self,
        d_model: int,
        dim_feedforward: int,
        dropout: float = 0.0,
        activation: str = "gelu",
        use_swiglu: bool = True,
        # --- MoE å‚æ•° ---
        use_moe: bool = False,
        num_experts: int = 8,
        top_k: int = 1,
        capacity_factor: float = 1.25,
        shared_expert: bool = True,
        noisy_gating: bool = True,
    ):
        super().__init__()
        self.use_moe = use_moe
        self.d_model = d_model
        self.top_k = top_k
        self.capacity_factor = capacity_factor
        self.use_shared_expert = shared_expert and use_moe

        if not use_moe:
            # --- Dense æ¨¡å¼ ---
            if use_swiglu:
                self.dense_ffn = SwiGLU(d_model, dim_feedforward, d_model, dropout=dropout)
            else:
                self.dense_ffn = nn.Sequential(
                    nn.Linear(d_model, dim_feedforward),
                    nn.GELU() if activation == "gelu" else nn.ReLU(),
                    nn.Dropout(dropout),
                    nn.Linear(dim_feedforward, d_model),
                )
        else:
            # --- MoE æ¨¡å¼ ---
            self.router = TopKRouter(
                d_model=d_model,
                num_experts=num_experts,
                top_k=top_k,
                noisy_gating=noisy_gating,
            )

            # åˆ›å»º num_experts ä¸ªç‹¬ç«‹ FFN ä¸“å®¶
            experts = []
            for _ in range(num_experts):
                if use_swiglu:
                    experts.append(SwiGLU(d_model, dim_feedforward, d_model, dropout=dropout))
                else:
                    experts.append(nn.Sequential(
                        nn.Linear(d_model, dim_feedforward),
                        nn.GELU() if activation == "gelu" else nn.ReLU(),
                        nn.Dropout(dropout),
                        nn.Linear(dim_feedforward, d_model),
                    ))
            self.experts = nn.ModuleList(experts)

            # å¯é€‰ shared expert (always-on dense ä¸“å®¶)
            if self.use_shared_expert:
                if use_swiglu:
                    self.shared_expert_ffn = SwiGLU(d_model, dim_feedforward, d_model, dropout=dropout)
                else:
                    self.shared_expert_ffn = nn.Sequential(
                        nn.Linear(d_model, dim_feedforward),
                        nn.GELU() if activation == "gelu" else nn.ReLU(),
                        nn.Dropout(dropout),
                        nn.Linear(dim_feedforward, d_model),
                    )
                # èåˆé—¨æ§ï¼šæ§åˆ¶ shared_expert å’Œ routed_expert çš„æ¯”ä¾‹
                self.shared_gate = nn.Linear(d_model, 1, bias=True)

    def forward(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        å‚æ•°:
            x: (B, T, D)

        è¿”å›:
            output:   (B, T, D)
            aux_loss: scalar (Dense æ¨¡å¼ä¸‹ä¸º 0)
        """
        if not self.use_moe:
            return self.dense_ffn(x), torch.tensor(0.0, device=x.device, dtype=x.dtype)

        # --- MoE dispatch ---
        B, T, D = x.shape
        weights, indices, aux_loss = self.router(x)  # (B,T,k), (B,T,k), scalar

        # Token-level dispatch: å¯¹æ¯ä¸ª token åŠ æƒç»„åˆ top_k ä¸“å®¶è¾“å‡º
        # ä½¿ç”¨é«˜æ•ˆçš„ batch å®ç°ï¼Œé¿å… python-level loop over tokens
        x_flat = x.view(B * T, D)                                  # (N, D)
        weights_flat = weights.view(B * T, self.top_k)              # (N, k)
        indices_flat = indices.view(B * T, self.top_k)              # (N, k)

        # æ”¶é›†æ‰€æœ‰ä¸“å®¶éœ€è¦å¤„ç†çš„ token å¹¶æ‰¹é‡æ‰§è¡Œ
        output = torch.zeros_like(x_flat)  # (N, D)

        for k_idx in range(self.top_k):
            expert_indices_k = indices_flat[:, k_idx]   # (N,)
            weights_k = weights_flat[:, k_idx]           # (N,)

            for e_idx in range(len(self.experts)):
                mask = (expert_indices_k == e_idx)       # (N,)
                if mask.any():
                    expert_input = x_flat[mask]          # (n_e, D)
                    expert_output = self.experts[e_idx](expert_input)  # (n_e, D)
                    output[mask] += weights_k[mask].unsqueeze(-1) * expert_output

        output = output.view(B, T, D)

        # --- Shared expert ---
        if self.use_shared_expert:
            shared_out = self.shared_expert_ffn(x)                      # (B,T,D)
            gate = torch.sigmoid(self.shared_gate(x.mean(dim=1, keepdim=True)))  # (B,1,1)
            output = gate * shared_out + (1.0 - gate) * output

        return output, aux_loss


class PositionalEncoding(nn.Module):
    """ä½ç½®ç¼–ç å®ç°ï¼Œæ”¯æŒåŠ¨æ€æ‰©å±•"""
    def __init__(self, d_model, max_len=5000, dropout=0.1):
        super().__init__()
        self.d_model = d_model
        self.dropout = nn.Dropout(p=dropout)
        # é¢„å…ˆè®¡ç®— max_len ä¸ªä½ç½®ç¼–ç 
        pe = torch.zeros(max_len, d_model)
        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
        div_term = torch.exp(torch.arange(0, d_model, 2, dtype=torch.float) * (-math.log(10000.0) / d_model))
        pe[:, 0::2] = torch.sin(position * div_term)
        pe[:, 1::2] = torch.cos(position * div_term)
        pe = pe.unsqueeze(0)  # shape: [1, max_len, d_model]
        self.register_buffer('pe', pe)

    def forward(self, x):
        """
        å‚æ•°:
            x: [batch_size, seq_len, embedding_dim]
        è¿”å›:
            x åŠ ä¸Šä½ç½®ç¼–ç 
        """
        seq_len = x.size(1)
        if seq_len > self.pe.size(1):
            device = x.device
            extra_len = seq_len - self.pe.size(1)
            # ç”Ÿæˆé¢å¤–çš„ä½ç½®ç¼–ç 
            pe_extra = torch.zeros(extra_len, self.d_model, device=device)
            position = torch.arange(self.pe.size(1), seq_len, dtype=torch.float, device=device).unsqueeze(1)
            div_term = torch.exp(torch.arange(0, self.d_model, 2, dtype=torch.float, device=device) * (-math.log(10000.0) / self.d_model))
            pe_extra[:, 0::2] = torch.sin(position * div_term)
            pe_extra[:, 1::2] = torch.cos(position * div_term)
            pe_extra = pe_extra.unsqueeze(0)  # shape: [1, extra_len, d_model]
            pe = torch.cat([self.pe, pe_extra], dim=1)
        else:
            pe = self.pe
        return self.dropout(x + pe[:, :seq_len, :])


# ä½ å¯ä»¥åœ¨æ­¤å¤„å®šä¹‰ä¸€ä¸ªå…¨å±€æ—¥å¿—æ–‡ä»¶åï¼Œç¡®ä¿æ¯æ¬¡éƒ½å¾€åŒä¸€ä¸ªæ–‡ä»¶è¿½åŠ å†™å…¥
DEBUG_LOG_FILE = "autopoietic_debug.log"

class AutopoieticAttention(nn.Module):
    """è‡ªç”Ÿæˆæ³¨æ„åŠ›æœºåˆ¶ï¼ˆä¼˜åŒ–ç‰ˆï¼‰

    è®¾è®¡ç›®æ ‡ï¼š
    - çƒ­è·¯å¾„ä¼˜å…ˆï¼šå°½é‡è®©æ³¨æ„åŠ›èµ°åˆ° PyTorch SDPAï¼ˆFlash/Math/Memory-efficientï¼‰å®ç°
    - ä¿ç•™â€œè‡ªç”Ÿæˆâ€å‘³é“ï¼šç”¨ä½ç§©è‡ªç”Ÿæˆåˆ†é‡ + é—¨æ§æ¸©åº¦ Ï„ å¯¹æ³¨æ„åŠ›è¾“å‡ºåšå¯å­¦ä¹ æ‰°åŠ¨
    - ä¿æŒæ¥å£å…¼å®¹ï¼šforward è¿”å› (attn_out, attn_weights_or_None)
    """

    def __init__(
        self,
        embed_dim: int,
        num_heads: int,
        dropout: float = 0.0,
        eps: float = 1e-6,
        alpha: float = 0.1,
        init_tau: float = 1.0,
        sr_ratio: int = 4,
        use_autopoietic: bool = True,
        batch_first: bool = True,
        use_dbc_dac: bool = True,
        debug_mode: bool = False,
        rank_ratio_proj: float = 0.1,
        rank_ratio_res: float = 0.05,
        dbc_threshold: float = 1e-6,
        dbc_iterations: int = 1,
        use_fused_qkv: bool = True,
    ):
        super().__init__()
        assert embed_dim % num_heads == 0, "embed_dim must be divisible by num_heads"
        self.embed_dim = embed_dim
        self.num_heads = num_heads
        self.dropout = float(dropout)
        self.head_dim = embed_dim // num_heads
        self.batch_first = batch_first
        self.use_autopoietic = use_autopoietic
        self.alpha = float(alpha)
        self.eps = float(eps)
        self.debug_mode = bool(debug_mode)

        # fused QKV for better kernel shapes
        self.use_fused_qkv = bool(use_fused_qkv)
        if self.use_fused_qkv:
            self.qkv_proj = nn.Linear(embed_dim, 3 * embed_dim)
            self.q_proj = self.k_proj = self.v_proj = None
        else:
            self.q_proj = nn.Linear(embed_dim, embed_dim)
            self.k_proj = nn.Linear(embed_dim, embed_dim)
            self.v_proj = nn.Linear(embed_dim, embed_dim)
            self.qkv_proj = None

        self.out_proj = nn.Linear(embed_dim, embed_dim)

        # å¯å­¦ä¹ æ¸©åº¦ï¼ˆç”¨äºâ€œè‡ªç”Ÿæˆé—¨æ§â€ï¼‰
        self.tau = nn.Parameter(torch.ones(1) * float(init_tau))

        # ä½ç§©â€œè‡ªç”Ÿæˆâ€åˆ†é‡ï¼šÎ” = (x U) V
        # sr_ratio è¶Šå¤§ï¼Œrank è¶Šå°ï¼ˆæ›´çœ FLOPsï¼‰
        r = max(4, embed_dim // max(1, int(sr_ratio)))
        self.auto_u = nn.Linear(embed_dim, r, bias=False)
        self.auto_v = nn.Linear(r, embed_dim, bias=False)
        self.auto_gate = nn.Linear(embed_dim, num_heads, bias=True)

        self.dropout_layer = nn.Dropout(self.dropout)

    def _shape(self, x: torch.Tensor) -> torch.Tensor:
        # (B,T,C) -> (B,H,T,D)
        b, t, c = x.shape
        x = x.view(b, t, self.num_heads, self.head_dim).transpose(1, 2).contiguous()
        return x

    def _merge(self, x: torch.Tensor) -> torch.Tensor:
        # (B,H,T,D) -> (B,T,C)
        b, h, t, d = x.shape
        return x.transpose(1, 2).contiguous().view(b, t, h * d)

    def forward(
        self,
        query: torch.Tensor,
        key: torch.Tensor,
        value: torch.Tensor,
        attn_mask: Optional[torch.Tensor] = None,
        key_padding_mask: Optional[torch.Tensor] = None,
        need_weights: bool = False,
        is_causal: bool = False,
    ):
        # ç»Ÿä¸€ batch_first
        if not self.batch_first:
            # (T,B,C) -> (B,T,C)
            query = query.transpose(0, 1)
            key = key.transpose(0, 1)
            value = value.transpose(0, 1)

        b, t, c = query.shape

        # Patch 5: åªæœ‰çœŸæ­£ self-attnï¼ˆq/k/v æŒ‡å‘åŒä¸€å¼ é‡ï¼‰æ‰èµ° fused QKV
        is_self_attn = (query is key) and (key is value)
        use_fused = self.use_fused_qkv and is_self_attn

        if use_fused:
            qkv = self.qkv_proj(query)  # (B,T,3C)
            q, k, v = qkv.chunk(3, dim=-1)
        else:
            if self.use_fused_qkv:
                # fused proj ä¸é€‚ç”¨ cross-attnï¼Œé€€åŒ–ï¼šç”¨åŒä¸€çŸ©é˜µåˆ†åˆ«æŠ•å½± q/k/v
                q = self.qkv_proj(query)[..., :c]
                k = self.qkv_proj(key)[..., c:2*c]
                v = self.qkv_proj(value)[..., 2*c:]
            else:
                q = self.q_proj(query)
                k = self.k_proj(key)
                v = self.v_proj(value)

        q = self._shape(q)
        k = self._shape(k)
        v = self._shape(v)

        # key_padding_mask: (B,T) -> (B,1,1,T) additive mask for SDPA when needed
        # SDPA in PyTorch supports attn_mask; we compose a boolean mask if possible.
        composed_mask = None
        if key_padding_mask is not None:
            # True means "pad" (masked)
            kpm = key_padding_mask.view(b, 1, 1, -1).to(dtype=torch.bool)
            composed_mask = kpm if composed_mask is None else (composed_mask | kpm)

        if attn_mask is not None:
            # accept (T,T), (B,T,T), or already broadcastable
            am = attn_mask
            # ç»Ÿä¸€è½¬ä¸º boolï¼š-inf / éé›¶ â†’ Trueï¼ˆé®æ‰ï¼‰ï¼Œ0.0 â†’ Falseï¼ˆæ”¾é€šï¼‰
            # è¿™æ · float causal mask å’Œ bool kpm å¯ä»¥å®‰å…¨åœ°ç”¨ | åˆå¹¶ï¼Œ
            # é¿å… float mask åœ¨ composed_mask å·²å­˜åœ¨æ—¶è¢«é™é»˜ä¸¢å¼ƒã€‚
            if am.dtype != torch.bool:
                am = am.bool()
            # broadcast to (B,1,T,T)
            if am.dim() == 2:
                am = am.view(1, 1, am.size(0), am.size(1)).expand(b, 1, -1, -1).contiguous()
            elif am.dim() == 3:
                am = am.view(b, 1, am.size(-2), am.size(-1)).contiguous()
            composed_mask = am if composed_mask is None else (composed_mask | am)

        # SDPA fast path
        if hasattr(F, "scaled_dot_product_attention"):
            # dropout only during training
            dropout_p = self.dropout if self.training else 0.0
            attn_out = F.scaled_dot_product_attention(
                q, k, v,
                attn_mask=composed_mask,
                dropout_p=dropout_p,
                is_causal=is_causal
            )  # (B,H,T,D)
            attn_weights = None
        else:
            # fallback: explicit softmax
            scale = 1.0 / math.sqrt(self.head_dim)
            scores = torch.matmul(q, k.transpose(-2, -1)) * scale  # (B,H,T,T)
            if composed_mask is not None:
                if composed_mask.dtype == torch.bool:
                    scores = scores.masked_fill(composed_mask, float("-inf"))
                else:
                    scores = scores + composed_mask
            if is_causal:
                causal = torch.triu(torch.ones(t, t, device=scores.device, dtype=torch.bool), diagonal=1)
                scores = scores.masked_fill(causal.view(1,1,t,t), float("-inf"))
            attn = torch.softmax(scores, dim=-1)
            attn = self.dropout_layer(attn)
            attn_out = torch.matmul(attn, v)
            attn_weights = attn if need_weights else None

        out = self._merge(attn_out)  # (B,T,C)

        # è‡ªç”Ÿæˆæ‰°åŠ¨ï¼šä½ç§©åˆ†é‡ + é—¨æ§æ¸©åº¦
        if self.use_autopoietic:
            # gate per-head from mean token embedding
            g = torch.sigmoid(self.auto_gate(query.mean(dim=1)))  # (B,H)
            g = g.view(b, self.num_heads, 1, 1)
            delta = self._shape(self.auto_v(self.auto_u(query)))  # (B,H,T,D)
            # æ¸©åº¦ Ï„ ä¸ alpha æ§åˆ¶æ‰°åŠ¨å¼ºåº¦
            out = out + self.alpha * torch.tanh(self.tau) * self._merge(delta * g)

        out = self.out_proj(out)

        if not self.batch_first:
            out = out.transpose(0, 1)

        return out, attn_weights

class APTEncoderLayer(nn.Module):
    """
    APTç¼–ç å™¨å±‚
    é›†æˆè‡ªç”Ÿæˆæ³¨æ„åŠ›æœºåˆ¶ + å·¦æ—‹å¹³æ»‘æ®‹å·®è¿æ¥ + MoE-Ready FFN
    """
    def __init__(
        self,
        d_model: int,
        nhead: int,
        dim_feedforward: int = 2048,
        dropout: float = 0.1,
        activation: str = "gelu",
        batch_first: bool = True,
        eps: float = 1e-6,
        alpha: float = 0.1,
        init_tau: float = 1.0,
        sr_ratio: int = 4,
        use_autopoietic: bool = True,
        # DBC-DACç›¸å…³å‚æ•°
        use_dbc_dac: bool = True,
        rank_ratio_proj: float = 0.1,
        rank_ratio_res: float = 0.05,
        dbc_threshold: float = 1e-6,
        dbc_iterations: int = 1,
        # å·¦æ—‹å¹³æ»‘å‚æ•°
        use_left_spin: bool = True,
        left_spin_alpha: float = 0.5,
        left_spin_tau: float = 0.3,
        left_spin_beta: float = 0.7,
        # ç°ä»£åŒ–ç»„ä»¶å¼€å…³
        use_rmsnorm: bool = True,
        use_swiglu: bool = True,
        # FFN æ‰©å±•å€ç‡ï¼ˆPhi-3 é£æ ¼ï¼‰
        ffn_ratio: Optional[float] = None,
        # MoE å‚æ•°ï¼ˆé»˜è®¤å…¨å…³é—­ï¼Œä¸å½±å“ç°æœ‰è®­ç»ƒï¼‰
        use_moe: bool = False,
        moe_num_experts: int = 8,
        moe_top_k: int = 1,
        moe_capacity_factor: float = 1.25,
        moe_shared_expert: bool = True,
        moe_noisy_gating: bool = True,
    ):
        super().__init__()
        self.use_rmsnorm = use_rmsnorm
        self.use_swiglu = use_swiglu

        # FFN æ‰©å±•å€ç‡è¦†ç›–
        if ffn_ratio is not None:
            dim_feedforward = round(d_model * ffn_ratio)

        # è‡ªç”Ÿæˆæ³¨æ„åŠ›å±‚
        self.self_attn = AutopoieticAttention(
            embed_dim=d_model,
            num_heads=nhead,
            dropout=dropout,
            eps=eps,
            alpha=alpha,
            init_tau=init_tau,
            sr_ratio=sr_ratio,
            use_autopoietic=use_autopoietic,
            batch_first=batch_first,
            use_dbc_dac=use_dbc_dac,
            rank_ratio_proj=rank_ratio_proj,
            rank_ratio_res=rank_ratio_res,
            dbc_threshold=dbc_threshold,
            dbc_iterations=dbc_iterations
        )

        # å‰é¦ˆç½‘ç»œï¼šç»Ÿä¸€èµ° MoEFFN (Dense / MoE å¯åˆ‡æ¢)
        self.ffn = MoEFFN(
            d_model=d_model,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            activation=activation,
            use_swiglu=use_swiglu,
            use_moe=use_moe,
            num_experts=moe_num_experts,
            top_k=moe_top_k,
            capacity_factor=moe_capacity_factor,
            shared_expert=moe_shared_expert,
            noisy_gating=moe_noisy_gating,
        )
        # å…¼å®¹æ—§ä»£ç çš„å­—æ®µå¼•ç”¨
        self.swiglu = None
        self.linear1 = None
        self.linear2 = None
        self.dropout = None

        # å±‚å½’ä¸€åŒ–
        self.norm1 = RMSNorm(d_model, eps=eps) if self.use_rmsnorm else nn.LayerNorm(d_model)
        self.norm2 = RMSNorm(d_model, eps=eps) if self.use_rmsnorm else nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)

        # æ¿€æ´»å‡½æ•°
        self.activation_fn = F.gelu if activation == "gelu" else F.relu

        # é…ç½®
        self.batch_first = batch_first
        self.use_left_spin = use_left_spin

        # è¾…åŠ©æŸå¤±ç¼“å­˜ï¼ˆæ¯å±‚ç‹¬ç«‹ï¼‰
        self._aux_loss = torch.tensor(0.0)

        # ğŸš€ å·¦æ—‹å¹³æ»‘æ®‹å·®è¿æ¥ï¼ˆæ›¿æ¢ä¼ ç»Ÿæ³°å‹’å±•å¼€ï¼‰
        if use_left_spin:
            self.left_spin_attn = LeftSpinResidual(
                alpha=alpha,  # ä¿®å¤ï¼šä½¿ç”¨ä¸ AutopoieticAttention ä¸€è‡´çš„ alpha
                tau=init_tau,  # ä¿®å¤ï¼šä½¿ç”¨ä¸ AutopoieticAttention ä¸€è‡´çš„ init_tau
                beta=left_spin_beta,
                gate_type='normalized',
                adaptive=True
            )
            self.left_spin_ffn = LeftSpinResidual(
                alpha=alpha,  # ä¿®å¤ï¼šä½¿ç”¨ä¸ AutopoieticAttention ä¸€è‡´çš„ alpha
                tau=init_tau,  # ä¿®å¤ï¼šä½¿ç”¨ä¸ AutopoieticAttention ä¸€è‡´çš„ init_tau
                beta=left_spin_beta,
                gate_type='normalized',
                adaptive=True
            )
        else:
            self.left_spin_attn = None
            self.left_spin_ffn = None

        self.debug_mode = False

    def forward(
        self,
        src: torch.Tensor,
        src_mask: Optional[torch.Tensor] = None,
        src_key_padding_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        ç¼–ç å™¨å±‚å‰å‘ä¼ æ’­ï¼ˆé›†æˆå·¦æ—‹å¹³æ»‘ + MoEï¼‰

        å‚æ•°:
            src: è¾“å…¥å¼ é‡ [seq_len, batch_size, d_model] æˆ– [batch_size, seq_len, d_model]
            src_mask: åºåˆ—æ©ç  [seq_len, seq_len] æˆ– [batch_size, seq_len, seq_len]
            src_key_padding_mask: å¡«å……æ©ç  [batch_size, seq_len]

        è¿”å›:
            output: ç¼–ç å™¨å±‚è¾“å‡º
        """
        # ğŸš€ è‡ªæ³¨æ„åŠ›å­å±‚ï¼ˆå·¦æ—‹å¹³æ»‘æ®‹å·®è¿æ¥ï¼‰
        src2, _ = self.self_attn(
            query=src,
            key=src,
            value=src,
            attn_mask=src_mask,
            key_padding_mask=src_key_padding_mask
        )
        src2_dropout = self.dropout1(src2)

        if self.use_left_spin and self.left_spin_attn is not None:
            src = self.left_spin_attn(src, src2_dropout)
        else:
            src = src + src2_dropout

        src = self.norm1(src)

        # ğŸš€ å‰é¦ˆç½‘ç»œå­å±‚ï¼ˆMoE-Ready: è¿”å› (output, aux_loss)ï¼‰
        src2, aux_loss = self.ffn(src)
        self._aux_loss = aux_loss
        src2_dropout = self.dropout2(src2)

        if self.use_left_spin and self.left_spin_ffn is not None:
            src = self.left_spin_ffn(src, src2_dropout)
        else:
            src = src + src2_dropout

        src = self.norm2(src)

        return src


class APTDecoderLayer(nn.Module):
    """
    APTè§£ç å™¨å±‚
    é›†æˆè‡ªç”Ÿæˆæ³¨æ„åŠ›æœºåˆ¶ + å·¦æ—‹å¹³æ»‘æ®‹å·®è¿æ¥ + MoE-Ready FFN
    """
    def __init__(
        self,
        d_model: int,
        nhead: int,
        dim_feedforward: int = 2048,
        dropout: float = 0.1,
        activation: str = "gelu",
        batch_first: bool = True,
        eps: float = 1e-6,
        alpha: float = 0.1,
        init_tau: float = 1.0,
        sr_ratio: int = 4,
        use_autopoietic: bool = True,
        # DBC-DACç›¸å…³å‚æ•°
        use_dbc_dac: bool = True,
        rank_ratio_proj: float = 0.1,
        rank_ratio_res: float = 0.05,
        dbc_threshold: float = 1e-6,
        dbc_iterations: int = 1,
        # å·¦æ—‹å¹³æ»‘å‚æ•°
        use_left_spin: bool = True,
        left_spin_alpha: float = 0.5,
        left_spin_tau: float = 0.3,
        left_spin_beta: float = 0.7,
        # ç°ä»£åŒ–ç»„ä»¶å¼€å…³
        use_rmsnorm: bool = True,
        use_swiglu: bool = True,
        # GPT-only å¼€å…³ï¼šFalse æ—¶ cross-attn è¢«æ—è·¯
        use_cross_attn: bool = True,
        # FFN æ‰©å±•å€ç‡ï¼ˆPhi-3 é£æ ¼ï¼‰
        ffn_ratio: Optional[float] = None,
        # MoE å‚æ•°ï¼ˆé»˜è®¤å…¨å…³é—­ï¼Œä¸å½±å“ç°æœ‰è®­ç»ƒï¼‰
        use_moe: bool = False,
        moe_num_experts: int = 8,
        moe_top_k: int = 1,
        moe_capacity_factor: float = 1.25,
        moe_shared_expert: bool = True,
        moe_noisy_gating: bool = True,
    ):
        super().__init__()
        self.use_rmsnorm = use_rmsnorm
        self.use_swiglu = use_swiglu
        self.use_cross_attn = use_cross_attn

        # FFN æ‰©å±•å€ç‡è¦†ç›–
        if ffn_ratio is not None:
            dim_feedforward = round(d_model * ffn_ratio)

        # è‡ªæ³¨æ„åŠ›å±‚(æ©ç )
        self.self_attn = AutopoieticAttention(
            embed_dim=d_model,
            num_heads=nhead,
            dropout=dropout,
            eps=eps,
            alpha=alpha,
            init_tau=init_tau,
            sr_ratio=sr_ratio,
            use_autopoietic=use_autopoietic,
            batch_first=batch_first,
            use_dbc_dac=use_dbc_dac,
            rank_ratio_proj=rank_ratio_proj,
            rank_ratio_res=rank_ratio_res,
            dbc_threshold=dbc_threshold,
            dbc_iterations=dbc_iterations
        )

        # ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›å±‚
        self.multihead_attn = AutopoieticAttention(
            embed_dim=d_model,
            num_heads=nhead,
            dropout=dropout,
            eps=eps,
            alpha=alpha,
            init_tau=init_tau,
            sr_ratio=sr_ratio,
            use_autopoietic=use_autopoietic,
            batch_first=batch_first,
            use_dbc_dac=use_dbc_dac,
            rank_ratio_proj=rank_ratio_proj,
            rank_ratio_res=rank_ratio_res,
            dbc_threshold=dbc_threshold,
            dbc_iterations=dbc_iterations
        )

        # å‰é¦ˆç½‘ç»œï¼šç»Ÿä¸€èµ° MoEFFN (Dense / MoE å¯åˆ‡æ¢)
        self.ffn = MoEFFN(
            d_model=d_model,
            dim_feedforward=dim_feedforward,
            dropout=dropout,
            activation=activation,
            use_swiglu=use_swiglu,
            use_moe=use_moe,
            num_experts=moe_num_experts,
            top_k=moe_top_k,
            capacity_factor=moe_capacity_factor,
            shared_expert=moe_shared_expert,
            noisy_gating=moe_noisy_gating,
        )
        # å…¼å®¹æ—§ä»£ç çš„å­—æ®µå¼•ç”¨
        self.swiglu = None
        self.linear1 = None
        self.linear2 = None
        self.dropout = None

        # å±‚å½’ä¸€åŒ–
        self.norm1 = RMSNorm(d_model, eps=eps) if self.use_rmsnorm else nn.LayerNorm(d_model)
        self.norm2 = RMSNorm(d_model, eps=eps) if self.use_rmsnorm else nn.LayerNorm(d_model)
        self.norm3 = RMSNorm(d_model, eps=eps) if self.use_rmsnorm else nn.LayerNorm(d_model)
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)
        self.dropout3 = nn.Dropout(dropout)

        # æ¿€æ´»å‡½æ•°
        self.activation_fn = F.gelu if activation == "gelu" else F.relu

        # é…ç½®
        self.batch_first = batch_first
        self.use_left_spin = use_left_spin

        # è¾…åŠ©æŸå¤±ç¼“å­˜ï¼ˆæ¯å±‚ç‹¬ç«‹ï¼‰
        self._aux_loss = torch.tensor(0.0)

        # ğŸš€ å·¦æ—‹å¹³æ»‘æ®‹å·®è¿æ¥ï¼ˆ3ä¸ªå­å±‚ï¼‰
        if use_left_spin:
            self.left_spin_self_attn = LeftSpinResidual(
                alpha=alpha,  # ä¿®å¤ï¼šä½¿ç”¨ä¸ AutopoieticAttention ä¸€è‡´çš„ alpha
                tau=init_tau,  # ä¿®å¤ï¼šä½¿ç”¨ä¸ AutopoieticAttention ä¸€è‡´çš„ init_tau
                beta=left_spin_beta,
                gate_type='normalized',
                adaptive=True
            )
            self.left_spin_cross_attn = LeftSpinResidual(
                alpha=alpha,  # ä¿®å¤ï¼šä½¿ç”¨ä¸ AutopoieticAttention ä¸€è‡´çš„ alpha
                tau=init_tau,  # ä¿®å¤ï¼šä½¿ç”¨ä¸ AutopoieticAttention ä¸€è‡´çš„ init_tau
                beta=left_spin_beta,
                gate_type='normalized',
                adaptive=True
            )
            self.left_spin_ffn = LeftSpinResidual(
                alpha=alpha,  # ä¿®å¤ï¼šä½¿ç”¨ä¸ AutopoieticAttention ä¸€è‡´çš„ alpha
                tau=init_tau,  # ä¿®å¤ï¼šä½¿ç”¨ä¸ AutopoieticAttention ä¸€è‡´çš„ init_tau
                beta=left_spin_beta,
                gate_type='normalized',
                adaptive=True
            )
        else:
            self.left_spin_self_attn = None
            self.left_spin_cross_attn = None
            self.left_spin_ffn = None

    def forward(
        self,
        tgt: torch.Tensor,
        memory: Optional[torch.Tensor] = None,
        tgt_mask: Optional[torch.Tensor] = None,
        memory_mask: Optional[torch.Tensor] = None,
        tgt_key_padding_mask: Optional[torch.Tensor] = None,
        memory_key_padding_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        è§£ç å™¨å±‚å‰å‘ä¼ æ’­ï¼ˆé›†æˆå·¦æ—‹å¹³æ»‘ + MoEï¼‰

        å‚æ•°:
            tgt: ç›®æ ‡åºåˆ— [seq_len, batch_size, d_model] æˆ– [batch_size, seq_len, d_model]
            memory: ç¼–ç å™¨è¾“å‡ºï¼ˆå¯ä¸º Noneï¼Œæ­¤æ—¶å®Œå…¨è·³è¿‡ cross-attnï¼Œç­‰ä»· GPT blockï¼‰
            tgt_mask: ç›®æ ‡åºåˆ—æ©ç  [tgt_len, tgt_len] æˆ– [batch_size, tgt_len, tgt_len]
            memory_mask: è®°å¿†æ©ç  [tgt_len, src_len]
            tgt_key_padding_mask: ç›®æ ‡å¡«å……æ©ç  [batch_size, tgt_len]
            memory_key_padding_mask: è®°å¿†å¡«å……æ©ç  [batch_size, src_len]

        è¿”å›:
            output: è§£ç å™¨å±‚è¾“å‡º
        """
        # ğŸš€ è‡ªæ³¨æ„åŠ›å­å±‚ï¼ˆå·¦æ—‹å¹³æ»‘æ®‹å·®è¿æ¥ï¼‰
        tgt2, _ = self.self_attn(
            query=tgt,
            key=tgt,
            value=tgt,
            attn_mask=tgt_mask,
            key_padding_mask=tgt_key_padding_mask
        )
        tgt2_dropout = self.dropout1(tgt2)

        if self.use_left_spin and self.left_spin_self_attn is not None:
            tgt = self.left_spin_self_attn(tgt, tgt2_dropout)
        else:
            tgt = tgt + tgt2_dropout

        tgt = self.norm1(tgt)

        # ğŸš€ ç¼–ç å™¨-è§£ç å™¨æ³¨æ„åŠ›å­å±‚ï¼ˆæ—è·¯å¼ï¼šmemory=None æˆ– use_cross_attn=False æ—¶è·³è¿‡ï¼‰
        do_cross = (
            memory is not None
            and getattr(self, "use_cross_attn", True)
            and getattr(self, "multihead_attn", None) is not None
        )
        if do_cross:
            tgt2, _ = self.multihead_attn(
                query=tgt,
                key=memory,
                value=memory,
                attn_mask=memory_mask,
                key_padding_mask=memory_key_padding_mask
            )
            tgt2_dropout = self.dropout2(tgt2)

            if self.use_left_spin and self.left_spin_cross_attn is not None:
                tgt = self.left_spin_cross_attn(tgt, tgt2_dropout)
            else:
                tgt = tgt + tgt2_dropout

            tgt = self.norm2(tgt)

        # ğŸš€ å‰é¦ˆç½‘ç»œå­å±‚ï¼ˆMoE-Ready: è¿”å› (output, aux_loss)ï¼‰
        tgt2, aux_loss = self.ffn(tgt)
        self._aux_loss = aux_loss
        tgt2_dropout = self.dropout3(tgt2)

        if self.use_left_spin and self.left_spin_ffn is not None:
            tgt = self.left_spin_ffn(tgt, tgt2_dropout)
        else:
            tgt = tgt + tgt2_dropout

        tgt = self.norm3(tgt)

        return tgt


class APTModelConfiguration:
    """APTæ¨¡å‹é…ç½®ç±»ï¼ˆé›†æˆå·¦æ—‹å¹³æ»‘ + MoEï¼‰"""
    def __init__(
        self,
        vocab_size: int = 30522,  # è¯æ±‡è¡¨å¤§å°
        d_model: int = 768,  # æ¨¡å‹ç»´åº¦
        max_seq_len: int = 2048,  # æœ€å¤§åºåˆ—é•¿åº¦
        num_encoder_layers: int = 12,  # ç¼–ç å™¨å±‚æ•°
        num_decoder_layers: int = 12,  # è§£ç å™¨å±‚æ•°
        num_heads: int = 12,  # æ³¨æ„åŠ›å¤´æ•°
        d_ff: int = 3072,  # å‰é¦ˆç½‘ç»œç»´åº¦
        dropout: float = 0.1,  # Dropoutæ¯”ç‡
        activation: str = "gelu",  # æ¿€æ´»å‡½æ•°
        epsilon: float = 1e-6,  # è‡ªç”Ÿæˆæ— ç©·å€’æ•°ç¼©æ”¾å› å­
        alpha: float = 0.1,  # æ³°å‹’å±•å¼€ç³»æ•°ï¼ˆå·²è¢«å·¦æ—‹å¹³æ»‘æ›¿æ¢ï¼‰
        beta: float = 0.01,  # åŠ¨æ€è°ƒèŠ‚ç³»æ•°
        init_tau: float = 1.0,  # åˆå§‹æ¸©åº¦
        sr_ratio: int = 4,  # è‡ªç”ŸæˆçŸ©é˜µå‹ç¼©æ¯”
        use_autopoietic: bool = True,  # æ˜¯å¦ä½¿ç”¨è‡ªç”Ÿæˆæœºåˆ¶
        base_lr: float = 3e-5,  # åŸºå‡†å­¦ä¹ ç‡(ç”¨äºåŠ¨æ€å‚æ•°è°ƒæ•´)
        batch_first: bool = True,  # æ˜¯å¦ä½¿ç”¨batch_firstæ ¼å¼
        pad_token_id: int = 0,  # å¡«å……token ID
        bos_token_id: int = 101,  # å¼€å§‹token ID
        eos_token_id: int = 102,  # ç»“æŸtoken ID
        # DBC-DAC ç›¸å…³å‚æ•°
        use_dbc_dac: bool = True,  # æ˜¯å¦ä½¿ç”¨DBC-DACç¨³å®š
        rank_ratio_proj: float = 0.1,  # DBCæŠ•å½±æ¯”ä¾‹
        rank_ratio_res: float = 0.05,  # DACæ®‹å·®æ¯”ä¾‹
        dbc_threshold: float = 1e-6,  # DBCé˜ˆå€¼
        dbc_iterations: int = 1,  # DACè¿­ä»£æ¬¡æ•°
        # ğŸš€ å·¦æ—‹å¹³æ»‘ç›¸å…³å‚æ•°ï¼ˆæ›¿æ¢æ³°å‹’å±•å¼€ï¼‰
        use_left_spin: bool = True,  # æ˜¯å¦ä½¿ç”¨å·¦æ—‹å¹³æ»‘æ®‹å·®
        left_spin_alpha: float = 0.5,  # ç¼“å†²å¼ºåº¦ç³»æ•°
        left_spin_tau: float = 0.3,  # å°–ç‚¹é˜ˆå€¼
        left_spin_beta: float = 0.7,  # æƒ¯æ€§ç³»æ•°
        # GPT-only å¼€å…³ï¼ˆæ—è·¯å¼ï¼Œä¿ç•™ Encoder ç»“æ„ä¸åˆ é™¤ï¼‰
        decoder_only: bool = True,   # True=GPT-only forwardï¼›False=seq2seq forward
        use_cross_attn: bool = False,  # DecoderLayer æ˜¯å¦å¯ç”¨ cross-attn
        # FFN æ‰©å±•å€ç‡ï¼ˆPhi-3 é£æ ¼ï¼ŒNone ä¿æŒ d_ff ä¸å˜ï¼‰
        ffn_ratio: Optional[float] = None,
        # MoE å‚æ•°ï¼ˆé»˜è®¤å…¨å…³é—­ï¼Œä¸å½±å“ç°æœ‰è®­ç»ƒï¼‰
        use_moe: bool = False,              # æ˜¯å¦å¯ç”¨ MoE FFN
        moe_num_experts: int = 8,           # ä¸“å®¶æ€»æ•°ï¼ˆå¸¸ç”¨ 8/16ï¼‰
        moe_top_k: int = 1,                 # æ¯ token é€‰å‡ ä¸ªä¸“å®¶ï¼ˆå…ˆ Top-1 æœ€ç¨³ï¼‰
        moe_capacity_factor: float = 1.25,  # å®¹é‡å› å­ï¼ˆé˜²æº¢å‡ºï¼‰
        moe_aux_weight: float = 0.01,       # è¾…åŠ©æŸå¤±æƒé‡ï¼ˆè®­ç»ƒæ—¶ç”¨ï¼‰
        moe_shared_expert: bool = True,     # æ˜¯å¦å¯ç”¨ shared expertï¼ˆalways-on åº•ç›˜ï¼‰
        moe_noisy_gating: bool = True,      # è®­ç»ƒæ—¶è·¯ç”±å™¨åŠ å™ªå£°æ¢ç´¢
        moe_router_z_loss: float = 0.0,     # z-loss é¢å¤–ç¼©æ”¾ï¼ˆ0 = èµ°é»˜è®¤ï¼‰
        **kwargs  # å…¶ä»–å‚æ•°
    ):
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.max_seq_len = max_seq_len
        self.num_encoder_layers = num_encoder_layers
        self.num_decoder_layers = num_decoder_layers
        self.num_heads = num_heads
        self.d_ff = d_ff
        self.dropout = dropout
        self.activation = activation
        self.epsilon = epsilon
        self.alpha = alpha
        self.beta = beta
        self.init_tau = init_tau
        self.sr_ratio = sr_ratio
        self.use_autopoietic = use_autopoietic
        self.base_lr = base_lr
        self.batch_first = batch_first
        self.pad_token_id = pad_token_id
        self.bos_token_id = bos_token_id
        self.eos_token_id = eos_token_id

        # DBC-DACç›¸å…³å‚æ•°
        self.use_dbc_dac = use_dbc_dac
        self.rank_ratio_proj = rank_ratio_proj
        self.rank_ratio_res = rank_ratio_res
        self.dbc_threshold = dbc_threshold
        self.dbc_iterations = dbc_iterations

        # ğŸš€ å·¦æ—‹å¹³æ»‘ç›¸å…³å‚æ•°
        self.use_left_spin = use_left_spin
        self.left_spin_alpha = left_spin_alpha
        self.left_spin_tau = left_spin_tau
        self.left_spin_beta = left_spin_beta

        # GPT-only å¼€å…³
        self.decoder_only = decoder_only
        self.use_cross_attn = use_cross_attn

        # FFN æ‰©å±•å€ç‡
        self.ffn_ratio = ffn_ratio

        # MoE å‚æ•°
        self.use_moe = use_moe
        self.moe_num_experts = moe_num_experts
        self.moe_top_k = moe_top_k
        self.moe_capacity_factor = moe_capacity_factor
        self.moe_aux_weight = moe_aux_weight
        self.moe_shared_expert = moe_shared_expert
        self.moe_noisy_gating = moe_noisy_gating
        self.moe_router_z_loss = moe_router_z_loss

        # æ·»åŠ ä»»ä½•é¢å¤–å‚æ•°
        for key, value in kwargs.items():
            setattr(self, key, value)
    
    def to_dict(self):
        """å°†é…ç½®è½¬æ¢ä¸ºå­—å…¸"""
        return {k: v for k, v in self.__dict__.items()}
    
    @classmethod
    def from_dict(cls, config_dict):
        """ä»å­—å…¸åˆ›å»ºé…ç½®"""
        return cls(**config_dict)
    
    def save_pretrained(self, save_directory):
        """ä¿å­˜é…ç½®åˆ°æŒ‡å®šç›®å½•"""
        import os
        import json
        
        os.makedirs(save_directory, exist_ok=True)
        config_file = os.path.join(save_directory, "config.json")
        
        with open(config_file, "w", encoding="utf-8") as f:
            json.dump(self.to_dict(), f, indent=2)
    
    @classmethod
    def from_pretrained(cls, model_path):
        """ä»é¢„è®­ç»ƒç›®å½•åŠ è½½é…ç½®"""
        import os
        import json
        
        config_file = os.path.join(model_path, "config.json")
        
        if not os.path.exists(config_file):
            raise ValueError(f"åœ¨ {model_path} ä¸­æ‰¾ä¸åˆ°config.json")
        
        with open(config_file, "r", encoding="utf-8") as f:
            config_dict = json.load(f)
        
        return cls.from_dict(config_dict)


class APTModel(nn.Module):
    """
    è‡ªç”Ÿæˆå˜æ¢å™¨(APT)æ¨¡å‹
    é›†æˆäº†è‡ªç”Ÿæˆæ³¨æ„åŠ›æœºåˆ¶å’ŒDBC-DACç¨³å®šæŠ€æœ¯çš„å®Œæ•´Transformeræ¨¡å‹
    æ”¯æŒç¼–ç å™¨-è§£ç å™¨æ¶æ„ï¼Œé€‚ç”¨äºå„ç§åºåˆ—åˆ°åºåˆ—ä»»åŠ¡
    """
    def __init__(self, config: APTModelConfiguration):
        super().__init__()
        self.config = config
        use_rmsnorm = getattr(config, 'use_rmsnorm', True)
        use_swiglu = getattr(config, 'use_swiglu', True)
        # è¯åµŒå…¥
        self.token_embedding = nn.Embedding(
            config.vocab_size, 
            config.d_model, 
            padding_idx=config.pad_token_id
        )
        
        # ä½ç½®ç¼–ç 
        self.positional_encoding = PositionalEncoding(
            config.d_model,
            max_len=config.max_seq_len,
            dropout=config.dropout
        )
        
        # å…¼å®¹ç¼ºå¤±çš„å¯é€‰é…ç½®é¡¹ï¼Œæä¾›åˆç†çš„é»˜è®¤å€¼
        use_autopoietic = getattr(config, "use_autopoietic", True)
        use_dbc_dac = getattr(config, "use_dbc_dac", False)
        rank_ratio_proj = getattr(config, "rank_ratio_proj", 0.1)
        rank_ratio_res = getattr(config, "rank_ratio_res", 0.05)
        dbc_threshold = getattr(config, "dbc_threshold", 1e-6)
        dbc_iterations = getattr(config, "dbc_iterations", 1)

        # ğŸš€ å·¦æ—‹å¹³æ»‘å‚æ•°
        use_left_spin = getattr(config, "use_left_spin", True)
        left_spin_alpha = getattr(config, "left_spin_alpha", 0.5)
        left_spin_tau = getattr(config, "left_spin_tau", 0.3)
        left_spin_beta = getattr(config, "left_spin_beta", 0.7)

        # GPT-only å¼€å…³
        self.decoder_only = bool(getattr(config, "decoder_only", True))
        use_cross_attn = bool(getattr(config, "use_cross_attn", False))

        # MoE å‚æ•°
        ffn_ratio = getattr(config, "ffn_ratio", None)
        use_moe = getattr(config, "use_moe", False)
        moe_num_experts = getattr(config, "moe_num_experts", 8)
        moe_top_k = getattr(config, "moe_top_k", 1)
        moe_capacity_factor = getattr(config, "moe_capacity_factor", 1.25)
        moe_shared_expert = getattr(config, "moe_shared_expert", True)
        moe_noisy_gating = getattr(config, "moe_noisy_gating", True)
        self.moe_aux_weight = float(getattr(config, "moe_aux_weight", 0.01))

        # åˆ›å»ºç¼–ç å™¨å±‚
        encoder_layers = []
        for _ in range(config.num_encoder_layers):
            encoder_layers.append(
                APTEncoderLayer(d_model=config.d_model,
                    nhead=config.num_heads,
                    dim_feedforward=config.d_ff,
                    dropout=config.dropout,
                    activation=config.activation,
                    batch_first=config.batch_first,
                    eps=config.epsilon,
                    alpha=config.alpha,
                    init_tau=config.init_tau,
                    sr_ratio=config.sr_ratio,
                    use_autopoietic=use_autopoietic,
                    use_dbc_dac=use_dbc_dac,
                    rank_ratio_proj=rank_ratio_proj,
                    rank_ratio_res=rank_ratio_res,
                    dbc_threshold=dbc_threshold,
                    dbc_iterations=dbc_iterations,
                    # ğŸš€ å·¦æ—‹å¹³æ»‘å‚æ•°
                    use_left_spin=use_left_spin,
                    left_spin_alpha=left_spin_alpha,
                    left_spin_tau=left_spin_tau,
                    left_spin_beta=left_spin_beta,
                    use_rmsnorm=use_rmsnorm,
                    use_swiglu=use_swiglu,
                    # MoE å‚æ•°
                    ffn_ratio=ffn_ratio,
                    use_moe=use_moe,
                    moe_num_experts=moe_num_experts,
                    moe_top_k=moe_top_k,
                    moe_capacity_factor=moe_capacity_factor,
                    moe_shared_expert=moe_shared_expert,
                    moe_noisy_gating=moe_noisy_gating)
            )

        # åˆ›å»ºè§£ç å™¨å±‚
        decoder_layers = []
        for _ in range(config.num_decoder_layers):
            decoder_layers.append(
                APTDecoderLayer(d_model=config.d_model,
                    nhead=config.num_heads,
                    dim_feedforward=config.d_ff,
                    dropout=config.dropout,
                    activation=config.activation,
                    batch_first=config.batch_first,
                    eps=config.epsilon,
                    alpha=config.alpha,
                    init_tau=config.init_tau,
                    sr_ratio=config.sr_ratio,
                    use_autopoietic=use_autopoietic,
                    use_dbc_dac=use_dbc_dac,
                    rank_ratio_proj=rank_ratio_proj,
                    rank_ratio_res=rank_ratio_res,
                    dbc_threshold=dbc_threshold,
                    dbc_iterations=dbc_iterations,
                    # ğŸš€ å·¦æ—‹å¹³æ»‘å‚æ•°
                    use_left_spin=use_left_spin,
                    left_spin_alpha=left_spin_alpha,
                    left_spin_tau=left_spin_tau,
                    left_spin_beta=left_spin_beta,
                    use_rmsnorm=use_rmsnorm,
                    use_swiglu=use_swiglu,
                    use_cross_attn=use_cross_attn,
                    # MoE å‚æ•°
                    ffn_ratio=ffn_ratio,
                    use_moe=use_moe,
                    moe_num_experts=moe_num_experts,
                    moe_top_k=moe_top_k,
                    moe_capacity_factor=moe_capacity_factor,
                    moe_shared_expert=moe_shared_expert,
                    moe_noisy_gating=moe_noisy_gating)
            )
        
        # ç¼–ç å™¨å’Œè§£ç å™¨
        self.encoder_layers = nn.ModuleList(encoder_layers)
        self.decoder_layers = nn.ModuleList(decoder_layers)
        
        # æœ€ç»ˆå±‚å½’ä¸€åŒ–
        self.encoder_norm = RMSNorm(config.d_model, eps=getattr(config, 'layer_norm_eps', 1e-6)) if use_rmsnorm else nn.LayerNorm(config.d_model)
        self.decoder_norm = RMSNorm(config.d_model, eps=getattr(config, 'layer_norm_eps', 1e-6)) if use_rmsnorm else nn.LayerNorm(config.d_model)
        
        # è¾“å‡ºæŠ•å½±
        self.output_projection = nn.Linear(config.d_model, config.vocab_size, bias=False)
        
        # æƒé‡å…±äº«(å¯é€‰)
        self.output_projection.weight = self.token_embedding.weight
        
        # åˆå§‹åŒ–DBC-DACä¼˜åŒ–å™¨
        self.dbc_dac_optimizer = DBCDAC_Optimizer(
            rank_ratio_proj=rank_ratio_proj,
            rank_ratio_res=rank_ratio_res,
            threshold=dbc_threshold,
            iterations=dbc_iterations,
            apply_to_gradients=True
        ) if use_dbc_dac else None
        
        # æ·»åŠ æ¢¯åº¦ç¨³å®šé’©å­
        self.gradient_hooks = add_gradient_hooks_to_model(self, self.dbc_dac_optimizer) if self.dbc_dac_optimizer else []
            
        # åˆå§‹åŒ–å‚æ•°
        self._reset_parameters()
    
    def _reset_parameters(self):
        """åˆå§‹åŒ–æ¨¡å‹å‚æ•°"""
        # åˆå§‹åŒ–åµŒå…¥
        nn.init.normal_(self.token_embedding.weight, mean=0.0, std=0.02)
        
        # å¯¹äºpadding_idxï¼Œå°†åµŒå…¥å‘é‡ç½®é›¶
        if self.token_embedding.padding_idx is not None:
            with torch.no_grad():
                self.token_embedding.weight[self.token_embedding.padding_idx].fill_(0)

    # ------------------------------------------------------------------
    # MoE aux_loss èšåˆ
    # ------------------------------------------------------------------

    def _gather_aux_loss(self, layers) -> torch.Tensor:
        """ä»æ‰€æœ‰å±‚æ”¶é›† MoE è¾…åŠ©æŸå¤±å¹¶æ±‚å’Œã€‚"""
        total = torch.tensor(0.0, device=next(self.parameters()).device)
        for layer in layers:
            layer_aux = getattr(layer, "_aux_loss", None)
            if layer_aux is not None and isinstance(layer_aux, torch.Tensor):
                total = total + layer_aux
        return total

    # ------------------------------------------------------------------
    # GPT-only è·¯å¾„ï¼ˆPatch 1ï¼‰
    # ------------------------------------------------------------------

    def _build_causal_mask(self, tgt_len: int, device) -> torch.Tensor:
        """æ„å»º causal maskï¼šbool çŸ©é˜µï¼ŒTrue è¡¨ç¤ºã€Œè¢«é®æ‰ï¼ˆä¸å¯è§ï¼‰ã€ã€‚"""
        return torch.triu(
            torch.ones((tgt_len, tgt_len), device=device, dtype=torch.bool),
            diagonal=1
        )

    def forward_lm(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        return_hidden: bool = False,
    ) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
        """
        Decoder-only LM forwardï¼ˆçº¯ GPT è·¯å¾„ï¼‰ã€‚

        å‚æ•°:
            input_ids: (B, S) â€” token id
            attention_mask: (B, S)ï¼Œå¯é€‰ã€‚
                - dtype=bool â†’ True=keepï¼ˆHF é£æ ¼ï¼‰ï¼Œå†…éƒ¨è½¬ä¸º True=mask
                - dtype=int/float â†’ 1=keep, 0=pad
            return_hidden: æ˜¯å¦åŒæ—¶è¿”å›æœ€åä¸€å±‚ hidden states

        è¿”å›:
            logits (B, S, vocab_size)ï¼Œæˆ– (logits, hidden) å½“ return_hidden=True
            æ³¨: å½“ use_moe=True æ—¶ï¼Œå¯é€šè¿‡ model.last_aux_loss è·å–è¾…åŠ©æŸå¤±
        """
        bsz, seqlen = input_ids.shape
        device = input_ids.device

        x = self.token_embedding(input_ids)
        x = self.positional_encoding(x)

        # causal mask (S, S)ï¼ŒTrue=mask
        causal_mask = self._build_causal_mask(seqlen, device=device)

        # key padding mask (B, S)ï¼ŒTrue=mask
        key_padding_mask: Optional[torch.Tensor] = None
        if attention_mask is not None:
            if attention_mask.dtype == torch.bool:
                key_padding_mask = ~attention_mask
            else:
                key_padding_mask = (attention_mask == 0)

        for layer in self.decoder_layers:
            x = layer(
                tgt=x,
                memory=None,
                tgt_mask=causal_mask,
                tgt_key_padding_mask=key_padding_mask,
                memory_mask=None,
                memory_key_padding_mask=None,
            )

        x = self.decoder_norm(x)
        logits = self.output_projection(x)

        # èšåˆ MoE è¾…åŠ©æŸå¤±ï¼ˆæŒ‚åœ¨ self ä¸Šä¾›è®­ç»ƒè„šæœ¬ä½¿ç”¨ï¼‰
        self.last_aux_loss = self._gather_aux_loss(self.decoder_layers) * self.moe_aux_weight

        if return_hidden:
            return logits, x
        return logits

    # ------------------------------------------------------------------
    # ä¿ç•™ Encoder ç›¸å…³æ–¹æ³•ï¼ˆseq2seq è·¯å¾„éšæ—¶å¯åˆ‡å›ï¼‰
    # ------------------------------------------------------------------

    def encode(
        self,
        src_tokens: torch.Tensor,
        src_mask: Optional[torch.Tensor] = None,
        src_key_padding_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        ç¼–ç å™¨å‰å‘ä¼ æ’­
    
        å‚æ•°:
            src_tokens: æºåºåˆ—token ID [batch_size, src_len]
            src_mask: æºåºåˆ—æ©ç 
            src_key_padding_mask: æºåºåˆ—å¡«å……æ©ç 
    
        è¿”å›:
            memory: ç¼–ç å™¨è¾“å‡º
        """
        # è·å–è¯åµŒå…¥
        src = self.token_embedding(src_tokens)
    
        # æ·»åŠ ä½ç½®ç¼–ç 
        src = self.positional_encoding(src)
    
        # é€šè¿‡ç¼–ç å™¨å±‚
        for layer in self.encoder_layers:
            src = layer(
                src=src,
                src_mask=src_mask,
                src_key_padding_mask=src_key_padding_mask
            )
    
        # æœ€ç»ˆå±‚å½’ä¸€åŒ–
        memory = self.encoder_norm(src)
    
        return memory
    
    def decode(
        self,
        tgt_tokens: torch.Tensor,
        memory: torch.Tensor,
        tgt_mask: Optional[torch.Tensor] = None,
        memory_mask: Optional[torch.Tensor] = None,
        tgt_key_padding_mask: Optional[torch.Tensor] = None,
        memory_key_padding_mask: Optional[torch.Tensor] = None
    ) -> torch.Tensor:
        """
        è§£ç å™¨å‰å‘ä¼ æ’­
    
        å‚æ•°:
            tgt_tokens: ç›®æ ‡åºåˆ—token ID [batch_size, tgt_len]
            memory: ç¼–ç å™¨è¾“å‡º
            tgt_mask: ç›®æ ‡åºåˆ—æ©ç 
            memory_mask: è®°å¿†æ©ç 
            tgt_key_padding_mask: ç›®æ ‡åºåˆ—å¡«å……æ©ç 
            memory_key_padding_mask: è®°å¿†å¡«å……æ©ç 
    
        è¿”å›:
            output: è§£ç å™¨è¾“å‡º
        """
        # è·å–è¯åµŒå…¥
        tgt = self.token_embedding(tgt_tokens)
    
        # æ·»åŠ ä½ç½®ç¼–ç 
        tgt = self.positional_encoding(tgt)
    
        # å¦‚æœæ²¡æœ‰æä¾›ç›®æ ‡æ©ç ï¼Œåˆ›å»ºè‡ªå›å½’æ©ç ï¼ˆä¸Šä¸‰è§’çŸ©é˜µï¼‰
        if tgt_mask is None and self.config.batch_first:
            tgt_len = tgt.size(1)
            device = tgt.device
            tgt_mask = torch.triu(
                torch.full((tgt_len, tgt_len), float('-inf'), device=device),
                diagonal=1
            )
    
        # é€šè¿‡è§£ç å™¨å±‚
        for layer in self.decoder_layers:
            tgt = layer(
                tgt=tgt,
                memory=memory,
                tgt_mask=tgt_mask,
                memory_mask=memory_mask,
                tgt_key_padding_mask=tgt_key_padding_mask,
                memory_key_padding_mask=memory_key_padding_mask
            )
    
        # æœ€ç»ˆå±‚å½’ä¸€åŒ–
        output = self.decoder_norm(tgt)
    
        return output
    
    def forward(
        self,
        src_tokens: torch.Tensor = None,
        tgt_tokens: Optional[torch.Tensor] = None,
        src_mask: Optional[torch.Tensor] = None,
        tgt_mask: Optional[torch.Tensor] = None,
        memory_mask: Optional[torch.Tensor] = None,
        src_key_padding_mask: Optional[torch.Tensor] = None,
        tgt_key_padding_mask: Optional[torch.Tensor] = None,
        memory_key_padding_mask: Optional[torch.Tensor] = None,
        return_dict: bool = False,
        # å…¼å®¹AutopoieticAttentioné£æ ¼å‚æ•°
        query: torch.Tensor = None,
        key: torch.Tensor = None,
        value: torch.Tensor = None,
        attn_mask: Optional[torch.Tensor] = None,
        key_padding_mask: Optional[torch.Tensor] = None,
        need_weights: bool = True,
        **kwargs
    ) -> Union[torch.Tensor, Dict[str, torch.Tensor]]:
        """å‰å‘è·¯ç”±å™¨ï¼šdecoder_only=True æ—¶èµ° GPT è·¯å¾„ï¼Œå¦åˆ™èµ° seq2seq è·¯å¾„ã€‚"""
        if getattr(self, "decoder_only", True):
            # GPT-only è·¯å¾„
            input_ids = src_tokens if src_tokens is not None else kwargs.get("input_ids")
            if input_ids is None and query is not None:
                input_ids = query
            attention_mask = src_key_padding_mask if src_key_padding_mask is not None else kwargs.get("attention_mask")
            return_hidden = kwargs.get("return_hidden", False)
            return self.forward_lm(
                input_ids=input_ids,
                attention_mask=attention_mask,
                return_hidden=return_hidden,
            )
        # seq2seq è·¯å¾„ï¼ˆä¿ç•™æ—§é€»è¾‘ï¼‰
        return self.forward_seq2seq(
            src_tokens=src_tokens,
            tgt_tokens=tgt_tokens,
            src_mask=src_mask,
            tgt_mask=tgt_mask,
            memory_mask=memory_mask,
            src_key_padding_mask=src_key_padding_mask,
            tgt_key_padding_mask=tgt_key_padding_mask,
            memory_key_padding_mask=memory_key_padding_mask,
            return_dict=return_dict,
            query=query,
            key=key,
            value=value,
            attn_mask=attn_mask,
            key_padding_mask=key_padding_mask,
            need_weights=need_weights,
            **kwargs,
        )

    def forward_seq2seq(
        self,
        src_tokens: torch.Tensor = None,
        tgt_tokens: Optional[torch.Tensor] = None,
        src_mask: Optional[torch.Tensor] = None,
        tgt_mask: Optional[torch.Tensor] = None,
        memory_mask: Optional[torch.Tensor] = None,
        src_key_padding_mask: Optional[torch.Tensor] = None,
        tgt_key_padding_mask: Optional[torch.Tensor] = None,
        memory_key_padding_mask: Optional[torch.Tensor] = None,
        return_dict: bool = False,
        # å…¼å®¹AutopoieticAttentioné£æ ¼å‚æ•°
        query: torch.Tensor = None,
        key: torch.Tensor = None,
        value: torch.Tensor = None,
        attn_mask: Optional[torch.Tensor] = None,
        key_padding_mask: Optional[torch.Tensor] = None,
        need_weights: bool = True,
        **kwargs
    ) -> Union[torch.Tensor, Dict[str, torch.Tensor]]:
        
        # å°†è‡ªæ³¨æ„åŠ›æ¥å£çš„å‚æ•°æ˜ å°„åˆ°Transformeræ¥å£
        if src_tokens is None and query is not None:
            src_tokens = query
        if tgt_tokens is None:
            if key is not None:
                tgt_tokens = key
            else:
                tgt_tokens = src_tokens
        if src_mask is None and attn_mask is not None:
            src_mask = attn_mask
        if src_key_padding_mask is None and key_padding_mask is not None:
            src_key_padding_mask = key_padding_mask
        
        # **ç¡®ä¿æ©ç æ˜¯boolç±»å‹**ï¼ˆè‹¥åŸæœ¬æ˜¯floatæˆ–longï¼Œåˆ™è½¬ä¸ºboolï¼‰
        if src_mask is not None and src_mask.dtype != torch.bool:
            src_mask = src_mask.to(torch.bool)
        if tgt_mask is not None and tgt_mask.dtype != torch.bool:
            tgt_mask = tgt_mask.to(torch.bool)
        if memory_mask is not None and memory_mask.dtype != torch.bool:
            memory_mask = memory_mask.to(torch.bool)
        if src_key_padding_mask is not None and src_key_padding_mask.dtype != torch.bool:
            src_key_padding_mask = src_key_padding_mask.to(torch.bool)
        if tgt_key_padding_mask is not None and tgt_key_padding_mask.dtype != torch.bool:
            tgt_key_padding_mask = tgt_key_padding_mask.to(torch.bool)
        if memory_key_padding_mask is not None and memory_key_padding_mask.dtype != torch.bool:
            memory_key_padding_mask = memory_key_padding_mask.to(torch.bool)
    
        memory = self.encode(
            src_tokens=src_tokens,
            src_mask=src_mask,
            src_key_padding_mask=src_key_padding_mask
        )
        decoder_output = self.decode(
            tgt_tokens=tgt_tokens,
            memory=memory,
            tgt_mask=tgt_mask,
            memory_mask=memory_mask,
            tgt_key_padding_mask=tgt_key_padding_mask,
            memory_key_padding_mask=memory_key_padding_mask if memory_key_padding_mask is not None else src_key_padding_mask
        )
        
        # ç”Ÿæˆlogits
        logits = self.output_projection(decoder_output)

        # èšåˆ MoE è¾…åŠ©æŸå¤±
        aux_enc = self._gather_aux_loss(self.encoder_layers)
        aux_dec = self._gather_aux_loss(self.decoder_layers)
        self.last_aux_loss = (aux_enc + aux_dec) * self.moe_aux_weight

        # æ ¹æ®return_dictå‚æ•°å†³å®šè¿”å›å½¢å¼
        if return_dict:
            return {
                "logits": logits,
                "encoder_output": memory,
                "decoder_output": decoder_output,
                "aux_loss": self.last_aux_loss,
            }
        else:
            # é»˜è®¤ç›´æ¥è¿”å›logits
            return logits

    def generate(
        self,
        input_ids,
        max_length=50,
        temperature=1.0,
        top_p=0.9,
        top_k=50,
        repetition_penalty=1.0,
        do_sample=True,
        num_beams=1,
        eos_token_id=None,
        pad_token_id=None,
    ):
        """ç”Ÿæˆè·¯ç”±å™¨ï¼šdecoder_only=True èµ° LM è·¯å¾„ï¼Œå¦åˆ™èµ° seq2seq è·¯å¾„ã€‚"""
        if getattr(self, "decoder_only", True):
            return self.generate_lm(
                input_ids=input_ids,
                max_new_tokens=max_length,
                temperature=temperature,
                top_k=top_k,
                top_p=top_p,
                repetition_penalty=repetition_penalty,
                do_sample=do_sample,
                eos_token_id=eos_token_id,
                pad_token_id=pad_token_id,
            )
        return self.generate_seq2seq(
            input_ids=input_ids,
            max_length=max_length,
            temperature=temperature,
            top_p=top_p,
            top_k=top_k,
            repetition_penalty=repetition_penalty,
            do_sample=do_sample,
            num_beams=num_beams,
            eos_token_id=eos_token_id,
            pad_token_id=pad_token_id,
        )

    @torch.no_grad()
    def generate_lm(
        self,
        input_ids: torch.Tensor,
        max_new_tokens: int = 50,
        temperature: float = 1.0,
        top_k: int = 50,
        top_p: float = 0.9,
        repetition_penalty: float = 1.0,
        do_sample: bool = True,
        eos_token_id: Optional[int] = None,
        pad_token_id: Optional[int] = None,
    ) -> torch.Tensor:
        """
        Decoder-only (GPT) è‡ªå›å½’ç”Ÿæˆã€‚

        å‚æ•°:
            input_ids: (B, S) â€” prompt token ids
            max_new_tokens: æœ€å¤šé¢å¤–ç”Ÿæˆçš„ token æ•°é‡
            å…¶ä½™å‚æ•°åŒ generate()

        è¿”å›:
            generated_ids: (B, max_new_tokens) â€” ä»…æ–°ç”Ÿæˆçš„éƒ¨åˆ†
        """
        if input_ids is None:
            raise ValueError("input_ids ä¸èƒ½ä¸ºç©º")

        device = input_ids.device
        batch_size = input_ids.size(0)

        if eos_token_id is None:
            eos_token_id = getattr(self.config, "eos_token_id", 3)
        if pad_token_id is None:
            pad_token_id = getattr(self.config, "pad_token_id", 0)
        unk_token_id = getattr(self.config, "unk_token_id", None)

        # å½“å‰åºåˆ—ä» prompt å¼€å§‹ï¼ŒæŒç»­ append æ–° token
        cur_ids = input_ids.clone()
        generated_ids = torch.empty((batch_size, 0), device=device, dtype=torch.long)

        was_training = self.training
        self.eval()

        try:
            for _ in range(max_new_tokens):
                # GPT forwardï¼šåªæœ‰ input_idsï¼Œæ—  memory
                logits = self.forward_lm(cur_ids)          # (B, S, V)
                next_token_logits = logits[:, -1, :]       # (B, V)

                # é‡å¤æƒ©ç½š
                if repetition_penalty != 1.0:
                    for i in range(batch_size):
                        history = set(cur_ids[i].tolist())
                        for tid in history:
                            if next_token_logits[i, tid] > 0:
                                next_token_logits[i, tid] /= repetition_penalty
                            else:
                                next_token_logits[i, tid] *= repetition_penalty

                # æ¸©åº¦
                next_token_logits = next_token_logits / max(float(temperature), 1e-5)

                # å±è”½ç‰¹æ®Šç¬¦å·
                if pad_token_id is not None and 0 <= pad_token_id < next_token_logits.size(-1):
                    next_token_logits[:, pad_token_id] = -float("inf")
                if unk_token_id is not None and 0 <= unk_token_id < next_token_logits.size(-1):
                    next_token_logits[:, unk_token_id] = -float("inf")

                # é‡‡æ · / è´ªå¿ƒ
                if do_sample:
                    if top_k > 0:
                        v, _ = torch.topk(next_token_logits, min(top_k, next_token_logits.size(-1)))
                        next_token_logits[next_token_logits < v[:, [-1]]] = -float("inf")
                    if 0 < top_p < 1.0:
                        sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
                        sorted_probs = F.softmax(sorted_logits, dim=-1)
                        cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
                        sorted_indices_to_remove = cumulative_probs > top_p
                        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                        sorted_indices_to_remove[..., 0] = 0
                        for i in range(batch_size):
                            indices_to_remove = sorted_indices[i][sorted_indices_to_remove[i]]
                            next_token_logits[i, indices_to_remove] = -float("inf")
                    probs = F.softmax(next_token_logits, dim=-1)
                    next_token = torch.multinomial(probs, num_samples=1)
                else:
                    next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)

                generated_ids = torch.cat([generated_ids, next_token], dim=1)
                cur_ids = torch.cat([cur_ids, next_token], dim=1)

                if (next_token == eos_token_id).all():
                    break
        finally:
            if was_training:
                self.train()

        return generated_ids

    def generate_seq2seq(
        self,
        input_ids,
        max_length=50,
        temperature=1.0,
        top_p=0.9,
        top_k=50,
        repetition_penalty=1.0,
        do_sample=True,
        num_beams=1,
        eos_token_id=None,
        pad_token_id=None,
    ):
        """
        â­ ä¿®å¤åçš„æ–‡æœ¬ç”Ÿæˆæ–¹æ³• (Encoder-Decoder é€»è¾‘ä¿®æ­£ç‰ˆ)

        Args:
            input_ids: è¾“å…¥token IDs [batch_size, seq_len]
            max_length: æœ€å¤§ç”Ÿæˆé•¿åº¦
            temperature: é‡‡æ ·æ¸©åº¦
            top_p: nucleusé‡‡æ ·å‚æ•°
            top_k: top-ké‡‡æ ·å‚æ•°
            repetition_penalty: é‡å¤æƒ©ç½š
            do_sample: æ˜¯å¦é‡‡æ ·(Falseåˆ™è´ªå¿ƒè§£ç )
            num_beams: beam searchçš„beamæ•°é‡
            eos_token_id: ç»“æŸæ ‡è®°ID
            pad_token_id: å¡«å……æ ‡è®°ID

        Returns:
            ç”Ÿæˆçš„token IDs [batch_size, generated_length]
        """
        del num_beams  # å½“å‰å®ç°ä¸æ”¯æŒbeam searchï¼Œé¿å…æœªä½¿ç”¨å‚æ•°è­¦å‘Š

        if input_ids is None:
            raise ValueError("input_ids ä¸èƒ½ä¸ºç©º")

        device = input_ids.device
        batch_size = input_ids.size(0)

        # 1. å‡†å¤‡ç‰¹æ®Š Token
        bos_token_id = getattr(self.config, "bos_token_id", 2)
        if eos_token_id is None:
            eos_token_id = getattr(self.config, "eos_token_id", 3)
        if pad_token_id is None:
            pad_token_id = getattr(self.config, "pad_token_id", 0)
        unk_token_id = getattr(self.config, "unk_token_id", None)

        # ------------------------------------------------------------------
        # ğŸš€ æ ¸å¿ƒé€»è¾‘ä¿®å¤ï¼šä» GPT æ¨¡å¼åˆ‡æ¢å› Encoder-Decoder æ¨¡å¼
        # ------------------------------------------------------------------
        
        # 2. ç¼–ç é˜¶æ®µ (Encoder)
        # ä¸€æ¬¡æ€§è¯»æ‡‚ Promptï¼Œè·å–è®°å¿†
        memory = self.encode(
            src_tokens=input_ids,
            src_key_padding_mask=(input_ids == pad_token_id)
        )

        # 3. è§£ç å‡†å¤‡ (Decoder)
        # ç»™è§£ç å™¨ä¸€å¼ ç™½çº¸ï¼Œåªå†™ä¸€ä¸ª [BOS] å¼€å¤´
        # ç»å¯¹ä¸èƒ½æŠŠ input_ids å–‚ç»™è§£ç å™¨ï¼Œå¦åˆ™å®ƒçœ‹åˆ° EOS å°±ä¼šåœæ­¢ï¼
        decoder_input = torch.full((batch_size, 1), bos_token_id, device=device, dtype=torch.long)
        
        # ç”¨äºä¿å­˜ç”Ÿæˆç»“æœ (ä¸åŒ…å« BOS)
        generated_ids = torch.empty((batch_size, 0), device=device, dtype=torch.long)

        was_training = self.training
        self.eval()

        try:
            with torch.no_grad():
                # å¾ªç¯ç”Ÿæˆ Response
                for step in range(max_length):
                    # å‰å‘è§£ç ï¼šä¼ å…¥ memory å’Œ å½“å‰å·²ç”Ÿæˆçš„ decoder_input
                    # æ³¨æ„ï¼šæˆ‘ä»¬ä½¿ç”¨ decode() æ–¹æ³•è€Œä¸æ˜¯ forward()
                    decoder_output = self.decode(
                        tgt_tokens=decoder_input,
                        memory=memory,
                        tgt_mask=None, # å†…éƒ¨ä¼šè‡ªåŠ¨ç”Ÿæˆå› æœæ©ç 
                        memory_mask=None,
                        tgt_key_padding_mask=None,
                        memory_key_padding_mask=(input_ids == pad_token_id)
                    )
                    
                    # æ˜ å°„åˆ°è¯è¡¨
                    logits = self.output_projection(decoder_output)
                    next_token_logits = logits[:, -1, :] # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥

                    # --- é‡å¤æƒ©ç½šé€»è¾‘ ---
                    if repetition_penalty != 1.0:
                        for i in range(batch_size):
                            # æ³¨æ„ï¼šæˆ‘ä»¬æ£€æŸ¥çš„æ˜¯å·²ç»ç”Ÿæˆçš„ generated_ids (ä¸å« prompt)
                            history = set(generated_ids[i].tolist())
                            if not history:
                                continue
                            
                            # å°† tensor è½¬ä¸º list ä»¥ä¾¿ç´¢å¼•ï¼Œæˆ–è€…ç›´æ¥ä½¿ç”¨ scatter/gather ä¼˜åŒ–
                            # è¿™é‡Œä¸ºäº†å…¼å®¹æ€§ä¿æŒå¾ªç¯å†™æ³•ï¼Œä½†åŠ å…¥äº† logits æ­£è´Ÿå€¼çš„æ­£ç¡®å¤„ç†
                            for token_id in history:
                                if next_token_logits[i, token_id] > 0:
                                    next_token_logits[i, token_id] /= repetition_penalty
                                else:
                                    next_token_logits[i, token_id] *= repetition_penalty

                    # æ¸©åº¦è°ƒèŠ‚
                    temperature = max(float(temperature), 1e-5)
                    next_token_logits = next_token_logits / temperature

                    # å±è”½ç‰¹æ®Šç¬¦å· (PAD, UNK)
                    if pad_token_id is not None and 0 <= pad_token_id < next_token_logits.size(-1):
                        next_token_logits[:, pad_token_id] = -float('inf')
                    if unk_token_id is not None and 0 <= unk_token_id < next_token_logits.size(-1):
                        next_token_logits[:, unk_token_id] = -float('inf')

                    # é‡‡æ ·
                    if do_sample:
                        # Top-K
                        if top_k > 0:
                            v, _ = torch.topk(next_token_logits, min(top_k, next_token_logits.size(-1)))
                            next_token_logits[next_token_logits < v[:, [-1]]] = -float('inf')
                        
                        # Top-P
                        if 0 < top_p < 1.0:
                            sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
                            sorted_probs = F.softmax(sorted_logits, dim=-1)
                            cumulative_probs = torch.cumsum(sorted_probs, dim=-1)
                            sorted_indices_to_remove = cumulative_probs > top_p
                            sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                            sorted_indices_to_remove[..., 0] = 0
                            for i in range(batch_size):
                                indices_to_remove = sorted_indices[i][sorted_indices_to_remove[i]]
                                next_token_logits[i, indices_to_remove] = -float('inf')

                        probs = F.softmax(next_token_logits, dim=-1)
                        next_token = torch.multinomial(probs, num_samples=1)
                    else:
                        # è´ªå©ªæœç´¢
                        next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)

                    # æ‹¼æ¥åˆ°ç»“æœä¸­
                    generated_ids = torch.cat([generated_ids, next_token], dim=1)
                    decoder_input = torch.cat([decoder_input, next_token], dim=1)

                    # æ£€æŸ¥ EOS
                    if (next_token == eos_token_id).all():
                        break
                        
        finally:
            if was_training:
                self.train()

        # è¿”å›ç”Ÿæˆç»“æœ (åªè¿”å›ç”Ÿæˆçš„å›å¤éƒ¨åˆ†)
        return generated_ids


class APTLargeModel(APTModel):
    """APTLargeModelæ˜¯APTModelçš„åˆ«åï¼Œç”¨äºå…¼å®¹æ€§ç›®çš„"""
    def __init__(self, config):
        super().__init__(config)
        # åˆå§‹åŒ–Taylorå‚æ•°ï¼Œç¡®ä¿å­˜åœ¨è¿™äº›å±æ€§
        self.register_parameter(
            'taylor_epsilon', 
            nn.Parameter(torch.tensor(config.epsilon, dtype=torch.float))
        )
        self.register_parameter(
            'taylor_alpha', 
            nn.Parameter(torch.tensor(config.alpha, dtype=torch.float))
        )
    
    def update_dynamic_taylor_parameters(self, learning_rate):
        """æ›´æ–°åŠ¨æ€Taylorå±•å¼€å‚æ•°"""
        try:
            # å¦‚æœä½¿ç”¨LRè°ƒåº¦å™¨ï¼Œéœ€è¦æ ¹æ®å½“å‰å­¦ä¹ ç‡è°ƒæ•´å‚æ•°
            lr_factor = float(learning_rate) / float(self.config.base_lr)
            
            # å®‰å…¨åœ°æ›´æ–°å‚æ•°
            if hasattr(self, 'taylor_epsilon'):
                self.taylor_epsilon.data = torch.clamp(
                    self.taylor_epsilon * (1.0 + self.config.alpha * lr_factor),
                    min=0.1, max=10.0
                )
            
            if hasattr(self, 'taylor_alpha'):
                self.taylor_alpha.data = torch.clamp(
                    self.taylor_alpha * (1.0 - self.config.beta * lr_factor),
                    min=0.001, max=0.1
                )
            
            # æ›´æ–°æ‰€æœ‰æ³¨æ„åŠ›å±‚çš„å‚æ•°
            for name, module in self.named_modules():
                if hasattr(module, 'tau') and isinstance(module.tau, torch.nn.Parameter):
                    # è°ƒæ•´æ¸©åº¦å‚æ•°
                    module.tau.data = torch.clamp(
                        module.tau * (1.0 - 0.01 * lr_factor),
                        min=0.5, max=2.0
                    )
        except Exception as e:
            # å¦‚æœå‡ºç°ä»»ä½•å¼‚å¸¸ï¼Œè®°å½•ä½†ä¸ä¸­æ–­è®­ç»ƒ
            print(f"è­¦å‘Š: åŠ¨æ€å‚æ•°æ›´æ–°å¤±è´¥: {e}")
            # ç®€å•åœ°é€šè¿‡ï¼Œç¡®ä¿æ–¹æ³•ä¸ä¼šæŠ›å‡ºå¼‚å¸¸
            pass