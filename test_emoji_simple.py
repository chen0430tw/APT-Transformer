#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ç®€åŒ–ç‰ˆ emoji tokenizer æµ‹è¯•
ç›´æ¥æ¨¡æ‹Ÿ ChineseTokenizer çš„è¡Œä¸º
"""

def test_emoji_tokenization():
    """æµ‹è¯• emoji ç¼–ç è¡Œä¸º"""

    print("="*70)
    print("æ¨¡æ‹Ÿ ChineseTokenizer å¯¹ emoji çš„å¤„ç†")
    print("="*70)

    # æ¨¡æ‹Ÿ ChineseTokenizer çš„è¯æ±‡è¡¨æ„å»º
    encoder = {}
    decoder = {}

    # 1. æ·»åŠ ç‰¹æ®Š token
    special_tokens = ["<|pad|>", "<|endoftext|>"]
    for i, token in enumerate(special_tokens):
        encoder[token] = i
        decoder[i] = token

    # 2. æ·»åŠ  ASCII å­—ç¬¦
    for i in range(32, 127):
        char = chr(i)
        if char not in encoder:
            encoder[char] = len(encoder)

    # 3. æ·»åŠ å¸¸ç”¨æ±‰å­—
    for i in range(0x4e00, 0x5000):  # åªæ·»åŠ ä¸€å°éƒ¨åˆ†æ±‰å­—ä½œä¸ºç¤ºä¾‹
        char = chr(i)
        if char not in encoder:
            encoder[char] = len(encoder)

    decoder = {v: k for k, v in encoder.items()}

    print(f"\nè¯æ±‡è¡¨å¤§å°: {len(encoder)}")
    print(f"ç‰¹æ®Š token: {special_tokens}")

    # æµ‹è¯• emoji ç¼–ç ï¼ˆæ¨¡æ‹Ÿ ChineseTokenizer.encode çš„è¡Œä¸ºï¼‰
    def encode(text):
        """å­—ç¬¦çº§ç¼–ç ï¼Œè·³è¿‡æœªçŸ¥å­—ç¬¦"""
        ids = []
        for char in text:
            if char in encoder:
                ids.append(encoder[char])
            else:
                # æœªçŸ¥å­—ç¬¦è¢«è·³è¿‡ï¼ˆline 161 in chinese_tokenizer.pyï¼‰
                pass
        return ids

    def decode(ids):
        """è§£ç """
        chars = []
        for id in ids:
            if id in decoder:
                chars.append(decoder[id])
        return ''.join(chars)

    # æµ‹è¯•ç”¨ä¾‹
    test_cases = [
        "ğŸŒ§ï¸",
        "ä½ å¥½ğŸŒ§ï¸ä¸–ç•Œ",
        "HelloğŸŒ§ï¸World",
        "ğŸ˜€",
        "ğŸ‰ğŸŠğŸˆ",
        "æ™®é€šæ–‡æœ¬",
        "Hello",
    ]

    print("\næµ‹è¯•ç»“æœ:")
    print("-"*70)

    for text in test_cases:
        ids = encode(text)
        decoded = decode(ids)

        chars_in_vocab = [c for c in text if c in encoder]
        chars_not_in_vocab = [c for c in text if c not in encoder]

        print(f"\nåŸæ–‡æœ¬: {text!r}")
        print(f"  ç¼–ç  ID: {ids}")
        print(f"  ID é•¿åº¦: {len(ids)} (åŸæ–‡æœ¬é•¿åº¦: {len(text)})")
        print(f"  è§£ç ç»“æœ: {decoded!r}")
        print(f"  åœ¨è¯æ±‡è¡¨ä¸­: {[c for c in text if c in encoder]}")
        print(f"  ä¸åœ¨è¯æ±‡è¡¨: {[c for c in text if c not in encoder]}")

        if text != decoded:
            print(f"  âš ï¸  ç¼–ç /è§£ç æœ‰æŸå¤±ï¼ä¸¢å¤±äº† {len(text) - len(decoded)} ä¸ªå­—ç¬¦")
        else:
            print(f"  âœ“  ç¼–ç /è§£ç æ— æŸ")

    # æ£€æŸ¥æ˜¯å¦æœ‰ UNK token
    print("\n" + "="*70)
    print("æ£€æŸ¥è¯æ±‡è¡¨ä¸­çš„ UNK token:")
    print("-"*70)

    unk_tokens = ["<unk>", "[UNK]", "<UNK>", "ï¿½"]
    found_unk = False
    for token in unk_tokens:
        if token in encoder:
            print(f"  æ‰¾åˆ°: {token} (ID = {encoder[token]})")
            found_unk = True

    if not found_unk:
        print("  âŒ è¯æ±‡è¡¨ä¸­æ²¡æœ‰ä»»ä½• UNK token")

    # æœ€ç»ˆç»“è®º
    print("\n" + "="*70)
    print("ç»“è®º:")
    print("-"*70)

    emoji_text = "ğŸŒ§ï¸"
    emoji_ids = encode(emoji_text)

    if len(emoji_ids) == 0:
        print(f"  â€¢ emoji '{emoji_text}' è¢«ç¼–ç ä¸ºç©ºåˆ—è¡¨ []")
        print(f"  â€¢ æœªçŸ¥å­—ç¬¦è¢«ç›´æ¥è·³è¿‡ï¼ˆchinese_tokenizer.py line 161ï¼‰")
        print(f"  â€¢ âŒ æ²¡æœ‰ä½¿ç”¨ [UNK] token")
        print(f"  â€¢ âŒ ç¼–ç /è§£ç ä¼šä¸¢å¤± emoji")
    else:
        print(f"  â€¢ emoji '{emoji_text}' è¢«ç¼–ç ä¸º: {emoji_ids}")
        decoded_emoji = decode(emoji_ids)
        print(f"  â€¢ è§£ç ç»“æœ: {decoded_emoji!r}")
        if decoded_emoji == emoji_text:
            print(f"  â€¢ âœ“ emoji å¯ä»¥æ­£ç¡®ç¼–ç /è§£ç ")
        else:
            print(f"  â€¢ âš ï¸  ç¼–ç /è§£ç æœ‰æŸå¤±")

    print("="*70)

if __name__ == "__main__":
    test_emoji_tokenization()
