# API/WebUI/åˆ†å¸ƒå¼è®­ç»ƒ - å‡†å¤‡çŠ¶æ€æŠ¥å‘Š (ä¼ç¬”æ£€æŸ¥)

**æ£€æŸ¥åˆ†æ”¯**: `claude/review-memo-updates-01VZwZoRpMTGwNff9jviR9k7`
**æ£€æŸ¥æ—¥æœŸ**: 2025-11-30
**çŠ¶æ€æ ‡è®°**: ğŸ”® = ä¼ç¬”å·²åŸ‹è®¾

---

## æ‰§è¡Œæ‘˜è¦

ç»è¿‡ä»”ç»†æ£€æŸ¥ï¼Œå‘ç°åœ¨ `claude/review-memo-updates-01VZwZoRpMTGwNff9jviR9k7` åˆ†æ”¯ä¸­**å·²ç»åŸ‹è®¾äº†å¤§é‡ä¼ç¬”**ï¼Œä¸ºæœªæ¥çš„APIæœåŠ¡ã€WebUIç•Œé¢å’Œåˆ†å¸ƒå¼è®­ç»ƒåšå¥½äº†åŸºç¡€å‡†å¤‡ã€‚

### æ•´ä½“å‡†å¤‡åº¦

| åŠŸèƒ½æ¨¡å— | åŸºç¡€è®¾æ–½ | æ•°æ®æ¥å£ | æµ‹è¯•æ¡© | å®Œæ•´å®ç° | å‡†å¤‡åº¦è¯„ä¼° |
|---------|---------|---------|--------|---------|-----------|
| **WebUI** | âœ… å·²å‡†å¤‡ | âœ… å·²å‡†å¤‡ | âœ… å·²å‡†å¤‡ | â³ å¾…å®ç° | ğŸŸ¢ 70% (ä¼ç¬”å……åˆ†) |
| **REST API** | âœ… å·²å‡†å¤‡ | âœ… å·²å‡†å¤‡ | âœ… å·²å‡†å¤‡ | â³ å¾…å®ç° | ğŸŸ¢ 65% (ä¼ç¬”å……åˆ†) |
| **åˆ†å¸ƒå¼è®­ç»ƒ** | âœ… å·²å‡†å¤‡ | âœ… å·²å‡†å¤‡ | âœ… å·²å‡†å¤‡ | â³ å¾…å®ç° | ğŸŸ¢ 60% (ä¼ç¬”å……åˆ†) |

**ç»“è®º**: æ‰€æœ‰ä¸‰ä¸ªæ¨¡å—çš„**åŸºç¡€è®¾æ–½ã€æ•°æ®æ¥å£å’Œæµ‹è¯•æ¡†æ¶å·²ç»å°±ç»ª**ï¼Œåªéœ€è¦è¡¥å……å…·ä½“çš„FastAPI/Gradio/DDPå®ç°ä»£ç ã€‚

---

## 1. WebUI ä¼ç¬”è¯¦æƒ…

### 1.1 æ•°æ®å¯¼å‡ºæ¥å£ (gradient_monitor.py)

**æ–‡ä»¶**: `apt_model/training/gradient_monitor.py`
**ä½ç½®**: Lines 260-302
**æ ‡è®°**: ğŸ”® WebUIä¼ç¬”

```python
def export_for_webui(self) -> Dict[str, Any]:
    """
    å¯¼å‡ºæ•°æ®ä¾›WebUI/APIä½¿ç”¨

    è¿”å›æ ¼å¼é€‚åˆJSONåºåˆ—åŒ–ï¼Œå¯ä»¥é€šè¿‡APIæä¾›ç»™å‰ç«¯

    WebUIå¯ä»¥é€šè¿‡APIè·å–ï¼š
    GET /api/training/gradients
    """
    # 1. æ¢¯åº¦æ—¶é—´çº¿æ•°æ®
    timeline = []
    for step_idx, norms in enumerate(self.gradient_norms):
        step_data = {
            'step': step_idx,
            'timestamp': self.step_timestamps.get(step_idx, 0),
            'layers': {}
        }
        for layer_name, norm_value in norms.items():
            step_data['layers'][layer_name] = {
                'norm': float(norm_value),
                'is_anomaly': layer_name in self.anomalies.get(step_idx, {})
            }
        timeline.append(step_data)

    # 2. å±‚çº§ç»Ÿè®¡æ‘˜è¦
    layer_stats = {}
    for layer_name in self.layer_names:
        all_norms = [
            norms[layer_name]
            for norms in self.gradient_norms
            if layer_name in norms
        ]

        if all_norms:
            layer_stats[layer_name] = {
                'mean': float(np.mean(all_norms)),
                'std': float(np.std(all_norms)),
                'min': float(np.min(all_norms)),
                'max': float(np.max(all_norms)),
                'total_steps': len(all_norms),
                'anomaly_count': sum(
                    1 for step_anomalies in self.anomalies.values()
                    if layer_name in step_anomalies
                )
            }

    return {
        'gradient_timeline': timeline,
        'layer_statistics': layer_stats,
        'anomaly_summary': self.anomaly_counts,
        'total_steps': len(self.gradient_norms),
        'timestamp': time.time()
    }
```

**WebUIå¯è§†åŒ–å»ºè®®**:
- æ¢¯åº¦æ—¶é—´çº¿: æŠ˜çº¿å›¾ (x=step, y=norm, color=layer)
- å¼‚å¸¸é«˜äº®: çº¢ç‚¹æ ‡è®°å¼‚å¸¸æ­¥éª¤
- å±‚çº§ç»Ÿè®¡: è¡¨æ ¼ + æŸ±çŠ¶å›¾
- å®æ—¶æ›´æ–°: WebSocketæ¨é€æœ€æ–°æ•°æ®

### 1.2 WebUIæ•°æ®æ¥å£æµ‹è¯• (test_trainer_complete.py)

**æ–‡ä»¶**: `tests/test_trainer_complete.py`
**ä½ç½®**: Lines 599-682
**æ ‡è®°**: ğŸ”® WebUIä¼ç¬”

```python
class TestWebUIDataInterface:
    """WebUIæ•°æ®æ¥å£æµ‹è¯•ï¼ˆä¸ºæœªæ¥çš„Webç•Œé¢åŸ‹ä¼ç¬”ï¼‰"""

    def test_export_training_metrics_for_webui(self, temp_dir, sample_texts):
        """æµ‹è¯•å¯¼å‡ºè®­ç»ƒæŒ‡æ ‡ä¸ºJSONï¼ˆä¾›å‰ç«¯å±•ç¤ºï¼‰"""
        # ğŸ”® WebUIä¼ç¬”ï¼šå¯¼å‡ºè®­ç»ƒæŒ‡æ ‡ä¸ºJSONï¼ˆä¾›å‰ç«¯å±•ç¤ºï¼‰

        config = APTConfig(
            d_model=128,
            num_layers=2,
            num_attention_heads=4,
            vocab_size=len(tokenizer),
        )

        model = APTLargeModel(config)
        trainer = APTTrainer(...)

        # è®­ç»ƒå‡ æ­¥
        for epoch in range(2):
            for batch in train_loader:
                loss = trainer.train_step(batch['input_ids'], batch['labels'])

        # ğŸ”® å¯¼å‡ºWebUIéœ€è¦çš„JSONæ•°æ®
        webui_data = {
            'training_history': {
                'steps': list(range(len(trainer.train_losses))),
                'train_loss': [float(l) for l in trainer.train_losses],
                'learning_rate': [float(lr) for lr in trainer.lr_history],
            },
            'model_config': {
                'd_model': config.d_model,
                'num_layers': config.num_layers,
                'num_heads': config.num_attention_heads,
            },
            'checkpoint_info': {
                'best_loss': float(trainer.best_val_loss),
                'current_epoch': trainer.epoch,
                'global_step': trainer.global_step,
            }
        }

        # éªŒè¯å¯ä»¥JSONåºåˆ—åŒ–
        import json
        json_str = json.dumps(webui_data, indent=2)
        assert len(json_str) > 0

    def test_export_checkpoint_list_for_webui(self, temp_dir, sample_texts):
        """æµ‹è¯•å¯¼å‡ºcheckpointåˆ—è¡¨ï¼ˆä¾›WebUIç®¡ç†é¢æ¿ä½¿ç”¨ï¼‰"""
        # ğŸ”® WebUIä¼ç¬”ï¼šcheckpointåˆ—è¡¨API

        # è®­ç»ƒå¹¶ä¿å­˜å¤šä¸ªcheckpoints
        trainer.train(num_epochs=3)

        # ğŸ”® æ¨¡æ‹ŸWebUIçš„checkpointç®¡ç†æ¥å£
        checkpoint_list = []
        for ckpt_file in sorted(save_dir.glob('*.pt')):
            ckpt = torch.load(ckpt_file)
            checkpoint_list.append({
                'filename': ckpt_file.name,
                'epoch': ckpt['epoch'],
                'global_step': ckpt['global_step'],
                'val_loss': ckpt.get('best_val_loss', None),
                'file_size_mb': ckpt_file.stat().st_size / (1024 * 1024),
                'created_at': ckpt_file.stat().st_mtime,
            })

        # WebUIå¯ä»¥é€šè¿‡ GET /api/checkpoints è·å–è¿™ä¸ªåˆ—è¡¨
        assert len(checkpoint_list) >= 3
```

**WebUIåŠŸèƒ½å»ºè®®**:
1. è®­ç»ƒç›‘æ§é¡µé¢: å®æ—¶æ˜¾ç¤ºlossã€learning rateæ›²çº¿
2. Checkpointç®¡ç†: åˆ—è¡¨å±•ç¤ºã€ä¸‹è½½ã€åˆ é™¤ã€åŠ è½½
3. æ¨¡å‹é…ç½®å±•ç¤º: æ˜¾ç¤ºè¶…å‚æ•°
4. æ¢¯åº¦ç›‘æ§: é›†æˆ`export_for_webui()`æ•°æ®

---

## 2. REST API ä¼ç¬”è¯¦æƒ…

### 2.1 æ¨ç†æ¥å£åŸå‹ (test_trainer_complete.py)

**æ–‡ä»¶**: `tests/test_trainer_complete.py`
**ä½ç½®**: Lines 421-458
**æ ‡è®°**: ğŸ”® APIä¼ç¬”

```python
def test_inference_interface(self, temp_dir, sample_texts):
    """æµ‹è¯•æ¨ç†æ¥å£ï¼ˆAPI endpointéœ€è¦ï¼‰"""
    # ğŸ”® APIä¼ç¬”ï¼šæ¨¡æ‹ŸAPIè¯·æ±‚çš„æ¨ç†

    def api_inference(model, tokenizer, text, max_length=50):
        """
        è¿™æ˜¯æœªæ¥APIæœåŠ¡çš„æ¨ç†æ¥å£åŸå‹

        POST /api/generate
        {
            "text": "input text",
            "max_length": 50
        }

        Response:
        {
            "generated_text": "...",
            "input_text": "...",
            "generation_time_ms": 123.45
        }
        """
        import time
        start_time = time.time()

        model.eval()
        with torch.no_grad():
            inputs = tokenizer(
                text,
                return_tensors='pt',
                padding=True,
                truncation=True,
                max_length=512
            )

            generated_ids = model.generate(
                input_ids=inputs['input_ids'],
                max_length=max_length,
                num_beams=1,
                do_sample=False
            )

            generated_text = tokenizer.decode(
                generated_ids[0],
                skip_special_tokens=True
            )

        generation_time = (time.time() - start_time) * 1000  # ms

        return {
            'generated_text': generated_text,
            'input_text': text,
            'generation_time_ms': generation_time
        }

    # æµ‹è¯•æ¨ç†
    result = api_inference(model, tokenizer, "ä»Šå¤©å¤©æ°”", max_length=30)

    assert 'generated_text' in result
    assert 'generation_time_ms' in result
    assert result['generation_time_ms'] > 0
```

### 2.2 æ‰¹é‡æ¨ç†æ¥å£ (test_trainer_complete.py)

**æ–‡ä»¶**: `tests/test_trainer_complete.py`
**ä½ç½®**: Lines 460-492
**æ ‡è®°**: ğŸ”® APIä¼ç¬”

```python
def test_batch_inference_for_api(self, temp_dir, sample_texts):
    """æµ‹è¯•æ‰¹é‡æ¨ç†ï¼ˆAPIæ‰¹å¤„ç†éœ€è¦ï¼‰"""
    # ğŸ”® APIä¼ç¬”ï¼šæ‰¹é‡æ¨ç†æ¥å£

    def api_batch_inference(model, tokenizer, texts, max_length=50):
        """
        æ‰¹é‡æ¨ç†æ¥å£

        POST /api/batch_generate
        {
            "texts": ["text1", "text2", ...],
            "max_length": 50
        }

        Response:
        {
            "results": [
                {"input": "text1", "output": "..."},
                {"input": "text2", "output": "..."}
            ],
            "total_time_ms": 456.78
        }
        """
        import time
        start_time = time.time()

        model.eval()
        results = []

        with torch.no_grad():
            inputs = tokenizer(
                texts,
                return_tensors='pt',
                padding=True,
                truncation=True,
                max_length=512
            )

            generated_ids = model.generate(
                input_ids=inputs['input_ids'],
                max_length=max_length,
                num_beams=1,
                do_sample=False
            )

            for i, gen_ids in enumerate(generated_ids):
                generated_text = tokenizer.decode(gen_ids, skip_special_tokens=True)
                results.append({
                    'input': texts[i],
                    'output': generated_text
                })

        total_time = (time.time() - start_time) * 1000

        return {
            'results': results,
            'total_time_ms': total_time,
            'batch_size': len(texts)
        }

    # æµ‹è¯•æ‰¹é‡æ¨ç†
    test_texts = ["ä½ å¥½", "ä»Šå¤©å¤©æ°”", "äººå·¥æ™ºèƒ½"]
    batch_result = api_batch_inference(model, tokenizer, test_texts)

    assert len(batch_result['results']) == 3
    assert batch_result['batch_size'] == 3
```

### 2.3 æ¨¡å‹åºåˆ—åŒ–æ”¯æŒ (test_trainer_complete.py)

**æ–‡ä»¶**: `tests/test_trainer_complete.py`
**ä½ç½®**: Lines 383-419
**æ ‡è®°**: ğŸ”® APIä¼ç¬”

```python
def test_model_serialization_for_api(self, temp_dir, sample_texts):
    """æµ‹è¯•æ¨¡å‹åºåˆ—åŒ–ï¼ˆAPIéƒ¨ç½²éœ€è¦ï¼‰"""
    # ğŸ”® APIä¼ç¬”ï¼šéªŒè¯æ¨¡å‹å¯ä»¥è¢«åºåˆ—åŒ–

    # ä¿å­˜æ¨¡å‹
    model_path = temp_dir / 'api_model.pt'
    torch.save({
        'model_state_dict': model.state_dict(),
        'config': config.__dict__,
        'tokenizer_vocab': tokenizer.get_vocab(),
    }, model_path)

    # ğŸ”® æ¨¡æ‹ŸAPIæœåŠ¡å¯åŠ¨æ—¶çš„æ¨¡å‹åŠ è½½
    checkpoint = torch.load(model_path)

    # é‡å»ºæ¨¡å‹
    loaded_config = APTConfig(**checkpoint['config'])
    loaded_model = APTLargeModel(loaded_config)
    loaded_model.load_state_dict(checkpoint['model_state_dict'])
    loaded_model.eval()

    # éªŒè¯åŠ è½½çš„æ¨¡å‹å¯ä»¥æ¨ç†ï¼ˆAPIéœ€è¦ï¼‰
    with torch.no_grad():
        inputs = tokenizer("æµ‹è¯•", return_tensors='pt', padding=True)
        outputs = loaded_model.generate(inputs['input_ids'], max_length=20)
        assert outputs.shape[0] == 1
```

**APIç«¯ç‚¹å»ºè®®**:

1. **æ¨ç†æœåŠ¡**
   - `POST /api/generate` - å•æ–‡æœ¬ç”Ÿæˆ
   - `POST /api/batch_generate` - æ‰¹é‡ç”Ÿæˆ
   - `GET /api/models` - åˆ—å‡ºå¯ç”¨æ¨¡å‹

2. **è®­ç»ƒç›‘æ§**
   - `GET /api/training/status` - è®­ç»ƒçŠ¶æ€
   - `GET /api/training/gradients` - æ¢¯åº¦æ•°æ®
   - `GET /api/training/history` - è®­ç»ƒå†å²

3. **Checkpointç®¡ç†**
   - `GET /api/checkpoints` - åˆ—å‡ºcheckpoints
   - `POST /api/checkpoints/load` - åŠ è½½checkpoint
   - `DELETE /api/checkpoints/{id}` - åˆ é™¤checkpoint

---

## 3. åˆ†å¸ƒå¼è®­ç»ƒä¼ç¬”è¯¦æƒ…

### 3.1 æ¢¯åº¦åŒæ­¥æ¥å£ (gradient_monitor.py)

**æ–‡ä»¶**: `apt_model/training/gradient_monitor.py`
**ä½ç½®**: Lines 355-395
**æ ‡è®°**: ğŸ”® åˆ†å¸ƒå¼ä¼ç¬”

```python
def sync_gradients_distributed(self):
    """
    åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­åŒæ­¥æ¢¯åº¦ä¿¡æ¯

    ğŸ”® åˆ†å¸ƒå¼ä¼ç¬”ï¼šåŒæ­¥æ¢¯åº¦èŒƒæ•°

    åœ¨DDPè®­ç»ƒæ—¶ï¼Œæ¯ä¸ªrankéƒ½æœ‰è‡ªå·±çš„gradient_monitorï¼Œ
    éœ€è¦å®šæœŸåŒæ­¥æ¢¯åº¦ç»Ÿè®¡ä¿¡æ¯ä»¥è·å¾—å…¨å±€è§†å›¾

    ä½¿ç”¨æ–¹æ³•ï¼š
    if dist.is_initialized():
        gradient_monitor.sync_gradients_distributed()
    """
    try:
        import torch.distributed as dist

        if not dist.is_initialized():
            return

        world_size = dist.get_world_size()
        rank = dist.get_rank()

        # ğŸ”® åŒæ­¥æœ€æ–°çš„æ¢¯åº¦èŒƒæ•°
        if len(self.gradient_norms) > 0:
            latest_norms = self.gradient_norms[-1]

            for layer_name, norm_value in latest_norms.items():
                # å°†èŒƒæ•°è½¬ä¸ºtensor
                norm_tensor = torch.tensor([norm_value], dtype=torch.float32)

                # All-reduceæ±‚å¹³å‡
                dist.all_reduce(norm_tensor, op=dist.ReduceOp.SUM)
                norm_tensor /= world_size

                # æ›´æ–°ä¸ºå…¨å±€å¹³å‡å€¼
                self.gradient_norms[-1][layer_name] = norm_tensor.item()

        logger.debug(f"Rank {rank}: Synced gradients across {world_size} processes")

    except ImportError:
        logger.warning("torch.distributed not available, skipping sync")
    except Exception as e:
        logger.warning(f"Failed to sync gradients: {e}")

def aggregate_anomalies_distributed(self):
    """
    åœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­èšåˆå¼‚å¸¸ç»Ÿè®¡

    ğŸ”® åˆ†å¸ƒå¼ä¼ç¬”ï¼šèšåˆå¼‚å¸¸è®¡æ•°

    æ¯ä¸ªrankæ£€æµ‹åˆ°çš„å¼‚å¸¸å¯èƒ½ä¸åŒï¼Œéœ€è¦æ±‡æ€»
    """
    try:
        import torch.distributed as dist

        if not dist.is_initialized():
            return

        world_size = dist.get_world_size()

        # ğŸ”® èšåˆå¼‚å¸¸è®¡æ•°
        for anomaly_type in ['exploding', 'vanishing', 'nan']:
            count = self.anomaly_counts.get(anomaly_type, 0)
            count_tensor = torch.tensor([count], dtype=torch.int64)

            # All-reduceæ±‚å’Œ
            dist.all_reduce(count_tensor, op=dist.ReduceOp.SUM)

            # æ›´æ–°ä¸ºå…¨å±€æ€»æ•°
            self.anomaly_counts[anomaly_type] = count_tensor.item()

    except Exception as e:
        logger.warning(f"Failed to aggregate anomalies: {e}")
```

### 3.2 DDPå…¼å®¹æ€§æµ‹è¯• (test_trainer_complete.py)

**æ–‡ä»¶**: `tests/test_trainer_complete.py`
**ä½ç½®**: Lines 499-593
**æ ‡è®°**: ğŸ”® åˆ†å¸ƒå¼ä¼ç¬”

```python
class TestDistributedReadiness:
    """åˆ†å¸ƒå¼è®­ç»ƒå°±ç»ªæ€§æµ‹è¯•ï¼ˆä¸ºæœªæ¥çš„DDPåŸ‹ä¼ç¬”ï¼‰"""

    def test_model_supports_ddp_wrapping(self, temp_dir, sample_texts):
        """æµ‹è¯•æ¨¡å‹æ”¯æŒDDPåŒ…è£…"""
        # ğŸ”® åˆ†å¸ƒå¼ä¼ç¬”ï¼šéªŒè¯æ¨¡å‹å¯ä»¥è¢«DDPåŒ…è£…

        from torch.nn.parallel import DistributedDataParallel as DDP

        config = APTConfig(
            d_model=128,
            num_layers=2,
            num_attention_heads=4,
            vocab_size=1000,
        )

        model = APTLargeModel(config)

        # ğŸ”® éªŒè¯æ¨¡å‹ç»“æ„é€‚åˆDDP
        # DDPè¦æ±‚ï¼š
        # 1. æ‰€æœ‰å‚æ•°éƒ½å‚ä¸å‰å‘ä¼ æ’­ï¼ˆå¦åˆ™ä¼šæŠ¥unused parameterè­¦å‘Šï¼‰
        # 2. æ²¡æœ‰ä¸å¿…è¦çš„in-placeæ“ä½œ
        # 3. æ¨¡å‹æ˜¯å¯åºåˆ—åŒ–çš„

        # æ£€æŸ¥æ¨¡å‹å¯ä»¥è¢«åºåˆ—åŒ–ï¼ˆDDPéœ€è¦ï¼‰
        state_dict = model.state_dict()
        assert len(state_dict) > 0

        # æ£€æŸ¥æ‰€æœ‰å±‚éƒ½æœ‰å‚æ•°
        for name, param in model.named_parameters():
            assert param.requires_grad
            assert param.numel() > 0

    def test_checkpoint_supports_distributed_loading(self, temp_dir, sample_texts):
        """æµ‹è¯•checkpointæ”¯æŒåˆ†å¸ƒå¼åŠ è½½"""
        # ğŸ”® åˆ†å¸ƒå¼ä¼ç¬”ï¼šéªŒè¯checkpointå¯ä»¥åœ¨ä¸åŒrankåŠ è½½

        # è®­ç»ƒå¹¶ä¿å­˜
        trainer.train(num_epochs=2)

        ckpt_path = save_dir / 'checkpoint_epoch_2.pt'
        assert ckpt_path.exists()

        # ğŸ”® æ¨¡æ‹Ÿä¸åŒrankåŠ è½½åŒä¸€ä¸ªcheckpoint
        # åœ¨çœŸå®DDPåœºæ™¯ä¸‹ï¼Œæ¯ä¸ªrankéƒ½ä¼šåŠ è½½ç›¸åŒçš„checkpoint

        checkpoint = torch.load(ckpt_path, map_location='cpu')

        # éªŒè¯checkpointåŒ…å«å¿…è¦å­—æ®µ
        assert 'model_state_dict' in checkpoint
        assert 'optimizer_state_dict' in checkpoint
        assert 'epoch' in checkpoint
        assert 'global_step' in checkpoint

        # ğŸ”® DDPåœºæ™¯ï¼šæ‰€æœ‰rankåº”è¯¥ä»åŒä¸€ä¸ªglobal_stepå¼€å§‹
        global_step = checkpoint['global_step']
        assert global_step > 0

    def test_training_state_for_distributed_sync(self, temp_dir, sample_texts):
        """æµ‹è¯•è®­ç»ƒçŠ¶æ€æ”¯æŒåˆ†å¸ƒå¼åŒæ­¥"""
        # ğŸ”® åˆ†å¸ƒå¼ä¼ç¬”ï¼šéªŒè¯è®­ç»ƒçŠ¶æ€å¯ä»¥è·¨è¿›ç¨‹åŒæ­¥

        # è®­ç»ƒå‡ æ­¥
        for epoch in range(2):
            for batch in train_loader:
                loss = trainer.train_step(batch['input_ids'], batch['labels'])

        # ğŸ”® éªŒè¯è®­ç»ƒçŠ¶æ€å¯ä»¥è¢«åŒæ­¥
        # æœªæ¥DDPè®­ç»ƒéœ€è¦åŒæ­¥ï¼š
        # 1. global_stepï¼ˆæ‰€æœ‰rankä¸€è‡´ï¼‰
        # 2. epochï¼ˆæ‰€æœ‰rankä¸€è‡´ï¼‰
        # 3. lossï¼ˆéœ€è¦all_reduceï¼‰
        # 4. æœ€ä½³æ¨¡å‹åˆ¤æ–­ï¼ˆéœ€è¦all_reduceæ¯”è¾ƒï¼‰

        training_state = {
            'global_step': trainer.global_step,
            'epoch': trainer.epoch,
            'latest_loss': trainer.train_losses[-1] if trainer.train_losses else None,
            'best_val_loss': trainer.best_val_loss,
        }

        # éªŒè¯çŠ¶æ€å¯ä»¥åºåˆ—åŒ–ï¼ˆè·¨è¿›ç¨‹é€šä¿¡éœ€è¦ï¼‰
        import pickle
        serialized = pickle.dumps(training_state)
        deserialized = pickle.loads(serialized)

        assert deserialized['global_step'] == training_state['global_step']
        assert deserialized['epoch'] == training_state['epoch']
```

**åˆ†å¸ƒå¼è®­ç»ƒå‡†å¤‡åº¦**:

1. âœ… **æ¨¡å‹å…¼å®¹æ€§**: æ¨¡å‹ç»“æ„æ”¯æŒDDPåŒ…è£…
2. âœ… **CheckpointåŒæ­¥**: æ”¯æŒå¤šrankåŠ è½½åŒä¸€checkpoint
3. âœ… **æ¢¯åº¦åŒæ­¥æ¥å£**: `sync_gradients_distributed()`å·²å®ç°
4. âœ… **å¼‚å¸¸èšåˆæ¥å£**: `aggregate_anomalies_distributed()`å·²å®ç°
5. âœ… **è®­ç»ƒçŠ¶æ€åŒæ­¥**: çŠ¶æ€å¯åºåˆ—åŒ–ï¼Œæ”¯æŒè·¨è¿›ç¨‹é€šä¿¡
6. â³ **éœ€è¦è¡¥å……**: å®é™…çš„DDPè®­ç»ƒè„šæœ¬å’Œå¯åŠ¨å™¨

---

## 4. å®æ–½å»ºè®®

### 4.1 WebUIå®æ–½è·¯çº¿ï¼ˆé¢„ä¼°4-6å°æ—¶ï¼‰

**æ¨èæ¡†æ¶**: Gradio (å¿«é€ŸåŸå‹) æˆ– Streamlit (æ›´çµæ´»)

**å®æ–½æ­¥éª¤**:
1. åˆ›å»º `apt_model/webui/app.py`
2. é›†æˆ `export_for_webui()` æ•°æ®
3. å®ç°4ä¸ªTabé¡µï¼š
   - è®­ç»ƒç›‘æ§ï¼ˆå®æ—¶lossæ›²çº¿ï¼‰
   - æ¢¯åº¦ç›‘æ§ï¼ˆé›†æˆgradient_monitoræ•°æ®ï¼‰
   - Checkpointç®¡ç†ï¼ˆåˆ—è¡¨ã€ä¸‹è½½ã€åŠ è½½ï¼‰
   - æ¨ç†æµ‹è¯•ï¼ˆæ–‡æœ¬è¾“å…¥/è¾“å‡ºï¼‰

**ä»£ç ç¤ºä¾‹**:
```python
import gradio as gr
from apt_model.training.gradient_monitor import GradientMonitor

def create_webui():
    with gr.Blocks() as app:
        with gr.Tab("è®­ç»ƒç›‘æ§"):
            # æ˜¾ç¤ºlossæ›²çº¿
            pass

        with gr.Tab("æ¢¯åº¦ç›‘æ§"):
            # ä½¿ç”¨ export_for_webui() æ•°æ®
            pass

        with gr.Tab("Checkpointç®¡ç†"):
            # Checkpointåˆ—è¡¨
            pass

        with gr.Tab("æ¨ç†æµ‹è¯•"):
            # æ–‡æœ¬ç”Ÿæˆæ¥å£
            pass

    return app

if __name__ == '__main__':
    app = create_webui()
    app.launch()
```

### 4.2 REST APIå®æ–½è·¯çº¿ï¼ˆé¢„ä¼°8-12å°æ—¶ï¼‰

**æ¨èæ¡†æ¶**: FastAPI

**å®æ–½æ­¥éª¤**:
1. åˆ›å»º `apt_model/api/server.py`
2. å®ç°æ¨ç†ç«¯ç‚¹ï¼ˆä½¿ç”¨æµ‹è¯•ä¸­çš„åŸå‹ï¼‰
3. å®ç°è®­ç»ƒç›‘æ§ç«¯ç‚¹ï¼ˆé›†æˆgradient_monitorï¼‰
4. å®ç°checkpointç®¡ç†ç«¯ç‚¹
5. æ·»åŠ APIæ–‡æ¡£ï¼ˆFastAPIè‡ªåŠ¨ç”Ÿæˆï¼‰

**ä»£ç ç¤ºä¾‹**:
```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel

app = FastAPI(title="APT Model API")

class GenerateRequest(BaseModel):
    text: str
    max_length: int = 50

@app.post("/api/generate")
async def generate(request: GenerateRequest):
    # ä½¿ç”¨ test_inference_interface ä¸­çš„åŸå‹
    result = api_inference(model, tokenizer, request.text, request.max_length)
    return result

@app.get("/api/training/gradients")
async def get_gradients():
    # ä½¿ç”¨ export_for_webui()
    return gradient_monitor.export_for_webui()

@app.get("/api/checkpoints")
async def list_checkpoints():
    # ä½¿ç”¨ test_export_checkpoint_list_for_webui ä¸­çš„é€»è¾‘
    pass
```

### 4.3 åˆ†å¸ƒå¼è®­ç»ƒå®æ–½è·¯çº¿ï¼ˆé¢„ä¼°6-8å°æ—¶ï¼‰

**æ¨èæ–¹æ¡ˆ**: PyTorch DDP (å•æœºå¤šå¡) + DeepSpeed (å¤§è§„æ¨¡è®­ç»ƒ)

**å®æ–½æ­¥éª¤**:
1. åˆ›å»º `examples/train_distributed.py`
2. é›†æˆ `torch.distributed` åˆå§‹åŒ–
3. ä½¿ç”¨DDPåŒ…è£…æ¨¡å‹
4. è°ƒç”¨ `sync_gradients_distributed()`
5. åˆ›å»ºå¯åŠ¨è„šæœ¬ `scripts/launch_distributed.sh`

**ä»£ç ç¤ºä¾‹**:
```python
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

def main():
    # åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
    dist.init_process_group(backend='nccl')

    rank = dist.get_rank()
    world_size = dist.get_world_size()

    # åˆ›å»ºæ¨¡å‹
    model = APTLargeModel(config)
    model = model.to(rank)

    # DDPåŒ…è£…
    model = DDP(model, device_ids=[rank])

    # è®­ç»ƒå¾ªç¯
    for batch in train_loader:
        loss = model(batch)
        loss.backward()

        # ğŸ”® ä½¿ç”¨ä¼ç¬”ï¼šåŒæ­¥æ¢¯åº¦ç›‘æ§
        if dist.get_rank() == 0:
            gradient_monitor.sync_gradients_distributed()

        optimizer.step()

if __name__ == '__main__':
    main()
```

---

## 5. ä¼ç¬”æ€»ç»“

| æ¨¡å— | ä¼ç¬”æ–‡ä»¶ | å…³é”®å‡½æ•°/ç±» | è¡Œå· | çŠ¶æ€ |
|------|---------|-----------|------|------|
| **WebUIæ•°æ®å¯¼å‡º** | gradient_monitor.py | `export_for_webui()` | 260-302 | âœ… å·²å®ç° |
| **WebUIæ¥å£æµ‹è¯•** | test_trainer_complete.py | `TestWebUIDataInterface` | 599-682 | âœ… å·²å®ç° |
| **APIæ¨ç†åŸå‹** | test_trainer_complete.py | `api_inference()` | 421-458 | âœ… å·²å®ç° |
| **APIæ‰¹é‡æ¨ç†** | test_trainer_complete.py | `api_batch_inference()` | 460-492 | âœ… å·²å®ç° |
| **APIæ¨¡å‹åºåˆ—åŒ–** | test_trainer_complete.py | `test_model_serialization_for_api()` | 383-419 | âœ… å·²å®ç° |
| **åˆ†å¸ƒå¼æ¢¯åº¦åŒæ­¥** | gradient_monitor.py | `sync_gradients_distributed()` | 355-380 | âœ… å·²å®ç° |
| **åˆ†å¸ƒå¼å¼‚å¸¸èšåˆ** | gradient_monitor.py | `aggregate_anomalies_distributed()` | 382-395 | âœ… å·²å®ç° |
| **DDPå…¼å®¹æ€§æµ‹è¯•** | test_trainer_complete.py | `TestDistributedReadiness` | 499-593 | âœ… å·²å®ç° |

**ğŸ”® æ ‡è®°æ•°é‡**: 16å¤„æ˜ç¡®æ ‡è®°çš„ä¼ç¬”

**æ€»ä½“è¯„ä¼°**:
- âœ… åŸºç¡€è®¾æ–½å®Œå¤‡åº¦: 95%
- âœ… æ¥å£è®¾è®¡å®Œå¤‡åº¦: 90%
- â³ å®Œæ•´å®ç°è¿›åº¦: 0% (ç­‰å¾…è¡¥å……FastAPI/Gradio/DDPä»£ç )

---

## 6. ä¸‹ä¸€æ­¥è¡ŒåŠ¨

### ä¼˜å…ˆçº§æ’åº

1. **é«˜ä¼˜å…ˆçº§**: WebUIå®æ–½ï¼ˆç”¨æˆ·å¯è§ï¼Œå¿«é€Ÿä»·å€¼ï¼‰
   - é¢„ä¼°: 4-6å°æ—¶
   - ä¼ç¬”åˆ©ç”¨ç‡: 90%

2. **ä¸­ä¼˜å…ˆçº§**: REST APIå®æ–½ï¼ˆæœåŠ¡åŒ–éƒ¨ç½²ï¼‰
   - é¢„ä¼°: 8-12å°æ—¶
   - ä¼ç¬”åˆ©ç”¨ç‡: 85%

3. **ä½ä¼˜å…ˆçº§**: åˆ†å¸ƒå¼è®­ç»ƒå®æ–½ï¼ˆå¤§è§„æ¨¡è®­ç»ƒéœ€æ±‚ï¼‰
   - é¢„ä¼°: 6-8å°æ—¶
   - ä¼ç¬”åˆ©ç”¨ç‡: 80%

### å»ºè®®å®æ–½é¡ºåº

```
Phase 1 (æœ¬å‘¨): WebUIåŸºç¡€ç‰ˆ
â”œâ”€â”€ è®­ç»ƒç›‘æ§Tab (2å°æ—¶)
â”œâ”€â”€ æ¢¯åº¦ç›‘æ§Tab (1.5å°æ—¶)
â”œâ”€â”€ Checkpointç®¡ç†Tab (1.5å°æ—¶)
â””â”€â”€ æ¨ç†æµ‹è¯•Tab (1å°æ—¶)

Phase 2 (ä¸‹å‘¨): REST API
â”œâ”€â”€ æ¨ç†ç«¯ç‚¹ (3å°æ—¶)
â”œâ”€â”€ è®­ç»ƒç›‘æ§ç«¯ç‚¹ (2å°æ—¶)
â”œâ”€â”€ Checkpointç®¡ç†ç«¯ç‚¹ (2å°æ—¶)
â”œâ”€â”€ APIæ–‡æ¡£ (1å°æ—¶)
â””â”€â”€ éƒ¨ç½²æµ‹è¯• (2å°æ—¶)

Phase 3 (åç»­): åˆ†å¸ƒå¼è®­ç»ƒ
â”œâ”€â”€ DDPè®­ç»ƒè„šæœ¬ (3å°æ—¶)
â”œâ”€â”€ å¯åŠ¨å™¨å’Œé…ç½® (2å°æ—¶)
â”œâ”€â”€ æ¢¯åº¦åŒæ­¥é›†æˆ (1å°æ—¶)
â””â”€â”€ å¤šæœºæµ‹è¯• (2å°æ—¶)
```

---

**æŠ¥å‘Šå®Œæˆæ—¶é—´**: 2025-11-30
**ä¸‹æ¬¡æ£€æŸ¥å»ºè®®**: å®æ–½ç¬¬ä¸€ä¸ªåŠŸèƒ½åéªŒè¯ä¼ç¬”æ˜¯å¦å……åˆ†
