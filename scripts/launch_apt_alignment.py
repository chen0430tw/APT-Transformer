#!/usr/bin/env python3
"""
ğŸš€ APTå¯¹é½è®­ç»ƒå¯åŠ¨å™¨
å¿«é€Ÿå¯åŠ¨APTæ¨ç†ä¸å¯¹é½è®­ç»ƒ

é¢„è®¾é…ç½®:
1. æ ‡å‡†å¯¹é½ (SFT â†’ GRPO)
2. å¿ è¯šåº¦è®­ç»ƒ (Loyalty)
3. æš´é£é›¨è®­ç»ƒ (Storm - åŠ¨æ€æ¨ç†)
4. å®Œæ•´æµç¨‹ (All stages)

ä½œè€…: chen0430tw
æ—¥æœŸ: 2024-12-23
"""

import os
import sys
import subprocess
from pathlib import Path

# é¡¹ç›®æ ¹ç›®å½•
PROJECT_ROOT = Path(__file__).parent.parent.absolute()
os.chdir(PROJECT_ROOT)

# ANSIé¢œè‰²
CYAN = '\033[96m'
GREEN = '\033[92m'
YELLOW = '\033[93m'
RED = '\033[91m'
RESET = '\033[0m'
BOLD = '\033[1m'


def print_header(text):
    """æ‰“å°æ ‡é¢˜"""
    print(f"\n{CYAN}{BOLD}{'='*60}")
    print(f"{text}")
    print(f"{'='*60}{RESET}\n")


def print_success(text):
    """æ‰“å°æˆåŠŸä¿¡æ¯"""
    print(f"{GREEN}âœ“{RESET} {text}")


def print_warning(text):
    """æ‰“å°è­¦å‘Š"""
    print(f"{YELLOW}âš {RESET}  {text}")


def print_error(text):
    """æ‰“å°é”™è¯¯"""
    print(f"{RED}âœ—{RESET} {text}")


def check_dependencies():
    """æ£€æŸ¥Pythonä¾èµ–"""
    print_header("æ£€æŸ¥Pythonä¾èµ–...")

    try:
        import torch
        print_success(f"PyTorch {torch.__version__}")
    except ImportError:
        print_error("PyTorch æœªå®‰è£…")
        return False

    try:
        import numpy
        print_success(f"NumPy {numpy.__version__}")
    except ImportError:
        print_error("NumPy æœªå®‰è£…")
        return False

    return True


def get_main_action():
    """è·å–ä¸»è¦æ“ä½œ"""
    print_header("é€‰æ‹©æ“ä½œ")

    actions = {
        '1': {
            'name': 'ğŸ“¦ å‡†å¤‡æ•°æ®é›†',
            'type': 'prepare_data',
            'desc': 'ä¸‹è½½å’Œé¢„å¤„ç†HuggingFaceæ•°æ®é›†'
        },
        '2': {
            'name': 'ğŸš€ å¼€å§‹è®­ç»ƒ',
            'type': 'train',
            'desc': 'å¯åŠ¨APTå¯¹é½è®­ç»ƒæµç¨‹'
        },
        '3': {
            'name': 'ğŸ“Š æŸ¥çœ‹æ•°æ®é›†ä¿¡æ¯',
            'type': 'show_datasets',
            'desc': 'æ˜¾ç¤ºå·²å‡†å¤‡çš„æ•°æ®é›†ç»Ÿè®¡'
        }
    }

    print("æ“ä½œ:")
    for key, action in actions.items():
        print(f"  {CYAN}{key}{RESET}. {action['name']}")
        print(f"     {action['desc']}")

    choice = input(f"\n{CYAN}é€‰æ‹©æ“ä½œ [1-3]:{RESET} ").strip()

    if choice not in actions:
        print_error("æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨é»˜è®¤: å¼€å§‹è®­ç»ƒ")
        choice = '2'

    return actions[choice]


def get_training_mode():
    """è·å–è®­ç»ƒæ¨¡å¼"""
    print_header("é€‰æ‹©è®­ç»ƒæ¨¡å¼")

    modes = {
        '1': {
            'name': 'æ ‡å‡†å¯¹é½ (SFT â†’ GRPO)',
            'skip': 'dpo,loyalty,storm',
            'desc': 'åŸºç¡€æŒ‡ä»¤å¾®è°ƒ + ç­–ç•¥ä¼˜åŒ–',
            'datasets': ['coig-cqia', 'hh-rlhf']
        },
        '2': {
            'name': 'å¿ è¯šåº¦è®­ç»ƒ (Loyalty)',
            'skip': 'sft,dpo,grpo,storm',
            'desc': 'åŒºåˆ†ä¸»äºº vs å¤§ä¼—å“åº”',
            'datasets': ['loyalty_template']
        },
        '3': {
            'name': 'æš´é£é›¨è®­ç»ƒ (Storm)',
            'skip': 'sft,dpo,grpo,loyalty',
            'desc': 'åŠ¨æ€æ¨ç† + å†…åŒ–CoT',
            'datasets': ['s1k']
        },
        '4': {
            'name': 'å®Œæ•´æµç¨‹ (All Stages)',
            'skip': '',
            'desc': 'SFT â†’ GRPO â†’ Loyalty â†’ Storm',
            'datasets': ['coig-cqia', 'hh-rlhf', 's1k', 'loyalty_template']
        }
    }

    print("è®­ç»ƒæ¨¡å¼:")
    for key, mode in modes.items():
        print(f"  {CYAN}{key}{RESET}. {BOLD}{mode['name']}{RESET}")
        print(f"     {mode['desc']}")

    choice = input(f"\n{CYAN}é€‰æ‹©æ¨¡å¼ [1-4]:{RESET} ").strip()

    if choice not in modes:
        print_error("æ— æ•ˆé€‰æ‹©ï¼Œä½¿ç”¨é»˜è®¤: æ ‡å‡†å¯¹é½")
        choice = '1'

    return modes[choice]


def prepare_datasets():
    """æ•°æ®é›†å‡†å¤‡äº¤äº’å¼ç•Œé¢"""
    print_header("ğŸ“¦ æ•°æ®é›†å‡†å¤‡")

    print("æ¨èæ•°æ®é›†:")
    print(f"  {CYAN}1{RESET}. COIG-CQIA (48Kä¸­æ–‡æŒ‡ä»¤) - {BOLD}SFTé˜¶æ®µ{RESET}")
    print(f"  {CYAN}2{RESET}. simplescaling/s1K (1Kæ¨ç†traces) - {BOLD}Stormé˜¶æ®µ{RESET}")
    print(f"  {CYAN}3{RESET}. HH-RLHF (160Kåå¥½æ•°æ®) - {BOLD}GRPOé˜¶æ®µ{RESET}")
    print(f"  {CYAN}4{RESET}. å¼±æ™ºå§å­é›† (ä»COIG-CQIAæå–) - {BOLD}æå‡æ¨ç†{RESET}")
    print(f"  {CYAN}5{RESET}. å¿ è¯šåº¦æ¨¡æ¿ (åŸºäºHH-RLHF) - {BOLD}Loyaltyé˜¶æ®µ{RESET}")
    print(f"  {CYAN}6{RESET}. ä¸‹è½½å…¨éƒ¨æ¨èæ•°æ®é›†")

    choice = input(f"\n{CYAN}é€‰æ‹©è¦å‡†å¤‡çš„æ•°æ®é›† [1-6]:{RESET} ").strip()

    # æ„å»ºæ•°æ®å‡†å¤‡å‘½ä»¤
    prepare_script = PROJECT_ROOT / "scripts" / "prepare_apt_datasets.py"

    if choice == '1':
        cmd = [sys.executable, str(prepare_script), '--sft']
    elif choice == '2':
        cmd = [sys.executable, str(prepare_script), '--cot']
    elif choice == '3':
        cmd = [sys.executable, str(prepare_script), '--dpo']
    elif choice == '4':
        cmd = [sys.executable, str(prepare_script), '--ruozhiba']
    elif choice == '5':
        cmd = [sys.executable, str(prepare_script), '--loyalty-template']
    elif choice == '6':
        cmd = [sys.executable, str(prepare_script), '--all', '--ruozhiba', '--loyalty-template']
    else:
        print_error("æ— æ•ˆé€‰æ‹©")
        return

    # æ˜¾ç¤ºå‘½ä»¤
    print_header("æ‰§è¡Œå‘½ä»¤")
    print(f"{CYAN}{' '.join(cmd)}{RESET}\n")

    # æ‰§è¡Œ
    try:
        subprocess.run(cmd, check=True)
        print_success("\næ•°æ®é›†å‡†å¤‡å®Œæˆï¼")
    except subprocess.CalledProcessError as e:
        print_error(f"æ•°æ®é›†å‡†å¤‡å¤±è´¥: {e}")
    except KeyboardInterrupt:
        print_warning("\næ“ä½œè¢«ä¸­æ–­")


def show_dataset_info():
    """æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯"""
    print_header("ğŸ“Š æ•°æ®é›†ä¿¡æ¯")

    data_dir = PROJECT_ROOT / "data" / "apt_datasets"

    if not data_dir.exists():
        print_warning(f"æ•°æ®ç›®å½•ä¸å­˜åœ¨: {data_dir}")
        print_info("è¯·å…ˆä½¿ç”¨ 'å‡†å¤‡æ•°æ®é›†' åŠŸèƒ½ä¸‹è½½æ•°æ®")
        return

    import json

    # æ‰«ææ•°æ®æ–‡ä»¶
    datasets = []
    for file_path in data_dir.glob("*.json"):
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                datasets.append({
                    'name': file_path.stem,
                    'path': file_path,
                    'size': len(data),
                    'file_size': file_path.stat().st_size / (1024 * 1024)  # MB
                })
        except Exception as e:
            print_warning(f"æ— æ³•è¯»å– {file_path.name}: {e}")

    if not datasets:
        print_warning("æœªæ‰¾åˆ°ä»»ä½•æ•°æ®é›†")
        print_info("è¯·å…ˆä½¿ç”¨ 'å‡†å¤‡æ•°æ®é›†' åŠŸèƒ½ä¸‹è½½æ•°æ®")
        return

    # æ˜¾ç¤ºç»Ÿè®¡
    print(f"æ•°æ®ç›®å½•: {CYAN}{data_dir}{RESET}\n")
    print(f"{'æ•°æ®é›†åç§°':<30} {'æ ·æœ¬æ•°':>10} {'æ–‡ä»¶å¤§å°':>12}")
    print("-" * 55)

    total_samples = 0
    total_size = 0

    for ds in sorted(datasets, key=lambda x: x['size'], reverse=True):
        print(f"{ds['name']:<30} {ds['size']:>10,} {ds['file_size']:>10.2f} MB")
        total_samples += ds['size']
        total_size += ds['file_size']

    print("-" * 55)
    print(f"{'æ€»è®¡':<30} {total_samples:>10,} {total_size:>10.2f} MB")


def check_required_datasets(mode):
    """æ£€æŸ¥æ‰€éœ€æ•°æ®é›†æ˜¯å¦å·²å‡†å¤‡"""
    data_dir = PROJECT_ROOT / "data" / "apt_datasets"

    if not data_dir.exists():
        print_warning("\næ•°æ®é›†ç›®å½•ä¸å­˜åœ¨ï¼Œå»ºè®®å…ˆå‡†å¤‡æ•°æ®é›†")
        prepare = input(f"{CYAN}æ˜¯å¦ç°åœ¨å‡†å¤‡æ•°æ®é›†? [y/N]:{RESET} ").strip().lower()
        if prepare in ['y', 'yes']:
            prepare_datasets()
            return True
        return False

    # æ£€æŸ¥æ‰€éœ€æ–‡ä»¶
    required = mode.get('datasets', [])
    missing = []

    for dataset_name in required:
        file_path = data_dir / f"{dataset_name}_train.json"
        if not file_path.exists():
            missing.append(dataset_name)

    if missing:
        print_warning(f"\nç¼ºå°‘æ•°æ®é›†: {', '.join(missing)}")
        prepare = input(f"{CYAN}æ˜¯å¦ç°åœ¨å‡†å¤‡ç¼ºå¤±çš„æ•°æ®é›†? [y/N]:{RESET} ").strip().lower()
        if prepare in ['y', 'yes']:
            prepare_datasets()
            return True

    return True


def build_command(mode):
    """æ„å»ºè®­ç»ƒå‘½ä»¤"""
    print_header("è®­ç»ƒé…ç½®")

    cmd = [
        sys.executable,
        "training/train_apt_alignment.py"
    ]

    # æ•°æ®é›†ç›®å½•
    data_dir = PROJECT_ROOT / "data" / "apt_datasets"

    # æ ¹æ®æ¨¡å¼æ·»åŠ å‚æ•°
    if 'æ ‡å‡†å¯¹é½' in mode['name']:
        print(f"æ¨¡å¼: {BOLD}æ ‡å‡†å¯¹é½{RESET}")
        print(f"  â†’ SFTæ•°æ®é›†: coig-cqia_train.json")
        print(f"  â†’ GRPO prompts: hh-rlhf_train.json")

        cmd.extend([
            '--sft-data', str(data_dir / 'coig-cqia_train.json'),
            '--prompts', str(data_dir / 'hh-rlhf_train.json')
        ])

    elif 'å¿ è¯šåº¦' in mode['name']:
        print(f"æ¨¡å¼: {BOLD}å¿ è¯šåº¦è®­ç»ƒ{RESET}")
        print(f"  â†’ å¿ è¯šåº¦æ¨¡æ¿: loyalty_template.json")
        print(f"  â†’ å¥–åŠ±åŠ æˆ: +2.0")

        cmd.extend([
            '--loyalty-data', str(data_dir / 'loyalty_template.json'),
            '--owner-bonus', '2.0'
        ])

    elif 'æš´é£é›¨' in mode['name']:
        print(f"æ¨¡å¼: {BOLD}æš´é£é›¨è®­ç»ƒ{RESET}")
        print(f"  â†’ æ¨ç†æ•°æ®: s1k_train.json")
        print(f"  â†’ å™ªéŸ³æ¯”ä¾‹: 0.3")
        print(f"  â†’ å™ªéŸ³ç­–ç•¥: cosine")
        print(f"  â†’ å†…åŒ–CoT: æ˜¯")

        cmd.extend([
            '--reasoning-data', str(data_dir / 's1k_train.json'),
            '--noise-ratio', '0.3',
            '--noise-schedule', 'cosine',
            '--internalize-cot'
        ])

    elif 'å®Œæ•´æµç¨‹' in mode['name']:
        print(f"æ¨¡å¼: {BOLD}å®Œæ•´æµç¨‹{RESET}")
        print(f"  â†’ åŒ…å«æ‰€æœ‰é˜¶æ®µ (SFT â†’ GRPO â†’ Loyalty â†’ Storm)")

        cmd.extend([
            '--sft-data', str(data_dir / 'coig-cqia_train.json'),
            '--prompts', str(data_dir / 'hh-rlhf_train.json'),
            '--loyalty-data', str(data_dir / 'loyalty_template.json'),
            '--reasoning-data', str(data_dir / 's1k_train.json')
        ])

    # é€šç”¨å‚æ•°
    cmd.extend([
        '--output-dir', './apt_aligned_models',
        '--device', 'cuda'
    ])

    # è·³è¿‡é˜¶æ®µ
    if mode['skip']:
        cmd.extend(['--skip', mode['skip']])

    return cmd


def main():
    print_header("ğŸš€ APTå¯¹é½è®­ç»ƒå¯åŠ¨å™¨")

    print(f"é¡¹ç›®ç›®å½•: {PROJECT_ROOT}\n")

    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        print_error("ä¾èµ–æ£€æŸ¥å¤±è´¥ï¼Œè¯·å®‰è£…ç¼ºå¤±çš„åŒ…")
        sys.exit(1)

    # é€‰æ‹©ä¸»è¦æ“ä½œ
    action = get_main_action()

    if action['type'] == 'prepare_data':
        # æ•°æ®é›†å‡†å¤‡
        prepare_datasets()

    elif action['type'] == 'show_datasets':
        # æ˜¾ç¤ºæ•°æ®é›†ä¿¡æ¯
        show_dataset_info()

    elif action['type'] == 'train':
        # è®­ç»ƒæµç¨‹
        mode = get_training_mode()

        # æ£€æŸ¥æ‰€éœ€æ•°æ®é›†
        if not check_required_datasets(mode):
            print_warning("æ•°æ®é›†æ£€æŸ¥æœªé€šè¿‡ï¼Œè®­ç»ƒå·²å–æ¶ˆ")
            sys.exit(0)

        # æ„å»ºå‘½ä»¤
        cmd = build_command(mode)

        # æ˜¾ç¤ºå‘½ä»¤
        print_header("å¯åŠ¨å‘½ä»¤")
        print(f"{CYAN}{' '.join(cmd)}{RESET}\n")

        # ç¡®è®¤å¯åŠ¨
        confirm = input(f"{YELLOW}æ˜¯å¦å¼€å§‹è®­ç»ƒ? [y/N]:{RESET} ").strip().lower()

        if confirm not in ['y', 'yes']:
            print_warning("è®­ç»ƒå·²å–æ¶ˆ")
            sys.exit(0)

        # å¯åŠ¨è®­ç»ƒ
        print_header("ğŸš€ å¼€å§‹è®­ç»ƒ...")
        try:
            subprocess.run(cmd, check=True)
            print_success("è®­ç»ƒå®Œæˆï¼")
        except subprocess.CalledProcessError as e:
            print_error(f"è®­ç»ƒå¤±è´¥: {e}")
            sys.exit(1)
        except KeyboardInterrupt:
            print_warning("\nè®­ç»ƒè¢«ä¸­æ–­")
            sys.exit(1)


if __name__ == "__main__":
    main()
