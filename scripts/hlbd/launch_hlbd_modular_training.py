#!/usr/bin/env python3
"""
HLBDæ¨¡å—åŒ–è®­ç»ƒå¯åŠ¨å™¨
è‡ªåŠ¨ç»„åˆå¤šä¸ªHLBDæ•°æ®é›†è¿›è¡Œè”åˆè®­ç»ƒ

ç‰¹æ€§:
- ğŸ”— è‡ªåŠ¨åŠ è½½HLBD Full V2 (5000æ ·æœ¬) + HLBD Hardcore V2 (5042æ ·æœ¬)
- ğŸ“Š æ€»è®¡çº¦10,000ä¸ªè®­ç»ƒæ ·æœ¬
- ğŸ² è‡ªåŠ¨æ··åˆæ‰“æ•£ï¼Œé˜²æ­¢æ¨¡å¼åç¼©
- ğŸ“ˆ ç»Ÿä¸€è®­ç»ƒæµç¨‹
"""

import os
import sys
import subprocess
from pathlib import Path


def check_datasets():
    """æ£€æŸ¥æ•°æ®é›†æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    datasets = [
        'data/HLBD_Full_V2.json',
        'data/HLBD_Hardcore_Full_V2.json'
    ]

    print("=" * 60)
    print("æ£€æŸ¥æ•°æ®é›†æ–‡ä»¶...")
    print("=" * 60)

    missing = []
    for dataset in datasets:
        if Path(dataset).exists():
            size = Path(dataset).stat().st_size / (1024 * 1024)  # MB
            print(f"âœ“ {dataset} ({size:.1f} MB)")
        else:
            print(f"âœ— {dataset} (ä¸å­˜åœ¨)")
            missing.append(dataset)

    if missing:
        print("\nâŒ ç¼ºå°‘æ•°æ®é›†æ–‡ä»¶:")
        for m in missing:
            print(f"   - {m}")
        print("\nè¯·å…ˆç”Ÿæˆæ•°æ®é›†:")
        print("   python3 tools/generate_hlbd_full_v2.py")
        print("   python3 tools/generate_hlbd_hardcore_v2.py")
        return False

    return True


def check_dependencies():
    """æ£€æŸ¥Pythonä¾èµ–"""
    print("\n" + "=" * 60)
    print("æ£€æŸ¥Pythonä¾èµ–...")
    print("=" * 60)

    dependencies = {
        'torch': 'PyTorch',
        'numpy': 'NumPy'
    }

    missing = []
    for module, name in dependencies.items():
        try:
            __import__(module)
            print(f"âœ“ {name}")
        except ImportError:
            print(f"âœ— {name}")
            missing.append(name)

    if missing:
        print("\nâŒ ç¼ºå°‘ä¾èµ–:")
        for m in missing:
            print(f"   - {m}")
        print("\nå®‰è£…å‘½ä»¤:")
        print("   pip install torch numpy")
        return False

    return True


def find_latest_checkpoint(save_dir='hlbd_modular'):
    """æŸ¥æ‰¾æœ€æ–°çš„checkpoint"""
    save_path = Path(save_dir)
    if not save_path.exists():
        return None

    # æŸ¥æ‰¾æ‰€æœ‰checkpointæ–‡ä»¶
    checkpoints = list(save_path.glob('checkpoint_epoch_*.pt'))
    if not checkpoints:
        return None

    # æå–epochæ•°å­—å¹¶æ’åº
    checkpoint_epochs = []
    for ckpt in checkpoints:
        try:
            # æ–‡ä»¶åæ ¼å¼: checkpoint_epoch_10.pt
            epoch_num = int(ckpt.stem.split('_')[-1])
            checkpoint_epochs.append((epoch_num, ckpt))
        except ValueError:
            continue

    if not checkpoint_epochs:
        return None

    # è¿”å›æœ€æ–°çš„checkpoint
    checkpoint_epochs.sort(key=lambda x: x[0], reverse=True)
    latest_epoch, latest_ckpt = checkpoint_epochs[0]

    return {
        'path': latest_ckpt,
        'epoch': latest_epoch,
        'size_mb': latest_ckpt.stat().st_size / (1024 * 1024)
    }


def ask_resume_training(checkpoint_info):
    """è¯¢é—®æ˜¯å¦æ¢å¤è®­ç»ƒ"""
    print("\n" + "=" * 60)
    print("ğŸ” å‘ç°å·²æœ‰checkpoint")
    print("=" * 60)
    print(f"æ–‡ä»¶: {checkpoint_info['path']}")
    print(f"Epoch: {checkpoint_info['epoch']}")
    print(f"å¤§å°: {checkpoint_info['size_mb']:.1f} MB")
    print()

    while True:
        response = input("æ˜¯å¦ä»æ­¤checkpointæ¢å¤è®­ç»ƒ? [Y/n]: ").strip().lower()
        if response in ['', 'y', 'yes']:
            return True
        elif response in ['n', 'no']:
            print("\nâš ï¸  å°†ä»epoch 1é‡æ–°å¼€å§‹è®­ç»ƒï¼ˆå·²æœ‰checkpointå°†è¢«è¦†ç›–ï¼‰")
            confirm = input("ç¡®è®¤é‡æ–°å¼€å§‹? [y/N]: ").strip().lower()
            if confirm in ['y', 'yes']:
                return False
            # å¦åˆ™ç»§ç»­å¾ªç¯è¯¢é—®
        else:
            print("è¯·è¾“å…¥ y æˆ– n")


def main():
    """ä¸»å¯åŠ¨æµç¨‹"""
    # ç¡®ä¿åœ¨é¡¹ç›®æ ¹ç›®å½•è¿è¡Œ
    script_dir = Path(__file__).parent
    project_root = script_dir.parent.parent
    os.chdir(project_root)

    print("=" * 60)
    print("ğŸ”— HLBDæ¨¡å—åŒ–è®­ç»ƒå¯åŠ¨å™¨")
    print("=" * 60)
    print(f"é¡¹ç›®ç›®å½•: {project_root}")
    print()

    # æ£€æŸ¥æ•°æ®é›†
    if not check_datasets():
        return 1

    # æ£€æŸ¥ä¾èµ–
    if not check_dependencies():
        return 1

    # æ£€æŸ¥æ˜¯å¦æœ‰å·²å­˜åœ¨çš„checkpoint
    checkpoint_info = find_latest_checkpoint('hlbd_modular')
    resume_training = False

    if checkpoint_info:
        resume_training = ask_resume_training(checkpoint_info)

    # è®­ç»ƒé…ç½®
    print("\n" + "=" * 60)
    print("è®­ç»ƒé…ç½®")
    print("=" * 60)
    print("æ•°æ®é›†: HLBD Full V2 + HLBD Hardcore V2")
    print("æ€»æ ·æœ¬: ~10,000")
    print("è®­ç»ƒè½®æ•°: 50")
    print("æ‰¹æ¬¡å¤§å°: 16 (æ¢¯åº¦ç´¯ç§¯x2)")
    print("ä¿å­˜ç›®å½•: hlbd_modular")
    if resume_training:
        print(f"æ¢å¤æ¨¡å¼: ä»Epoch {checkpoint_info['epoch']} ç»§ç»­")
    else:
        print("è®­ç»ƒæ¨¡å¼: ä»å¤´å¼€å§‹")
    print("=" * 60)
    print()

    # æ„å»ºè®­ç»ƒå‘½ä»¤
    cmd = [
        sys.executable,
        'training/train_hlbd_playground.py',
        '--datasets',
        'data/HLBD_Full_V2.json',
        'data/HLBD_Hardcore_Full_V2.json',
        '--epochs', '50',
        '--save-dir', 'hlbd_modular',
        '--save-interval', '10'
    ]

    # æ·»åŠ æ¢å¤å‚æ•°
    if resume_training:
        cmd.extend(['--resume', str(checkpoint_info['path'])])

    print("å¯åŠ¨å‘½ä»¤:")
    print(" ".join(cmd))
    print()
    print("=" * 60)
    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    print("=" * 60)
    print()

    # å¯åŠ¨è®­ç»ƒ
    try:
        subprocess.run(cmd, check=True)

        print("\n" + "=" * 60)
        print("âœ… è®­ç»ƒå®Œæˆï¼")
        print("=" * 60)
        print()
        print("æ¨¡å‹ä¿å­˜ä½ç½®: hlbd_modular/")
        print("æŸ¥çœ‹è®­ç»ƒè¿›åº¦: hlbd_modular/experiment_report.json")
        return 0

    except subprocess.CalledProcessError as e:
        print(f"\nâŒ è®­ç»ƒå¤±è´¥: {e}")
        return 1
    except KeyboardInterrupt:
        print("\nâš ï¸  è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        return 130


if __name__ == "__main__":
    sys.exit(main())
