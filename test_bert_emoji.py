#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æµ‹è¯• BertTokenizer å¯¹ emoji çš„å¤„ç†
ï¼ˆtest_hlbd_quick_learning.py ä½¿ç”¨çš„æ˜¯ BertTokenizerï¼‰
"""

import os
import sys

# æ·»åŠ é¡¹ç›®è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)

try:
    from transformers import BertTokenizer

    # ä½¿ç”¨æœ¬åœ° bert-base-chinese
    bert_path = os.path.join(current_dir, 'bert', 'bert-base-chinese')

    print("="*70)
    print("æµ‹è¯• BertTokenizer (bert-base-chinese) å¯¹ emoji çš„å¤„ç†")
    print("="*70)

    if not os.path.exists(bert_path):
        print(f"\nâŒ æœªæ‰¾åˆ°æœ¬åœ° BERT æ¨¡å‹: {bert_path}")
        print("è¯·å…ˆä¸‹è½½ bert-base-chinese æ¨¡å‹")
        sys.exit(1)

    # åŠ è½½ tokenizer
    tokenizer = BertTokenizer.from_pretrained(
        bert_path,
        local_files_only=True,
        vocab_file=os.path.join(bert_path, 'vocab.txt')
    )

    print(f"\nåˆ†è¯å™¨ç±»å‹: {type(tokenizer).__name__}")
    print(f"è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")

    # æ£€æŸ¥ç‰¹æ®Š token
    print(f"\nç‰¹æ®Š token:")
    print(f"  [UNK]: {tokenizer.unk_token} (ID={tokenizer.unk_token_id})")
    print(f"  [PAD]: {tokenizer.pad_token} (ID={tokenizer.pad_token_id})")
    print(f"  [CLS]: {tokenizer.cls_token} (ID={tokenizer.cls_token_id})")
    print(f"  [SEP]: {tokenizer.sep_token} (ID={tokenizer.sep_token_id})")

    # æµ‹è¯• emoji ç¼–ç 
    test_cases = [
        "ğŸŒ§ï¸",           # é›¨æ»´ emojiï¼ˆtest_hlbd ä¸­ä½¿ç”¨ï¼‰
        "â¤ï¸",           # çˆ±å¿ƒ emojiï¼ˆtest_hlbd ä¸­ä½¿ç”¨ï¼‰
        "ğŸ½ï¸",           # é¤å…· emojiï¼ˆtest_hlbd ä¸­ä½¿ç”¨ï¼‰
        "ğŸ“–",           # ä¹¦æœ¬ emojiï¼ˆtest_hlbd ä¸­ä½¿ç”¨ï¼‰
        "ä½ å¥½ğŸŒ§ï¸ä¸–ç•Œ",   # ä¸­æ–‡ + emoji
        "HelloğŸŒ§ï¸World", # è‹±æ–‡ + emoji
        "æˆ‘çˆ±ä½ ",        # çº¯ä¸­æ–‡
        "Hello",        # çº¯è‹±æ–‡
    ]

    print("\n" + "="*70)
    print("æµ‹è¯•ç»“æœ:")
    print("-"*70)

    for text in test_cases:
        # ç¼–ç ï¼ˆä¸æ·»åŠ ç‰¹æ®Š tokenï¼‰
        ids = tokenizer.encode(text, add_special_tokens=False)

        # è§£ç 
        decoded = tokenizer.decode(ids, skip_special_tokens=True)

        # è·å– token åˆ—è¡¨
        tokens = tokenizer.tokenize(text)

        print(f"\nåŸæ–‡æœ¬: {text!r}")
        print(f"  Token: {tokens}")
        print(f"  ç¼–ç  ID: {ids}")
        print(f"  ID é•¿åº¦: {len(ids)} (åŸæ–‡æœ¬é•¿åº¦: {len(text)})")
        print(f"  è§£ç ç»“æœ: {decoded!r}")

        # æ£€æŸ¥æ˜¯å¦æœ‰ UNK
        if tokenizer.unk_token_id in ids:
            unk_count = ids.count(tokenizer.unk_token_id)
            print(f"  âš ï¸  åŒ…å« {unk_count} ä¸ª [UNK] token")

        if text != decoded:
            print(f"  âš ï¸  ç¼–ç /è§£ç æœ‰æŸå¤±ï¼")
        else:
            print(f"  âœ“  ç¼–ç /è§£ç æ— æŸ")

    # ä¸“é—¨æµ‹è¯• emoji
    print("\n" + "="*70)
    print("Emoji è¯¦ç»†åˆ†æ:")
    print("-"*70)

    emoji_text = "ğŸŒ§ï¸"
    tokens = tokenizer.tokenize(emoji_text)
    ids = tokenizer.encode(emoji_text, add_special_tokens=False)
    decoded = tokenizer.decode(ids, skip_special_tokens=True)

    print(f"åŸæ–‡æœ¬: {emoji_text!r}")
    print(f"  Unicode: {[hex(ord(c)) for c in emoji_text]}")
    print(f"  Tokenize: {tokens}")
    print(f"  ç¼–ç  ID: {ids}")
    print(f"  è§£ç ç»“æœ: {decoded!r}")

    if tokenizer.unk_token_id in ids:
        print(f"\nâœ“ emoji è¢«ç¼–ç ä¸º [UNK] token (ID={tokenizer.unk_token_id})")
        print(f"  è¿™æ„å‘³ç€ BertTokenizer è¯†åˆ« emoji ä¸ºæœªçŸ¥å­—ç¬¦")
        print(f"  ä½†ä¸ä¼šåƒ ChineseTokenizer é‚£æ ·ç›´æ¥è·³è¿‡")
    else:
        print(f"\nâœ“ emoji è¢«æ­£å¸¸ç¼–ç ï¼ˆæœªä½¿ç”¨ UNKï¼‰")

    print("="*70)
    print("\nç»“è®º:")
    print("-"*70)
    print("BertTokenizer å¤„ç† emoji çš„æ–¹å¼ï¼š")
    if tokenizer.unk_token_id in tokenizer.encode("ğŸŒ§ï¸", add_special_tokens=False):
        print("  â€¢ emoji è¢«ç¼–ç ä¸º [UNK] token")
        print("  â€¢ ä¸ä¼šä¸¢å¤± emoji ä¿¡æ¯ï¼ˆç”¨ UNK å ä½ï¼‰")
        print("  â€¢ ç¼–ç /è§£ç è¿‡ç¨‹ä¿ç•™äº† emoji çš„ä½ç½®")
        print("  â€¢ âœ“ æ¯” ChineseTokenizer çš„ç›´æ¥è·³è¿‡æ›´åˆç†")
    else:
        print("  â€¢ emoji å¯ä»¥æ­£å¸¸ç¼–ç ")

    print("="*70)

except ImportError as e:
    print(f"\nâŒ ç¼ºå°‘ä¾èµ–åº“: {e}")
    print("è¯·å®‰è£…: pip install transformers")
    sys.exit(1)
except Exception as e:
    print(f"\nâŒ é”™è¯¯: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)
