#!/bin/bash
# APT-Transformer æ ¹ç›®å½•è‡ªåŠ¨æ•´ç†è„šæœ¬

set -e  # é‡åˆ°é”™è¯¯ç«‹å³é€€å‡º

echo "ğŸš€ å¼€å§‹æ•´ç† APT-Transformer æ ¹ç›®å½•..."
echo ""

# Step 1: åˆ›å»ºæ–°ç›®å½•
echo "ğŸ“ Step 1: åˆ›å»ºæ–°ç›®å½•ç»“æ„..."
mkdir -p training
mkdir -p tools
mkdir -p data
mkdir -p archived/pr
mkdir -p docs/testing
mkdir -p docs/reports
mkdir -p scripts/testing
mkdir -p scripts/setup
echo "   âœ… ç›®å½•åˆ›å»ºå®Œæˆ"
echo ""

# Step 2: ç§»åŠ¨è®­ç»ƒè„šæœ¬
echo "ğŸš‚ Step 2: ç§»åŠ¨è®­ç»ƒè„šæœ¬åˆ° training/..."
for file in train.py train_apt_playground.py train_azure_ml.py train_control_experiment.py \
            train_deepspeed.py train_hf_trainer.py train_hlbd_playground.py; do
    if [ -f "$file" ]; then
        git mv "$file" training/
        echo "   âœ“ $file â†’ training/"
    fi
done

if [ -f "training_resume_guide.py" ]; then
    git mv training_resume_guide.py training/resume_guide.py
    echo "   âœ“ training_resume_guide.py â†’ training/resume_guide.py"
fi
echo ""

# Step 3: ç§»åŠ¨å·¥å…·è„šæœ¬
echo "ğŸ”§ Step 3: ç§»åŠ¨å·¥å…·è„šæœ¬åˆ° tools/..."
for file in check_training_backends.py diagnose_issues.py generate_hlbd_hardcore.py \
            monitor_all_trainings.py verify_hlbd_model.py visualize_training.py \
            demo_visualization.py test_vocab_size.py mascot_render_fused45.py; do
    if [ -f "$file" ]; then
        git mv "$file" tools/
        echo "   âœ“ $file â†’ tools/"
    fi
done
echo ""

# Step 4: ç§»åŠ¨æ•°æ®æ–‡ä»¶
echo "ğŸ“Š Step 4: ç§»åŠ¨æ•°æ®æ–‡ä»¶åˆ° data/..."
if [ -f "HLBD_Hardcore_Full.json" ]; then
    git mv HLBD_Hardcore_Full.json data/
    echo "   âœ“ HLBD_Hardcore_Full.json â†’ data/"
fi
echo ""

# Step 5: ç§»åŠ¨æ–‡æ¡£
echo "ğŸ“š Step 5: æ•´ç†æ–‡æ¡£åˆ° docs/..."
if [ -f "TRAINING_BACKENDS.md" ]; then
    git mv TRAINING_BACKENDS.md docs/
    echo "   âœ“ TRAINING_BACKENDS.md â†’ docs/"
fi

if [ -f "VISUALIZATION_GUIDE.md" ]; then
    git mv VISUALIZATION_GUIDE.md docs/
    echo "   âœ“ VISUALIZATION_GUIDE.md â†’ docs/"
fi

if [ -f "README_TEST.md" ]; then
    git mv README_TEST.md docs/testing/
    echo "   âœ“ README_TEST.md â†’ docs/testing/"
fi

if [ -f "æµ‹è¯•å·¥å…·ä½¿ç”¨æŒ‡å—.md" ]; then
    git mv æµ‹è¯•å·¥å…·ä½¿ç”¨æŒ‡å—.md docs/testing/
    echo "   âœ“ æµ‹è¯•å·¥å…·ä½¿ç”¨æŒ‡å—.md â†’ docs/testing/"
fi

if [ -f "command_verification_report.md" ]; then
    git mv command_verification_report.md docs/reports/
    echo "   âœ“ command_verification_report.md â†’ docs/reports/"
fi
echo ""

# Step 6: å½’æ¡£PRç›¸å…³æ–‡ä»¶
echo "ğŸ—„ï¸  Step 6: å½’æ¡£PRç›¸å…³æ–‡ä»¶åˆ° archived/pr/..."
for file in PR_DESCRIPTION.md PR_DESCRIPTION_FULL.md PULL_REQUEST.md CONFLICT_RESOLUTION.md; do
    if [ -f "$file" ]; then
        git mv "$file" archived/pr/
        echo "   âœ“ $file â†’ archived/pr/"
    fi
done
echo ""

# Step 7: ç§»åŠ¨æµ‹è¯•è„šæœ¬
echo "ğŸ§ª Step 7: ç§»åŠ¨æµ‹è¯•è„šæœ¬åˆ° scripts/testing/..."
if [ -f "test_all_commands.py" ]; then
    git mv test_all_commands.py scripts/testing/
    echo "   âœ“ test_all_commands.py â†’ scripts/testing/"
fi

for file in quick_test.sh quick_test.bat quick_test.ps1; do
    if [ -f "$file" ]; then
        git mv "$file" scripts/testing/
        echo "   âœ“ $file â†’ scripts/testing/"
    fi
done
echo ""

# Step 8: ç§»åŠ¨å®‰è£…è„šæœ¬
echo "ğŸ”¨ Step 8: ç§»åŠ¨å®‰è£…è„šæœ¬åˆ° scripts/setup/..."
for file in install_dependencies.sh fix_issues.sh; do
    if [ -f "$file" ]; then
        git mv "$file" scripts/setup/
        echo "   âœ“ $file â†’ scripts/setup/"
    fi
done
echo ""

# Step 9: åˆ›å»ºå„ç›®å½•çš„README
echo "ğŸ“ Step 9: åˆ›å»ºç›®å½•è¯´æ˜æ–‡æ¡£..."

cat > training/README.md << 'EOF'
# Training Scripts

This directory contains all training scripts for APT-Transformer.

## Available Training Backends

- `train.py` - Unified training launcher (supports all backends)
- `train_apt_playground.py` - APT Playground training
- `train_hlbd_playground.py` - HLBD Playground training
- `train_deepspeed.py` - DeepSpeed distributed training
- `train_azure_ml.py` - Azure ML cloud training
- `train_hf_trainer.py` - HuggingFace Trainer integration
- `train_control_experiment.py` - Control experiment training
- `resume_guide.py` - Training resume guide

## Quick Start

```bash
# List all available backends
python training/train.py --list-backends

# Train with specific backend
python training/train.py --backend playground --epochs 100
```

## Documentation

See [TRAINING_BACKENDS.md](../docs/TRAINING_BACKENDS.md) for detailed usage guide.
EOF

cat > tools/README.md << 'EOF'
# Tools and Utilities

This directory contains diagnostic, generation, and monitoring tools for APT-Transformer.

## Available Tools

### Diagnostic Tools
- `check_training_backends.py` - Code quality checker for training backends
- `diagnose_issues.py` - System diagnostic tool

### Data Generation
- `generate_hlbd_hardcore.py` - HLBD Hardcore dataset generator

### Monitoring & Visualization
- `monitor_all_trainings.py` - Multi-training monitor
- `visualize_training.py` - Real-time sci-fi style visualization
- `demo_visualization.py` - Visualization demo

### Model Verification
- `verify_hlbd_model.py` - HLBD model verification tool

### Other Utilities
- `test_vocab_size.py` - Vocabulary size tester
- `mascot_render_fused45.py` - Mascot rendering tool

## Usage Examples

```bash
# Check training backend code quality
python tools/check_training_backends.py

# Generate HLBD dataset
python tools/generate_hlbd_hardcore.py

# Verify trained model
python tools/verify_hlbd_model.py --model path/to/model.pt

# Real-time visualization
python tools/visualize_training.py --log-dir training_logs --mode realtime
```
EOF

cat > data/README.md << 'EOF'
# Data Directory

This directory contains datasets for APT-Transformer training.

## Available Datasets

- `HLBD_Hardcore_Full.json` - HLBD Hardcore dataset (575 samples)
  - 5 modules: geometry, arithmetic, zodiac, physics, reverse_english
  - Designed to prevent shortcut learning

## Dataset Format

All datasets use JSON format with the following structure:

```json
{
  "metadata": {
    "total_samples": 575,
    "modules": ["geometry", "arithmetic", "zodiac", "physics", "reverse_english"]
  },
  "data": {
    "module_name": [
      {"input": "...", "output": "..."},
      ...
    ]
  }
}
```

## Generating New Datasets

Use the dataset generation tool:

```bash
python tools/generate_hlbd_hardcore.py
```
EOF

cat > archived/README.md << 'EOF'
# Archived Files

This directory contains archived files that are no longer actively used but kept for reference.

## Structure

- `pr/` - Pull request descriptions and conflict resolutions

These files are kept for historical reference and should not be modified.
EOF

echo "   âœ… READMEæ–‡æ¡£åˆ›å»ºå®Œæˆ"
echo ""

echo "=" * 60
echo "âœ¨ æ•´ç†å®Œæˆï¼"
echo "=" * 60
echo ""
echo "ğŸ“Š ç»Ÿè®¡ï¼š"
echo "   - training/ : $(ls -1 training/*.py 2>/dev/null | wc -l) ä¸ªè®­ç»ƒè„šæœ¬"
echo "   - tools/    : $(ls -1 tools/*.py 2>/dev/null | wc -l) ä¸ªå·¥å…·è„šæœ¬"
echo "   - data/     : $(ls -1 data/*.json 2>/dev/null | wc -l) ä¸ªæ•°æ®æ–‡ä»¶"
echo "   - docs/     : $(find docs -name '*.md' 2>/dev/null | wc -l) ä¸ªæ–‡æ¡£"
echo "   - archived/ : $(find archived -type f 2>/dev/null | wc -l) ä¸ªå½’æ¡£æ–‡ä»¶"
echo ""
echo "âš ï¸  ä¸‹ä¸€æ­¥ï¼š"
echo "   1. è¿è¡Œ: bash update_paths.sh  # æ›´æ–°æ‰€æœ‰è·¯å¾„å¼•ç”¨"
echo "   2. æµ‹è¯•æ‰€æœ‰åŠŸèƒ½æ˜¯å¦æ­£å¸¸"
echo "   3. æäº¤æ›´æ”¹: git commit -m 'Reorganize project structure'"
echo ""
