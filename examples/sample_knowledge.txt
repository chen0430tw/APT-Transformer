# APT知识图谱示例数据
# 格式: 头实体<Tab>关系<Tab>尾实体
# 这是一个关于人工智能和机器学习的示例知识图谱

# 基础概念
人工智能	是	计算机科学的分支
人工智能	包括	机器学习
人工智能	包括	自然语言处理
人工智能	包括	计算机视觉
人工智能	应用于	自动驾驶
人工智能	应用于	医疗诊断

# 机器学习
机器学习	是	人工智能的子领域
机器学习	包括	监督学习
机器学习	包括	无监督学习
机器学习	包括	强化学习
机器学习	包括	深度学习

# 深度学习
深度学习	是	机器学习的分支
深度学习	基于	神经网络
深度学习	用于	图像识别
深度学习	用于	语音识别
深度学习	用于	自然语言处理

# 神经网络
神经网络	包括	卷积神经网络
神经网络	包括	循环神经网络
神经网络	包括	Transformer

# Transformer架构
Transformer	是	深度学习架构
Transformer	发明于	2017年
Transformer	基于	注意力机制
Transformer	用于	自然语言处理
Transformer	应用于	机器翻译

# 具体模型
BERT	是	预训练语言模型
BERT	基于	Transformer
BERT	用于	文本理解
BERT	发布于	2018年

GPT	是	预训练语言模型
GPT	基于	Transformer
GPT	用于	文本生成
GPT	发布于	2018年

APT	是	预训练语言模型
APT	基于	Transformer
APT	结合	先验知识
APT	用于	文本生成

# 应用领域
自然语言处理	包括	机器翻译
自然语言处理	包括	文本分类
自然语言处理	包括	情感分析
自然语言处理	包括	问答系统

计算机视觉	包括	图像分类
计算机视觉	包括	目标检测
计算机视觉	包括	语义分割

# 训练技术
监督学习	需要	标注数据
无监督学习	不需要	标注数据
强化学习	基于	奖励机制

预训练	用于	语言模型
微调	用于	下游任务

# 评估指标
准确率	用于	分类任务
BLEU	用于	机器翻译
F1分数	用于	信息检索
困惑度	用于	语言模型

# 研究机构
OpenAI	开发了	GPT
Google	开发了	BERT
Google	开发了	Transformer
Meta	开发了	LLaMA

# 数据集
ImageNet	用于	图像分类
COCO	用于	目标检测
GLUE	用于	自然语言理解
SQuAD	用于	问答系统
