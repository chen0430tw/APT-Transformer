#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å¯è§†åŒ–çŸ¥è¯†è’¸é¦ä½¿ç”¨ç¤ºä¾‹

å±•ç¤ºå¦‚ä½•ä½¿ç”¨å¯è§†åŒ–è’¸é¦æ’ä»¶è¿›è¡Œæ¨¡å‹è®­ç»ƒ
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader, TensorDataset
from transformers import AutoTokenizer

# å¯¼å…¥å¯è§†åŒ–è’¸é¦æ’ä»¶
import sys
sys.path.append('..')
from apt_model.plugins.visual_distillation_plugin import (
    VisualDistillationPlugin,
    quick_visual_distill
)


def create_dummy_data(num_samples=100, seq_len=32):
    """åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®ç”¨äºæ¼”ç¤º"""
    # éšæœºç”Ÿæˆè¾“å…¥ID
    input_ids = torch.randint(0, 50000, (num_samples, seq_len))

    # åˆ›å»ºæ•°æ®é›†
    dataset = TensorDataset(input_ids)
    dataloader = DataLoader(dataset, batch_size=8, shuffle=True)

    return dataloader


def create_dummy_models(vocab_size=50000, hidden_size=768):
    """åˆ›å»ºæ¨¡æ‹Ÿçš„æ•™å¸ˆå’Œå­¦ç”Ÿæ¨¡å‹"""

    class SimpleLanguageModel(nn.Module):
        def __init__(self, vocab_size, hidden_size):
            super().__init__()
            self.embedding = nn.Embedding(vocab_size, hidden_size)
            self.transformer = nn.TransformerEncoder(
                nn.TransformerEncoderLayer(hidden_size, nhead=8, batch_first=True),
                num_layers=6
            )
            self.output = nn.Linear(hidden_size, vocab_size)

        def forward(self, input_ids, **kwargs):
            x = self.embedding(input_ids)
            x = self.transformer(x)
            logits = self.output(x)

            # è¿”å›ç±»ä¼¼HuggingFaceçš„è¾“å‡º
            class Output:
                def __init__(self, logits):
                    self.logits = logits

            return Output(logits)

    # æ•™å¸ˆæ¨¡å‹ï¼ˆå¤§ï¼‰
    teacher_model = SimpleLanguageModel(vocab_size, hidden_size=768)

    # å­¦ç”Ÿæ¨¡å‹ï¼ˆå°ï¼‰
    student_model = SimpleLanguageModel(vocab_size, hidden_size=384)

    return teacher_model, student_model


# ==================== ç¤ºä¾‹1: åŸºç¡€ä½¿ç”¨ ====================

def example_basic():
    """ç¤ºä¾‹1: åŸºç¡€å¯è§†åŒ–è’¸é¦"""
    print("\n" + "="*70)
    print("ç¤ºä¾‹1: åŸºç¡€å¯è§†åŒ–è’¸é¦".center(70))
    print("="*70 + "\n")

    # å‡†å¤‡æ•°æ®
    dataloader = create_dummy_data(num_samples=50, seq_len=32)

    # å‡†å¤‡æ¨¡å‹
    teacher_model, student_model = create_dummy_models()

    # å‡†å¤‡tokenizerï¼ˆä½¿ç”¨HuggingFaceçš„tokenizerï¼‰
    try:
        tokenizer = AutoTokenizer.from_pretrained("gpt2")
    except:
        print("âš ï¸  æœªæ‰¾åˆ°GPT-2 tokenizerï¼Œä½¿ç”¨ç®€åŒ–ç‰ˆ")
        # ç®€åŒ–çš„tokenizer
        class DummyTokenizer:
            def decode(self, token_ids, **kwargs):
                return f"ç”Ÿæˆçš„æ–‡æœ¬ (tokens: {len(token_ids)})"

        tokenizer = DummyTokenizer()

    # é…ç½®
    config = {
        'temperature': 4.0,
        'alpha': 0.7,
        'beta': 0.3,
        'show_samples': True,
        'sample_frequency': 5,  # æ¯5ä¸ªbatchæ˜¾ç¤ºä¸€æ¬¡
        'max_text_length': 80,
    }

    # åˆ›å»ºæ’ä»¶
    plugin = VisualDistillationPlugin(config)

    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(student_model.parameters(), lr=1e-4)

    # å¼€å§‹è’¸é¦ï¼ˆåªè®­ç»ƒ1ä¸ªepochç”¨äºæ¼”ç¤ºï¼‰
    plugin.visual_distill_model(
        student_model=student_model,
        teacher_model=teacher_model,
        train_dataloader=dataloader,
        optimizer=optimizer,
        tokenizer=tokenizer,
        num_epochs=1,
        device='cpu'  # CPUæ¼”ç¤º
    )

    print("\nâœ… ç¤ºä¾‹1å®Œæˆï¼")


# ==================== ç¤ºä¾‹2: å¿«é€Ÿå¯åŠ¨ ====================

def example_quick_start():
    """ç¤ºä¾‹2: ä½¿ç”¨å¿«æ·å‡½æ•°"""
    print("\n" + "="*70)
    print("ç¤ºä¾‹2: å¿«é€Ÿå¯åŠ¨è’¸é¦".center(70))
    print("="*70 + "\n")

    # å‡†å¤‡
    dataloader = create_dummy_data(num_samples=30, seq_len=16)
    teacher_model, student_model = create_dummy_models()

    class DummyTokenizer:
        def decode(self, token_ids, **kwargs):
            return f"ç¤ºä¾‹æ–‡æœ¬ (é•¿åº¦: {len(token_ids)})"

    tokenizer = DummyTokenizer()

    # ä½¿ç”¨å¿«æ·å‡½æ•°
    quick_visual_distill(
        student_model=student_model,
        teacher_model=teacher_model,
        train_dataloader=dataloader,
        tokenizer=tokenizer,
        num_epochs=1,
        device='cpu'
    )

    print("\nâœ… ç¤ºä¾‹2å®Œæˆï¼")


# ==================== ç¤ºä¾‹3: è‡ªå®šä¹‰é…ç½® ====================

def example_custom_config():
    """ç¤ºä¾‹3: è‡ªå®šä¹‰é…ç½®"""
    print("\n" + "="*70)
    print("ç¤ºä¾‹3: è‡ªå®šä¹‰é…ç½®".center(70))
    print("="*70 + "\n")

    # è‡ªå®šä¹‰é…ç½®
    custom_config = {
        # è’¸é¦å‚æ•°
        'temperature': 6.0,  # æ›´é«˜çš„æ¸©åº¦ï¼Œæ›´å¹³æ»‘çš„åˆ†å¸ƒ
        'alpha': 0.8,        # æ›´é‡è§†è’¸é¦æŸå¤±
        'beta': 0.2,         # è¾ƒä½çš„çœŸå®æ ‡ç­¾æƒé‡

        # å¯è§†åŒ–å‚æ•°
        'show_samples': True,
        'show_diff': False,  # ä¸æ˜¾ç¤ºæ–‡æœ¬å·®å¼‚ï¼ˆç®€åŒ–è¾“å‡ºï¼‰
        'sample_frequency': 3,  # æ›´é¢‘ç¹åœ°æ˜¾ç¤ºæ ·æœ¬
        'max_text_length': 50,  # æ˜¾ç¤ºæ›´çŸ­çš„æ–‡æœ¬
    }

    # å‡†å¤‡
    dataloader = create_dummy_data(num_samples=20, seq_len=16)
    teacher_model, student_model = create_dummy_models()

    class DummyTokenizer:
        def decode(self, token_ids, **kwargs):
            topics = ['äº’è”ç½‘æŠ€æœ¯å‘å±•', 'äººå·¥æ™ºèƒ½åº”ç”¨', 'åŒ»ç–—å¥åº·åˆ›æ–°', 'æ•™è‚²æ”¹é©']
            import random
            return f"{random.choice(topics)}çš„ç›¸å…³å†…å®¹ä»‹ç»..."

    tokenizer = DummyTokenizer()

    # ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
    quick_visual_distill(
        student_model=student_model,
        teacher_model=teacher_model,
        train_dataloader=dataloader,
        tokenizer=tokenizer,
        config=custom_config,
        num_epochs=1,
        device='cpu'
    )

    print("\nâœ… ç¤ºä¾‹3å®Œæˆï¼")


# ==================== ç¤ºä¾‹4: é›†æˆåˆ°è®­ç»ƒæµç¨‹ ====================

def example_training_integration():
    """ç¤ºä¾‹4: é›†æˆåˆ°å®Œæ•´è®­ç»ƒæµç¨‹"""
    print("\n" + "="*70)
    print("ç¤ºä¾‹4: é›†æˆåˆ°è®­ç»ƒæµç¨‹".center(70))
    print("="*70 + "\n")

    print("ğŸ“š è¿™ä¸ªç¤ºä¾‹å±•ç¤ºå¦‚ä½•å°†å¯è§†åŒ–è’¸é¦é›†æˆåˆ°å®é™…è®­ç»ƒä¸­\n")

    # ä¼ªä»£ç ç¤ºä¾‹
    code_example = '''
# å®Œæ•´è®­ç»ƒæµç¨‹ç¤ºä¾‹

from apt_model.training.checkpoint import load_model
from apt_model.plugins.visual_distillation_plugin import quick_visual_distill

# 1. åŠ è½½æ•™å¸ˆæ¨¡å‹ï¼ˆå¤§æ¨¡å‹ï¼‰
teacher_model, tokenizer, config = load_model("apt_model_large")

# 2. åˆ›å»ºå­¦ç”Ÿæ¨¡å‹ï¼ˆå°æ¨¡å‹ï¼‰
student_model = create_student_model(config, compression_ratio=0.5)

# 3. å‡†å¤‡æ•°æ®
train_dataloader = get_training_dataloader()

# 4. å¯è§†åŒ–è’¸é¦è®­ç»ƒ
quick_visual_distill(
    student_model=student_model,
    teacher_model=teacher_model,
    train_dataloader=train_dataloader,
    tokenizer=tokenizer,
    num_epochs=5,
    device='cuda'
)

# 5. ä¿å­˜å­¦ç”Ÿæ¨¡å‹
save_model(student_model, "apt_model_distilled")

# 6. è¯„ä¼°
evaluate_model(student_model, test_dataloader)
    '''

    print(code_example)
    print("\nâœ… ç¤ºä¾‹4å®Œæˆï¼")


# ==================== ä¸»å‡½æ•° ====================

def main():
    """è¿è¡Œæ‰€æœ‰ç¤ºä¾‹"""
    print("\n" + "ğŸ¨"*35)
    print("å¯è§†åŒ–çŸ¥è¯†è’¸é¦æ’ä»¶ - ä½¿ç”¨ç¤ºä¾‹".center(70))
    print("ğŸ¨"*35 + "\n")

    print("æœ¬è„šæœ¬åŒ…å«4ä¸ªç¤ºä¾‹:")
    print("  1ï¸âƒ£  åŸºç¡€å¯è§†åŒ–è’¸é¦")
    print("  2ï¸âƒ£  å¿«é€Ÿå¯åŠ¨")
    print("  3ï¸âƒ£  è‡ªå®šä¹‰é…ç½®")
    print("  4ï¸âƒ£  é›†æˆåˆ°è®­ç»ƒæµç¨‹")
    print()

    choice = input("è¯·é€‰æ‹©è¦è¿è¡Œçš„ç¤ºä¾‹ (1-4, æˆ–æŒ‰Enterè¿è¡Œæ¼”ç¤º): ").strip()

    if choice == '1':
        example_basic()
    elif choice == '2':
        example_quick_start()
    elif choice == '3':
        example_custom_config()
    elif choice == '4':
        example_training_integration()
    else:
        print("\nè¿è¡Œå¿«é€Ÿæ¼”ç¤º...\n")
        # è¿è¡Œæ’ä»¶è‡ªå¸¦çš„æ¼”ç¤º
        from apt_model.plugins.visual_distillation_plugin import VisualDistillationPlugin

        config = {
            'temperature': 4.0,
            'alpha': 0.7,
            'beta': 0.3,
            'show_samples': True,
            'sample_frequency': 5,
            'max_text_length': 80,
        }

        plugin = VisualDistillationPlugin(config)

        plugin.print_header()
        plugin.print_epoch_header(1, 3)

        # æ¨¡æ‹Ÿæ ·æœ¬
        topics = ['äº’è”ç½‘', 'äººå·¥æ™ºèƒ½', 'åŒ»ç–—å¥åº·', 'åœ¨çº¿æ•™è‚²']

        for i in range(4):
            teacher_text = f"è¿™æ˜¯å…³äº{topics[i]}çš„è¯¦ç»†ä»‹ç»ï¼Œæ•™å¸ˆæ¨¡å‹æä¾›äº†æ·±å…¥çš„åˆ†æå’Œè§è§£..."
            student_text = f"å…³äº{topics[i]}çš„å­¦ä¹ ç¬”è®°ï¼Œå­¦ç”Ÿæ¨¡å‹æ­£åœ¨åŠªåŠ›ç†è§£å’Œå¸æ”¶çŸ¥è¯†..."

            laziness = 75 - i * 15
            loss = 2.5 - i * 0.4
            comment = plugin.generate_comment(laziness, loss)

            plugin.print_sample_comparison(
                epoch=1,
                batch_idx=i * 10,
                teacher_text=teacher_text,
                student_text=student_text,
                topic=topics[i],
                laziness=laziness,
                loss=loss,
                comment=comment
            )

        plugin.stats['avg_laziness'] = [75, 50, 30]
        plugin.stats['total_samples'] = 50

        plugin.print_epoch_summary(
            epoch=1,
            avg_loss=1.8,
            avg_laziness=55.0,
            topic_stats={'äº’è”ç½‘': 15, 'äººå·¥æ™ºèƒ½': 12, 'åŒ»ç–—å¥åº·': 10, 'åœ¨çº¿æ•™è‚²': 8}
        )

        plugin.print_final_summary()

    print("\n" + "="*70)
    print("ğŸ‰ æ‰€æœ‰ç¤ºä¾‹è¿è¡Œå®Œæˆï¼".center(70))
    print("="*70)
    print("\nğŸ’¡ æç¤º:")
    print("   - åœ¨å®é™…ä½¿ç”¨ä¸­ï¼Œæ›¿æ¢ä¸ºçœŸå®çš„æ¨¡å‹å’Œæ•°æ®")
    print("   - è°ƒæ•´ sample_frequency æ§åˆ¶æ˜¾ç¤ºé¢‘ç‡")
    print("   - è°ƒæ•´ temperature æ§åˆ¶è’¸é¦å¼ºåº¦")
    print("   - æŸ¥çœ‹ VISUAL_DISTILLATION_GUIDE.md äº†è§£æ›´å¤š\n")


if __name__ == "__main__":
    main()
