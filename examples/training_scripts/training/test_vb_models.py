#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
è™šæ‹ŸBlackwellæ¨¡å‹æµ‹è¯•è„šæœ¬

æµ‹è¯•APT-Transformerå„ç§æ¨¡å‹åœ¨è™šæ‹ŸBlackwellä¼˜åŒ–ä¸‹çš„æ€§èƒ½è¡¨ç°ã€‚

ç”¨æ³•:
    python test_vb_models.py --model apt --batch-size 4 --seq-len 128
    python test_vb_models.py --model all --epochs 5
"""

import sys
import os
import time
import argparse
import warnings
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

# æ£€æŸ¥ä¾èµ–
try:
    import torch
    import torch.nn as nn
    import numpy as np
    TORCH_AVAILABLE = True
except ImportError:
    print("âŒ é”™è¯¯: éœ€è¦å®‰è£…PyTorch")
    print("è¯·è¿è¡Œ: pip install torch")
    sys.exit(1)

from apt.perf.optimization import enable_vb_optimization, VB_TORCH_AVAILABLE

# æŠ‘åˆ¶è­¦å‘Š
warnings.filterwarnings('ignore')
os.environ['SUPPRESS_APT_WARNINGS'] = 'True'


def print_header(title):
    """æ‰“å°æ ‡é¢˜"""
    print("\n" + "="*70)
    print(f"{title:^70}")
    print("="*70)


def print_section(title):
    """æ‰“å°ç« èŠ‚"""
    print(f"\n{'â”€'*70}")
    print(f"  {title}")
    print(f"{'â”€'*70}")


class ModelTester:
    """æ¨¡å‹æµ‹è¯•å™¨"""

    def __init__(self, device='cpu', batch_size=4, seq_len=128, d_model=512):
        self.device = device
        self.batch_size = batch_size
        self.seq_len = seq_len
        self.d_model = d_model

        print(f"\nâš™ï¸  æµ‹è¯•é…ç½®:")
        print(f"   è®¾å¤‡: {device}")
        print(f"   Batchå¤§å°: {batch_size}")
        print(f"   åºåˆ—é•¿åº¦: {seq_len}")
        print(f"   æ¨¡å‹ç»´åº¦: {d_model}")

    def create_dummy_input(self):
        """åˆ›å»ºæµ‹è¯•è¾“å…¥"""
        return torch.randn(
            self.batch_size,
            self.seq_len,
            self.d_model,
            device=self.device
        )

    def benchmark_model(self, model_fn, model_name, num_iterations=10, enable_vb=False):
        """åŸºå‡†æµ‹è¯•å•ä¸ªæ¨¡å‹"""
        print_section(f"æµ‹è¯• {model_name} {'(è™šæ‹ŸBlackwell)' if enable_vb else '(åŸå§‹)'}")

        # åˆ›å»ºæ¨¡å‹
        try:
            model = model_fn()
            model = model.to(self.device)
            model.eval()
        except Exception as e:
            print(f"âŒ æ¨¡å‹åˆ›å»ºå¤±è´¥: {e}")
            return None

        # åº”ç”¨è™šæ‹ŸBlackwellä¼˜åŒ–
        if enable_vb:
            try:
                print("\nğŸš€ åº”ç”¨è™šæ‹ŸBlackwellä¼˜åŒ–...")
                model = enable_vb_optimization(
                    model,
                    mode='training',
                    enable_quantization=True,
                    replace_pattern='large'  # åªä¼˜åŒ–å¤§å‹å±‚
                )
            except Exception as e:
                print(f"âš ï¸  è™šæ‹ŸBlackwellä¼˜åŒ–å¤±è´¥: {e}")
                print("   ç»§ç»­ä½¿ç”¨åŸå§‹æ¨¡å‹...")

        # ç»Ÿè®¡å‚æ•°
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

        print(f"\nğŸ“Š æ¨¡å‹ä¿¡æ¯:")
        print(f"   æ€»å‚æ•°: {total_params:,}")
        print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        print(f"   æ¨¡å‹å¤§å°: {total_params * 4 / 1024 / 1024:.2f} MB (FP32)")

        # åˆ›å»ºè¾“å…¥
        x = self.create_dummy_input()

        # é¢„çƒ­
        print(f"\nğŸ”¥ é¢„çƒ­ {num_iterations//2} æ¬¡...")
        with torch.no_grad():
            for _ in range(num_iterations // 2):
                try:
                    _ = model(x)
                except Exception as e:
                    print(f"âŒ é¢„çƒ­å¤±è´¥: {e}")
                    return None

        # åŸºå‡†æµ‹è¯•
        print(f"\nâ±ï¸  è¿è¡Œ {num_iterations} æ¬¡è¿­ä»£...")
        times = []

        with torch.no_grad():
            for i in range(num_iterations):
                if self.device == 'cuda':
                    torch.cuda.synchronize()

                start = time.time()
                try:
                    output = model(x)
                except Exception as e:
                    print(f"âŒ è¿­ä»£ {i+1} å¤±è´¥: {e}")
                    return None

                if self.device == 'cuda':
                    torch.cuda.synchronize()

                elapsed = time.time() - start
                times.append(elapsed)

                if (i + 1) % (num_iterations // 2) == 0:
                    print(f"   è¿›åº¦: {i+1}/{num_iterations}")

        # ç»Ÿè®¡ç»“æœ
        times = np.array(times)
        mean_time = np.mean(times)
        std_time = np.std(times)
        median_time = np.median(times)

        result = {
            'model_name': model_name,
            'vb_enabled': enable_vb,
            'total_params': total_params,
            'mean_time': mean_time,
            'std_time': std_time,
            'median_time': median_time,
            'times': times,
            'output_shape': output.shape if output is not None else None
        }

        print(f"\nğŸ“ˆ æ€§èƒ½æŒ‡æ ‡:")
        print(f"   å¹³å‡æ—¶é—´: {mean_time*1000:.2f} Â± {std_time*1000:.2f} ms")
        print(f"   ä¸­ä½æ•°æ—¶é—´: {median_time*1000:.2f} ms")
        print(f"   ååé‡: {self.batch_size / mean_time:.2f} samples/sec")
        if output is not None:
            print(f"   è¾“å‡ºå½¢çŠ¶: {output.shape}")

        # å¦‚æœå¯ç”¨äº†VBï¼Œæ˜¾ç¤ºç»Ÿè®¡
        if enable_vb and hasattr(model, 'print_all_stats'):
            try:
                model.print_all_stats()
            except:
                pass

        return result

    def compare_models(self, model_fn, model_name, num_iterations=10):
        """å¯¹æ¯”åŸå§‹æ¨¡å‹å’ŒVBä¼˜åŒ–æ¨¡å‹"""
        print_header(f"å¯¹æ¯”æµ‹è¯•: {model_name}")

        # æµ‹è¯•åŸå§‹æ¨¡å‹
        result_orig = self.benchmark_model(
            model_fn, model_name, num_iterations, enable_vb=False
        )

        if result_orig is None:
            print(f"\nâš ï¸  {model_name} åŸå§‹æ¨¡å‹æµ‹è¯•å¤±è´¥ï¼Œè·³è¿‡VBæµ‹è¯•")
            return None

        # æµ‹è¯•VBä¼˜åŒ–æ¨¡å‹
        result_vb = self.benchmark_model(
            model_fn, model_name, num_iterations, enable_vb=True
        )

        if result_vb is None:
            print(f"\nâš ï¸  {model_name} VBä¼˜åŒ–æµ‹è¯•å¤±è´¥")
            return {'original': result_orig, 'vb': None}

        # è®¡ç®—åŠ é€Ÿæ¯”
        speedup = result_orig['mean_time'] / result_vb['mean_time']

        print_section("å¯¹æ¯”ç»“æœ")
        print(f"\nåŸå§‹æ¨¡å‹:")
        print(f"   å¹³å‡æ—¶é—´: {result_orig['mean_time']*1000:.2f} ms")
        print(f"   ååé‡: {self.batch_size / result_orig['mean_time']:.2f} samples/sec")

        print(f"\nVBä¼˜åŒ–æ¨¡å‹:")
        print(f"   å¹³å‡æ—¶é—´: {result_vb['mean_time']*1000:.2f} ms")
        print(f"   ååé‡: {self.batch_size / result_vb['mean_time']:.2f} samples/sec")

        print(f"\nğŸ¯ åŠ é€Ÿæ¯”: {speedup:.2f}Ã—")

        if speedup > 1.2:
            print(f"   âœ… æ˜¾è‘—åŠ é€Ÿ!")
        elif speedup > 1.0:
            print(f"   âœ“ è½»å¾®åŠ é€Ÿ")
        elif speedup > 0.8:
            print(f"   â‰ˆ æ€§èƒ½ç›¸å½“")
        else:
            print(f"   âš ï¸  æ€§èƒ½ä¸‹é™ (é¢„æœŸåœ¨CPUä¸Š)")

        return {
            'original': result_orig,
            'vb': result_vb,
            'speedup': speedup
        }


def test_apt_model(tester):
    """æµ‹è¯•APTæ¨¡å‹"""
    from apt.core.modeling.apt_model import APTLargeModel
    from apt.core.config.apt_config import APTConfig

    def create_model():
        config = APTConfig(
            d_model=tester.d_model,
            n_heads=8,
            n_layers=6,
            d_ff=tester.d_model * 4,
            vocab_size=30000,
            max_seq_length=tester.seq_len
        )
        return APTLargeModel(config)

    return tester.compare_models(create_model, "APTæ¨¡å‹")


def test_gpt5_model(tester):
    """æµ‹è¯•GPT-5æ¨¡å‹"""
    try:
        from apt.core.modeling.gpt5_model import GPT5Model, GPT5Config

        def create_model():
            config = GPT5Config(
                d_model=tester.d_model,
                n_heads=8,
                n_layers=6,
                d_ff=tester.d_model * 4,
                vocab_size=30000,
                max_seq_length=tester.seq_len
            )
            return GPT5Model(config)

        return tester.compare_models(create_model, "GPT-5æ¨¡å‹")
    except Exception as e:
        print(f"âš ï¸  GPT-5æ¨¡å‹ä¸å¯ç”¨: {e}")
        return None


def test_claude4_model(tester):
    """æµ‹è¯•Claude4æ¨¡å‹"""
    try:
        from apt.core.modeling.claude4_model import Claude4Model, Claude4Config

        def create_model():
            config = Claude4Config(
                d_model=tester.d_model,
                n_heads=8,
                n_layers=6,
                d_ff=tester.d_model * 4,
                vocab_size=30000,
                max_seq_length=tester.seq_len
            )
            return Claude4Model(config)

        return tester.compare_models(create_model, "Claude4æ¨¡å‹")
    except Exception as e:
        print(f"âš ï¸  Claude4æ¨¡å‹ä¸å¯ç”¨: {e}")
        return None


def test_multimodal_model(tester):
    """æµ‹è¯•å¤šæ¨¡æ€æ¨¡å‹"""
    try:
        from apt.core.modeling.multimodal_model import MultimodalAPTModel
        from apt.core.config.multimodal_config import MultimodalConfig

        def create_model():
            config = MultimodalConfig(
                d_model=tester.d_model,
                n_heads=8,
                n_layers=6,
                d_ff=tester.d_model * 4,
                vocab_size=30000,
                max_seq_length=tester.seq_len
            )
            return MultimodalAPTModel(config)

        return tester.compare_models(create_model, "å¤šæ¨¡æ€æ¨¡å‹")
    except Exception as e:
        print(f"âš ï¸  å¤šæ¨¡æ€æ¨¡å‹ä¸å¯ç”¨: {e}")
        return None


def generate_report(all_results):
    """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
    print_header("æµ‹è¯•æŠ¥å‘Šæ±‡æ€»")

    successful_tests = {k: v for k, v in all_results.items() if v is not None}

    if not successful_tests:
        print("\nâŒ æ²¡æœ‰æˆåŠŸçš„æµ‹è¯•")
        return

    print(f"\nâœ… æˆåŠŸæµ‹è¯•: {len(successful_tests)}/{len(all_results)}")
    print(f"\n{'æ¨¡å‹':<20} {'åŸå§‹(ms)':<15} {'VB(ms)':<15} {'åŠ é€Ÿæ¯”':<10} {'çŠ¶æ€'}")
    print("â”€" * 70)

    for model_name, result in successful_tests.items():
        if result is None or 'original' not in result:
            continue

        orig_time = result['original']['mean_time'] * 1000

        if result.get('vb') is not None:
            vb_time = result['vb']['mean_time'] * 1000
            speedup = result.get('speedup', 0)

            if speedup > 1.2:
                status = "âœ… æ˜¾è‘—åŠ é€Ÿ"
            elif speedup > 1.0:
                status = "âœ“ è½»å¾®åŠ é€Ÿ"
            elif speedup > 0.8:
                status = "â‰ˆ æ€§èƒ½ç›¸å½“"
            else:
                status = "âš ï¸ æ€§èƒ½ä¸‹é™"

            print(f"{model_name:<20} {orig_time:<15.2f} {vb_time:<15.2f} {speedup:<10.2f} {status}")
        else:
            print(f"{model_name:<20} {orig_time:<15.2f} {'N/A':<15} {'N/A':<10} âŒ VBå¤±è´¥")

    print("\n" + "="*70)
    print("è¯´æ˜:")
    print("  - åœ¨CPUç¯å¢ƒä¸‹ï¼ŒVBä¼˜åŒ–å¯èƒ½å› å¼€é”€>æ”¶ç›Šè€Œå˜æ…¢")
    print("  - åœ¨GPUç¯å¢ƒä¸‹ï¼Œé¢„æœŸæœ‰2-4Ã—çš„åŠ é€Ÿæ•ˆæœ")
    print("  - å»ºè®®åœ¨GPUä¸Šè¿è¡Œä»¥è·å¾—æœ€ä½³æ€§èƒ½")
    print("="*70)


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è™šæ‹ŸBlackwellæ¨¡å‹æ€§èƒ½æµ‹è¯•')
    parser.add_argument('--model', type=str, default='apt',
                      choices=['apt', 'gpt5', 'claude4', 'multimodal', 'all'],
                      help='è¦æµ‹è¯•çš„æ¨¡å‹')
    parser.add_argument('--batch-size', type=int, default=4,
                      help='æ‰¹é‡å¤§å°')
    parser.add_argument('--seq-len', type=int, default=128,
                      help='åºåˆ—é•¿åº¦')
    parser.add_argument('--d-model', type=int, default=512,
                      help='æ¨¡å‹ç»´åº¦')
    parser.add_argument('--iterations', type=int, default=10,
                      help='æµ‹è¯•è¿­ä»£æ¬¡æ•°')
    parser.add_argument('--device', type=str, default='auto',
                      choices=['auto', 'cpu', 'cuda'],
                      help='è¿è¡Œè®¾å¤‡')

    args = parser.parse_args()

    # ç¡®å®šè®¾å¤‡
    if args.device == 'auto':
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
    else:
        device = args.device

    if device == 'cuda' and not torch.cuda.is_available():
        print("âš ï¸  CUDAä¸å¯ç”¨ï¼Œåˆ‡æ¢åˆ°CPU")
        device = 'cpu'

    # æ‰“å°æ¬¢è¿ä¿¡æ¯
    print("\n" + "â•”" + "="*68 + "â•—")
    print("â•‘" + " "*15 + "è™šæ‹ŸBlackwellæ¨¡å‹æ€§èƒ½æµ‹è¯•" + " "*17 + "â•‘")
    print("â•š" + "="*68 + "â•")

    if not VB_TORCH_AVAILABLE:
        print("\nâš ï¸  è­¦å‘Š: PyTorché›†æˆæ¨¡å—ä¸å¯ç”¨")
        print("   å°†åªæ˜¾ç¤ºæ¨¡å‹ä¿¡æ¯ï¼Œæ— æ³•è¿›è¡ŒVBä¼˜åŒ–")

    # åˆ›å»ºæµ‹è¯•å™¨
    tester = ModelTester(
        device=device,
        batch_size=args.batch_size,
        seq_len=args.seq_len,
        d_model=args.d_model
    )

    # è¿è¡Œæµ‹è¯•
    all_results = {}

    if args.model == 'all':
        models_to_test = ['apt', 'gpt5', 'claude4', 'multimodal']
    else:
        models_to_test = [args.model]

    for model_name in models_to_test:
        try:
            if model_name == 'apt':
                result = test_apt_model(tester)
            elif model_name == 'gpt5':
                result = test_gpt5_model(tester)
            elif model_name == 'claude4':
                result = test_claude4_model(tester)
            elif model_name == 'multimodal':
                result = test_multimodal_model(tester)
            else:
                result = None

            all_results[model_name] = result

        except Exception as e:
            print(f"\nâŒ {model_name} æµ‹è¯•å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            all_results[model_name] = None

    # ç”ŸæˆæŠ¥å‘Š
    generate_report(all_results)

    print("\nğŸ‰ æµ‹è¯•å®Œæˆ!")
    print("\nğŸ’¡ æç¤º:")
    print("   - åœ¨GPUç¯å¢ƒè¿è¡Œå¯è·å¾—æ˜¾è‘—åŠ é€Ÿ")
    print("   - ä½¿ç”¨ --device cuda æŒ‡å®šGPUè®¾å¤‡")
    print("   - ä½¿ç”¨ --model all æµ‹è¯•æ‰€æœ‰æ¨¡å‹")
    print("   - è°ƒæ•´ --batch-size å’Œ --seq-len æµ‹è¯•ä¸åŒåœºæ™¯")


if __name__ == "__main__":
    main()
