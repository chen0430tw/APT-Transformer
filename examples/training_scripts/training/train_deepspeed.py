#!/usr/bin/env python3
"""
APTæ¨¡å‹ + DeepSpeedé›†æˆ
æ”¯æŒZeRO-2å’ŒZeRO-3ä¼˜åŒ–ï¼Œå®ç°è¶…å¤§è§„æ¨¡æ¨¡å‹è®­ç»ƒ

ç‰¹æ€§:
- ZeRO-2: åˆ†å¸ƒå¼ä¼˜åŒ–å™¨å’Œæ¢¯åº¦ (é€‚åˆå¤šGPU)
- ZeRO-3: åˆ†å¸ƒå¼å‚æ•° (é€‚åˆ100B+æ¨¡å‹)
- CPUå¸è½½ä¼˜åŒ–
- æ··åˆç²¾åº¦è®­ç»ƒ (FP16/BF16)
- æ¢¯åº¦ç´¯ç§¯
"""

import os
import sys
import json
import argparse
from pathlib import Path

import torch
import torch.nn as nn
from torch.utils.data import DataLoader

try:
    import deepspeed
    from deepspeed.ops.adam import DeepSpeedCPUAdam
    DEEPSPEED_AVAILABLE = True
except ImportError:
    DEEPSPEED_AVAILABLE = False
    print("âš ï¸  DeepSpeedæœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install deepspeed")

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from apt.apt_model.modeling.apt_model import APTModel, APTModelConfiguration
from train_hlbd_playground import DynamicTagTokenizer, HLBDPlaygroundDataset, collate_fn


# ============================================================================
# DeepSpeedé…ç½®ç”Ÿæˆ
# ============================================================================

def create_deepspeed_config(
    stage: int = 2,
    train_batch_size: int = 64,
    gradient_accumulation_steps: int = 4,
    enable_fp16: bool = True,
    enable_bf16: bool = False,
    cpu_offload: bool = False,
    learning_rate: float = 3e-4,
    weight_decay: float = 0.01
):
    """
    ç”ŸæˆDeepSpeedé…ç½®

    Args:
        stage: ZeROä¼˜åŒ–é˜¶æ®µ (1, 2, 3)
        train_batch_size: æ€»batchå¤§å°
        gradient_accumulation_steps: æ¢¯åº¦ç´¯ç§¯æ­¥æ•°
        enable_fp16: å¯ç”¨FP16æ··åˆç²¾åº¦
        enable_bf16: å¯ç”¨BF16æ··åˆç²¾åº¦ (éœ€è¦Ampere+ GPU)
        cpu_offload: å¯ç”¨CPUå¸è½½
        learning_rate: å­¦ä¹ ç‡
        weight_decay: æƒé‡è¡°å‡
    """

    config = {
        "train_batch_size": train_batch_size,
        "gradient_accumulation_steps": gradient_accumulation_steps,
        "gradient_clipping": 1.0,

        "optimizer": {
            "type": "AdamW",
            "params": {
                "lr": learning_rate,
                "betas": [0.9, 0.999],
                "eps": 1e-8,
                "weight_decay": weight_decay
            }
        },

        "scheduler": {
            "type": "WarmupDecayLR",
            "params": {
                "warmup_min_lr": 0,
                "warmup_max_lr": learning_rate,
                "warmup_num_steps": 500,
                "total_num_steps": 10000
            }
        },

        "zero_optimization": {
            "stage": stage,
            "contiguous_gradients": True,
            "overlap_comm": True,
            "reduce_scatter": True,
            "reduce_bucket_size": 2e8,
            "allgather_bucket_size": 2e8
        },

        "steps_per_print": 100,
        "wall_clock_breakdown": False
    }

    # ZeRO-2/3 ä¼˜åŒ–å™¨å¸è½½
    if stage >= 2 and cpu_offload:
        config["zero_optimization"]["offload_optimizer"] = {
            "device": "cpu",
            "pin_memory": True
        }

    # ZeRO-3 å‚æ•°å¸è½½
    if stage == 3:
        config["zero_optimization"]["stage3_prefetch_bucket_size"] = 5e7
        config["zero_optimization"]["stage3_param_persistence_threshold"] = 1e6

        if cpu_offload:
            config["zero_optimization"]["offload_param"] = {
                "device": "cpu",
                "pin_memory": True
            }

    # æ··åˆç²¾åº¦é…ç½®
    if enable_fp16:
        config["fp16"] = {
            "enabled": True,
            "loss_scale": 0,
            "loss_scale_window": 1000,
            "hysteresis": 2,
            "min_loss_scale": 1,
            "initial_scale_power": 16
        }
    elif enable_bf16:
        config["bf16"] = {
            "enabled": True
        }

    return config


# ============================================================================
# DeepSpeedè®­ç»ƒå™¨
# ============================================================================

class DeepSpeedAPTTrainer:
    """APTæ¨¡å‹DeepSpeedè®­ç»ƒå™¨"""

    def __init__(
        self,
        model: APTModel,
        train_loader: DataLoader,
        tokenizer: DynamicTagTokenizer,
        ds_config: dict,
        device: str = 'cuda'
    ):
        """
        Args:
            model: APTæ¨¡å‹
            train_loader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
            tokenizer: Tokenizer
            ds_config: DeepSpeedé…ç½®å­—å…¸
            device: è®¾å¤‡ï¼ˆDeepSpeedä¼šè‡ªåŠ¨å¤„ç†ï¼‰
        """
        if not DEEPSPEED_AVAILABLE:
            raise ImportError("DeepSpeedæœªå®‰è£…")

        self.train_loader = train_loader
        self.tokenizer = tokenizer
        self.device = device

        # ä½¿ç”¨DeepSpeedåˆå§‹åŒ–æ¨¡å‹
        self.model_engine, self.optimizer, _, self.lr_scheduler = deepspeed.initialize(
            model=model,
            config=ds_config
        )

        # æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss(ignore_index=-100)

        # ç»Ÿè®¡
        self.losses = []
        self.global_step = 0

    def train_epoch(self, epoch: int):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model_engine.train()
        total_loss = 0
        num_batches = 0

        for batch_idx, batch in enumerate(self.train_loader):
            # DeepSpeedè‡ªåŠ¨å¤„ç†è®¾å¤‡ç§»åŠ¨
            input_ids = batch['input_ids'].to(self.model_engine.device)
            labels = batch['labels'].to(self.model_engine.device)

            # Forward
            outputs = self.model_engine(input_ids)

            # è®¡ç®—æŸå¤±
            loss = self.criterion(
                outputs.view(-1, outputs.size(-1)),
                labels.view(-1)
            )

            # DeepSpeed backward
            self.model_engine.backward(loss)

            # DeepSpeed step (è‡ªåŠ¨å¤„ç†æ¢¯åº¦ç´¯ç§¯)
            self.model_engine.step()

            self.global_step += 1
            total_loss += loss.item()
            num_batches += 1

            # æ‰“å°è¿›åº¦
            if batch_idx % 100 == 0:
                current_lr = self.optimizer.param_groups[0]['lr']
                print(f"   Batch {batch_idx}/{len(self.train_loader)} | "
                      f"Loss: {loss.item():.4f} | "
                      f"LR: {current_lr:.6f} | "
                      f"Step: {self.global_step}")

        avg_loss = total_loss / num_batches
        self.losses.append(avg_loss)

        return avg_loss

    def save_checkpoint(self, save_dir: str, epoch: int):
        """ä¿å­˜DeepSpeed checkpoint"""
        save_path = Path(save_dir) / f"checkpoint_epoch_{epoch}"

        # DeepSpeedä¿å­˜
        self.model_engine.save_checkpoint(str(save_path))

        # é¢å¤–ä¿å­˜tokenizer
        tokenizer_path = save_path / "tokenizer_state.json"
        with open(tokenizer_path, 'w') as f:
            json.dump({
                'char_to_id': self.tokenizer.char_to_id,
                'id_to_char': {str(k): v for k, v in self.tokenizer.id_to_char.items()},
                'next_id': self.tokenizer.next_id,
                'vocab_size': self.tokenizer.vocab_size
            }, f, ensure_ascii=False, indent=2)

        print(f"âœ… DeepSpeed checkpointå·²ä¿å­˜: {save_path}")

    def load_checkpoint(self, load_dir: str):
        """åŠ è½½DeepSpeed checkpoint"""
        load_path = Path(load_dir)

        # DeepSpeedåŠ è½½
        _, client_state = self.model_engine.load_checkpoint(str(load_path))

        # åŠ è½½tokenizer
        tokenizer_path = load_path / "tokenizer_state.json"
        if tokenizer_path.exists():
            with open(tokenizer_path) as f:
                state = json.load(f)
                self.tokenizer.char_to_id = state['char_to_id']
                self.tokenizer.id_to_char = {int(k): v for k, v in state['id_to_char'].items()}
                self.tokenizer.next_id = state['next_id']
                self.tokenizer.vocab_size = state['vocab_size']

        print(f"âœ… DeepSpeed checkpointå·²åŠ è½½: {load_path}")


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description='APT + DeepSpeedè®­ç»ƒ')

    # æ•°æ®å’Œæ¨¡å‹å‚æ•°
    parser.add_argument('--dataset', type=str, default='../data/HLBD_Hardcore_Full.json',
                       help='HLBDæ•°æ®é›†è·¯å¾„')
    parser.add_argument('--epochs', type=int, default=100,
                       help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--save-dir', type=str, default='deepspeed_output',
                       help='ä¿å­˜ç›®å½•')

    # DeepSpeedå‚æ•°
    parser.add_argument('--deepspeed-config', type=str, default=None,
                       help='DeepSpeedé…ç½®æ–‡ä»¶è·¯å¾„ (å¦‚æœæä¾›ï¼Œå¿½ç•¥å…¶ä»–DSå‚æ•°)')
    parser.add_argument('--zero-stage', type=int, default=2, choices=[1, 2, 3],
                       help='ZeROä¼˜åŒ–é˜¶æ®µ')
    parser.add_argument('--train-batch-size', type=int, default=64,
                       help='æ€»è®­ç»ƒbatchå¤§å°')
    parser.add_argument('--gradient-accumulation', type=int, default=4,
                       help='æ¢¯åº¦ç´¯ç§¯æ­¥æ•°')
    parser.add_argument('--cpu-offload', action='store_true',
                       help='å¯ç”¨CPUå¸è½½')
    parser.add_argument('--fp16', action='store_true',
                       help='å¯ç”¨FP16æ··åˆç²¾åº¦')
    parser.add_argument('--bf16', action='store_true',
                       help='å¯ç”¨BF16æ··åˆç²¾åº¦')

    # æ¨¡å‹å‚æ•°
    parser.add_argument('--d-model', type=int, default=256,
                       help='æ¨¡å‹ç»´åº¦')
    parser.add_argument('--n-layers', type=int, default=6,
                       help='å±‚æ•°')
    parser.add_argument('--n-heads', type=int, default=8,
                       help='æ³¨æ„åŠ›å¤´æ•°')

    # DeepSpeedéœ€è¦çš„å‚æ•°
    parser.add_argument('--local_rank', type=int, default=-1,
                       help='DeepSpeedåˆ†å¸ƒå¼è®­ç»ƒlocal rank')

    args = parser.parse_args()

    # DeepSpeedåˆå§‹åŒ–
    deepspeed.init_distributed()

    # è·å–æˆ–åˆ›å»ºDeepSpeedé…ç½®
    if args.deepspeed_config and Path(args.deepspeed_config).exists():
        print(f"ğŸ“„ åŠ è½½DeepSpeedé…ç½®: {args.deepspeed_config}")
        with open(args.deepspeed_config) as f:
            ds_config = json.load(f)
    else:
        print(f"ğŸ”§ ç”ŸæˆDeepSpeedé…ç½® (ZeRO-{args.zero_stage})")
        ds_config = create_deepspeed_config(
            stage=args.zero_stage,
            train_batch_size=args.train_batch_size,
            gradient_accumulation_steps=args.gradient_accumulation,
            enable_fp16=args.fp16,
            enable_bf16=args.bf16,
            cpu_offload=args.cpu_offload
        )

        # ä¿å­˜é…ç½®ä¾›å‚è€ƒ
        config_path = Path(args.save_dir) / "deepspeed_config.json"
        config_path.parent.mkdir(parents=True, exist_ok=True)
        with open(config_path, 'w') as f:
            json.dump(ds_config, f, indent=2)
        print(f"   é…ç½®å·²ä¿å­˜: {config_path}")

    # æœ¬åœ°rank
    local_rank = args.local_rank if args.local_rank != -1 else 0

    if local_rank == 0:
        print(f"\nğŸš€ APT + DeepSpeedè®­ç»ƒ")
        print(f"   ZeROé˜¶æ®µ: {args.zero_stage}")
        print(f"   Batchå¤§å°: {args.train_batch_size}")
        print(f"   æ¢¯åº¦ç´¯ç§¯: {args.gradient_accumulation}")
        print(f"   æ··åˆç²¾åº¦: {'FP16' if args.fp16 else 'BF16' if args.bf16 else 'FP32'}")
        print(f"   CPUå¸è½½: {args.cpu_offload}")

    # Tokenizer
    if local_rank == 0:
        print(f"\nğŸ”¤ åˆå§‹åŒ–Tokenizer...")
    tokenizer = DynamicTagTokenizer(vocab_size=5000)

    # æ•°æ®é›†
    dataset = HLBDPlaygroundDataset(args.dataset, tokenizer)

    # DataLoader
    train_loader = DataLoader(
        dataset,
        batch_size=args.train_batch_size // args.gradient_accumulation,
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=0
    )

    # æ¨¡å‹
    if local_rank == 0:
        print(f"\nğŸ—ï¸  æ„å»ºAPTæ¨¡å‹...")

    model_config = APTModelConfiguration(
        vocab_size=tokenizer.vocab_size,
        d_model=args.d_model,
        n_heads=args.n_heads,
        num_encoder_layers=args.n_layers,
        num_decoder_layers=args.n_layers,
        d_ff=args.d_model * 4,
        max_seq_len=256,
        dropout=0.1,
        use_dbc_dac=True
    )

    model = APTModel(model_config)

    if local_rank == 0:
        total_params = sum(p.numel() for p in model.parameters())
        print(f"   æ€»å‚æ•°: {total_params:,}")

    # è®­ç»ƒå™¨
    trainer = DeepSpeedAPTTrainer(
        model=model,
        train_loader=train_loader,
        tokenizer=tokenizer,
        ds_config=ds_config
    )

    # è®­ç»ƒå¾ªç¯
    if local_rank == 0:
        print("\n" + "=" * 60)
        print("ğŸš€ å¼€å§‹DeepSpeedè®­ç»ƒ")
        print("=" * 60)

    for epoch in range(args.epochs):
        if local_rank == 0:
            print(f"\nğŸ“ Epoch {epoch + 1}/{args.epochs}")

        # è®­ç»ƒ
        avg_loss = trainer.train_epoch(epoch)

        if local_rank == 0:
            print(f"   å¹³å‡Loss: {avg_loss:.4f}")

            # å®šæœŸä¿å­˜
            if (epoch + 1) % 25 == 0:
                trainer.save_checkpoint(args.save_dir, epoch + 1)

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    if local_rank == 0:
        trainer.save_checkpoint(args.save_dir, args.epochs)
        print("\n" + "=" * 60)
        print("âœ¨ DeepSpeedè®­ç»ƒå®Œæˆï¼")
        print("=" * 60)


if __name__ == "__main__":
    main()
