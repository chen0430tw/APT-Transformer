#!/usr/bin/env python3
"""
APTæ¨¡å‹ + Azure MLé›†æˆ
æ”¯æŒäº‘ç«¯åˆ†å¸ƒå¼è®­ç»ƒã€MLflowå®éªŒè·Ÿè¸ªã€è‡ªåŠ¨è¶…å‚æ•°è°ƒä¼˜

ç‰¹æ€§:
- Azure MLè®¡ç®—é›†ç¾¤è‡ªåŠ¨ç®¡ç†
- MLflowå®éªŒè·Ÿè¸ªå’Œæ¨¡å‹æ³¨å†Œ
- åˆ†å¸ƒå¼è®­ç»ƒï¼ˆPyTorch DDPï¼‰
- è¶…å‚æ•°æ‰«æï¼ˆSweep jobsï¼‰
- äº‘ç«¯checkpointç®¡ç†
- TensorBoardé›†æˆ

ä½¿ç”¨å‰å‡†å¤‡:
1. å®‰è£…Azure ML SDK: pip install azure-ai-ml mlflow azureml-mlflow
2. é…ç½®Azureå‡­è¯: az login
3. åˆ›å»ºworkspaceé…ç½®æ–‡ä»¶
"""

import os
import sys
import json
import argparse
from pathlib import Path
from typing import Optional

try:
    from azure.ai.ml import MLClient, command, Input
    from azure.ai.ml.entities import Environment, AmlCompute
    from azure.identity import DefaultAzureCredential
    from azure.ai.ml.sweep import Choice, Uniform
    import mlflow
    import mlflow.pytorch
    AZURE_ML_AVAILABLE = True
except ImportError:
    AZURE_ML_AVAILABLE = False
    print("âš ï¸  Azure ML SDKæœªå®‰è£…ï¼Œè¯·è¿è¡Œ:")
    print("   pip install azure-ai-ml mlflow azureml-mlflow")

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))


# ============================================================================
# Azure MLé…ç½®
# ============================================================================

class AzureMLConfig:
    """Azure MLé…ç½®ç±»"""

    def __init__(
        self,
        subscription_id: str,
        resource_group: str,
        workspace_name: str,
        compute_name: str = "gpu-cluster",
        experiment_name: str = "apt-training"
    ):
        self.subscription_id = subscription_id
        self.resource_group = resource_group
        self.workspace_name = workspace_name
        self.compute_name = compute_name
        self.experiment_name = experiment_name


def create_ml_client(config: AzureMLConfig) -> MLClient:
    """åˆ›å»ºAzure MLå®¢æˆ·ç«¯"""
    credential = DefaultAzureCredential()

    ml_client = MLClient(
        credential=credential,
        subscription_id=config.subscription_id,
        resource_group_name=config.resource_group,
        workspace_name=config.workspace_name
    )

    print(f"âœ… è¿æ¥åˆ°Azure ML Workspace: {config.workspace_name}")
    return ml_client


def create_or_get_compute(ml_client: MLClient, compute_name: str, vm_size: str = "Standard_NC6s_v3"):
    """åˆ›å»ºæˆ–è·å–è®¡ç®—é›†ç¾¤"""
    try:
        compute = ml_client.compute.get(compute_name)
        print(f"âœ… ä½¿ç”¨ç°æœ‰è®¡ç®—é›†ç¾¤: {compute_name}")
        return compute
    except Exception:
        print(f"ğŸ”§ åˆ›å»ºæ–°è®¡ç®—é›†ç¾¤: {compute_name} ({vm_size})")

        compute_config = AmlCompute(
            name=compute_name,
            type="amlcompute",
            size=vm_size,
            min_instances=0,
            max_instances=4,
            idle_time_before_scale_down=300
        )

        compute = ml_client.compute.begin_create_or_update(compute_config).result()
        print(f"âœ… è®¡ç®—é›†ç¾¤å·²åˆ›å»º")
        return compute


def create_environment(ml_client: MLClient, environment_name: str = "apt-training-env"):
    """åˆ›å»ºè®­ç»ƒç¯å¢ƒ"""

    # Condaä¾èµ–é…ç½®
    conda_yaml = """
name: apt-training
channels:
  - pytorch
  - conda-forge
  - defaults
dependencies:
  - python=3.9
  - pytorch::pytorch>=2.0.0
  - pytorch::torchvision
  - pytorch::torchaudio
  - pip
  - pip:
    - mlflow
    - azureml-mlflow
    - numpy
    - matplotlib
"""

    # DockeråŸºç¡€é•œåƒ
    dockerfile = """
FROM mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.8-cudnn8-ubuntu22.04

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /workspace

# å¤åˆ¶condaé…ç½®
COPY conda.yaml /workspace/conda.yaml

# å®‰è£…condaç¯å¢ƒ
RUN conda env create -f conda.yaml && conda clean -a -y
"""

    env = Environment(
        name=environment_name,
        description="APTæ¨¡å‹è®­ç»ƒç¯å¢ƒ (PyTorch + CUDA)",
        conda_file=conda_yaml,
        image="mcr.microsoft.com/azureml/openmpi4.1.0-cuda11.8-cudnn8-ubuntu22.04"
    )

    env = ml_client.environments.create_or_update(env)
    print(f"âœ… ç¯å¢ƒå·²åˆ›å»º: {environment_name}")
    return env


# ============================================================================
# Azure MLè®­ç»ƒè„šæœ¬ç”Ÿæˆå™¨
# ============================================================================

def generate_training_script() -> str:
    """ç”ŸæˆAzure MLè®­ç»ƒè„šæœ¬

    è¿™ä¸ªè„šæœ¬å°†åœ¨Azure MLè®¡ç®—é›†ç¾¤ä¸Šæ‰§è¡Œ
    """

    script = '''#!/usr/bin/env python3
"""
Azure MLè®­ç»ƒè„šæœ¬
åœ¨Azureè®¡ç®—é›†ç¾¤ä¸Šæ‰§è¡Œ
"""

import os
import sys
import json
import argparse
from pathlib import Path

import torch
import torch.nn as nn
from torch.utils.data import DataLoader
from torch.cuda.amp import autocast, GradScaler
import mlflow
import mlflow.pytorch

# å¯¼å…¥APTæ¨¡å‹
from apt_model.modeling.apt_model import APTModel, APTModelConfiguration
from train_hlbd_playground import DynamicTagTokenizer, HLBDPlaygroundDataset, collate_fn


def train_with_mlflow(args):
    """å¸¦MLflowè·Ÿè¸ªçš„è®­ç»ƒå‡½æ•°"""

    # MLflowè®¾ç½®
    mlflow.set_experiment(args.experiment_name)

    with mlflow.start_run(run_name=args.run_name):
        # è®°å½•è¶…å‚æ•°
        mlflow.log_params({
            "d_model": args.d_model,
            "n_layers": args.n_layers,
            "n_heads": args.n_heads,
            "batch_size": args.batch_size,
            "learning_rate": args.learning_rate,
            "epochs": args.epochs,
            "weight_decay": args.weight_decay
        })

        # è®¾å¤‡
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")
        mlflow.log_param("device", device)

        # Tokenizer
        tokenizer = DynamicTagTokenizer(vocab_size=5000)

        # æ•°æ®é›†
        dataset = HLBDPlaygroundDataset(args.dataset, tokenizer)
        train_loader = DataLoader(
            dataset,
            batch_size=args.batch_size,
            shuffle=True,
            collate_fn=collate_fn,
            num_workers=0
        )

        # æ¨¡å‹
        model_config = APTModelConfiguration(
            vocab_size=tokenizer.vocab_size,
            d_model=args.d_model,
            n_heads=args.n_heads,
            num_encoder_layers=args.n_layers,
            num_decoder_layers=args.n_layers,
            d_ff=args.d_model * 4,
            max_seq_len=256,
            dropout=0.1,
            use_dbc_dac=True
        )
        model = APTModel(model_config).to(device)

        total_params = sum(p.numel() for p in model.parameters())
        print(f"   æ€»å‚æ•°: {total_params:,}")
        mlflow.log_param("total_params", total_params)

        # ä¼˜åŒ–å™¨
        optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=args.learning_rate,
            weight_decay=args.weight_decay
        )

        # å­¦ä¹ ç‡è°ƒåº¦å™¨
        scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            optimizer,
            T_0=10,
            T_mult=2,
            eta_min=1e-5
        )

        # æ··åˆç²¾åº¦
        scaler = GradScaler() if args.mixed_precision else None

        # æŸå¤±å‡½æ•°
        criterion = nn.CrossEntropyLoss(ignore_index=-100)

        # è®­ç»ƒå¾ªç¯
        print("\\n" + "=" * 60)
        print("ğŸš€ å¼€å§‹Azure MLè®­ç»ƒ")
        print("=" * 60)

        for epoch in range(args.epochs):
            model.train()
            total_loss = 0
            num_batches = 0

            for batch_idx, batch in enumerate(train_loader):
                input_ids = batch['input_ids'].to(device)
                labels = batch['labels'].to(device)

                # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
                with autocast(enabled=args.mixed_precision):
                    logits = model(input_ids)
                    loss = criterion(
                        logits.view(-1, logits.size(-1)),
                        labels.view(-1)
                    )

                # åå‘ä¼ æ’­
                if scaler:
                    scaler.scale(loss).backward()
                    scaler.step(optimizer)
                    scaler.update()
                else:
                    loss.backward()
                    optimizer.step()

                optimizer.zero_grad()

                total_loss += loss.item()
                num_batches += 1

                if batch_idx % 20 == 0:
                    current_lr = scheduler.get_last_lr()[0]
                    print(f"   Batch {batch_idx}/{len(train_loader)} | "
                          f"Loss: {loss.item():.4f} | LR: {current_lr:.6f}")

            # æ›´æ–°å­¦ä¹ ç‡
            scheduler.step()

            avg_loss = total_loss / num_batches
            current_lr = scheduler.get_last_lr()[0]

            print(f"\\nğŸ“ Epoch {epoch + 1}/{args.epochs}")
            print(f"   Avg Loss: {avg_loss:.4f} | LR: {current_lr:.6f}")

            # MLflowè®°å½•æŒ‡æ ‡
            mlflow.log_metrics({
                "train_loss": avg_loss,
                "learning_rate": current_lr
            }, step=epoch)

            # å®šæœŸä¿å­˜checkpoint
            if (epoch + 1) % args.save_interval == 0:
                checkpoint_path = f"checkpoint_epoch_{epoch + 1}.pt"
                torch.save({
                    'epoch': epoch + 1,
                    'model_state_dict': model.state_dict(),
                    'optimizer_state_dict': optimizer.state_dict(),
                    'scheduler_state_dict': scheduler.state_dict(),
                    'loss': avg_loss,
                    'tokenizer_char_to_id': tokenizer.char_to_id,
                    'tokenizer_id_to_char': tokenizer.id_to_char,
                    'tokenizer_next_id': tokenizer.next_id,
                    'tokenizer_vocab_size': tokenizer.vocab_size,
                }, checkpoint_path)

                # ä¸Šä¼ åˆ°MLflow
                mlflow.log_artifact(checkpoint_path)
                print(f"   âœ… Checkpointå·²ä¸Šä¼ åˆ°MLflow")

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = "final_model.pt"
        torch.save({
            'model_state_dict': model.state_dict(),
            'model_config': model_config.__dict__,
            'tokenizer_char_to_id': tokenizer.char_to_id,
            'tokenizer_id_to_char': tokenizer.id_to_char,
            'tokenizer_next_id': tokenizer.next_id,
            'tokenizer_vocab_size': tokenizer.vocab_size,
        }, final_model_path)

        # æ³¨å†Œæ¨¡å‹åˆ°MLflow
        mlflow.pytorch.log_model(model, "apt_model")
        mlflow.log_artifact(final_model_path)

        print("\\n" + "=" * 60)
        print("âœ¨ Azure MLè®­ç»ƒå®Œæˆï¼")
        print("=" * 60)


def main():
    parser = argparse.ArgumentParser(description='Azure ML APTè®­ç»ƒè„šæœ¬')

    # æ•°æ®å‚æ•°
    parser.add_argument('--dataset', type=str, default='../data/HLBD_Hardcore_Full.json')
    parser.add_argument('--epochs', type=int, default=100)
    parser.add_argument('--batch-size', type=int, default=16)
    parser.add_argument('--save-interval', type=int, default=25)

    # æ¨¡å‹å‚æ•°
    parser.add_argument('--d-model', type=int, default=256)
    parser.add_argument('--n-layers', type=int, default=6)
    parser.add_argument('--n-heads', type=int, default=8)

    # è®­ç»ƒå‚æ•°
    parser.add_argument('--learning-rate', type=float, default=3e-4)
    parser.add_argument('--weight-decay', type=float, default=0.01)
    parser.add_argument('--mixed-precision', action='store_true', default=True)

    # MLflowå‚æ•°
    parser.add_argument('--experiment-name', type=str, default='apt-training')
    parser.add_argument('--run-name', type=str, default='hlbd-playground')

    args = parser.parse_args()

    train_with_mlflow(args)


if __name__ == "__main__":
    main()
'''

    return script


# ============================================================================
# Azure MLä»»åŠ¡æäº¤
# ============================================================================

def submit_training_job(
    ml_client: MLClient,
    config: AzureMLConfig,
    environment_name: str,
    dataset_path: str,
    args: argparse.Namespace
):
    """æäº¤è®­ç»ƒä»»åŠ¡åˆ°Azure ML"""

    # ç”Ÿæˆè®­ç»ƒè„šæœ¬
    print("ğŸ“ ç”Ÿæˆè®­ç»ƒè„šæœ¬...")
    training_script = generate_training_script()

    # ä¿å­˜åˆ°æœ¬åœ°
    script_path = Path("azure_training_script.py")
    with open(script_path, 'w', encoding='utf-8') as f:
        f.write(training_script)
    print(f"   âœ“ è„šæœ¬å·²ä¿å­˜: {script_path}")

    # ä¸Šä¼ æ•°æ®é›†ï¼ˆå¦‚æœéœ€è¦ï¼‰
    print("ğŸ“‚ å‡†å¤‡æ•°æ®é›†...")
    # è¿™é‡Œå¯ä»¥ä¸Šä¼ åˆ°Azure Blob Storageæˆ–ä½¿ç”¨Azure MLæ•°æ®èµ„äº§

    # åˆ›å»ºå‘½ä»¤ä»»åŠ¡
    print("ğŸš€ åˆ›å»ºè®­ç»ƒä»»åŠ¡...")

    job = command(
        code="./",  # ä»£ç ç›®å½•
        command=f"""
            python azure_training_script.py \\
                --dataset ${{inputs.dataset}} \\
                --epochs {args.epochs} \\
                --batch-size {args.batch_size} \\
                --d-model {args.d_model} \\
                --n-layers {args.n_layers} \\
                --n-heads {args.n_heads} \\
                --learning-rate {args.learning_rate} \\
                --weight-decay {args.weight_decay} \\
                --experiment-name {config.experiment_name} \\
                --run-name {args.run_name}
        """,
        inputs={
            "dataset": Input(type="uri_file", path=dataset_path)
        },
        environment=f"{environment_name}@latest",
        compute=config.compute_name,
        experiment_name=config.experiment_name,
        display_name=args.run_name,
        description="APTæ¨¡å‹HLBDè®­ç»ƒ",
    )

    # æäº¤ä»»åŠ¡
    print("ğŸ“¤ æäº¤ä»»åŠ¡åˆ°Azure ML...")
    returned_job = ml_client.jobs.create_or_update(job)

    print(f"\n{'=' * 60}")
    print(f"âœ… ä»»åŠ¡å·²æäº¤ï¼")
    print(f"{'=' * 60}")
    print(f"ä»»åŠ¡åç§°: {returned_job.name}")
    print(f"ä»»åŠ¡çŠ¶æ€: {returned_job.status}")
    print(f"å®éªŒåç§°: {config.experiment_name}")
    print(f"\nğŸ”— Azure ML Studioé“¾æ¥:")
    print(f"   {returned_job.studio_url}")
    print(f"\nğŸ’¡ æŸ¥çœ‹ä»»åŠ¡çŠ¶æ€:")
    print(f"   az ml job show --name {returned_job.name}")
    print(f"\nğŸ“Š æŸ¥çœ‹å®æ—¶æ—¥å¿—:")
    print(f"   az ml job stream --name {returned_job.name}")

    return returned_job


def submit_sweep_job(
    ml_client: MLClient,
    config: AzureMLConfig,
    environment_name: str,
    dataset_path: str
):
    """æäº¤è¶…å‚æ•°æ‰«æä»»åŠ¡"""

    from azure.ai.ml.sweep import Choice, Uniform

    print("ğŸ”¬ åˆ›å»ºè¶…å‚æ•°æ‰«æä»»åŠ¡...")

    # åŸºç¡€å‘½ä»¤
    base_job = command(
        code="./",
        command="""
            python azure_training_script.py \\
                --dataset ${{inputs.dataset}} \\
                --epochs 50 \\
                --batch-size ${{search_space.batch_size}} \\
                --d-model ${{search_space.d_model}} \\
                --n-layers ${{search_space.n_layers}} \\
                --learning-rate ${{search_space.learning_rate}} \\
                --weight-decay ${{search_space.weight_decay}}
        """,
        inputs={"dataset": Input(type="uri_file", path=dataset_path)},
        environment=f"{environment_name}@latest",
        compute=config.compute_name,
    )

    # æœç´¢ç©ºé—´
    sweep_job = base_job.sweep(
        sampling_algorithm="random",
        primary_metric="train_loss",
        goal="minimize",
        max_total_trials=20,
        max_concurrent_trials=4,
        search_space={
            "batch_size": Choice([8, 16, 32]),
            "d_model": Choice([128, 256, 512]),
            "n_layers": Choice([4, 6, 8]),
            "learning_rate": Uniform(1e-5, 1e-3),
            "weight_decay": Uniform(0.001, 0.1)
        }
    )

    # æäº¤
    returned_sweep = ml_client.jobs.create_or_update(sweep_job)

    print(f"âœ… è¶…å‚æ•°æ‰«æä»»åŠ¡å·²æäº¤: {returned_sweep.name}")
    print(f"ğŸ”— æŸ¥çœ‹è¿›åº¦: {returned_sweep.studio_url}")

    return returned_sweep


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description='Azure ML APTè®­ç»ƒç®¡ç†')

    # Azure MLé…ç½®
    parser.add_argument('--subscription-id', type=str, required=True,
                       help='Azureè®¢é˜…ID')
    parser.add_argument('--resource-group', type=str, required=True,
                       help='èµ„æºç»„åç§°')
    parser.add_argument('--workspace-name', type=str, required=True,
                       help='Azure MLå·¥ä½œåŒºåç§°')
    parser.add_argument('--compute-name', type=str, default='gpu-cluster',
                       help='è®¡ç®—é›†ç¾¤åç§°')
    parser.add_argument('--experiment-name', type=str, default='apt-training',
                       help='å®éªŒåç§°')

    # è®¡ç®—èµ„æº
    parser.add_argument('--vm-size', type=str, default='Standard_NC6s_v3',
                       help='VMè§„æ ¼ (Standard_NC6s_v3 = 1x V100)')
    parser.add_argument('--environment-name', type=str, default='apt-training-env',
                       help='è®­ç»ƒç¯å¢ƒåç§°')

    # æ•°æ®å’Œè®­ç»ƒå‚æ•°
    parser.add_argument('--dataset', type=str, default='../data/HLBD_Hardcore_Full.json',
                       help='æ•°æ®é›†è·¯å¾„')
    parser.add_argument('--epochs', type=int, default=100,
                       help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--batch-size', type=int, default=16,
                       help='batchå¤§å°')

    # æ¨¡å‹å‚æ•°
    parser.add_argument('--d-model', type=int, default=256)
    parser.add_argument('--n-layers', type=int, default=6)
    parser.add_argument('--n-heads', type=int, default=8)

    # è®­ç»ƒå‚æ•°
    parser.add_argument('--learning-rate', type=float, default=3e-4)
    parser.add_argument('--weight-decay', type=float, default=0.01)

    # è¿è¡Œé…ç½®
    parser.add_argument('--run-name', type=str, default='hlbd-playground',
                       help='è¿è¡Œåç§°')
    parser.add_argument('--sweep', action='store_true',
                       help='å¯ç”¨è¶…å‚æ•°æ‰«æ')

    args = parser.parse_args()

    if not AZURE_ML_AVAILABLE:
        print("\nâŒ Azure ML SDKæœªå®‰è£…")
        print("   è¯·è¿è¡Œ: pip install azure-ai-ml mlflow azureml-mlflow")
        return

    print("\nğŸš€ Azure ML APTè®­ç»ƒ")
    print("=" * 60)

    # åˆ›å»ºé…ç½®
    config = AzureMLConfig(
        subscription_id=args.subscription_id,
        resource_group=args.resource_group,
        workspace_name=args.workspace_name,
        compute_name=args.compute_name,
        experiment_name=args.experiment_name
    )

    # è¿æ¥åˆ°Azure ML
    print("\n1ï¸âƒ£  è¿æ¥Azure ML...")
    ml_client = create_ml_client(config)

    # åˆ›å»º/è·å–è®¡ç®—é›†ç¾¤
    print("\n2ï¸âƒ£  å‡†å¤‡è®¡ç®—èµ„æº...")
    create_or_get_compute(ml_client, args.compute_name, args.vm_size)

    # åˆ›å»ºç¯å¢ƒ
    print("\n3ï¸âƒ£  åˆ›å»ºè®­ç»ƒç¯å¢ƒ...")
    create_environment(ml_client, args.environment_name)

    # æäº¤ä»»åŠ¡
    print("\n4ï¸âƒ£  æäº¤è®­ç»ƒä»»åŠ¡...")
    if args.sweep:
        submit_sweep_job(ml_client, config, args.environment_name, args.dataset)
    else:
        submit_training_job(ml_client, config, args.environment_name, args.dataset, args)

    print("\n" + "=" * 60)
    print("âœ¨ ä»»åŠ¡æäº¤å®Œæˆï¼")
    print("=" * 60)
    print("\nğŸ’¡ æç¤º:")
    print("1. åœ¨Azure ML StudioæŸ¥çœ‹è®­ç»ƒè¿›åº¦")
    print("2. ä½¿ç”¨MLflowæŸ¥çœ‹å®éªŒç»“æœ")
    print("3. è®­ç»ƒå®Œæˆåæ¨¡å‹ä¼šè‡ªåŠ¨æ³¨å†Œåˆ°MLflow")


if __name__ == "__main__":
    main()
