#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
è™šæ‹ŸBlackwellå¿«é€Ÿæµ‹è¯•è„šæœ¬ï¼ˆç®€åŒ–ç‰ˆï¼‰

ä¸“é—¨ç”¨äºå¿«é€Ÿæµ‹è¯•ï¼Œä½¿ç”¨è¾ƒå°çš„æ¨¡å‹é…ç½®ã€‚

ç”¨æ³•:
    python test_vb_quick.py                    # é»˜è®¤é…ç½®
    python test_vb_quick.py --small            # è¶…å°æ¨¡å‹
    python test_vb_quick.py --iterations 20    # è‡ªå®šä¹‰è¿­ä»£æ¬¡æ•°
"""

import sys
import os
import time
import argparse
from pathlib import Path

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

# æ£€æŸ¥torch
try:
    import torch
    import torch.nn as nn
    import numpy as np
except ImportError:
    print("âŒ éœ€è¦å®‰è£…PyTorch: pip install torch numpy")
    sys.exit(1)

# å¯¼å…¥ä¼˜åŒ–æ¨¡å—
from apt.perf.optimization import enable_vb_optimization, VB_TORCH_AVAILABLE

# æŠ‘åˆ¶è­¦å‘Š
import warnings
warnings.filterwarnings('ignore')
os.environ['SUPPRESS_APT_WARNINGS'] = 'True'


class SimpleTransformerBlock(nn.Module):
    """ç®€å•çš„Transformerå—ç”¨äºå¿«é€Ÿæµ‹è¯•"""

    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):
        super().__init__()

        # Multi-head attention
        self.attn = nn.MultiheadAttention(d_model, n_heads, dropout=dropout, batch_first=True)

        # Feed-forward
        self.ffn = nn.Sequential(
            nn.Linear(d_model, d_ff),
            nn.GELU(),
            nn.Dropout(dropout),
            nn.Linear(d_ff, d_model),
            nn.Dropout(dropout)
        )

        # Layer norms
        self.norm1 = nn.LayerNorm(d_model)
        self.norm2 = nn.LayerNorm(d_model)

    def forward(self, x):
        # Attention
        attn_out, _ = self.attn(x, x, x)
        x = self.norm1(x + attn_out)

        # FFN
        ffn_out = self.ffn(x)
        x = self.norm2(x + ffn_out)

        return x


class SimpleTransformer(nn.Module):
    """ç®€å•çš„Transformeræ¨¡å‹"""

    def __init__(self, vocab_size, d_model, n_heads, n_layers, d_ff, max_len):
        super().__init__()

        self.embedding = nn.Embedding(vocab_size, d_model)
        self.pos_embedding = nn.Embedding(max_len, d_model)

        self.blocks = nn.ModuleList([
            SimpleTransformerBlock(d_model, n_heads, d_ff)
            for _ in range(n_layers)
        ])

        self.output = nn.Linear(d_model, vocab_size)

    def forward(self, x):
        # x: (batch, seq_len, d_model) å¦‚æœå·²ç»æ˜¯embedding
        # æˆ– (batch, seq_len) å¦‚æœæ˜¯token ids

        if x.dim() == 2:  # token ids
            seq_len = x.size(1)
            pos = torch.arange(seq_len, device=x.device).unsqueeze(0)
            x = self.embedding(x) + self.pos_embedding(pos)
        elif x.dim() == 3:  # already embedded
            pass
        else:
            raise ValueError(f"Unexpected input dimension: {x.dim()}")

        for block in self.blocks:
            x = block(x)

        return self.output(x)


def benchmark_model(model, input_data, num_iterations=10, warmup=5):
    """åŸºå‡†æµ‹è¯•"""
    model.eval()

    # é¢„çƒ­
    with torch.no_grad():
        for _ in range(warmup):
            _ = model(input_data)

    # æµ‹è¯•
    times = []
    with torch.no_grad():
        for _ in range(num_iterations):
            start = time.time()
            _ = model(input_data)
            times.append(time.time() - start)

    times = np.array(times)
    return {
        'mean': np.mean(times),
        'std': np.std(times),
        'median': np.median(times),
        'min': np.min(times),
        'max': np.max(times)
    }


def count_linear_layers(model):
    """ç»Ÿè®¡çº¿æ€§å±‚æ•°é‡"""
    count = 0
    for module in model.modules():
        if isinstance(module, nn.Linear):
            count += 1
    return count


def main():
    parser = argparse.ArgumentParser(description='è™šæ‹ŸBlackwellå¿«é€Ÿæµ‹è¯•')
    parser.add_argument('--small', action='store_true',
                      help='ä½¿ç”¨è¶…å°æ¨¡å‹ï¼ˆæ›´å¿«ï¼‰')
    parser.add_argument('--iterations', type=int, default=10,
                      help='æµ‹è¯•è¿­ä»£æ¬¡æ•°')
    parser.add_argument('--batch-size', type=int, default=4,
                      help='æ‰¹é‡å¤§å°')
    parser.add_argument('--seq-len', type=int, default=64,
                      help='åºåˆ—é•¿åº¦')
    args = parser.parse_args()

    print("\n" + "="*70)
    print(" "*20 + "è™šæ‹ŸBlackwellå¿«é€Ÿæµ‹è¯•")
    print("="*70)

    # æ¨¡å‹é…ç½®
    if args.small:
        config = {
            'vocab_size': 10000,
            'd_model': 256,
            'n_heads': 4,
            'n_layers': 4,
            'd_ff': 256 * 4,
            'max_len': 128
        }
        print("\nğŸ“ é…ç½®: è¶…å°æ¨¡å‹")
    else:
        config = {
            'vocab_size': 30000,
            'd_model': 512,
            'n_heads': 8,
            'n_layers': 6,
            'd_ff': 512 * 4,
            'max_len': 256
        }
        print("\nğŸ“ é…ç½®: æ ‡å‡†æ¨¡å‹")

    print(f"   ç»´åº¦: {config['d_model']}")
    print(f"   å±‚æ•°: {config['n_layers']}")
    print(f"   æ³¨æ„åŠ›å¤´: {config['n_heads']}")
    print(f"   FFNç»´åº¦: {config['d_ff']}")

    print(f"\nâš™ï¸  æµ‹è¯•å‚æ•°:")
    print(f"   Batchå¤§å°: {args.batch_size}")
    print(f"   åºåˆ—é•¿åº¦: {args.seq_len}")
    print(f"   è¿­ä»£æ¬¡æ•°: {args.iterations}")

    # åˆ›å»ºè¾“å…¥
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"   è®¾å¤‡: {device}")

    input_data = torch.randn(
        args.batch_size,
        args.seq_len,
        config['d_model'],
        device=device
    )

    # æµ‹è¯•1: åŸå§‹æ¨¡å‹
    print("\n" + "â”€"*70)
    print("  æµ‹è¯•1: åŸå§‹æ¨¡å‹")
    print("â”€"*70)

    model_orig = SimpleTransformer(**config).to(device)
    total_params = sum(p.numel() for p in model_orig.parameters())
    n_linear = count_linear_layers(model_orig)

    print(f"\nğŸ“Š æ¨¡å‹ä¿¡æ¯:")
    print(f"   æ€»å‚æ•°: {total_params:,}")
    print(f"   çº¿æ€§å±‚æ•°é‡: {n_linear}")
    print(f"   æ¨¡å‹å¤§å°: {total_params * 4 / 1024 / 1024:.2f} MB")

    print(f"\nâ±ï¸  åŸºå‡†æµ‹è¯•...")
    result_orig = benchmark_model(model_orig, input_data, args.iterations)

    print(f"\nğŸ“ˆ æ€§èƒ½:")
    print(f"   å¹³å‡æ—¶é—´: {result_orig['mean']*1000:.2f} Â± {result_orig['std']*1000:.2f} ms")
    print(f"   ä¸­ä½æ•°: {result_orig['median']*1000:.2f} ms")
    print(f"   èŒƒå›´: [{result_orig['min']*1000:.2f}, {result_orig['max']*1000:.2f}] ms")
    print(f"   ååé‡: {args.batch_size / result_orig['mean']:.2f} samples/sec")

    # æµ‹è¯•2: VBä¼˜åŒ–æ¨¡å‹
    print("\n" + "â”€"*70)
    print("  æµ‹è¯•2: è™šæ‹ŸBlackwellä¼˜åŒ–æ¨¡å‹")
    print("â”€"*70)

    if not VB_TORCH_AVAILABLE:
        print("\nâš ï¸  VB PyTorché›†æˆä¸å¯ç”¨ï¼Œè·³è¿‡æµ‹è¯•")
        return

    model_vb = SimpleTransformer(**config).to(device)

    print(f"\nğŸš€ åº”ç”¨è™šæ‹ŸBlackwellä¼˜åŒ–...")
    try:
        model_vb = enable_vb_optimization(
            model_vb,
            mode='training',
            enable_quantization=False,  # ç¦ç”¨é‡åŒ–ï¼ˆå¤ªæ…¢ï¼Œæ¯ä¸ª8x8 blockéƒ½è¦åšSVDï¼‰
            replace_pattern='large'  # åªä¼˜åŒ–å¤§å‹å±‚
        )
    except Exception as e:
        print(f"âŒ VBä¼˜åŒ–å¤±è´¥: {e}")
        return

    print(f"\nâ±ï¸  åŸºå‡†æµ‹è¯•...")
    result_vb = benchmark_model(model_vb, input_data, args.iterations)

    print(f"\nğŸ“ˆ æ€§èƒ½:")
    print(f"   å¹³å‡æ—¶é—´: {result_vb['mean']*1000:.2f} Â± {result_vb['std']*1000:.2f} ms")
    print(f"   ä¸­ä½æ•°: {result_vb['median']*1000:.2f} ms")
    print(f"   èŒƒå›´: [{result_vb['min']*1000:.2f}, {result_vb['max']*1000:.2f}] ms")
    print(f"   ååé‡: {args.batch_size / result_vb['mean']:.2f} samples/sec")

    # VBç»Ÿè®¡
    try:
        model_vb.print_all_stats()
    except:
        pass

    # å¯¹æ¯”
    print("\n" + "="*70)
    print(" "*25 + "å¯¹æ¯”ç»“æœ")
    print("="*70)

    speedup = result_orig['mean'] / result_vb['mean']

    print(f"\nåŸå§‹æ¨¡å‹:")
    print(f"   æ—¶é—´: {result_orig['mean']*1000:.2f} ms")
    print(f"   ååé‡: {args.batch_size / result_orig['mean']:.2f} samples/sec")

    print(f"\nVBä¼˜åŒ–æ¨¡å‹:")
    print(f"   æ—¶é—´: {result_vb['mean']*1000:.2f} ms")
    print(f"   ååé‡: {args.batch_size / result_vb['mean']:.2f} samples/sec")

    print(f"\nğŸ¯ åŠ é€Ÿæ¯”: {speedup:.2f}Ã—")

    if speedup > 1.2:
        print("   âœ… æ˜¾è‘—åŠ é€Ÿ!")
        emoji = "ğŸš€"
    elif speedup > 1.0:
        print("   âœ“ è½»å¾®åŠ é€Ÿ")
        emoji = "âœ“"
    elif speedup > 0.8:
        print("   â‰ˆ æ€§èƒ½ç›¸å½“")
        emoji = "â‰ˆ"
    else:
        print("   âš ï¸  æ€§èƒ½ä¸‹é™")
        emoji = "âš ï¸"

        if device == 'cpu':
            print("\n   è¯´æ˜: åœ¨CPUç¯å¢ƒä¸‹ï¼Œè™šæ‹ŸåŒ–å¼€é”€å¯èƒ½è¶…è¿‡æ”¶ç›Š")
            print("   å»ºè®®: åœ¨GPUç¯å¢ƒæµ‹è¯•ä»¥è·å¾—æœ€ä½³æ€§èƒ½")

    # å¯è§†åŒ–
    print(f"\n{emoji} æ€§èƒ½å¯¹æ¯”:")
    bar_length = 50
    orig_bar = int(bar_length * result_orig['mean'] / max(result_orig['mean'], result_vb['mean']))
    vb_bar = int(bar_length * result_vb['mean'] / max(result_orig['mean'], result_vb['mean']))

    print(f"   åŸå§‹:  {'â–ˆ' * orig_bar}{' ' * (bar_length - orig_bar)} {result_orig['mean']*1000:.1f}ms")
    print(f"   VBä¼˜åŒ–: {'â–ˆ' * vb_bar}{' ' * (bar_length - vb_bar)} {result_vb['mean']*1000:.1f}ms")

    print("\n" + "="*70)
    print("æµ‹è¯•å®Œæˆ!")
    print("="*70)

    print("\nğŸ’¡ æç¤º:")
    print("   - ä½¿ç”¨ --small æµ‹è¯•æ›´å°çš„æ¨¡å‹ï¼ˆæ›´å¿«ï¼‰")
    print("   - ä½¿ç”¨ --iterations 20 å¢åŠ æµ‹è¯•æ¬¡æ•°ï¼ˆæ›´å‡†ç¡®ï¼‰")
    print("   - åœ¨GPUä¸Šè¿è¡Œå¯è·å¾—æ›´å¥½çš„åŠ é€Ÿæ•ˆæœ")
    print("   - å®Œæ•´æµ‹è¯•è¯·ä½¿ç”¨: python test_vb_models.py")


if __name__ == "__main__":
    main()
