#!/usr/bin/env python3
"""
APTæ¨¡å‹ + HuggingFace Traineré›†æˆ
å°†APTæ¨¡å‹é€‚é…åˆ°HuggingFaceç”Ÿæ€ç³»ç»Ÿ

ç‰¹æ€§:
- ğŸ¤— HuggingFace Trainer API
- ğŸ“Š Weights & Biases / TensorBoardé›†æˆ
- ğŸš€ è‡ªåŠ¨æ··åˆç²¾åº¦è®­ç»ƒ
- ğŸ’¾ HuggingFace Hubæ¨¡å‹ä¸Šä¼ 
- ğŸ”§ DeepSpeedé›†æˆï¼ˆé€šè¿‡HF Trainerï¼‰
- ğŸ“ˆ ä¸°å¯Œçš„è®­ç»ƒå›è°ƒï¼ˆEarlyStopping, LRç›‘æ§ç­‰ï¼‰

ä½¿ç”¨å‰å‡†å¤‡:
pip install transformers datasets accelerate wandb
"""

import os
import sys
import json
import argparse
from pathlib import Path
from typing import Dict, Optional, Any

import torch
import torch.nn as nn
from torch.utils.data import Dataset

try:
    from transformers import (
        PreTrainedModel,
        PretrainedConfig,
        Trainer,
        TrainingArguments,
        TrainerCallback,
        EarlyStoppingCallback
    )
    from transformers.modeling_outputs import CausalLMOutput
    from datasets import Dataset as HFDataset
    HF_AVAILABLE = True
except ImportError:
    HF_AVAILABLE = False
    print("âš ï¸  HuggingFace Transformersæœªå®‰è£…ï¼Œè¯·è¿è¡Œ:")
    print("   pip install transformers datasets accelerate")

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from apt_model.modeling.apt_model import APTModel, APTModelConfiguration
from train_hlbd_playground import DynamicTagTokenizer, HLBDPlaygroundDataset, collate_fn


# ============================================================================
# APTæ¨¡å‹HuggingFaceé€‚é…å™¨
# ============================================================================

class APTConfig(PretrainedConfig):
    """APTæ¨¡å‹é…ç½®ï¼ˆHuggingFaceå…¼å®¹ï¼‰"""

    model_type = "apt"

    def __init__(
        self,
        vocab_size: int = 5000,
        d_model: int = 256,
        n_heads: int = 8,
        num_encoder_layers: int = 6,
        num_decoder_layers: int = 6,
        d_ff: int = 1024,
        max_seq_len: int = 256,
        dropout: float = 0.1,
        use_dbc_dac: bool = True,
        **kwargs
    ):
        super().__init__(**kwargs)
        self.vocab_size = vocab_size
        self.d_model = d_model
        self.n_heads = n_heads
        self.num_encoder_layers = num_encoder_layers
        self.num_decoder_layers = num_decoder_layers
        self.d_ff = d_ff
        self.max_seq_len = max_seq_len
        self.dropout = dropout
        self.use_dbc_dac = use_dbc_dac


class APTForCausalLM(PreTrainedModel):
    """APTæ¨¡å‹HuggingFaceåŒ…è£…å™¨"""

    config_class = APTConfig

    def __init__(self, config: APTConfig):
        super().__init__(config)

        # åˆ›å»ºAPTæ¨¡å‹é…ç½®
        apt_config = APTModelConfiguration(
            vocab_size=config.vocab_size,
            d_model=config.d_model,
            n_heads=config.n_heads,
            num_encoder_layers=config.num_encoder_layers,
            num_decoder_layers=config.num_decoder_layers,
            d_ff=config.d_ff,
            max_seq_len=config.max_seq_len,
            dropout=config.dropout,
            use_dbc_dac=config.use_dbc_dac
        )

        # åŒ…è£…åŸå§‹APTæ¨¡å‹
        self.model = APTModel(apt_config)

        # æŸå¤±å‡½æ•°
        self.loss_fct = nn.CrossEntropyLoss(ignore_index=-100)

        # åˆå§‹åŒ–æƒé‡
        self.post_init()

    def forward(
        self,
        input_ids: torch.Tensor,
        attention_mask: Optional[torch.Tensor] = None,
        labels: Optional[torch.Tensor] = None,
        **kwargs
    ) -> CausalLMOutput:
        """
        HuggingFaceæ ‡å‡†å‰å‘ä¼ æ’­

        Args:
            input_ids: è¾“å…¥token IDs [batch, seq_len]
            attention_mask: æ³¨æ„åŠ›maskï¼ˆå¯é€‰ï¼‰
            labels: æ ‡ç­¾ï¼ˆç”¨äºè®¡ç®—æŸå¤±ï¼‰ [batch, seq_len]
        """

        # APTæ¨¡å‹å‰å‘ä¼ æ’­
        logits = self.model(input_ids)  # [batch, seq_len, vocab_size]

        # è®¡ç®—æŸå¤±
        loss = None
        if labels is not None:
            # ç§»ä½å¤„ç†ï¼ˆè¯­è¨€æ¨¡å‹æ ‡å‡†åšæ³•ï¼‰
            shift_logits = logits[..., :-1, :].contiguous()
            shift_labels = labels[..., 1:].contiguous()

            loss = self.loss_fct(
                shift_logits.view(-1, shift_logits.size(-1)),
                shift_labels.view(-1)
            )

        return CausalLMOutput(
            loss=loss,
            logits=logits,
            hidden_states=None,
            attentions=None
        )

    def generate(
        self,
        input_ids: torch.Tensor,
        max_length: int = 50,
        temperature: float = 1.0,
        top_k: int = 50,
        top_p: float = 0.9,
        **kwargs
    ) -> torch.Tensor:
        """
        ç®€å•çš„ç”Ÿæˆå‡½æ•°ï¼ˆè´ªå©ªè§£ç ï¼‰

        å¯¹äºæ›´å¤æ‚çš„ç”Ÿæˆï¼Œå¯ä»¥ä½¿ç”¨HuggingFaceçš„GenerationMixin
        """
        self.eval()

        with torch.no_grad():
            for _ in range(max_length):
                logits = self.model(input_ids)
                next_token_logits = logits[:, -1, :] / temperature

                # Top-kè¿‡æ»¤
                if top_k > 0:
                    indices_to_remove = next_token_logits < torch.topk(next_token_logits, top_k)[0][..., -1, None]
                    next_token_logits[indices_to_remove] = float('-inf')

                # Top-pè¿‡æ»¤
                if top_p < 1.0:
                    sorted_logits, sorted_indices = torch.sort(next_token_logits, descending=True)
                    cumulative_probs = torch.cumsum(torch.softmax(sorted_logits, dim=-1), dim=-1)
                    sorted_indices_to_remove = cumulative_probs > top_p
                    sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()
                    sorted_indices_to_remove[..., 0] = 0
                    indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)
                    next_token_logits[indices_to_remove] = float('-inf')

                # é‡‡æ ·
                probs = torch.softmax(next_token_logits, dim=-1)
                next_token = torch.multinomial(probs, num_samples=1)

                input_ids = torch.cat([input_ids, next_token], dim=-1)

                # æ£€æŸ¥EOS
                if next_token.item() == 3:  # EOS token
                    break

        return input_ids


# ============================================================================
# HuggingFace Dataseté€‚é…å™¨
# ============================================================================

class HLBDHFDataset(Dataset):
    """HLBDæ•°æ®é›†çš„HuggingFaceé€‚é…å™¨"""

    def __init__(self, json_path: str, tokenizer: DynamicTagTokenizer, max_len: int = 128):
        self.tokenizer = tokenizer
        self.max_len = max_len

        print(f"ğŸ“‚ åŠ è½½HLBDæ•°æ®é›†: {json_path}")
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # åˆå¹¶æ‰€æœ‰æ¨¡å—
        self.pairs = []
        for module_name, module_data in data['data'].items():
            for item in module_data:
                self.pairs.append((item['input'], item['output']))

        print(f"   âœ“ åŠ è½½ {len(self.pairs)} ä¸ªè®­ç»ƒå¯¹")

        # é¢„å¡«å……è¯æ±‡è¡¨
        print(f"   é¢„å¡«å……è¯æ±‡è¡¨...")
        for src, tgt in self.pairs:
            self.tokenizer.encode(src)
            self.tokenizer.encode(tgt)
        print(f"   âœ“ è¯æ±‡è¡¨å¤§å°: {len(self.tokenizer.char_to_id)}")

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        src, tgt = self.pairs[idx]

        # Encode
        src_ids = self.tokenizer.encode(src)
        tgt_ids = self.tokenizer.encode(tgt)

        # æ‹¼æ¥: [src, SEP, tgt]
        input_ids = src_ids + [1] + tgt_ids

        # æˆªæ–­
        if len(input_ids) > self.max_len:
            input_ids = input_ids[:self.max_len]

        # HuggingFaceæ ¼å¼
        return {
            'input_ids': torch.tensor(input_ids[:-1], dtype=torch.long),
            'labels': torch.tensor(input_ids[1:], dtype=torch.long)
        }


def data_collator(features):
    """HuggingFaceæ•°æ®æ•´ç†å™¨"""
    input_ids = [f['input_ids'] for f in features]
    labels = [f['labels'] for f in features]

    max_len = max(len(ids) for ids in input_ids)

    padded_input = []
    padded_labels = []

    for inp, lab in zip(input_ids, labels):
        pad_len = max_len - len(inp)
        padded_input.append(torch.cat([inp, torch.zeros(pad_len, dtype=torch.long)]))
        padded_labels.append(torch.cat([lab, torch.full((pad_len,), -100, dtype=torch.long)]))

    return {
        'input_ids': torch.stack(padded_input),
        'labels': torch.stack(padded_labels)
    }


# ============================================================================
# è‡ªå®šä¹‰è®­ç»ƒå›è°ƒ
# ============================================================================

class APTTrainingCallback(TrainerCallback):
    """APTè®­ç»ƒç›‘æ§å›è°ƒ"""

    def __init__(self, tokenizer: DynamicTagTokenizer):
        self.tokenizer = tokenizer
        self.best_loss = float('inf')

    def on_log(self, args, state, control, logs=None, **kwargs):
        """è®°å½•è®­ç»ƒæ—¥å¿—"""
        if logs:
            current_loss = logs.get('loss', None)
            if current_loss and current_loss < self.best_loss:
                self.best_loss = current_loss
                print(f"ğŸ¯ æ–°æœ€ä½³Loss: {self.best_loss:.4f}")

    def on_epoch_end(self, args, state, control, **kwargs):
        """Epochç»“æŸå›è°ƒ"""
        print(f"\nâœ… Epoch {state.epoch} å®Œæˆ")
        print(f"   å…¨å±€æ­¥æ•°: {state.global_step}")
        print(f"   æœ€ä½³Loss: {self.best_loss:.4f}")

    def on_train_end(self, args, state, control, **kwargs):
        """è®­ç»ƒç»“æŸå›è°ƒ"""
        print("\n" + "=" * 60)
        print("ğŸ‰ HuggingFaceè®­ç»ƒå®Œæˆï¼")
        print("=" * 60)
        print(f"æ€»æ­¥æ•°: {state.global_step}")
        print(f"æœ€ä½³Loss: {self.best_loss:.4f}")


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description='APT + HuggingFace Trainerè®­ç»ƒ')

    # æ•°æ®å‚æ•°
    parser.add_argument('--dataset', type=str, default='../data/HLBD_Hardcore_Full.json',
                       help='HLBDæ•°æ®é›†è·¯å¾„')
    parser.add_argument('--output-dir', type=str, default='hf_output',
                       help='è¾“å‡ºç›®å½•')

    # æ¨¡å‹å‚æ•°
    parser.add_argument('--d-model', type=int, default=256)
    parser.add_argument('--n-layers', type=int, default=6)
    parser.add_argument('--n-heads', type=int, default=8)

    # è®­ç»ƒå‚æ•°
    parser.add_argument('--epochs', type=int, default=100)
    parser.add_argument('--batch-size', type=int, default=16)
    parser.add_argument('--gradient-accumulation-steps', type=int, default=2)
    parser.add_argument('--learning-rate', type=float, default=3e-4)
    parser.add_argument('--weight-decay', type=float, default=0.01)
    parser.add_argument('--warmup-steps', type=int, default=500)
    parser.add_argument('--max-grad-norm', type=float, default=1.0)

    # HuggingFaceç‰¹æ€§
    parser.add_argument('--fp16', action='store_true',
                       help='å¯ç”¨FP16æ··åˆç²¾åº¦')
    parser.add_argument('--bf16', action='store_true',
                       help='å¯ç”¨BF16æ··åˆç²¾åº¦')
    parser.add_argument('--deepspeed', type=str, default=None,
                       help='DeepSpeedé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--logging-steps', type=int, default=20)
    parser.add_argument('--save-steps', type=int, default=500)
    parser.add_argument('--eval-steps', type=int, default=500)
    parser.add_argument('--save-total-limit', type=int, default=3)

    # Weights & Biases
    parser.add_argument('--wandb', action='store_true',
                       help='å¯ç”¨Weights & Biasesè·Ÿè¸ª')
    parser.add_argument('--wandb-project', type=str, default='apt-training',
                       help='W&Bé¡¹ç›®åç§°')

    # æ—©åœ
    parser.add_argument('--early-stopping', action='store_true',
                       help='å¯ç”¨æ—©åœ')
    parser.add_argument('--early-stopping-patience', type=int, default=5)

    args = parser.parse_args()

    if not HF_AVAILABLE:
        print("\nâŒ HuggingFace Transformersæœªå®‰è£…")
        print("   è¯·è¿è¡Œ: pip install transformers datasets accelerate")
        return

    print("\nğŸ¤— APT + HuggingFace Trainerè®­ç»ƒ")
    print("=" * 60)

    # Tokenizer
    print("\nğŸ”¤ åˆå§‹åŒ–Tokenizerï¼ˆæ”¯æŒåŠ¨æ€æ ‡ç­¾ï¼‰...")
    tokenizer = DynamicTagTokenizer(vocab_size=5000)

    # æ•°æ®é›†
    print("\nğŸ“‚ åŠ è½½æ•°æ®é›†...")
    dataset = HLBDHFDataset(args.dataset, tokenizer)

    # æ¨¡å‹é…ç½®
    print("\nğŸ—ï¸  æ„å»ºAPTæ¨¡å‹...")
    config = APTConfig(
        vocab_size=tokenizer.vocab_size,
        d_model=args.d_model,
        n_heads=args.n_heads,
        num_encoder_layers=args.n_layers,
        num_decoder_layers=args.n_layers,
        d_ff=args.d_model * 4,
        max_seq_len=256,
        dropout=0.1,
        use_dbc_dac=True
    )

    model = APTForCausalLM(config)

    total_params = sum(p.numel() for p in model.parameters())
    print(f"   æ€»å‚æ•°: {total_params:,}")

    # è®­ç»ƒå‚æ•°
    print("\nâš™ï¸  é…ç½®è®­ç»ƒå‚æ•°...")

    training_args = TrainingArguments(
        output_dir=args.output_dir,
        num_train_epochs=args.epochs,
        per_device_train_batch_size=args.batch_size,
        gradient_accumulation_steps=args.gradient_accumulation_steps,
        learning_rate=args.learning_rate,
        weight_decay=args.weight_decay,
        warmup_steps=args.warmup_steps,
        max_grad_norm=args.max_grad_norm,

        # æ··åˆç²¾åº¦
        fp16=args.fp16,
        bf16=args.bf16,

        # æ—¥å¿—å’Œä¿å­˜
        logging_dir=f"{args.output_dir}/logs",
        logging_steps=args.logging_steps,
        save_steps=args.save_steps,
        save_total_limit=args.save_total_limit,

        # DeepSpeed
        deepspeed=args.deepspeed,

        # æŠ¥å‘Š
        report_to="wandb" if args.wandb else "none",
        run_name="apt-hlbd-training" if args.wandb else None,

        # å…¶ä»–
        remove_unused_columns=False,
        dataloader_num_workers=0,
        load_best_model_at_end=True if args.early_stopping else False,
    )

    # Weights & Biases
    if args.wandb:
        try:
            import wandb
            wandb.init(
                project=args.wandb_project,
                config=vars(args)
            )
            print("   âœ“ Weights & Biaseså·²å¯ç”¨")
        except ImportError:
            print("   âš ï¸  wandbæœªå®‰è£…ï¼Œè·³è¿‡W&Bé›†æˆ")

    # å›è°ƒ
    callbacks = [APTTrainingCallback(tokenizer)]

    if args.early_stopping:
        callbacks.append(EarlyStoppingCallback(
            early_stopping_patience=args.early_stopping_patience
        ))
        print(f"   âœ“ æ—©åœå·²å¯ç”¨ (patience={args.early_stopping_patience})")

    # Trainer
    print("\nğŸš€ åˆå§‹åŒ–HuggingFace Trainer...")
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=dataset,
        data_collator=data_collator,
        callbacks=callbacks
    )

    # è®­ç»ƒ
    print("\n" + "=" * 60)
    print("ğŸš€ å¼€å§‹HuggingFaceè®­ç»ƒ")
    print("=" * 60)

    trainer.train()

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    print("\nğŸ’¾ ä¿å­˜æ¨¡å‹...")
    final_path = Path(args.output_dir) / "final_model"
    trainer.save_model(str(final_path))

    # ä¿å­˜tokenizerçŠ¶æ€
    tokenizer_path = final_path / "tokenizer_state.json"
    with open(tokenizer_path, 'w') as f:
        json.dump({
            'char_to_id': tokenizer.char_to_id,
            'id_to_char': {str(k): v for k, v in tokenizer.id_to_char.items()},
            'next_id': tokenizer.next_id,
            'vocab_size': tokenizer.vocab_size
        }, f, ensure_ascii=False, indent=2)

    print(f"âœ… æ¨¡å‹å·²ä¿å­˜: {final_path}")

    # ä¸Šä¼ åˆ°HuggingFace Hubï¼ˆå¯é€‰ï¼‰
    if os.getenv('HF_HUB_TOKEN'):
        print("\nğŸ“¤ ä¸Šä¼ åˆ°HuggingFace Hub...")
        try:
            hub_model_id = f"apt-model-{args.d_model}d-{args.n_layers}l"
            trainer.push_to_hub(hub_model_id)
            print(f"âœ… æ¨¡å‹å·²ä¸Šä¼ : https://huggingface.co/{hub_model_id}")
        except Exception as e:
            print(f"âš ï¸  ä¸Šä¼ å¤±è´¥: {e}")

    print("\n" + "=" * 60)
    print("âœ¨ è®­ç»ƒå®Œæˆï¼")
    print("=" * 60)


if __name__ == "__main__":
    main()
