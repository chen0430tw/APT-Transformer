# æ ¸å¿ƒåŠŸèƒ½100%å®Œå–„è®¡åˆ’

**åŸåˆ™**: å…ˆæŠŠåŸºç¡€æ‰“ç‰¢ï¼Œå†è€ƒè™‘æ‰©å±•
**ç›®æ ‡**: 4ä¸ªæ ¸å¿ƒé¢†åŸŸå…¨éƒ¨è¾¾åˆ°100%æˆç†Ÿåº¦

---

## ğŸ¯ èšç„¦çš„4ä¸ªæ ¸å¿ƒé¢†åŸŸ

| é¢†åŸŸ | å½“å‰ | ç›®æ ‡ | å·®è· | ä¼˜å…ˆçº§ |
|------|------|------|------|--------|
| 1. æ ¸å¿ƒè®­ç»ƒåŠŸèƒ½ | 80% | 100% | 20% | P0 |
| 2. é”™è¯¯å¤„ç†ç³»ç»Ÿ | 90% | 100% | 10% | P0 |
| 3. æ’ä»¶ç³»ç»Ÿ | 70% | 100% | 30% | P0 |
| 4. å¤šæ¨¡æ€æ”¯æŒ | 50% | 100% | 50% | P1 |

---

## 1ï¸âƒ£ æ ¸å¿ƒè®­ç»ƒåŠŸèƒ½ 80% â†’ 100%

### ğŸ“‹ å®Œå–„æ¸…å•ï¼ˆæŒ‰ä¼˜å…ˆçº§ï¼‰

#### T1.1: å®Œå–„å•å…ƒæµ‹è¯•è¦†ç›– âœ… P0
**å½“å‰çŠ¶æ€**:
- âŒ æ— è®­ç»ƒå™¨æµ‹è¯•
- âŒ æ— checkpointæµ‹è¯•
- âŒ æ— æ•°æ®åŠ è½½æµ‹è¯•

**éœ€è¦å®ç°**:
```python
# tests/test_trainer.py
import pytest
from apt_model.training.trainer import train_model
from apt_model.training.checkpoint import CheckpointManager

class TestTrainer:
    def test_training_basic(self, tmp_path):
        """æµ‹è¯•åŸºç¡€è®­ç»ƒæµç¨‹"""
        model, tokenizer, config = train_model(
            epochs=2,
            batch_size=4,
            checkpoint_dir=tmp_path / "outputs",
            texts=["test text 1", "test text 2"]
        )
        assert model is not None
        assert (tmp_path / "outputs" / "checkpoints").exists()

    def test_checkpoint_save_load(self, tmp_path):
        """æµ‹è¯•checkpointä¿å­˜å’ŒåŠ è½½"""
        # è®­ç»ƒå¹¶ä¿å­˜
        model, tokenizer, config = train_model(
            epochs=3,
            checkpoint_dir=tmp_path / "outputs"
        )

        # æ£€æŸ¥checkpointæ–‡ä»¶
        checkpoint_files = list((tmp_path / "outputs" / "checkpoints").glob("*.pt"))
        assert len(checkpoint_files) == 3  # 3ä¸ªepoch

        # æµ‹è¯•åŠ è½½
        mgr = CheckpointManager(save_dir=tmp_path / "outputs")
        epoch, step, loss_history, metrics = mgr.load_checkpoint(
            model=model,
            checkpoint_path=checkpoint_files[-1]
        )
        assert epoch == 2  # æœ€åä¸€ä¸ªepoch
        assert len(loss_history) > 0

    def test_resume_training(self, tmp_path):
        """æµ‹è¯•æ¢å¤è®­ç»ƒ"""
        # ç¬¬ä¸€æ¬¡è®­ç»ƒåˆ°epoch 3
        model1, tokenizer1, config1 = train_model(
            epochs=3,
            checkpoint_dir=tmp_path / "outputs"
        )

        # æ¢å¤è®­ç»ƒåˆ°epoch 6
        model2, tokenizer2, config2 = train_model(
            epochs=6,
            checkpoint_dir=tmp_path / "outputs",
            resume_from=tmp_path / "outputs" / "checkpoints" / "apt_model_epoch2_*.pt"
        )

        # éªŒè¯ç»§ç»­è®­ç»ƒ
        checkpoint_files = list((tmp_path / "outputs" / "checkpoints").glob("*.pt"))
        assert len(checkpoint_files) == 6

    def test_early_stopping(self, tmp_path):
        """æµ‹è¯•æ—©åœæœºåˆ¶"""
        model, tokenizer, config = train_model(
            epochs=100,  # è®¾ç½®å¾ˆå¤šepoch
            checkpoint_dir=tmp_path / "outputs"
        )

        # éªŒè¯æ—©åœç”Ÿæ•ˆï¼ˆåº”è¯¥<100ä¸ªcheckpointï¼‰
        checkpoint_files = list((tmp_path / "outputs" / "checkpoints").glob("*.pt"))
        assert len(checkpoint_files) < 100

    def test_gradient_accumulation(self, tmp_path):
        """æµ‹è¯•æ¢¯åº¦ç´¯ç§¯"""
        # å°batch + ç´¯ç§¯ vs å¤§batch
        # éªŒè¯æŸå¤±ä¸€è‡´æ€§
        pass

    def test_mixed_precision(self, tmp_path):
        """æµ‹è¯•æ··åˆç²¾åº¦è®­ç»ƒ"""
        if not torch.cuda.is_available():
            pytest.skip("éœ€è¦GPU")

        # éªŒè¯AMPæ­£å¸¸å·¥ä½œ
        pass

    def test_temp_checkpoint(self, tmp_path):
        """æµ‹è¯•ä¸´æ—¶checkpoint"""
        model, tokenizer, config = train_model(
            epochs=2,
            checkpoint_dir=tmp_path / "outputs",
            temp_checkpoint_freq=10  # æ¯10æ­¥
        )

        # éªŒè¯tempæ–‡ä»¶è¢«åˆ›å»ºå’Œæ¸…ç†
        temp_dir = Path(".cache/temp")
        # è®­ç»ƒååº”è¯¥è¢«æ¸…ç†
        temp_files = list(temp_dir.glob("temp_*.pt"))
        assert len(temp_files) == 0


# tests/test_checkpoint.py
class TestCheckpointManager:
    def test_save_complete_state(self, tmp_path):
        """æµ‹è¯•ä¿å­˜å®Œæ•´è®­ç»ƒçŠ¶æ€"""
        # åˆ›å»ºè™šæ‹Ÿæ¨¡å‹å’Œä¼˜åŒ–å™¨
        checkpoint = torch.load(checkpoint_path)

        # éªŒè¯åŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µ
        assert 'model_state_dict' in checkpoint
        assert 'optimizer_state_dict' in checkpoint
        assert 'scheduler_state_dict' in checkpoint
        assert 'epoch' in checkpoint
        assert 'global_step' in checkpoint
        assert 'loss_history' in checkpoint
        assert 'metrics' in checkpoint

    def test_load_checkpoint(self, tmp_path):
        """æµ‹è¯•åŠ è½½checkpoint"""
        # ä¿å­˜
        mgr = CheckpointManager(save_dir=tmp_path)
        mgr.save_checkpoint(...)

        # åŠ è½½
        epoch, step, loss_history, metrics = mgr.load_checkpoint(...)

        # éªŒè¯çŠ¶æ€æ­£ç¡®æ¢å¤
        assert epoch == saved_epoch
        assert step == saved_step

    def test_best_checkpoint_tracking(self, tmp_path):
        """æµ‹è¯•æœ€ä½³æ¨¡å‹è¿½è¸ª"""
        # ä¿å­˜å¤šä¸ªcheckpointï¼Œæ ‡è®°is_best
        # éªŒè¯åªæœ‰ä¸€ä¸ªis_best
        pass

    def test_metadata_consistency(self, tmp_path):
        """æµ‹è¯•å…ƒæ•°æ®ä¸€è‡´æ€§"""
        # éªŒè¯metadata.jsonæ­£ç¡®è®°å½•æ‰€æœ‰checkpoint
        pass


# tests/test_data_loader.py
class TestDataLoader:
    def test_batch_generation(self):
        """æµ‹è¯•batchç”Ÿæˆ"""
        pass

    def test_padding(self):
        """æµ‹è¯•paddingæ­£ç¡®æ€§"""
        pass

    def test_tokenization(self):
        """æµ‹è¯•åˆ†è¯ä¸€è‡´æ€§"""
        pass


# tests/test_callbacks.py
class TestCallbacks:
    def test_progress_callback(self):
        """æµ‹è¯•è¿›åº¦æ¡å›è°ƒ"""
        pass

    def test_early_stopping_callback(self):
        """æµ‹è¯•æ—©åœå›è°ƒ"""
        pass

    def test_lr_scheduler_callback(self):
        """æµ‹è¯•å­¦ä¹ ç‡è°ƒåº¦å›è°ƒ"""
        pass
```

**å·¥ä½œé‡**: 24-30å°æ—¶
**éªŒæ”¶æ ‡å‡†**: pytestè¦†ç›–ç‡ > 80%

---

#### T1.2: æ¢¯åº¦ç›‘æ§å’Œè°ƒè¯•å·¥å…· âœ… P0
**éœ€æ±‚**: è¯†åˆ«è®­ç»ƒé—®é¢˜ï¼ˆæ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸ï¼‰

**å®ç°**:
```python
# apt_model/training/gradient_monitor.py
import torch
import numpy as np
from collections import defaultdict

class GradientMonitor:
    """æ¢¯åº¦ç›‘æ§å·¥å…·"""

    def __init__(self, model, logger=None):
        self.model = model
        self.logger = logger
        self.gradient_history = defaultdict(list)
        self.gradient_norms = []

    def check_gradient_flow(self):
        """æ£€æŸ¥æ¢¯åº¦æµï¼Œè¯†åˆ«æ¢¯åº¦æ¶ˆå¤±/çˆ†ç‚¸"""
        gradients = {}

        for name, param in self.model.named_parameters():
            if param.grad is not None:
                grad_norm = param.grad.norm().item()
                gradients[name] = grad_norm
                self.gradient_history[name].append(grad_norm)

        # æ£€æµ‹å¼‚å¸¸
        issues = []
        for name, grad_norm in gradients.items():
            if grad_norm < 1e-7:
                issues.append(f"âš ï¸  æ¢¯åº¦æ¶ˆå¤±: {name} (norm={grad_norm:.2e})")
            elif grad_norm > 1e3:
                issues.append(f"âš ï¸  æ¢¯åº¦çˆ†ç‚¸: {name} (norm={grad_norm:.2e})")
            elif torch.isnan(torch.tensor(grad_norm)):
                issues.append(f"âŒ NaNæ¢¯åº¦: {name}")

        if issues and self.logger:
            for issue in issues:
                self.logger.warning(issue)

        return gradients, issues

    def log_gradient_norms(self, step):
        """è®°å½•æ¢¯åº¦èŒƒæ•°"""
        total_norm = 0
        for param in self.model.parameters():
            if param.grad is not None:
                param_norm = param.grad.data.norm(2)
                total_norm += param_norm.item() ** 2

        total_norm = total_norm ** 0.5
        self.gradient_norms.append((step, total_norm))

        if self.logger:
            self.logger.info(f"Step {step}: Total gradient norm = {total_norm:.4f}")

        return total_norm

    def detect_gradient_anomalies(self):
        """æ£€æµ‹æ¢¯åº¦å¼‚å¸¸ï¼ˆNaN, Infç­‰ï¼‰"""
        anomalies = []

        for name, param in self.model.named_parameters():
            if param.grad is not None:
                if torch.isnan(param.grad).any():
                    anomalies.append(f"NaN in {name}")
                if torch.isinf(param.grad).any():
                    anomalies.append(f"Inf in {name}")

        return anomalies

    def plot_gradient_flow(self, save_path=None):
        """å¯è§†åŒ–æ¢¯åº¦æµ"""
        import matplotlib.pyplot as plt

        fig, ax = plt.subplots(figsize=(12, 6))

        # ç»˜åˆ¶æ¯å±‚çš„æ¢¯åº¦èŒƒæ•°
        layers = []
        avg_grads = []

        for name, grad_list in self.gradient_history.items():
            if len(grad_list) > 0:
                layers.append(name)
                avg_grads.append(np.mean(grad_list))

        ax.bar(range(len(layers)), avg_grads, alpha=0.7)
        ax.set_xticks(range(len(layers)))
        ax.set_xticklabels(layers, rotation=90, ha='right')
        ax.set_ylabel('Average Gradient Norm')
        ax.set_title('Gradient Flow Across Layers')
        ax.set_yscale('log')

        plt.tight_layout()

        if save_path:
            plt.savefig(save_path)

        return fig

    def get_gradient_stats(self):
        """è·å–æ¢¯åº¦ç»Ÿè®¡ä¿¡æ¯"""
        stats = {}

        for name, grad_list in self.gradient_history.items():
            if len(grad_list) > 0:
                stats[name] = {
                    'mean': np.mean(grad_list),
                    'std': np.std(grad_list),
                    'min': np.min(grad_list),
                    'max': np.max(grad_list)
                }

        return stats


# é›†æˆåˆ°trainer.py
def train_model(..., enable_gradient_monitoring=False):
    """
    å‚æ•°:
        enable_gradient_monitoring: å¯ç”¨æ¢¯åº¦ç›‘æ§ï¼ˆè°ƒè¯•ç”¨ï¼‰
    """

    if enable_gradient_monitoring:
        gradient_monitor = GradientMonitor(model, logger=logger)

    for epoch in range(epochs):
        for batch in dataloader:
            # ... è®­ç»ƒä»£ç  ...

            if enable_gradient_monitoring:
                # æ£€æŸ¥æ¢¯åº¦æµ
                gradients, issues = gradient_monitor.check_gradient_flow()

                # è®°å½•æ¢¯åº¦èŒƒæ•°
                gradient_monitor.log_gradient_norms(global_step)

                # æ£€æµ‹å¼‚å¸¸
                anomalies = gradient_monitor.detect_gradient_anomalies()
                if anomalies:
                    logger.error(f"æ£€æµ‹åˆ°æ¢¯åº¦å¼‚å¸¸: {anomalies}")

    # è®­ç»ƒç»“æŸåç”ŸæˆæŠ¥å‘Š
    if enable_gradient_monitoring:
        gradient_monitor.plot_gradient_flow("gradient_flow.png")
        stats = gradient_monitor.get_gradient_stats()
        logger.info(f"æ¢¯åº¦ç»Ÿè®¡: {stats}")
```

**å·¥ä½œé‡**: 8-10å°æ—¶

---

#### T1.3: è®­ç»ƒå¯è§†åŒ–é¢æ¿å¢å¼º âœ… P1
**éœ€æ±‚**: å®æ—¶ç›‘æ§è®­ç»ƒçŠ¶æ€

**å®ç°**:
```python
# apt_model/training/visualizer.py
from torch.utils.tensorboard import SummaryWriter
import matplotlib.pyplot as plt

class TrainingVisualizer:
    """è®­ç»ƒå¯è§†åŒ–å·¥å…·"""

    def __init__(self, log_dir="runs", use_wandb=False):
        self.tensorboard = SummaryWriter(log_dir)
        self.use_wandb = use_wandb

        if use_wandb:
            import wandb
            self.wandb = wandb

    def log_training_step(self, metrics, step):
        """è®°å½•è®­ç»ƒæ­¥éª¤"""
        for key, value in metrics.items():
            self.tensorboard.add_scalar(f'train/{key}', value, step)

            if self.use_wandb:
                self.wandb.log({f'train/{key}': value}, step=step)

    def log_validation_step(self, metrics, step):
        """è®°å½•éªŒè¯æ­¥éª¤"""
        for key, value in metrics.items():
            self.tensorboard.add_scalar(f'val/{key}', value, step)

    def plot_model_architecture(self, model, input_sample):
        """ç»˜åˆ¶æ¨¡å‹æ¶æ„å›¾"""
        try:
            from torchviz import make_dot

            output = model(input_sample)
            dot = make_dot(output, params=dict(model.named_parameters()))

            self.tensorboard.add_graph(model, input_sample)

            return dot
        except ImportError:
            logger.warning("éœ€è¦å®‰è£…torchviz: pip install torchviz")

    def plot_gradient_flow(self, model, step):
        """ç»˜åˆ¶æ¢¯åº¦æµå›¾"""
        gradient_norms = []
        layer_names = []

        for name, param in model.named_parameters():
            if param.grad is not None:
                gradient_norms.append(param.grad.norm().item())
                layer_names.append(name)

        # åˆ›å»ºæ¡å½¢å›¾
        fig, ax = plt.subplots(figsize=(15, 5))
        ax.bar(range(len(gradient_norms)), gradient_norms)
        ax.set_xticks(range(len(layer_names)))
        ax.set_xticklabels(layer_names, rotation=90)
        ax.set_ylabel('Gradient Norm')
        ax.set_title(f'Gradient Flow - Step {step}')

        self.tensorboard.add_figure('gradients/flow', fig, step)
        plt.close(fig)

    def plot_weight_distributions(self, model, step):
        """ç»˜åˆ¶æƒé‡åˆ†å¸ƒç›´æ–¹å›¾"""
        for name, param in model.named_parameters():
            if 'weight' in name:
                self.tensorboard.add_histogram(f'weights/{name}', param, step)
            if 'bias' in name:
                self.tensorboard.add_histogram(f'biases/{name}', param, step)

    def log_attention_weights(self, attention_weights, step):
        """è®°å½•æ³¨æ„åŠ›æƒé‡"""
        # å¯è§†åŒ–æ³¨æ„åŠ›çƒ­åŠ›å›¾
        pass

    def close(self):
        """å…³é—­writer"""
        self.tensorboard.close()

        if self.use_wandb:
            self.wandb.finish()
```

**å·¥ä½œé‡**: 10-12å°æ—¶

---

#### T1.4: è‡ªåŠ¨è¶…å‚æ•°æœç´¢é›†æˆ âœ… P2
**å½“å‰çŠ¶æ€**: æœ‰Optunaæ–‡ä»¶ä½†æœªé›†æˆ

**å®ç°**:
```python
# apt_model/training/hyperparameter_search.py
import optuna
from optuna.visualization import plot_optimization_history, plot_param_importances

class HyperparameterSearcher:
    """è¶…å‚æ•°æœç´¢å™¨"""

    def __init__(self, search_space, n_trials=100, storage=None):
        """
        Args:
            search_space: æœç´¢ç©ºé—´å®šä¹‰
            n_trials: è¯•éªŒæ¬¡æ•°
            storage: Optunaå­˜å‚¨åç«¯
        """
        self.search_space = search_space
        self.n_trials = n_trials
        self.storage = storage or "sqlite:///apt_optuna.db"

    def objective(self, trial):
        """ä¼˜åŒ–ç›®æ ‡å‡½æ•°"""
        # æ ¹æ®æœç´¢ç©ºé—´å»ºè®®å‚æ•°
        params = {}
        for key, value_range in self.search_space.items():
            if isinstance(value_range, tuple):
                # è¿ç»­ç©ºé—´
                if isinstance(value_range[0], float):
                    params[key] = trial.suggest_float(key, value_range[0], value_range[1], log=True)
                else:
                    params[key] = trial.suggest_int(key, value_range[0], value_range[1])
            elif isinstance(value_range, list):
                # ç¦»æ•£ç©ºé—´
                params[key] = trial.suggest_categorical(key, value_range)

        # è¿è¡Œè®­ç»ƒ
        model, tokenizer, config = train_model(
            epochs=5,  # å¿«é€Ÿè¯•éªŒ
            batch_size=params.get('batch_size', 8),
            learning_rate=params.get('learning_rate', 3e-5),
            checkpoint_dir=f"./optuna_trial_{trial.number}"
        )

        # è¿”å›éªŒè¯æŸå¤±ï¼ˆè¶Šå°è¶Šå¥½ï¼‰
        # è¿™é‡Œéœ€è¦ä»è®­ç»ƒè¿”å›éªŒè¯æŒ‡æ ‡
        return validation_loss

    def optimize(self):
        """è¿è¡Œè¶…å‚æ•°æœç´¢"""
        study = optuna.create_study(
            direction="minimize",
            storage=self.storage,
            study_name="apt_hyperparameter_search"
        )

        study.optimize(self.objective, n_trials=self.n_trials)

        return study

    def get_best_params(self, study):
        """è·å–æœ€ä½³å‚æ•°"""
        return study.best_params

    def visualize_results(self, study, save_dir="./optuna_results"):
        """å¯è§†åŒ–æœç´¢ç»“æœ"""
        import os
        os.makedirs(save_dir, exist_ok=True)

        # ä¼˜åŒ–å†å²
        fig1 = plot_optimization_history(study)
        fig1.write_image(f"{save_dir}/optimization_history.png")

        # å‚æ•°é‡è¦æ€§
        fig2 = plot_param_importances(study)
        fig2.write_image(f"{save_dir}/param_importances.png")


# ä½¿ç”¨ç¤ºä¾‹
search_space = {
    'learning_rate': (1e-5, 1e-3),
    'batch_size': [8, 16, 32],
    'num_layers': [6, 12, 24],
    'dropout': (0.1, 0.5)
}

searcher = HyperparameterSearcher(search_space, n_trials=50)
study = searcher.optimize()
best_params = searcher.get_best_params(study)
searcher.visualize_results(study)
```

**å·¥ä½œé‡**: 12-16å°æ—¶

---

### âœ… æ ¸å¿ƒè®­ç»ƒåŠŸèƒ½å®Œå–„æ€»ç»“

| ä»»åŠ¡ | ä¼˜å…ˆçº§ | å·¥ä½œé‡ | å®Œæˆåæå‡ |
|------|--------|--------|-----------|
| T1.1 å•å…ƒæµ‹è¯• | P0 | 24-30h | 80% â†’ 90% |
| T1.2 æ¢¯åº¦ç›‘æ§ | P0 | 8-10h | 90% â†’ 95% |
| T1.3 å¯è§†åŒ–å¢å¼º | P1 | 10-12h | 95% â†’ 98% |
| T1.4 è¶…å‚æ•°æœç´¢ | P2 | 12-16h | 98% â†’ 100% |
| **æ€»è®¡** | - | **54-68h** | **80% â†’ 100%** |

---

## 2ï¸âƒ£ é”™è¯¯å¤„ç†ç³»ç»Ÿ 90% â†’ 100%

### ğŸ“‹ å®Œå–„æ¸…å•

#### E2.1: é”™è¯¯æŒä¹…åŒ–å’Œåˆ†æ âœ… P0
**å®ç°**:
```python
# apt_model/infrastructure/error_logger.py
import json
import sqlite3
from datetime import datetime
from pathlib import Path

class ErrorLogger:
    """é”™è¯¯æŒä¹…åŒ–æ—¥å¿—å™¨"""

    def __init__(self, db_path=".cache/errors.db"):
        self.db_path = Path(db_path)
        self.db_path.parent.mkdir(parents=True, exist_ok=True)
        self._init_database()

    def _init_database(self):
        """åˆå§‹åŒ–æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute("""
            CREATE TABLE IF NOT EXISTS errors (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                timestamp TEXT NOT NULL,
                error_type TEXT NOT NULL,
                error_message TEXT NOT NULL,
                context TEXT,
                stack_trace TEXT,
                resolved BOOLEAN DEFAULT FALSE
            )
        """)

        conn.commit()
        conn.close()

    def log_error(self, error, context="", stack_trace=""):
        """è®°å½•é”™è¯¯åˆ°æ•°æ®åº“"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        cursor.execute("""
            INSERT INTO errors (timestamp, error_type, error_message, context, stack_trace)
            VALUES (?, ?, ?, ?, ?)
        """, (
            datetime.now().isoformat(),
            type(error).__name__,
            str(error),
            context,
            stack_trace
        ))

        conn.commit()
        conn.close()

    def analyze_error_patterns(self, days=7):
        """åˆ†æé”™è¯¯æ¨¡å¼"""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        # è·å–æœ€è¿‘Nå¤©çš„é”™è¯¯
        cursor.execute("""
            SELECT error_type, COUNT(*) as count
            FROM errors
            WHERE timestamp > datetime('now', '-{} days')
            GROUP BY error_type
            ORDER BY count DESC
        """.format(days))

        patterns = cursor.fetchall()
        conn.close()

        return {error_type: count for error_type, count in patterns}

    def generate_error_report(self, save_path="error_report.json"):
        """ç”Ÿæˆé”™è¯¯æŠ¥å‘Š"""
        patterns = self.analyze_error_patterns()

        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()

        # è·å–æœ€è¿‘é”™è¯¯è¯¦æƒ…
        cursor.execute("""
            SELECT timestamp, error_type, error_message, context
            FROM errors
            ORDER BY timestamp DESC
            LIMIT 100
        """)

        recent_errors = [
            {
                'timestamp': row[0],
                'type': row[1],
                'message': row[2],
                'context': row[3]
            }
            for row in cursor.fetchall()
        ]

        conn.close()

        report = {
            'generated_at': datetime.now().isoformat(),
            'error_patterns': patterns,
            'recent_errors': recent_errors,
            'total_errors': sum(patterns.values())
        }

        with open(save_path, 'w') as f:
            json.dump(report, f, indent=2)

        return report
```

**å·¥ä½œé‡**: 6-8å°æ—¶

---

#### E2.2: åˆ†å¸ƒå¼é”™è¯¯åŒæ­¥ âœ… P1
**éœ€æ±‚**: å¤šGPU/å¤šæœºè®­ç»ƒæ—¶é”™è¯¯åŒæ­¥

**å®ç°**:
```python
# apt_model/infrastructure/distributed_error_handler.py
import torch.distributed as dist

class DistributedErrorHandler:
    """åˆ†å¸ƒå¼é”™è¯¯å¤„ç†å™¨"""

    def __init__(self, rank, world_size):
        self.rank = rank
        self.world_size = world_size

    def broadcast_error(self, error):
        """å¹¿æ’­é”™è¯¯åˆ°æ‰€æœ‰è¿›ç¨‹"""
        if not dist.is_initialized():
            return

        # åºåˆ—åŒ–é”™è¯¯ä¿¡æ¯
        error_info = {
            'type': type(error).__name__,
            'message': str(error),
            'rank': self.rank
        }

        # å¹¿æ’­åˆ°æ‰€æœ‰è¿›ç¨‹
        error_tensor = torch.tensor([1 if error else 0], dtype=torch.int)
        dist.broadcast(error_tensor, src=self.rank)

        if error_tensor.item() == 1:
            # å‘ç”Ÿé”™è¯¯ï¼Œæ‰€æœ‰è¿›ç¨‹åº”è¯¥åœæ­¢
            dist.barrier()
            raise RuntimeError(f"Process {self.rank} encountered error: {error}")

    def sync_checkpoint_on_error(self, model, optimizer, checkpoint_path):
        """é”™è¯¯æ—¶åŒæ­¥checkpoint"""
        try:
            # ä¸»è¿›ç¨‹ä¿å­˜checkpoint
            if self.rank == 0:
                torch.save({
                    'model': model.state_dict(),
                    'optimizer': optimizer.state_dict()
                }, checkpoint_path)

            # ç­‰å¾…æ‰€æœ‰è¿›ç¨‹
            dist.barrier()
        except Exception as e:
            self.broadcast_error(e)
```

**å·¥ä½œé‡**: 8-10å°æ—¶

---

### âœ… é”™è¯¯å¤„ç†ç³»ç»Ÿå®Œå–„æ€»ç»“

| ä»»åŠ¡ | ä¼˜å…ˆçº§ | å·¥ä½œé‡ | å®Œæˆåæå‡ |
|------|--------|--------|-----------|
| E2.1 é”™è¯¯æŒä¹…åŒ– | P0 | 6-8h | 90% â†’ 95% |
| E2.2 åˆ†å¸ƒå¼é”™è¯¯åŒæ­¥ | P1 | 8-10h | 95% â†’ 100% |
| **æ€»è®¡** | - | **14-18h** | **90% â†’ 100%** |

---

## 3ï¸âƒ£ æ’ä»¶ç³»ç»Ÿ 70% â†’ 100%

### ğŸ“‹ å®Œå–„æ¸…å•

#### P3.1: æ’ä»¶ç‰ˆæœ¬ç®¡ç† âœ… P0
**å®ç°**:
```python
# apt_model/console/plugin_version_manager.py
import semver
from typing import Dict, List

class PluginVersionManager:
    """æ’ä»¶ç‰ˆæœ¬ç®¡ç†å™¨"""

    def __init__(self, registry):
        self.registry = registry
        self.version_history = {}

    def check_updates(self, plugin_name):
        """æ£€æŸ¥æ’ä»¶æ›´æ–°"""
        current_version = self.registry.get_plugin_version(plugin_name)
        latest_version = self._fetch_latest_version(plugin_name)

        if semver.compare(latest_version, current_version) > 0:
            return {
                'has_update': True,
                'current': current_version,
                'latest': latest_version
            }

        return {'has_update': False}

    def upgrade_plugin(self, plugin_name, target_version=None):
        """å‡çº§æ’ä»¶"""
        if target_version is None:
            target_version = self._fetch_latest_version(plugin_name)

        # ä¿å­˜å½“å‰ç‰ˆæœ¬ï¼ˆä»¥ä¾¿å›æ»šï¼‰
        current_version = self.registry.get_plugin_version(plugin_name)
        self.version_history[plugin_name] = current_version

        # ä¸‹è½½æ–°ç‰ˆæœ¬
        plugin_package = self._download_plugin(plugin_name, target_version)

        # å¸è½½æ—§ç‰ˆæœ¬
        self.registry.unload_plugin(plugin_name)

        # å®‰è£…æ–°ç‰ˆæœ¬
        self.registry.install_plugin(plugin_package)

        return True

    def rollback_plugin(self, plugin_name):
        """å›æ»šæ’ä»¶ç‰ˆæœ¬"""
        if plugin_name not in self.version_history:
            raise ValueError(f"No rollback version for {plugin_name}")

        target_version = self.version_history[plugin_name]
        return self.upgrade_plugin(plugin_name, target_version)

    def resolve_version_conflicts(self, plugins: List[str]):
        """è§£å†³ç‰ˆæœ¬å†²çª"""
        dependencies = {}

        for plugin in plugins:
            deps = self.registry.get_plugin_dependencies(plugin)
            for dep_name, dep_version in deps.items():
                if dep_name in dependencies:
                    # æ£€æŸ¥ç‰ˆæœ¬å†²çª
                    if dependencies[dep_name] != dep_version:
                        # å°è¯•æ‰¾åˆ°å…¼å®¹ç‰ˆæœ¬
                        compatible = self._find_compatible_version(
                            dep_name,
                            [dependencies[dep_name], dep_version]
                        )
                        if compatible:
                            dependencies[dep_name] = compatible
                        else:
                            raise ValueError(
                                f"Version conflict for {dep_name}: "
                                f"{dependencies[dep_name]} vs {dep_version}"
                            )
                else:
                    dependencies[dep_name] = dep_version

        return dependencies
```

**å·¥ä½œé‡**: 12-16å°æ—¶

---

#### P3.2: æ’ä»¶å¸‚åœº/ä»“åº“ âœ… P0
**å®ç°**:
```python
# apt_model/console/plugin_marketplace.py
import requests
from pathlib import Path

class PluginMarketplace:
    """æ’ä»¶å¸‚åœºå®¢æˆ·ç«¯"""

    def __init__(self, server_url="https://apt-plugins.example.com"):
        self.server_url = server_url

    def search_plugins(self, keyword, category=None):
        """æœç´¢æ’ä»¶"""
        params = {'q': keyword}
        if category:
            params['category'] = category

        response = requests.get(f"{self.server_url}/api/search", params=params)
        return response.json()

    def get_plugin_info(self, plugin_name):
        """è·å–æ’ä»¶è¯¦ç»†ä¿¡æ¯"""
        response = requests.get(f"{self.server_url}/api/plugins/{plugin_name}")
        return response.json()

    def download_plugin(self, plugin_name, version="latest"):
        """ä¸‹è½½æ’ä»¶"""
        response = requests.get(
            f"{self.server_url}/api/download/{plugin_name}/{version}"
        )

        # ä¿å­˜åˆ°æœ¬åœ°
        plugin_path = Path(".cache/plugins") / f"{plugin_name}_{version}.apx"
        plugin_path.parent.mkdir(parents=True, exist_ok=True)

        with open(plugin_path, 'wb') as f:
            f.write(response.content)

        return plugin_path

    def publish_plugin(self, plugin_package, api_key):
        """å‘å¸ƒæ’ä»¶åˆ°å¸‚åœº"""
        files = {'package': open(plugin_package, 'rb')}
        headers = {'Authorization': f'Bearer {api_key}'}

        response = requests.post(
            f"{self.server_url}/api/publish",
            files=files,
            headers=headers
        )

        return response.json()

    def rate_plugin(self, plugin_name, rating, comment=""):
        """è¯„åˆ†æ’ä»¶"""
        data = {
            'plugin': plugin_name,
            'rating': rating,
            'comment': comment
        }

        response = requests.post(
            f"{self.server_url}/api/rate",
            json=data
        )

        return response.json()
```

**æ³¨**: éœ€è¦å•ç‹¬å®ç°æœåŠ¡å™¨ç«¯ï¼ˆFlask/FastAPIï¼‰

**å·¥ä½œé‡**: 24-30å°æ—¶ï¼ˆå«æœåŠ¡å™¨ï¼‰

---

#### P3.3: æ’ä»¶æ²™ç®±éš”ç¦» âœ… P0
**å®ç°**:
```python
# apt_model/console/plugin_sandbox.py
import sys
import io
from contextlib import redirect_stdout, redirect_stderr
import resource

class PluginSandbox:
    """æ’ä»¶æ²™ç®±ç¯å¢ƒ"""

    def __init__(self, allowed_imports=None, resource_limits=None):
        self.allowed_imports = allowed_imports or [
            'numpy', 'torch', 'transformers'
        ]
        self.resource_limits = resource_limits or {
            'max_memory_mb': 1024,
            'max_cpu_time_sec': 60
        }

    def execute_plugin(self, plugin_code, globals_dict=None):
        """åœ¨æ²™ç®±ä¸­æ‰§è¡Œæ’ä»¶"""
        if globals_dict is None:
            globals_dict = {}

        # é™åˆ¶å¯å¯¼å…¥æ¨¡å—
        safe_builtins = {
            '__import__': self._safe_import,
            '__builtins__': __builtins__
        }

        # é™åˆ¶èµ„æºä½¿ç”¨
        self._set_resource_limits()

        # æ•è·è¾“å‡º
        stdout_capture = io.StringIO()
        stderr_capture = io.StringIO()

        try:
            with redirect_stdout(stdout_capture), redirect_stderr(stderr_capture):
                exec(plugin_code, safe_builtins, globals_dict)
        except Exception as e:
            return {
                'success': False,
                'error': str(e),
                'stdout': stdout_capture.getvalue(),
                'stderr': stderr_capture.getvalue()
            }
        finally:
            self._reset_resource_limits()

        return {
            'success': True,
            'stdout': stdout_capture.getvalue(),
            'stderr': stderr_capture.getvalue(),
            'globals': globals_dict
        }

    def _safe_import(self, name, *args, **kwargs):
        """å®‰å…¨çš„importå‡½æ•°"""
        if name not in self.allowed_imports:
            raise ImportError(f"Import of {name} not allowed in sandbox")
        return __import__(name, *args, **kwargs)

    def _set_resource_limits(self):
        """è®¾ç½®èµ„æºé™åˆ¶"""
        # é™åˆ¶å†…å­˜
        max_mem_bytes = self.resource_limits['max_memory_mb'] * 1024 * 1024
        resource.setrlimit(resource.RLIMIT_AS, (max_mem_bytes, max_mem_bytes))

        # é™åˆ¶CPUæ—¶é—´
        max_cpu_time = self.resource_limits['max_cpu_time_sec']
        resource.setrlimit(resource.RLIMIT_CPU, (max_cpu_time, max_cpu_time))

    def _reset_resource_limits(self):
        """é‡ç½®èµ„æºé™åˆ¶"""
        resource.setrlimit(resource.RLIMIT_AS, (resource.RLIM_INFINITY, resource.RLIM_INFINITY))
        resource.setrlimit(resource.RLIMIT_CPU, (resource.RLIM_INFINITY, resource.RLIM_INFINITY))
```

**å·¥ä½œé‡**: 16-20å°æ—¶

---

#### P3.4: æ’ä»¶æ€§èƒ½ç›‘æ§ âœ… P1
**å®ç°**:
```python
# apt_model/console/plugin_profiler.py
import time
import psutil
import threading

class PluginProfiler:
    """æ’ä»¶æ€§èƒ½åˆ†æå™¨"""

    def __init__(self):
        self.metrics = {}

    def profile_plugin(self, plugin_func, *args, **kwargs):
        """åˆ†ææ’ä»¶æ€§èƒ½"""
        # å¼€å§‹ç›‘æ§
        process = psutil.Process()

        # è®°å½•å¼€å§‹çŠ¶æ€
        start_time = time.time()
        start_memory = process.memory_info().rss / 1024 / 1024  # MB
        start_cpu_percent = process.cpu_percent()

        # æ‰§è¡Œæ’ä»¶
        result = plugin_func(*args, **kwargs)

        # è®°å½•ç»“æŸçŠ¶æ€
        end_time = time.time()
        end_memory = process.memory_info().rss / 1024 / 1024
        end_cpu_percent = process.cpu_percent()

        # è®¡ç®—æŒ‡æ ‡
        metrics = {
            'execution_time_sec': end_time - start_time,
            'memory_used_mb': end_memory - start_memory,
            'avg_cpu_percent': (start_cpu_percent + end_cpu_percent) / 2
        }

        plugin_name = plugin_func.__name__
        self.metrics[plugin_name] = metrics

        return result, metrics

    def get_plugin_stats(self, plugin_name):
        """è·å–æ’ä»¶ç»Ÿè®¡ä¿¡æ¯"""
        return self.metrics.get(plugin_name, {})
```

**å·¥ä½œé‡**: 6-8å°æ—¶

---

#### P3.5: æ’ä»¶å•å…ƒæµ‹è¯• âœ… P0
**å®ç°**:
```python
# tests/test_plugin_system_complete.py
import pytest
from apt_model.console.plugin_loader import PluginLoader
from apt_model.console.plugin_sandbox import PluginSandbox
from apt_model.console.plugin_version_manager import PluginVersionManager

class TestPluginSystem:
    def test_plugin_load_unload(self):
        """æµ‹è¯•æ’ä»¶åŠ è½½å’Œå¸è½½"""
        loader = PluginLoader()

        # åŠ è½½æ’ä»¶
        plugin = loader.load_plugin("test_plugin.apx")
        assert plugin is not None

        # å¸è½½æ’ä»¶
        loader.unload_plugin("test_plugin")
        assert "test_plugin" not in loader.loaded_plugins

    def test_plugin_sandbox(self):
        """æµ‹è¯•æ’ä»¶æ²™ç®±"""
        sandbox = PluginSandbox(allowed_imports=['numpy'])

        # å…è®¸çš„å¯¼å…¥
        result = sandbox.execute_plugin("import numpy\nx = numpy.array([1,2,3])")
        assert result['success']

        # ç¦æ­¢çš„å¯¼å…¥
        result = sandbox.execute_plugin("import os\nos.system('ls')")
        assert not result['success']
        assert 'not allowed' in result['error']

    def test_plugin_resource_limits(self):
        """æµ‹è¯•èµ„æºé™åˆ¶"""
        sandbox = PluginSandbox(resource_limits={'max_memory_mb': 100})

        # è¶…å‡ºå†…å­˜é™åˆ¶
        code = "x = [0] * 10**9"  # å°è¯•åˆ†é…å¤§é‡å†…å­˜
        result = sandbox.execute_plugin(code)
        # åº”è¯¥å¤±è´¥æˆ–è¢«é™åˆ¶

    def test_plugin_version_management(self):
        """æµ‹è¯•ç‰ˆæœ¬ç®¡ç†"""
        version_mgr = PluginVersionManager(registry)

        # æ£€æŸ¥æ›´æ–°
        updates = version_mgr.check_updates("test_plugin")

        # å‡çº§
        if updates['has_update']:
            version_mgr.upgrade_plugin("test_plugin")

        # å›æ»š
        version_mgr.rollback_plugin("test_plugin")

    def test_plugin_dependency_resolution(self):
        """æµ‹è¯•ä¾èµ–è§£æ"""
        version_mgr = PluginVersionManager(registry)

        dependencies = version_mgr.resolve_version_conflicts([
            "plugin_a",  # ä¾èµ– plugin_c@1.0.0
            "plugin_b"   # ä¾èµ– plugin_c@1.1.0
        ])

        # åº”è¯¥æ‰¾åˆ°å…¼å®¹ç‰ˆæœ¬
        assert 'plugin_c' in dependencies
```

**å·¥ä½œé‡**: 12-16å°æ—¶

---

### âœ… æ’ä»¶ç³»ç»Ÿå®Œå–„æ€»ç»“

| ä»»åŠ¡ | ä¼˜å…ˆçº§ | å·¥ä½œé‡ | å®Œæˆåæå‡ |
|------|--------|--------|-----------|
| P3.1 ç‰ˆæœ¬ç®¡ç† | P0 | 12-16h | 70% â†’ 78% |
| P3.2 æ’ä»¶å¸‚åœº | P0 | 24-30h | 78% â†’ 85% |
| P3.3 æ²™ç®±éš”ç¦» | P0 | 16-20h | 85% â†’ 92% |
| P3.4 æ€§èƒ½ç›‘æ§ | P1 | 6-8h | 92% â†’ 96% |
| P3.5 å•å…ƒæµ‹è¯• | P0 | 12-16h | 96% â†’ 100% |
| **æ€»è®¡** | - | **70-90h** | **70% â†’ 100%** |

---

## 4ï¸âƒ£ å¤šæ¨¡æ€æ”¯æŒ 50% â†’ 100%

### ğŸ“‹ å®Œå–„æ¸…å•

#### M4.1: è§†è§‰ç¼–ç å™¨é›†æˆ âœ… P0
**å®ç°**:
```python
# apt_model/multimodal/vision_encoder.py
import torch
import torch.nn as nn
from transformers import CLIPModel, CLIPProcessor

class VisionEncoder(nn.Module):
    """è§†è§‰ç¼–ç å™¨"""

    def __init__(self, model_type='clip', freeze_encoder=True):
        super().__init__()

        if model_type == 'clip':
            self.model = CLIPModel.from_pretrained("openai/clip-vit-base-patch32")
            self.processor = CLIPProcessor.from_pretrained("openai/clip-vit-base-patch32")
        else:
            raise ValueError(f"Unsupported model type: {model_type}")

        if freeze_encoder:
            for param in self.model.parameters():
                param.requires_grad = False

    def encode_image(self, image):
        """å•å¼ å›¾åƒç¼–ç """
        inputs = self.processor(images=image, return_tensors="pt")
        image_features = self.model.get_image_features(**inputs)
        return image_features

    def encode_batch(self, images):
        """æ‰¹é‡å›¾åƒç¼–ç """
        inputs = self.processor(images=images, return_tensors="pt", padding=True)
        image_features = self.model.get_image_features(**inputs)
        return image_features

    def forward(self, images):
        """å‰å‘ä¼ æ’­"""
        return self.encode_batch(images)
```

**å·¥ä½œé‡**: 12-16å°æ—¶

---

#### M4.2: éŸ³é¢‘ç¼–ç å™¨ âœ… P1
**å®ç°**:
```python
# apt_model/multimodal/audio_encoder.py
import torch
import torch.nn as nn
from transformers import WhisperModel, WhisperProcessor

class AudioEncoder(nn.Module):
    """éŸ³é¢‘ç¼–ç å™¨"""

    def __init__(self, model_type='whisper'):
        super().__init__()

        if model_type == 'whisper':
            self.model = WhisperModel.from_pretrained("openai/whisper-base")
            self.processor = WhisperProcessor.from_pretrained("openai/whisper-base")

    def encode_audio(self, audio_path):
        """ç¼–ç éŸ³é¢‘æ–‡ä»¶"""
        import librosa

        # åŠ è½½éŸ³é¢‘
        audio, sr = librosa.load(audio_path, sr=16000)

        # å¤„ç†
        inputs = self.processor(audio, sampling_rate=16000, return_tensors="pt")

        # ç¼–ç 
        with torch.no_grad():
            audio_features = self.model.encoder(**inputs).last_hidden_state

        return audio_features
```

**å·¥ä½œé‡**: 10-12å°æ—¶

---

#### M4.3: è·¨æ¨¡æ€æ³¨æ„åŠ›æœºåˆ¶ âœ… P0
**å®ç°**:
```python
# apt_model/multimodal/cross_modal_attention.py
import torch
import torch.nn as nn

class CrossModalAttention(nn.Module):
    """è·¨æ¨¡æ€æ³¨æ„åŠ›"""

    def __init__(self, d_model=512, num_heads=8):
        super().__init__()

        self.multihead_attn = nn.MultiheadAttention(d_model, num_heads)
        self.layer_norm1 = nn.LayerNorm(d_model)
        self.layer_norm2 = nn.LayerNorm(d_model)

        self.feed_forward = nn.Sequential(
            nn.Linear(d_model, d_model * 4),
            nn.ReLU(),
            nn.Linear(d_model * 4, d_model)
        )

    def forward(self, text_embeds, image_embeds):
        """
        Args:
            text_embeds: [seq_len, batch, d_model]
            image_embeds: [num_patches, batch, d_model]

        Returns:
            fused_features: [seq_len, batch, d_model]
        """
        # æ–‡æœ¬attendåˆ°å›¾åƒ
        attn_output, attn_weights = self.multihead_attn(
            query=text_embeds,
            key=image_embeds,
            value=image_embeds
        )

        # æ®‹å·®è¿æ¥ + LayerNorm
        text_embeds = self.layer_norm1(text_embeds + attn_output)

        # Feed-forward
        ff_output = self.feed_forward(text_embeds)
        fused_features = self.layer_norm2(text_embeds + ff_output)

        return fused_features, attn_weights


class MultimodalFusion(nn.Module):
    """å¤šæ¨¡æ€èåˆæ¨¡å—"""

    def __init__(self, text_dim=512, image_dim=512, audio_dim=512, output_dim=512):
        super().__init__()

        # æŠ•å½±å±‚ï¼ˆç»Ÿä¸€ç»´åº¦ï¼‰
        self.text_proj = nn.Linear(text_dim, output_dim)
        self.image_proj = nn.Linear(image_dim, output_dim)
        self.audio_proj = nn.Linear(audio_dim, output_dim)

        # è·¨æ¨¡æ€æ³¨æ„åŠ›
        self.text_image_attn = CrossModalAttention(output_dim)
        self.text_audio_attn = CrossModalAttention(output_dim)

        # èåˆå±‚
        self.fusion_layer = nn.Sequential(
            nn.Linear(output_dim * 3, output_dim),
            nn.ReLU(),
            nn.Linear(output_dim, output_dim)
        )

    def forward(self, text_features, image_features=None, audio_features=None):
        """
        å¤šæ¨¡æ€èåˆ

        Args:
            text_features: æ–‡æœ¬ç‰¹å¾
            image_features: å›¾åƒç‰¹å¾ï¼ˆå¯é€‰ï¼‰
            audio_features: éŸ³é¢‘ç‰¹å¾ï¼ˆå¯é€‰ï¼‰

        Returns:
            fused_features: èåˆåçš„ç‰¹å¾
        """
        # æŠ•å½±åˆ°ç»Ÿä¸€ç»´åº¦
        text_proj = self.text_proj(text_features)

        features_list = [text_proj]

        if image_features is not None:
            image_proj = self.image_proj(image_features)
            text_image_fused, _ = self.text_image_attn(text_proj, image_proj)
            features_list.append(text_image_fused)

        if audio_features is not None:
            audio_proj = self.audio_proj(audio_features)
            text_audio_fused, _ = self.text_audio_attn(text_proj, audio_proj)
            features_list.append(text_audio_fused)

        # æ‹¼æ¥å¹¶èåˆ
        concatenated = torch.cat(features_list, dim=-1)
        fused_features = self.fusion_layer(concatenated)

        return fused_features
```

**å·¥ä½œé‡**: 16-20å°æ—¶

---

#### M4.4: å¤šæ¨¡æ€æ•°æ®åŠ è½½å™¨ âœ… P0
**å®ç°**:
```python
# apt_model/multimodal/multimodal_dataloader.py
import torch
from torch.utils.data import Dataset, DataLoader
from PIL import Image
import librosa

class MultimodalDataset(Dataset):
    """å¤šæ¨¡æ€æ•°æ®é›†"""

    def __init__(self, data_list, text_tokenizer, image_processor, audio_processor):
        """
        Args:
            data_list: æ•°æ®åˆ—è¡¨ï¼Œæ¯é¡¹åŒ…å«:
                {
                    'text': str,
                    'image_path': str (optional),
                    'audio_path': str (optional)
                }
        """
        self.data_list = data_list
        self.text_tokenizer = text_tokenizer
        self.image_processor = image_processor
        self.audio_processor = audio_processor

    def __len__(self):
        return len(self.data_list)

    def __getitem__(self, idx):
        item = self.data_list[idx]

        # å¤„ç†æ–‡æœ¬
        text_inputs = self.text_tokenizer(
            item['text'],
            padding='max_length',
            truncation=True,
            max_length=512,
            return_tensors='pt'
        )

        result = {
            'text_input_ids': text_inputs['input_ids'].squeeze(0),
            'text_attention_mask': text_inputs['attention_mask'].squeeze(0)
        }

        # å¤„ç†å›¾åƒï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'image_path' in item and item['image_path']:
            image = Image.open(item['image_path']).convert('RGB')
            image_inputs = self.image_processor(images=image, return_tensors='pt')
            result['image'] = image_inputs['pixel_values'].squeeze(0)

        # å¤„ç†éŸ³é¢‘ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'audio_path' in item and item['audio_path']:
            audio, sr = librosa.load(item['audio_path'], sr=16000)
            audio_inputs = self.audio_processor(audio, sampling_rate=16000, return_tensors='pt')
            result['audio'] = audio_inputs['input_features'].squeeze(0)

        return result


def create_multimodal_dataloader(data_list, tokenizer, image_processor, audio_processor, batch_size=8):
    """åˆ›å»ºå¤šæ¨¡æ€æ•°æ®åŠ è½½å™¨"""
    dataset = MultimodalDataset(data_list, tokenizer, image_processor, audio_processor)

    dataloader = DataLoader(
        dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=4,
        collate_fn=multimodal_collate_fn
    )

    return dataloader


def multimodal_collate_fn(batch):
    """å¤šæ¨¡æ€batchæ•´ç†å‡½æ•°"""
    # å¤„ç†å˜é•¿åºåˆ—
    result = {}

    # æ–‡æœ¬
    result['text_input_ids'] = torch.stack([item['text_input_ids'] for item in batch])
    result['text_attention_mask'] = torch.stack([item['text_attention_mask'] for item in batch])

    # å›¾åƒï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if 'image' in batch[0]:
        result['image'] = torch.stack([item['image'] for item in batch])

    # éŸ³é¢‘ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if 'audio' in batch[0]:
        result['audio'] = torch.stack([item['audio'] for item in batch])

    return result
```

**å·¥ä½œé‡**: 10-12å°æ—¶

---

#### M4.5: å¤šæ¨¡æ€APTæ¨¡å‹ âœ… P0
**å®ç°**:
```python
# apt_model/multimodal/multimodal_apt_model.py
import torch
import torch.nn as nn
from apt_model.modeling.apt_model import APTLargeModel
from apt_model.multimodal.vision_encoder import VisionEncoder
from apt_model.multimodal.audio_encoder import AudioEncoder
from apt_model.multimodal.cross_modal_attention import MultimodalFusion

class MultimodalAPTModel(nn.Module):
    """å¤šæ¨¡æ€APTæ¨¡å‹"""

    def __init__(self, apt_config, use_vision=True, use_audio=False):
        super().__init__()

        # æ–‡æœ¬ç¼–ç å™¨ï¼ˆåŸAPTæ¨¡å‹ï¼‰
        self.text_encoder = APTLargeModel(apt_config)

        # è§†è§‰ç¼–ç å™¨
        self.use_vision = use_vision
        if use_vision:
            self.vision_encoder = VisionEncoder(model_type='clip')

        # éŸ³é¢‘ç¼–ç å™¨
        self.use_audio = use_audio
        if use_audio:
            self.audio_encoder = AudioEncoder(model_type='whisper')

        # å¤šæ¨¡æ€èåˆ
        self.multimodal_fusion = MultimodalFusion(
            text_dim=apt_config.d_model,
            image_dim=512,
            audio_dim=512,
            output_dim=apt_config.d_model
        )

        # è¾“å‡ºå¤´
        self.lm_head = nn.Linear(apt_config.d_model, apt_config.vocab_size)

    def forward(self, text_input_ids, image=None, audio=None, text_attention_mask=None):
        """
        Args:
            text_input_ids: [batch, seq_len]
            image: [batch, 3, H, W] (optional)
            audio: [batch, audio_len] (optional)
            text_attention_mask: [batch, seq_len] (optional)

        Returns:
            logits: [batch, seq_len, vocab_size]
        """
        # ç¼–ç æ–‡æœ¬
        text_features = self.text_encoder(
            src_tokens=text_input_ids,
            tgt_tokens=text_input_ids,
            src_key_padding_mask=~text_attention_mask if text_attention_mask is not None else None
        )

        # ç¼–ç å›¾åƒ
        image_features = None
        if self.use_vision and image is not None:
            image_features = self.vision_encoder(image)

        # ç¼–ç éŸ³é¢‘
        audio_features = None
        if self.use_audio and audio is not None:
            audio_features = self.audio_encoder.encode_audio(audio)

        # å¤šæ¨¡æ€èåˆ
        fused_features = self.multimodal_fusion(
            text_features,
            image_features,
            audio_features
        )

        # ç”Ÿæˆè¾“å‡º
        logits = self.lm_head(fused_features)

        return logits

    def generate_from_multimodal(self, text, image=None, audio=None, max_length=100):
        """å¤šæ¨¡æ€ç”Ÿæˆ"""
        # TODO: å®ç°ç”Ÿæˆé€»è¾‘
        pass
```

**å·¥ä½œé‡**: 16-20å°æ—¶

---

#### M4.6: å¤šæ¨¡æ€è®­ç»ƒè„šæœ¬ âœ… P1
**å®ç°**:
```python
# apt_model/training/train_multimodal.py
def train_multimodal_model(
    data_list,
    epochs=10,
    batch_size=8,
    checkpoint_dir="./outputs_multimodal"
):
    """è®­ç»ƒå¤šæ¨¡æ€APTæ¨¡å‹"""

    # åˆå§‹åŒ–æ¨¡å‹
    config = APTConfig()
    model = MultimodalAPTModel(config, use_vision=True, use_audio=False)
    model = model.to(device)

    # åˆå§‹åŒ–ç¼–ç å™¨
    text_tokenizer = ...
    image_processor = ...
    audio_processor = ...

    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloader = create_multimodal_dataloader(
        data_list,
        text_tokenizer,
        image_processor,
        audio_processor,
        batch_size=batch_size
    )

    # ä¼˜åŒ–å™¨
    optimizer = torch.optim.AdamW(model.parameters(), lr=3e-5)
    scheduler = get_linear_schedule_with_warmup(optimizer, ...)

    # CheckpointManager
    checkpoint_mgr = CheckpointManager(save_dir=checkpoint_dir)

    # è®­ç»ƒå¾ªç¯
    for epoch in range(epochs):
        model.train()
        total_loss = 0

        for batch in dataloader:
            # å‰å‘ä¼ æ’­
            logits = model(
                text_input_ids=batch['text_input_ids'].to(device),
                image=batch.get('image').to(device) if 'image' in batch else None,
                text_attention_mask=batch['text_attention_mask'].to(device)
            )

            # è®¡ç®—æŸå¤±
            loss = F.cross_entropy(
                logits.view(-1, logits.size(-1)),
                batch['text_input_ids'].view(-1).to(device),
                ignore_index=tokenizer.pad_token_id
            )

            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            scheduler.step()

            total_loss += loss.item()

        avg_loss = total_loss / len(dataloader)
        print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")

        # ä¿å­˜checkpoint
        checkpoint_mgr.save_checkpoint(
            model=model,
            optimizer=optimizer,
            scheduler=scheduler,
            epoch=epoch,
            global_step=epoch * len(dataloader),
            loss_history=[avg_loss],
            metrics={'avg_loss': avg_loss},
            tokenizer=text_tokenizer,
            config=config
        )
```

**å·¥ä½œé‡**: 12-16å°æ—¶

---

#### M4.7: å¤šæ¨¡æ€æ¨ç†ç¤ºä¾‹ âœ… P2
**å®ç°**:
```python
# examples/multimodal_inference.py
from apt_model.multimodal.multimodal_apt_model import MultimodalAPTModel
from PIL import Image

# åŠ è½½æ¨¡å‹
model = MultimodalAPTModel.from_pretrained("./outputs_multimodal/checkpoints/best.pt")
model.eval()

# ç¤ºä¾‹1: å›¾åƒæè¿°ç”Ÿæˆ
image = Image.open("cat.jpg")
caption = model.generate_from_multimodal(
    text="Describe this image:",
    image=image,
    max_length=50
)
print(f"Caption: {caption}")

# ç¤ºä¾‹2: è§†è§‰é—®ç­”
question = "What color is the cat?"
answer = model.generate_from_multimodal(
    text=question,
    image=image,
    max_length=20
)
print(f"Answer: {answer}")

# ç¤ºä¾‹3: çº¯æ–‡æœ¬ï¼ˆåå‘å…¼å®¹ï¼‰
response = model.generate_from_multimodal(
    text="Hello, how are you?",
    max_length=30
)
print(f"Response: {response}")
```

**å·¥ä½œé‡**: 8-10å°æ—¶

---

#### M4.8: å¤šæ¨¡æ€å•å…ƒæµ‹è¯• âœ… P0
**å®ç°**:
```python
# tests/test_multimodal.py
import pytest
import torch
from apt_model.multimodal.multimodal_apt_model import MultimodalAPTModel

class TestMultimodal:
    def test_vision_encoder(self):
        """æµ‹è¯•è§†è§‰ç¼–ç å™¨"""
        from apt_model.multimodal.vision_encoder import VisionEncoder
        from PIL import Image

        encoder = VisionEncoder()
        image = Image.new('RGB', (224, 224))

        features = encoder.encode_image(image)
        assert features.shape[1] == 512  # CLIP feature dim

    def test_cross_modal_attention(self):
        """æµ‹è¯•è·¨æ¨¡æ€æ³¨æ„åŠ›"""
        from apt_model.multimodal.cross_modal_attention import CrossModalAttention

        attn = CrossModalAttention(d_model=512, num_heads=8)

        text_embeds = torch.randn(10, 2, 512)  # [seq_len, batch, d_model]
        image_embeds = torch.randn(49, 2, 512)  # [num_patches, batch, d_model]

        fused, weights = attn(text_embeds, image_embeds)

        assert fused.shape == text_embeds.shape
        assert weights.shape == (2, 8, 10, 49)  # [batch, heads, seq, patches]

    def test_multimodal_model_forward(self):
        """æµ‹è¯•å¤šæ¨¡æ€æ¨¡å‹å‰å‘ä¼ æ’­"""
        from apt_model.config.apt_config import APTConfig

        config = APTConfig()
        model = MultimodalAPTModel(config, use_vision=True, use_audio=False)

        # çº¯æ–‡æœ¬è¾“å…¥
        text_input_ids = torch.randint(0, config.vocab_size, (2, 128))
        logits = model(text_input_ids)
        assert logits.shape == (2, 128, config.vocab_size)

        # æ–‡æœ¬+å›¾åƒè¾“å…¥
        image = torch.randn(2, 3, 224, 224)
        logits = model(text_input_ids, image=image)
        assert logits.shape == (2, 128, config.vocab_size)

    def test_multimodal_dataloader(self):
        """æµ‹è¯•å¤šæ¨¡æ€æ•°æ®åŠ è½½å™¨"""
        # TODO: å®ç°
        pass
```

**å·¥ä½œé‡**: 12-16å°æ—¶

---

### âœ… å¤šæ¨¡æ€æ”¯æŒå®Œå–„æ€»ç»“

| ä»»åŠ¡ | ä¼˜å…ˆçº§ | å·¥ä½œé‡ | å®Œæˆåæå‡ |
|------|--------|--------|-----------|
| M4.1 è§†è§‰ç¼–ç å™¨ | P0 | 12-16h | 50% â†’ 60% |
| M4.2 éŸ³é¢‘ç¼–ç å™¨ | P1 | 10-12h | 60% â†’ 68% |
| M4.3 è·¨æ¨¡æ€æ³¨æ„åŠ› | P0 | 16-20h | 68% â†’ 78% |
| M4.4 å¤šæ¨¡æ€æ•°æ®åŠ è½½å™¨ | P0 | 10-12h | 78% â†’ 85% |
| M4.5 å¤šæ¨¡æ€APTæ¨¡å‹ | P0 | 16-20h | 85% â†’ 92% |
| M4.6 è®­ç»ƒè„šæœ¬ | P1 | 12-16h | 92% â†’ 96% |
| M4.7 æ¨ç†ç¤ºä¾‹ | P2 | 8-10h | 96% â†’ 98% |
| M4.8 å•å…ƒæµ‹è¯• | P0 | 12-16h | 98% â†’ 100% |
| **æ€»è®¡** | - | **96-122h** | **50% â†’ 100%** |

---

## ğŸ“Š æ€»ä½“å®Œå–„è®¡åˆ’æ±‡æ€»

### å·¥ä½œé‡ä¼°ç®—

| é¢†åŸŸ | å½“å‰æˆç†Ÿåº¦ | ç›®æ ‡ | ä»»åŠ¡æ•° | æ€»å·¥ä½œé‡ | å®Œæˆæ—¶é—´ï¼ˆ1äººï¼‰ | å®Œæˆæ—¶é—´ï¼ˆ2äººï¼‰ |
|------|-----------|------|--------|----------|----------------|----------------|
| 1. æ ¸å¿ƒè®­ç»ƒåŠŸèƒ½ | 80% | 100% | 4 | 54-68h | 1.5-2å‘¨ | 0.7-1å‘¨ |
| 2. é”™è¯¯å¤„ç†ç³»ç»Ÿ | 90% | 100% | 2 | 14-18h | 0.5å‘¨ | 0.3å‘¨ |
| 3. æ’ä»¶ç³»ç»Ÿ | 70% | 100% | 5 | 70-90h | 2-2.5å‘¨ | 1-1.3å‘¨ |
| 4. å¤šæ¨¡æ€æ”¯æŒ | 50% | 100% | 8 | 96-122h | 2.5-3å‘¨ | 1.3-1.5å‘¨ |
| **æ€»è®¡** | - | - | **19** | **234-298h** | **6.5-8å‘¨** | **3.3-4å‘¨** |

### å®æ–½é¡ºåºå»ºè®®

#### Sprint 1: æ ¸å¿ƒç¨³å®šï¼ˆ2å‘¨ï¼‰
**ç›®æ ‡**: æ‰“ç‰¢åŸºç¡€

1. æ ¸å¿ƒè®­ç»ƒåŠŸèƒ½å•å…ƒæµ‹è¯•ï¼ˆT1.1ï¼‰ - P0
2. æ¢¯åº¦ç›‘æ§å·¥å…·ï¼ˆT1.2ï¼‰ - P0
3. é”™è¯¯æŒä¹…åŒ–ï¼ˆE2.1ï¼‰ - P0
4. æ’ä»¶ç‰ˆæœ¬ç®¡ç†ï¼ˆP3.1ï¼‰ - P0

**å®Œæˆå**:
- æ ¸å¿ƒè®­ç»ƒåŠŸèƒ½: 80% â†’ 90%
- é”™è¯¯å¤„ç†ç³»ç»Ÿ: 90% â†’ 95%
- æ’ä»¶ç³»ç»Ÿ: 70% â†’ 78%

---

#### Sprint 2: æ’ä»¶ç”Ÿæ€ï¼ˆ2-3å‘¨ï¼‰
**ç›®æ ‡**: å®Œå–„æ’ä»¶ç³»ç»Ÿ

5. æ’ä»¶æ²™ç®±éš”ç¦»ï¼ˆP3.3ï¼‰ - P0
6. æ’ä»¶å¸‚åœºï¼ˆP3.2ï¼‰ - P0
7. æ’ä»¶å•å…ƒæµ‹è¯•ï¼ˆP3.5ï¼‰ - P0
8. æ’ä»¶æ€§èƒ½ç›‘æ§ï¼ˆP3.4ï¼‰ - P1

**å®Œæˆå**:
- æ’ä»¶ç³»ç»Ÿ: 78% â†’ 100% âœ…

---

#### Sprint 3: å¤šæ¨¡æ€åŸºç¡€ï¼ˆ2-3å‘¨ï¼‰
**ç›®æ ‡**: å»ºç«‹å¤šæ¨¡æ€èƒ½åŠ›

9. è§†è§‰ç¼–ç å™¨ï¼ˆM4.1ï¼‰ - P0
10. è·¨æ¨¡æ€æ³¨æ„åŠ›ï¼ˆM4.3ï¼‰ - P0
11. å¤šæ¨¡æ€æ•°æ®åŠ è½½å™¨ï¼ˆM4.4ï¼‰ - P0
12. å¤šæ¨¡æ€APTæ¨¡å‹ï¼ˆM4.5ï¼‰ - P0

**å®Œæˆå**:
- å¤šæ¨¡æ€æ”¯æŒ: 50% â†’ 85%

---

#### Sprint 4: å®Œå–„å’Œæµ‹è¯•ï¼ˆ1-2å‘¨ï¼‰
**ç›®æ ‡**: è¾¾åˆ°100%æˆç†Ÿåº¦

13. è®­ç»ƒå¯è§†åŒ–å¢å¼ºï¼ˆT1.3ï¼‰ - P1
14. è¶…å‚æ•°æœç´¢é›†æˆï¼ˆT1.4ï¼‰ - P2
15. éŸ³é¢‘ç¼–ç å™¨ï¼ˆM4.2ï¼‰ - P1
16. å¤šæ¨¡æ€è®­ç»ƒè„šæœ¬ï¼ˆM4.6ï¼‰ - P1
17. åˆ†å¸ƒå¼é”™è¯¯åŒæ­¥ï¼ˆE2.2ï¼‰ - P1
18. å¤šæ¨¡æ€æ¨ç†ç¤ºä¾‹ï¼ˆM4.7ï¼‰ - P2
19. å¤šæ¨¡æ€å•å…ƒæµ‹è¯•ï¼ˆM4.8ï¼‰ - P0

**å®Œæˆå**:
- æ ¸å¿ƒè®­ç»ƒåŠŸèƒ½: 90% â†’ 100% âœ…
- é”™è¯¯å¤„ç†ç³»ç»Ÿ: 95% â†’ 100% âœ…
- å¤šæ¨¡æ€æ”¯æŒ: 85% â†’ 100% âœ…

---

## ğŸ¯ å®Œæˆåçš„é¡¹ç›®çŠ¶æ€

### æŠ€æœ¯é¢†åŸŸæˆç†Ÿåº¦ï¼ˆé¢„æœŸï¼‰

| æŠ€æœ¯é¢†åŸŸ | å½“å‰ | å®Œæˆå | æå‡ |
|---------|------|--------|------|
| æ ¸å¿ƒè®­ç»ƒåŠŸèƒ½ | 80% | **100%** | +20% |
| å¤šè¯­è¨€æ”¯æŒ | 100% | **100%** | - |
| é”™è¯¯å¤„ç†ç³»ç»Ÿ | 90% | **100%** | +10% |
| å¯è§†åŒ–å·¥å…· | 80% | **100%** | +20% |
| æ’ä»¶ç³»ç»Ÿ | 70% | **100%** | +30% |
| å¤šæ¨¡æ€æ”¯æŒ | 50% | **100%** | +50% |
| åˆ†å¸ƒå¼è®­ç»ƒ | 40% | 40% | - |
| æ¨¡å‹å‹ç¼© | 60% | 60% | - |
| APIæœåŠ¡ | 20% | 20% | - |
| Webç•Œé¢ | 0% | 0% | - |
| **æ€»ä½“æˆç†Ÿåº¦** | **70%** | **81%** | **+11%** |

### æ ¸å¿ƒä¼˜åŠ¿

å®Œæˆ4ä¸ªæ ¸å¿ƒé¢†åŸŸ100%åï¼ŒAPT-Transformerå°†å…·å¤‡ï¼š

âœ… **å·¥ä¸šçº§è®­ç»ƒç³»ç»Ÿ**
- å®Œæ•´çš„checkpointç®¡ç†
- æ¢¯åº¦ç›‘æ§å’Œè°ƒè¯•
- è‡ªåŠ¨è¶…å‚æ•°æœç´¢
- å…¨é¢çš„å•å…ƒæµ‹è¯•è¦†ç›–

âœ… **ä¼ä¸šçº§æ’ä»¶ç”Ÿæ€**
- ç‰ˆæœ¬ç®¡ç†å’Œä¾èµ–è§£æ
- æ’ä»¶å¸‚åœº
- æ²™ç®±å®‰å…¨éš”ç¦»
- æ€§èƒ½ç›‘æ§

âœ… **ç”Ÿäº§çº§é”™è¯¯å¤„ç†**
- é”™è¯¯æŒä¹…åŒ–å’Œåˆ†æ
- åˆ†å¸ƒå¼é”™è¯¯åŒæ­¥
- è‡ªåŠ¨æ¢å¤æœºåˆ¶

âœ… **å®Œæ•´å¤šæ¨¡æ€èƒ½åŠ›**
- æ–‡æœ¬+å›¾åƒ+éŸ³é¢‘
- è·¨æ¨¡æ€æ³¨æ„åŠ›
- ç»Ÿä¸€çš„è®­ç»ƒå’Œæ¨ç†æ¥å£
- VQAã€å›¾åƒæè¿°ç­‰åº”ç”¨

---

## ğŸš€ å»ºè®®ç«‹å³å¼€å§‹

### Week 1-2: æ ¸å¿ƒç¨³å®šSprint
1. ç¼–å†™æ ¸å¿ƒè®­ç»ƒåŠŸèƒ½æµ‹è¯• (24-30h)
2. å®ç°æ¢¯åº¦ç›‘æ§å·¥å…· (8-10h)
3. é”™è¯¯æŒä¹…åŒ–ç³»ç»Ÿ (6-8h)
4. æ’ä»¶ç‰ˆæœ¬ç®¡ç† (12-16h)

**æŠ•å…¥**: 50-64å°æ—¶
**äº§å‡º**: 3ä¸ªæ ¸å¿ƒç³»ç»ŸåŸºç¡€ç¨³å›º

---

éœ€è¦æˆ‘å¼€å§‹å®æ–½å—ï¼Ÿä»å“ªä¸ªSprintå¼€å§‹ï¼ŸğŸ¯
