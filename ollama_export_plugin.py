"""
Ollama Export Plugin for APT Model
å°†APTæ¨¡å‹å¯¼å‡ºä¸ºOllamaæ ¼å¼,æ”¯æŒæœ¬åœ°éƒ¨ç½²
"""

import os
import json
import torch
import struct
import subprocess
from pathlib import Path
from typing import Dict, Any, Optional, List


class OllamaExportPlugin:
    """
    Ollamaå¯¼å‡ºæ’ä»¶
    
    åŠŸèƒ½:
    1. å¯¼å‡ºAPTæ¨¡å‹ä¸ºGGUFæ ¼å¼
    2. åˆ›å»ºModelfileé…ç½®
    3. è‡ªåŠ¨æ³¨å†Œåˆ°Ollama
    4. æ”¯æŒå¤šç§é‡åŒ–é€‰é¡¹
    """
    
    def __init__(self, config: Dict[str, Any]):
        self.name = "ollama-export"
        self.version = "1.0.0"
        self.config = config
        
        self.quantization = config.get('quantization', 'Q4_K_M')  # Q4/Q5/Q8
        self.context_length = config.get('context_length', 2048)
        self.temperature = config.get('temperature', 0.7)
    
    # ==================== GGUFæ ¼å¼è½¬æ¢ ====================
    
    def export_to_gguf(
        self,
        model_path: str,
        output_path: str,
        quantization: Optional[str] = None
    ) -> str:
        """
        å°†APTæ¨¡å‹è½¬æ¢ä¸ºGGUFæ ¼å¼
        
        Args:
            model_path: APTæ¨¡å‹è·¯å¾„
            output_path: è¾“å‡ºGGUFæ–‡ä»¶è·¯å¾„
            quantization: é‡åŒ–ç±»å‹ (Q4_0, Q4_K_M, Q5_K_M, Q8_0ç­‰)
            
        Returns:
            GGUFæ–‡ä»¶è·¯å¾„
        """
        quant = quantization or self.quantization
        print(f"ğŸ”„ æ­£åœ¨è½¬æ¢APTæ¨¡å‹ä¸ºGGUFæ ¼å¼ (é‡åŒ–: {quant})...")
        
        try:
            # åŠ è½½APTæ¨¡å‹
            model = torch.load(os.path.join(model_path, "pytorch_model.bin"))
            
            # åˆ›å»ºGGUFæ–‡ä»¶
            gguf_path = output_path if output_path.endswith('.gguf') else f"{output_path}.gguf"
            
            # å†™å…¥GGUFæ ¼å¼
            self._write_gguf(model, gguf_path, quant)
            
            print(f"âœ… GGUFæ–‡ä»¶å·²åˆ›å»º: {gguf_path}")
            return gguf_path
            
        except Exception as e:
            print(f"âŒ GGUFè½¬æ¢å¤±è´¥: {e}")
            raise
    
    def _write_gguf(
        self,
        model: Dict[str, torch.Tensor],
        output_path: str,
        quantization: str
    ):
        """å†™å…¥GGUFæ ¼å¼æ–‡ä»¶"""
        with open(output_path, 'wb') as f:
            # GGUFé­”æ•°å’Œç‰ˆæœ¬
            f.write(b'GGUF')
            f.write(struct.pack('I', 3))  # ç‰ˆæœ¬3
            
            # å†™å…¥å…ƒæ•°æ®
            metadata = self._create_gguf_metadata(model)
            self._write_metadata(f, metadata)
            
            # å†™å…¥å¼ é‡ä¿¡æ¯
            tensor_count = len(model)
            f.write(struct.pack('Q', tensor_count))
            
            # é‡åŒ–å¹¶å†™å…¥æƒé‡
            for name, tensor in model.items():
                quantized = self._quantize_tensor(tensor, quantization)
                self._write_tensor(f, name, quantized, quantization)
    
    def _create_gguf_metadata(self, model: Dict) -> Dict:
        """åˆ›å»ºGGUFå…ƒæ•°æ®"""
        return {
            'general.architecture': 'apt',
            'general.name': 'APT Model',
            'general.file_type': self._get_file_type(self.quantization),
            'apt.context_length': self.context_length,
            'apt.embedding_length': self._infer_embedding_dim(model),
            'apt.block_count': self._infer_layer_count(model),
            'tokenizer.ggml.model': 'gpt2',
        }
    
    def _quantize_tensor(self, tensor: torch.Tensor, quant_type: str) -> bytes:
        """é‡åŒ–å¼ é‡"""
        if quant_type == 'Q4_0':
            return self._quantize_q4_0(tensor)
        elif quant_type == 'Q4_K_M':
            return self._quantize_q4_k(tensor)
        elif quant_type == 'Q5_K_M':
            return self._quantize_q5_k(tensor)
        elif quant_type == 'Q8_0':
            return self._quantize_q8_0(tensor)
        else:
            # é»˜è®¤FP16
            return tensor.half().numpy().tobytes()
    
    def _quantize_q4_0(self, tensor: torch.Tensor) -> bytes:
        """4ä½é‡åŒ–(åŸºç¡€ç‰ˆæœ¬)"""
        # ç®€åŒ–å®ç°,å®é™…åº”è¯¥ç”¨llama.cppçš„é‡åŒ–æ–¹æ³•
        flat = tensor.flatten().float()
        
        # åˆ†å—é‡åŒ–
        block_size = 32
        n_blocks = (len(flat) + block_size - 1) // block_size
        quantized = bytearray()
        
        for i in range(n_blocks):
            block = flat[i*block_size:(i+1)*block_size]
            
            # è®¡ç®—scale
            abs_max = block.abs().max().item()
            scale = abs_max / 7.0 if abs_max > 0 else 1.0
            
            # é‡åŒ–
            quants = torch.clamp(torch.round(block / scale), -8, 7)
            
            # æ‰“åŒ…(æ¯ä¸ªå€¼4ä½)
            quantized.extend(struct.pack('f', scale))
            for j in range(0, len(quants), 2):
                v1 = int(quants[j].item()) & 0x0F
                v2 = int(quants[j+1].item() if j+1 < len(quants) else 0) & 0x0F
                quantized.append((v2 << 4) | v1)
        
        return bytes(quantized)
    
    def _quantize_q8_0(self, tensor: torch.Tensor) -> bytes:
        """8ä½é‡åŒ–"""
        flat = tensor.flatten().float()
        
        block_size = 32
        n_blocks = (len(flat) + block_size - 1) // block_size
        quantized = bytearray()
        
        for i in range(n_blocks):
            block = flat[i*block_size:(i+1)*block_size]
            
            abs_max = block.abs().max().item()
            scale = abs_max / 127.0 if abs_max > 0 else 1.0
            
            quants = torch.clamp(torch.round(block / scale), -128, 127)
            
            quantized.extend(struct.pack('f', scale))
            quantized.extend(quants.to(torch.int8).numpy().tobytes())
        
        return bytes(quantized)
    
    def _quantize_q4_k(self, tensor: torch.Tensor) -> bytes:
        """4ä½K-quants (æ”¹è¿›ç‰ˆæœ¬)"""
        # è¿™é‡Œåº”è¯¥å®ç°K-quantsç®—æ³•,ä¸ºç®€åŒ–ä½¿ç”¨Q4_0
        return self._quantize_q4_0(tensor)
    
    def _quantize_q5_k(self, tensor: torch.Tensor) -> bytes:
        """5ä½K-quants"""
        # ç®€åŒ–å®ç°
        return self._quantize_q4_0(tensor)
    
    # ==================== Modelfileåˆ›å»º ====================
    
    def create_modelfile(
        self,
        gguf_path: str,
        output_path: Optional[str] = None,
        system_prompt: Optional[str] = None,
        template: Optional[str] = None
    ) -> str:
        """
        åˆ›å»ºOllama Modelfile
        
        Args:
            gguf_path: GGUFæ¨¡å‹æ–‡ä»¶è·¯å¾„
            output_path: Modelfileè¾“å‡ºè·¯å¾„
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            template: å¯¹è¯æ¨¡æ¿
            
        Returns:
            Modelfileè·¯å¾„
        """
        print("ğŸ“ åˆ›å»ºModelfile...")
        
        if output_path is None:
            output_path = os.path.join(
                os.path.dirname(gguf_path),
                "Modelfile"
            )
        
        # é»˜è®¤ç³»ç»Ÿæç¤º
        if system_prompt is None:
            system_prompt = "You are a helpful AI assistant powered by APT model."
        
        # é»˜è®¤æ¨¡æ¿
        if template is None:
            template = """{{ if .System }}{{ .System }}{{ end }}
{{ if .Prompt }}User: {{ .Prompt }}{{ end }}
Assistant: """
        
        # åˆ›å»ºModelfileå†…å®¹
        modelfile_content = f"""# APT Model for Ollama
FROM {gguf_path}

# æ¨¡å‹å‚æ•°
PARAMETER temperature {self.temperature}
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER repeat_penalty 1.1
PARAMETER num_ctx {self.context_length}

# ç³»ç»Ÿæç¤º
SYSTEM \"\"\"{system_prompt}\"\"\"

# å¯¹è¯æ¨¡æ¿
TEMPLATE \"\"\"{template}\"\"\"

# åœæ­¢è¯
PARAMETER stop "<|im_end|>"
PARAMETER stop "</s>"
"""
        
        # å†™å…¥æ–‡ä»¶
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write(modelfile_content)
        
        print(f"âœ… Modelfileå·²åˆ›å»º: {output_path}")
        return output_path
    
    # ==================== Ollamaé›†æˆ ====================
    
    def register_to_ollama(
        self,
        modelfile_path: str,
        model_name: str
    ) -> bool:
        """
        å°†æ¨¡å‹æ³¨å†Œåˆ°Ollama
        
        Args:
            modelfile_path: Modelfileè·¯å¾„
            model_name: æ¨¡å‹åç§°(å¦‚: apt-chinese:latest)
            
        Returns:
            æ˜¯å¦æˆåŠŸ
        """
        print(f"ğŸš€ æ³¨å†Œæ¨¡å‹åˆ°Ollama: {model_name}")
        
        try:
            # æ£€æŸ¥Ollamaæ˜¯å¦å®‰è£…
            result = subprocess.run(
                ['ollama', '--version'],
                capture_output=True,
                text=True
            )
            
            if result.returncode != 0:
                print("âŒ Ollamaæœªå®‰è£…,è¯·å…ˆå®‰è£…Ollama")
                print("   è®¿é—®: https://ollama.ai/download")
                return False
            
            # åˆ›å»ºæ¨¡å‹
            result = subprocess.run(
                ['ollama', 'create', model_name, '-f', modelfile_path],
                capture_output=True,
                text=True
            )
            
            if result.returncode == 0:
                print(f"âœ… æ¨¡å‹å·²æ³¨å†Œåˆ°Ollama: {model_name}")
                print(f"\nä½¿ç”¨æ–¹æ³•:")
                print(f"  ollama run {model_name}")
                return True
            else:
                print(f"âŒ æ³¨å†Œå¤±è´¥: {result.stderr}")
                return False
                
        except FileNotFoundError:
            print("âŒ Ollamaå‘½ä»¤æœªæ‰¾åˆ°,è¯·ç¡®ä¿Ollamaå·²æ­£ç¡®å®‰è£…")
            return False
        except Exception as e:
            print(f"âŒ æ³¨å†Œè¿‡ç¨‹å‡ºé”™: {e}")
            return False
    
    def test_model(self, model_name: str, prompt: str = "ä½ å¥½ï¼") -> str:
        """
        æµ‹è¯•Ollamaæ¨¡å‹
        
        Args:
            model_name: æ¨¡å‹åç§°
            prompt: æµ‹è¯•æç¤ºè¯
            
        Returns:
            æ¨¡å‹å“åº”
        """
        print(f"ğŸ§ª æµ‹è¯•æ¨¡å‹: {model_name}")
        
        try:
            result = subprocess.run(
                ['ollama', 'run', model_name, prompt],
                capture_output=True,
                text=True,
                timeout=30
            )
            
            if result.returncode == 0:
                response = result.stdout.strip()
                print(f"âœ… æ¨¡å‹å“åº”:\n{response}")
                return response
            else:
                print(f"âŒ æµ‹è¯•å¤±è´¥: {result.stderr}")
                return ""
                
        except subprocess.TimeoutExpired:
            print("â±ï¸ æµ‹è¯•è¶…æ—¶")
            return ""
        except Exception as e:
            print(f"âŒ æµ‹è¯•å‡ºé”™: {e}")
            return ""
    
    # ==================== å®Œæ•´å¯¼å‡ºæµç¨‹ ====================
    
    def export_complete(
        self,
        model_path: str,
        output_dir: str,
        model_name: str = "apt-model",
        register: bool = True
    ) -> Dict[str, str]:
        """
        å®Œæ•´çš„å¯¼å‡ºæµç¨‹
        
        Args:
            model_path: APTæ¨¡å‹è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•
            model_name: Ollamaæ¨¡å‹åç§°
            register: æ˜¯å¦æ³¨å†Œåˆ°Ollama
            
        Returns:
            å¯¼å‡ºç»“æœ(åŒ…å«æ–‡ä»¶è·¯å¾„)
        """
        print(f"ğŸ¯ å¼€å§‹å®Œæ•´å¯¼å‡ºæµç¨‹...")
        
        os.makedirs(output_dir, exist_ok=True)
        
        results = {
            'gguf_path': None,
            'modelfile_path': None,
            'registered': False
        }
        
        try:
            # 1. è½¬æ¢ä¸ºGGUF
            gguf_path = os.path.join(output_dir, f"{model_name}.gguf")
            results['gguf_path'] = self.export_to_gguf(
                model_path, gguf_path
            )
            
            # 2. åˆ›å»ºModelfile
            results['modelfile_path'] = self.create_modelfile(
                results['gguf_path']
            )
            
            # 3. æ³¨å†Œåˆ°Ollama
            if register:
                results['registered'] = self.register_to_ollama(
                    results['modelfile_path'],
                    model_name
                )
            
            print("\n" + "="*50)
            print("âœ… å¯¼å‡ºå®Œæˆ!")
            print("="*50)
            print(f"GGUFæ–‡ä»¶: {results['gguf_path']}")
            print(f"Modelfile: {results['modelfile_path']}")
            if results['registered']:
                print(f"Ollamaæ¨¡å‹: {model_name}")
                print(f"\nä½¿ç”¨: ollama run {model_name}")
            
            return results
            
        except Exception as e:
            print(f"âŒ å¯¼å‡ºå¤±è´¥: {e}")
            raise
    
    # ==================== å·¥å…·æ–¹æ³• ====================
    
    def _get_file_type(self, quant: str) -> int:
        """è·å–GGUFæ–‡ä»¶ç±»å‹"""
        file_types = {
            'F32': 0,
            'F16': 1,
            'Q4_0': 2,
            'Q4_1': 3,
            'Q5_0': 6,
            'Q5_1': 7,
            'Q8_0': 8,
            'Q8_1': 9,
            'Q4_K_S': 12,
            'Q4_K_M': 13,
            'Q5_K_S': 14,
            'Q5_K_M': 15,
        }
        return file_types.get(quant, 1)
    
    def _infer_embedding_dim(self, model: Dict) -> int:
        """æ¨æ–­åµŒå…¥ç»´åº¦"""
        # ä»æ¨¡å‹æƒé‡ä¸­æ¨æ–­
        for name, tensor in model.items():
            if 'embed' in name.lower():
                return tensor.shape[-1]
        return 768  # é»˜è®¤
    
    def _infer_layer_count(self, model: Dict) -> int:
        """æ¨æ–­å±‚æ•°"""
        layer_nums = set()
        for name in model.keys():
            if 'layer' in name.lower() or 'block' in name.lower():
                # æå–å±‚ç¼–å·
                parts = name.split('.')
                for part in parts:
                    if part.isdigit():
                        layer_nums.add(int(part))
        return len(layer_nums) if layer_nums else 12  # é»˜è®¤
    
    def _write_metadata(self, f, metadata: Dict):
        """å†™å…¥GGUFå…ƒæ•°æ®"""
        # å…ƒæ•°æ®æ•°é‡
        f.write(struct.pack('Q', len(metadata)))
        
        for key, value in metadata.items():
            # é”®
            key_bytes = key.encode('utf-8')
            f.write(struct.pack('Q', len(key_bytes)))
            f.write(key_bytes)
            
            # å€¼(æ ¹æ®ç±»å‹)
            if isinstance(value, str):
                f.write(struct.pack('I', 8))  # å­—ç¬¦ä¸²ç±»å‹
                value_bytes = value.encode('utf-8')
                f.write(struct.pack('Q', len(value_bytes)))
                f.write(value_bytes)
            elif isinstance(value, int):
                f.write(struct.pack('I', 4))  # uint32ç±»å‹
                f.write(struct.pack('I', value))
            elif isinstance(value, float):
                f.write(struct.pack('I', 6))  # float32ç±»å‹
                f.write(struct.pack('f', value))
    
    def _write_tensor(self, f, name: str, data: bytes, quant_type: str):
        """å†™å…¥å¼ é‡æ•°æ®"""
        # å¼ é‡åç§°
        name_bytes = name.encode('utf-8')
        f.write(struct.pack('Q', len(name_bytes)))
        f.write(name_bytes)
        
        # å¼ é‡ç±»å‹
        f.write(struct.pack('I', self._get_file_type(quant_type)))
        
        # å¼ é‡æ•°æ®
        f.write(struct.pack('Q', len(data)))
        f.write(data)
    
    # ==================== æ’ä»¶é’©å­ ====================
    
    def on_training_end(self, context: Dict[str, Any]):
        """è®­ç»ƒç»“æŸåè‡ªåŠ¨å¯¼å‡º"""
        if self.config.get('auto_export', False):
            model_path = context.get('checkpoint_path')
            output_dir = self.config.get('output_dir', './ollama_export')
            
            self.export_complete(
                model_path,
                output_dir,
                register=self.config.get('auto_register', False)
            )


# ==================== ä½¿ç”¨ç¤ºä¾‹ ====================

if __name__ == "__main__":
    # é…ç½®
    config = {
        'quantization': 'Q4_K_M',  # Q4_0, Q4_K_M, Q5_K_M, Q8_0
        'context_length': 2048,
        'temperature': 0.7,
        'auto_export': True,
        'auto_register': False,
        'output_dir': './ollama_models'
    }
    
    plugin = OllamaExportPlugin(config)
    
    # å®Œæ•´å¯¼å‡ºæµç¨‹
    results = plugin.export_complete(
        model_path="./trained_model",
        output_dir="./ollama_export",
        model_name="apt-chinese",
        register=True
    )
    
    # æµ‹è¯•æ¨¡å‹
    if results['registered']:
        plugin.test_model("apt-chinese", "ä½ å¥½,ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±")
    
    print("\nâœ… Ollamaå¯¼å‡ºæ’ä»¶ç¤ºä¾‹å®Œæˆ!")
