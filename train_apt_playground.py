#!/usr/bin/env python3
"""
APT Playground Training Script
ä½¿ç”¨HuggingFaceæ•°æ®é›†è®­ç»ƒAPTæ¨¡å‹

ç‰¹æ€§ï¼š
1. HuggingFaceæ•°æ®é›†é›†æˆ (wikitext-103-v1æµå¼åŠ è½½)
2. "Playground Theory" - CosineAnnealingWarmRestartsåŠ¨æ€å­¦ä¹ ç‡
3. RTX 3070ä¼˜åŒ– (æ··åˆç²¾åº¦ + æ¢¯åº¦ç´¯ç§¯)
4. DBC-DACæ¢¯åº¦ç¨³å®š
5. æ”¯æŒHLBD Legacyæ¨¡å¼è°ƒè¯•
"""

import os
import sys
import time
import json
import argparse
from pathlib import Path
from typing import Optional, Dict, Any

import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from torch.cuda.amp import autocast, GradScaler

# HuggingFace datasets
try:
    from datasets import load_dataset
    HF_AVAILABLE = True
except ImportError:
    HF_AVAILABLE = False
    print("âš ï¸  è­¦å‘Š: datasetsåº“æœªå®‰è£…ï¼ŒHuggingFaceåŠŸèƒ½ä¸å¯ç”¨")
    print("   å®‰è£…: pip install datasets")

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent))

from apt_model.modeling.apt_model import APTModel, APTModelConfiguration
from apt_model.tokenization.char_tokenizer import CharacterTokenizer


# ============================================================================
# é…ç½®ç±»ï¼šProject Swift (RTX 3070ä¼˜åŒ–)
# ============================================================================

class ProjectSwiftConfig:
    """
    RTX 3070 Laptopä¼˜åŒ–é…ç½®

    ç¡¬ä»¶è§„æ ¼ï¼š
    - VRAM: 8GB
    - CUDA Cores: 5120
    - Memory Bus: 256-bit
    - TDP: 125W

    ä¼˜åŒ–ç­–ç•¥ï¼š
    - æ··åˆç²¾åº¦è®­ç»ƒ (FP16)
    - æ¢¯åº¦ç´¯ç§¯ (å‡å°‘batch sizeå†…å­˜å‹åŠ›)
    - æ¨¡å‹å¤§å°é€‚ä¸­ (d_model=512, n_layers=12)
    """

    # æ¨¡å‹é…ç½®
    d_model = 512
    n_layers = 12
    n_heads = 8
    d_ff = 2048
    max_seq_len = 512
    dropout = 0.1

    # è®­ç»ƒé…ç½®
    batch_size = 8           # å°batché€‚é…8GB VRAM
    gradient_accumulation_steps = 4  # æœ‰æ•ˆbatch=32
    mixed_precision = True   # FP16æ··åˆç²¾åº¦
    max_grad_norm = 1.0      # æ¢¯åº¦è£å‰ª

    # Playground Theory: åŠ¨æ€å­¦ä¹ ç‡
    base_lr = 3e-4
    min_lr = 1e-5
    T_0 = 10                 # é‡å¯å‘¨æœŸ (epochs)
    T_mult = 2               # å‘¨æœŸå€å¢

    # DBC-DACé…ç½®
    use_dbc_dac = True
    rank_ratio_proj = 0.95
    threshold = 1e-6


# ============================================================================
# HuggingFaceæ•°æ®é›†é€‚é…å™¨
# ============================================================================

class HuggingFaceDataset(Dataset):
    """
    HuggingFaceæ•°æ®é›†é€‚é…å™¨
    æ”¯æŒæµå¼åŠ è½½å¤§å‹æ•°æ®é›† (å¦‚wikitext-103-v1)
    """

    def __init__(
        self,
        dataset_name: str = "wikitext",
        dataset_config: str = "wikitext-103-v1",
        split: str = "train",
        tokenizer: Optional[CharacterTokenizer] = None,
        max_length: int = 512,
        streaming: bool = True,
        max_samples: Optional[int] = None
    ):
        """
        Args:
            dataset_name: HFæ•°æ®é›†åç§°
            dataset_config: æ•°æ®é›†é…ç½®
            split: æ•°æ®é›†åˆ’åˆ† (train/validation/test)
            tokenizer: å­—ç¬¦tokenizer
            max_length: æœ€å¤§åºåˆ—é•¿åº¦
            streaming: æ˜¯å¦æµå¼åŠ è½½
            max_samples: æœ€å¤§æ ·æœ¬æ•° (ç”¨äºå¿«é€Ÿæµ‹è¯•)
        """
        if not HF_AVAILABLE:
            raise ImportError("è¯·å®‰è£…datasetsåº“: pip install datasets")

        self.tokenizer = tokenizer or CharacterTokenizer()
        self.max_length = max_length

        print(f"ğŸ“¦ åŠ è½½HuggingFaceæ•°æ®é›†: {dataset_name}/{dataset_config} ({split})")
        print(f"   æµå¼åŠ è½½: {streaming}")

        # åŠ è½½æ•°æ®é›†
        self.dataset = load_dataset(
            dataset_name,
            dataset_config,
            split=split,
            streaming=streaming
        )

        # å¦‚æœæ˜¯æµå¼ï¼Œè½¬æ¢ä¸ºåˆ—è¡¨ï¼ˆé™åˆ¶æ ·æœ¬æ•°ï¼‰
        if streaming:
            print(f"   è½¬æ¢æµå¼æ•°æ®...")
            samples = []
            for i, example in enumerate(self.dataset):
                if max_samples and i >= max_samples:
                    break
                if example['text'].strip():  # è¿‡æ»¤ç©ºæ–‡æœ¬
                    samples.append(example)
            self.samples = samples
            print(f"   âœ“ åŠ è½½ {len(self.samples)} ä¸ªæ ·æœ¬")
        else:
            self.samples = [ex for ex in self.dataset if ex['text'].strip()]

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        text = self.samples[idx]['text']

        # å­—ç¬¦çº§tokenization
        tokens = self.tokenizer.encode(text)

        # æˆªæ–­æˆ–å¡«å……
        if len(tokens) > self.max_length:
            tokens = tokens[:self.max_length]

        # è½¬æ¢ä¸ºtensor
        input_ids = torch.tensor(tokens, dtype=torch.long)

        # è‡ªå›å½’ä»»åŠ¡: è¾“å…¥å’Œç›®æ ‡é”™ä½
        # input: [BOS, t1, t2, ..., tn-1]
        # target: [t1, t2, t3, ..., tn]
        return {
            'input_ids': input_ids[:-1] if len(input_ids) > 1 else input_ids,
            'labels': input_ids[1:] if len(input_ids) > 1 else input_ids
        }


# ============================================================================
# HLBD Legacyæ•°æ®é›† (ç”¨äºè°ƒè¯•å¯¹æ¯”)
# ============================================================================

class HLBDLegacyDataset(Dataset):
    """HLBD Hardcoreæ•°æ®é›†åŠ è½½å™¨"""

    def __init__(self, json_path: str, tokenizer: CharacterTokenizer):
        self.tokenizer = tokenizer

        print(f"ğŸ“‚ åŠ è½½HLBDæ•°æ®é›†: {json_path}")
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # åˆå¹¶æ‰€æœ‰æ¨¡å—çš„æ•°æ®
        self.pairs = []
        for module_name, module_data in data['data'].items():
            for item in module_data:
                self.pairs.append((item['input'], item['output']))

        print(f"   âœ“ åŠ è½½ {len(self.pairs)} ä¸ªè®­ç»ƒå¯¹")

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        src, tgt = self.pairs[idx]

        # Encode
        src_ids = self.tokenizer.encode(src)
        tgt_ids = self.tokenizer.encode(tgt)

        # æ‹¼æ¥: [src, SEP, tgt]
        # å‡è®¾vocab[1]æ˜¯SEP token
        input_ids = src_ids + [1] + tgt_ids

        return {
            'input_ids': torch.tensor(input_ids[:-1], dtype=torch.long),
            'labels': torch.tensor(input_ids[1:], dtype=torch.long)
        }


# ============================================================================
# Collateå‡½æ•°ï¼šåŠ¨æ€padding
# ============================================================================

def collate_fn(batch):
    """
    åŠ¨æ€paddingæ‰¹æ¬¡æ•°æ®
    """
    input_ids = [item['input_ids'] for item in batch]
    labels = [item['labels'] for item in batch]

    # æ‰¾åˆ°æœ€å¤§é•¿åº¦
    max_len = max(len(ids) for ids in input_ids)

    # Padding
    padded_input = []
    padded_labels = []

    for inp, lab in zip(input_ids, labels):
        pad_len = max_len - len(inp)
        padded_input.append(torch.cat([inp, torch.zeros(pad_len, dtype=torch.long)]))
        padded_labels.append(torch.cat([lab, torch.full((pad_len,), -100, dtype=torch.long)]))

    return {
        'input_ids': torch.stack(padded_input),
        'labels': torch.stack(padded_labels)
    }


# ============================================================================
# è®­ç»ƒå™¨
# ============================================================================

class APTPlaygroundTrainer:
    """
    APT Playgroundè®­ç»ƒå™¨
    å®ç°"Playground Theory"å’ŒRTX 3070ä¼˜åŒ–
    """

    def __init__(
        self,
        config: ProjectSwiftConfig,
        model: APTModel,
        train_loader: DataLoader,
        val_loader: Optional[DataLoader] = None,
        device: str = 'cuda'
    ):
        self.config = config
        self.model = model.to(device)
        self.train_loader = train_loader
        self.val_loader = val_loader
        self.device = device

        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=config.base_lr,
            weight_decay=0.01
        )

        # Playground Theory: CosineAnnealingWarmRestarts
        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(
            self.optimizer,
            T_0=config.T_0,
            T_mult=config.T_mult,
            eta_min=config.min_lr
        )

        # æ··åˆç²¾åº¦
        self.scaler = GradScaler() if config.mixed_precision else None

        # æŸå¤±å‡½æ•°
        self.criterion = nn.CrossEntropyLoss(ignore_index=-100)

        # ç»Ÿè®¡
        self.global_step = 0
        self.epoch = 0

    def train_epoch(self):
        """è®­ç»ƒä¸€ä¸ªepoch"""
        self.model.train()
        total_loss = 0
        num_batches = 0

        epoch_start = time.time()

        for batch_idx, batch in enumerate(self.train_loader):
            input_ids = batch['input_ids'].to(self.device)
            labels = batch['labels'].to(self.device)

            # æ··åˆç²¾åº¦ä¸Šä¸‹æ–‡
            with autocast(enabled=self.config.mixed_precision):
                # Forward
                logits = self.model(input_ids)

                # è®¡ç®—æŸå¤±
                loss = self.criterion(
                    logits.view(-1, logits.size(-1)),
                    labels.view(-1)
                )

                # æ¢¯åº¦ç´¯ç§¯
                loss = loss / self.config.gradient_accumulation_steps

            # Backward (æ··åˆç²¾åº¦)
            if self.scaler:
                self.scaler.scale(loss).backward()
            else:
                loss.backward()

            # æ¢¯åº¦ç´¯ç§¯æ­¥éª¤
            if (batch_idx + 1) % self.config.gradient_accumulation_steps == 0:
                # æ¢¯åº¦è£å‰ª
                if self.scaler:
                    self.scaler.unscale_(self.optimizer)

                torch.nn.utils.clip_grad_norm_(
                    self.model.parameters(),
                    self.config.max_grad_norm
                )

                # ä¼˜åŒ–å™¨æ­¥éª¤
                if self.scaler:
                    self.scaler.step(self.optimizer)
                    self.scaler.update()
                else:
                    self.optimizer.step()

                self.optimizer.zero_grad()
                self.global_step += 1

            total_loss += loss.item() * self.config.gradient_accumulation_steps
            num_batches += 1

            # æ¯100æ­¥æ‰“å°
            if batch_idx % 100 == 0:
                current_lr = self.scheduler.get_last_lr()[0]
                print(f"   Step {batch_idx}/{len(self.train_loader)} | "
                      f"Loss: {loss.item():.4f} | LR: {current_lr:.6f}")

        # Epochç»“æŸåæ›´æ–°å­¦ä¹ ç‡
        self.scheduler.step()

        avg_loss = total_loss / num_batches
        epoch_time = time.time() - epoch_start

        return avg_loss, epoch_time

    @torch.no_grad()
    def validate(self):
        """éªŒè¯"""
        if not self.val_loader:
            return None

        self.model.eval()
        total_loss = 0
        num_batches = 0

        for batch in self.val_loader:
            input_ids = batch['input_ids'].to(self.device)
            labels = batch['labels'].to(self.device)

            logits = self.model(input_ids)
            loss = self.criterion(
                logits.view(-1, logits.size(-1)),
                labels.view(-1)
            )

            total_loss += loss.item()
            num_batches += 1

        return total_loss / num_batches

    def save_checkpoint(self, save_path: str, **kwargs):
        """ä¿å­˜checkpoint"""
        checkpoint = {
            'epoch': self.epoch,
            'global_step': self.global_step,
            'model_state_dict': self.model.state_dict(),
            'optimizer_state_dict': self.optimizer.state_dict(),
            'scheduler_state_dict': self.scheduler.state_dict(),
            'config': self.config.__dict__,
            **kwargs
        }

        if self.scaler:
            checkpoint['scaler_state_dict'] = self.scaler.state_dict()

        torch.save(checkpoint, save_path)
        print(f"âœ… Checkpointå·²ä¿å­˜: {save_path}")


# ============================================================================
# ä¸»å‡½æ•°
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description="APT Playground Training")

    # æ•°æ®é›†é€‰æ‹©
    parser.add_argument('--dataset', type=str, default='wikitext',
                        choices=['wikitext', 'hlbd'],
                        help='æ•°æ®é›†ç±»å‹')
    parser.add_argument('--hlbd-path', type=str, default='HLBD_Hardcore_Full.json',
                        help='HLBDæ•°æ®é›†è·¯å¾„')
    parser.add_argument('--max-samples', type=int, default=10000,
                        help='æœ€å¤§è®­ç»ƒæ ·æœ¬æ•° (ç”¨äºå¿«é€Ÿæµ‹è¯•)')

    # è®­ç»ƒé…ç½®
    parser.add_argument('--epochs', type=int, default=50,
                        help='è®­ç»ƒè½®æ•°')
    parser.add_argument('--save-dir', type=str, default='playground_checkpoints',
                        help='Checkpointä¿å­˜ç›®å½•')
    parser.add_argument('--save-interval', type=int, default=10,
                        help='ä¿å­˜é—´éš” (epochs)')

    # DBC-DAC
    parser.add_argument('--no-dbc-dac', action='store_true',
                        help='ç¦ç”¨DBC-DAC')

    args = parser.parse_args()

    # è®¾å¤‡æ£€æŸ¥
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    print(f"ğŸš€ ä½¿ç”¨è®¾å¤‡: {device}")
    if device == 'cuda':
        print(f"   GPU: {torch.cuda.get_device_name(0)}")
        print(f"   VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB")

    # é…ç½®
    config = ProjectSwiftConfig()
    if args.no_dbc_dac:
        config.use_dbc_dac = False

    print("\n" + "=" * 60)
    print("APT Playground Training")
    print("Playground Theory: CosineAnnealingWarmRestarts")
    print("=" * 60)
    print(f"\nğŸ“‹ é…ç½®:")
    print(f"   æ¨¡å‹: d_model={config.d_model}, n_layers={config.n_layers}, n_heads={config.n_heads}")
    print(f"   Batch Size: {config.batch_size} Ã— {config.gradient_accumulation_steps} (ç´¯ç§¯)")
    print(f"   å­¦ä¹ ç‡: {config.base_lr} (Cosine Annealing, T_0={config.T_0})")
    print(f"   æ··åˆç²¾åº¦: {config.mixed_precision}")
    print(f"   DBC-DAC: {config.use_dbc_dac}")

    # Tokenizer
    print(f"\nğŸ”¤ åˆå§‹åŒ–Tokenizer...")
    tokenizer = CharacterTokenizer(vocab_size=2000)

    # æ•°æ®é›†
    print(f"\nğŸ“¦ å‡†å¤‡æ•°æ®é›†: {args.dataset}")
    if args.dataset == 'wikitext':
        train_dataset = HuggingFaceDataset(
            dataset_name='wikitext',
            dataset_config='wikitext-103-v1',
            split='train',
            tokenizer=tokenizer,
            max_length=config.max_seq_len,
            max_samples=args.max_samples
        )
        val_dataset = HuggingFaceDataset(
            dataset_name='wikitext',
            dataset_config='wikitext-103-v1',
            split='validation',
            tokenizer=tokenizer,
            max_length=config.max_seq_len,
            max_samples=args.max_samples // 10
        )
    else:  # HLBD
        train_dataset = HLBDLegacyDataset(args.hlbd_path, tokenizer)
        val_dataset = None

    # DataLoader
    train_loader = DataLoader(
        train_dataset,
        batch_size=config.batch_size,
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=0  # é¿å…å¤šè¿›ç¨‹é™·é˜±
    )

    val_loader = None
    if val_dataset:
        val_loader = DataLoader(
            val_dataset,
            batch_size=config.batch_size,
            shuffle=False,
            collate_fn=collate_fn,
            num_workers=0
        )

    # æ¨¡å‹
    print(f"\nğŸ—ï¸  æ„å»ºAPTæ¨¡å‹...")
    model_config = APTModelConfiguration(
        vocab_size=tokenizer.vocab_size,
        d_model=config.d_model,
        n_heads=config.n_heads,
        n_layers=config.n_layers,
        d_ff=config.d_ff,
        max_seq_len=config.max_seq_len,
        dropout=config.dropout,
        use_dbc_dac=config.use_dbc_dac,
        rank_ratio_proj=config.rank_ratio_proj,
        threshold=config.threshold
    )
    model = APTModel(model_config)

    # å‚æ•°ç»Ÿè®¡
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"   æ€»å‚æ•°: {total_params:,}")
    print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")

    # è®­ç»ƒå™¨
    trainer = APTPlaygroundTrainer(
        config=config,
        model=model,
        train_loader=train_loader,
        val_loader=val_loader,
        device=device
    )

    # åˆ›å»ºä¿å­˜ç›®å½•
    save_dir = Path(args.save_dir)
    save_dir.mkdir(parents=True, exist_ok=True)

    # è®­ç»ƒå¾ªç¯
    print("\n" + "=" * 60)
    print("å¼€å§‹è®­ç»ƒ")
    print("=" * 60)

    for epoch in range(args.epochs):
        trainer.epoch = epoch
        print(f"\nğŸ“ Epoch {epoch + 1}/{args.epochs}")

        # è®­ç»ƒ
        train_loss, epoch_time = trainer.train_epoch()
        print(f"   è®­ç»ƒæŸå¤±: {train_loss:.4f}")
        print(f"   ç”¨æ—¶: {epoch_time:.2f}s")

        # éªŒè¯
        if val_loader:
            val_loss = trainer.validate()
            print(f"   éªŒè¯æŸå¤±: {val_loss:.4f}")

        # ä¿å­˜checkpoint
        if (epoch + 1) % args.save_interval == 0:
            save_path = save_dir / f"checkpoint_epoch_{epoch + 1}.pt"
            trainer.save_checkpoint(
                str(save_path),
                train_loss=train_loss,
                val_loss=val_loss if val_loader else None
            )

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    final_path = save_dir / "final_model.pt"
    trainer.save_checkpoint(str(final_path))

    print("\n" + "=" * 60)
    print("âœ¨ è®­ç»ƒå®Œæˆï¼")
    print("=" * 60)


if __name__ == "__main__":
    main()
