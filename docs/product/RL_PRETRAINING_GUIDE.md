# å¼ºåŒ–å­¦ä¹ ä¸é¢„è®­ç»ƒæŒ‡å—

å®Œæ•´çš„RLå’Œè‡ªç›‘ç£é¢„è®­ç»ƒå®ç°æ–‡æ¡£

ä½œè€…: chen0430tw
æ›´æ–°æ—¥æœŸ: 2025-12-02

---

## ğŸ“‹ ç›®å½•

1. [æ¦‚è¿°](#æ¦‚è¿°)
2. [å¼ºåŒ–å­¦ä¹ æ¨¡å—](#å¼ºåŒ–å­¦ä¹ æ¨¡å—)
   - [å¥–åŠ±æ¨¡å‹](#å¥–åŠ±æ¨¡å‹)
   - [RLHFè®­ç»ƒå™¨](#rlhfè®­ç»ƒå™¨)
   - [DPOè®­ç»ƒå™¨](#dpoè®­ç»ƒå™¨)
   - [GRPOè®­ç»ƒå™¨](#grpoè®­ç»ƒå™¨)
3. [é¢„è®­ç»ƒæ¨¡å—](#é¢„è®­ç»ƒæ¨¡å—)
   - [å¯¹æ¯”å­¦ä¹ ](#å¯¹æ¯”å­¦ä¹ )
   - [MLMé¢„è®­ç»ƒ](#mlmé¢„è®­ç»ƒ)
4. [ä½¿ç”¨ç¤ºä¾‹](#ä½¿ç”¨ç¤ºä¾‹)
5. [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

---

## æ¦‚è¿°

APT-Transformerç°åœ¨æä¾›å®Œæ•´çš„å¼ºåŒ–å­¦ä¹ å’Œè‡ªç›‘ç£é¢„è®­ç»ƒåŠŸèƒ½:

### å¼ºåŒ–å­¦ä¹  (RL)
- **å¥–åŠ±æ¨¡å‹**: ä»äººç±»åå¥½å­¦ä¹ å¥–åŠ±å‡½æ•°
- **RLHF**: åŸºäºPPOçš„äººç±»åé¦ˆå¼ºåŒ–å­¦ä¹ 
- **DPO**: ç›´æ¥åå¥½ä¼˜åŒ–ï¼Œæ— éœ€å¥–åŠ±æ¨¡å‹
- **GRPO**: åˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼Œé«˜æ•ˆåœ¨çº¿å­¦ä¹ 

### è‡ªç›‘ç£é¢„è®­ç»ƒ
- **å¯¹æ¯”å­¦ä¹ **: SimCLR/MoCoé£æ ¼çš„å¯¹æ¯”å­¦ä¹ 
- **MLM**: BERTé£æ ¼çš„é®è”½è¯­è¨€æ¨¡å‹

---

## å¼ºåŒ–å­¦ä¹ æ¨¡å—

### å¥–åŠ±æ¨¡å‹

å¥–åŠ±æ¨¡å‹ç”¨äºä»äººç±»åå¥½æ•°æ®å­¦ä¹ å¥–åŠ±å‡½æ•°ã€‚

#### åŸºæœ¬ç”¨æ³•

```python
from apt_model.rl import create_reward_model, RewardModelTrainer
import torch.nn as nn

# å‡è®¾ä½ æœ‰ä¸€ä¸ªé¢„è®­ç»ƒçš„base model
base_model = YourPretrainedModel.from_pretrained("path/to/model")

# åˆ›å»ºå¥–åŠ±æ¨¡å‹
reward_model = create_reward_model(
    base_model=base_model,
    hidden_size=768,
    num_layers=2,
    use_pooling="last"  # "last", "mean", "max"
)

# åˆ›å»ºè®­ç»ƒå™¨
import torch.optim as optim
optimizer = optim.Adam(reward_model.parameters(), lr=1e-5)
trainer = RewardModelTrainer(
    reward_model=reward_model,
    optimizer=optimizer,
    margin=0.0
)

# è®­ç»ƒ
chosen_ids = torch.randint(0, vocab_size, (batch_size, seq_len))
rejected_ids = torch.randint(0, vocab_size, (batch_size, seq_len))

stats = trainer.train_step(chosen_ids, rejected_ids)
print(f"Loss: {stats['loss']:.4f}")
print(f"Accuracy: {stats['accuracy']:.2%}")
```

#### å¥–åŠ±æ¨¡å‹ç‰¹æ€§

1. **å¤šç§æ± åŒ–ç­–ç•¥**:
   - `last`: ä½¿ç”¨æœ€åä¸€ä¸ªtokençš„è¡¨ç¤º
   - `mean`: å¹³å‡æ± åŒ–
   - `max`: æœ€å¤§æ± åŒ–

2. **Bradley-TerryæŸå¤±**:
   ```
   L = -log(sigmoid(r_chosen - r_rejected))
   ```

3. **ç›´æ¥æ¯”è¾ƒå“åº”**:
   ```python
   chosen_rewards, rejected_rewards = reward_model.compare_responses(
       chosen_ids, rejected_ids
   )
   ```

---

### RLHFè®­ç»ƒå™¨

åŸºäºPPO (Proximal Policy Optimization) çš„RLHFå®ç°ã€‚

#### åŸºæœ¬ç”¨æ³•

```python
from apt_model.rl import create_rlhf_trainer, RLHFConfig

# é…ç½®
config = RLHFConfig(
    ppo_epochs=4,
    clip_epsilon=0.2,
    kl_coef=0.1,
    learning_rate=1e-5
)

# åˆ›å»ºè®­ç»ƒå™¨
trainer = create_rlhf_trainer(
    policy_model=your_model,
    reward_model=reward_model,
    config=config
)

# è®­ç»ƒ
prompts = torch.randint(0, vocab_size, (batch_size, prompt_len))
prompt_masks = torch.ones_like(prompts)

stats = trainer.train_step(prompts, prompt_masks)
print(f"Mean Reward: {stats['mean_reward']:.4f}")
print(f"KL Divergence: {stats['mean_kl']:.4f}")
print(f"PPO Loss: {stats['ppo_loss']:.4f}")
```

#### RLHFè®­ç»ƒæµç¨‹

1. **ç”Ÿæˆå“åº”**: ä½¿ç”¨ç­–ç•¥æ¨¡å‹ç”Ÿæˆå“åº”
2. **è®¡ç®—å¥–åŠ±**: ä½¿ç”¨å¥–åŠ±æ¨¡å‹è¯„åˆ†
3. **KLæƒ©ç½š**: é˜²æ­¢åç¦»å‚è€ƒæ¨¡å‹è¿‡è¿œ
4. **è®¡ç®—ä¼˜åŠ¿**: ä½¿ç”¨GAE (Generalized Advantage Estimation)
5. **PPOæ›´æ–°**: å¤šè½®ç­–ç•¥æ›´æ–°

#### å…³é”®å‚æ•°

- `ppo_epochs`: PPOå†…éƒ¨è®­ç»ƒè½®æ•° (é»˜è®¤: 4)
- `clip_epsilon`: PPOè£å‰ªå‚æ•° (é»˜è®¤: 0.2)
- `kl_coef`: KLæ•£åº¦æƒ©ç½šç³»æ•° (é»˜è®¤: 0.1)
- `gamma`: æŠ˜æ‰£å› å­ (é»˜è®¤: 0.99)
- `gae_lambda`: GAEå‚æ•° (é»˜è®¤: 0.95)

---

### DPOè®­ç»ƒå™¨

ç›´æ¥åå¥½ä¼˜åŒ–ï¼Œæ— éœ€è®­ç»ƒç‹¬ç«‹çš„å¥–åŠ±æ¨¡å‹ã€‚

#### åŸºæœ¬ç”¨æ³•

```python
from apt_model.rl import create_dpo_trainer, DPOConfig

# é…ç½®
config = DPOConfig(
    beta=0.1,  # æ¸©åº¦å‚æ•°
    label_smoothing=0.0,
    reference_free=False
)

# åˆ›å»ºè®­ç»ƒå™¨
trainer = create_dpo_trainer(
    policy_model=your_model,
    ref_policy_model=ref_model,  # å‚è€ƒæ¨¡å‹ (é€šå¸¸æ˜¯è®­ç»ƒå‰çš„å‰¯æœ¬)
    config=config
)

# è®­ç»ƒ
chosen_ids = torch.randint(0, vocab_size, (batch_size, seq_len))
rejected_ids = torch.randint(0, vocab_size, (batch_size, seq_len))

stats = trainer.train_step(chosen_ids, rejected_ids)
print(f"Loss: {stats['loss']:.4f}")
print(f"Accuracy: {stats['accuracy']:.2%}")
print(f"Chosen Reward: {stats['chosen_reward']:.4f}")
print(f"Rejected Reward: {stats['rejected_reward']:.4f}")
```

#### DPOæŸå¤±å‡½æ•°

```
L = -log(sigmoid(Î² * (log Ï€_Î¸(y_w|x) - log Ï€_Î¸(y_l|x)
                      - log Ï€_ref(y_w|x) + log Ï€_ref(y_l|x))))
```

å…¶ä¸­:
- `y_w`: é€‰ä¸­çš„å“åº” (chosen)
- `y_l`: æ‹’ç»çš„å“åº” (rejected)
- `Ï€_Î¸`: ç­–ç•¥æ¨¡å‹
- `Ï€_ref`: å‚è€ƒæ¨¡å‹
- `Î²`: æ¸©åº¦å‚æ•°

#### DPOä¼˜åŠ¿

1. **æ›´ç®€å•**: ä¸éœ€è¦å•ç‹¬è®­ç»ƒå¥–åŠ±æ¨¡å‹
2. **æ›´ç¨³å®š**: ç›´æ¥ä¼˜åŒ–åå¥½ï¼Œé¿å…å¥–åŠ±æ¨¡å‹çš„è¯¯å·®
3. **æ›´é«˜æ•ˆ**: è®­ç»ƒæ­¥éª¤æ›´å°‘
4. **æ€§èƒ½ç›¸å½“**: ä¸RLHFæ€§èƒ½ç›¸å½“

#### æ— å‚è€ƒæ¨¡å¼

```python
config = DPOConfig(reference_free=True)
```

åœ¨æ— å‚è€ƒæ¨¡å¼ä¸‹ï¼Œä¸ä½¿ç”¨å‚è€ƒæ¨¡å‹çš„log_probsã€‚

---

### GRPOè®­ç»ƒå™¨

åˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ï¼ŒDeepSeekMathä½¿ç”¨çš„æ–¹æ³•ã€‚

#### åŸºæœ¬ç”¨æ³•

```python
from apt_model.rl import create_grpo_trainer, GRPOConfig

# é…ç½®
config = GRPOConfig(
    group_size=4,  # æ¯ç»„çš„æ ·æœ¬æ•°
    advantage_type="relative",  # "relative", "normalized", "rank"
    learning_rate=1e-5
)

# åˆ›å»ºè®­ç»ƒå™¨
trainer = create_grpo_trainer(
    policy_model=your_model,
    reward_model=reward_model,  # å¯é€‰
    config=config
)

# è®­ç»ƒ
# ç”Ÿæˆå¤šä¸ªå“åº” (æ¯ç»„group_sizeä¸ª)
responses = torch.randint(0, vocab_size, (8, seq_len))  # 2ç»„ï¼Œæ¯ç»„4ä¸ª
response_masks = torch.ones_like(responses)

stats = trainer.train_step(responses, response_masks)
print(f"Mean Reward: {stats['mean_reward']:.4f}")
print(f"Group Variance: {stats['group_variance']:.4f}")
print(f"Policy Loss: {stats['policy_loss']:.4f}")
```

#### GRPOç®—æ³•æµç¨‹

1. **åˆ†ç»„**: å°†æ ·æœ¬åˆ†æˆå¤šä¸ªç»„ï¼Œæ¯ç»„`group_size`ä¸ª
2. **è®¡ç®—å¥–åŠ±**: å¯¹æ¯ä¸ªå“åº”è®¡ç®—å¥–åŠ±
3. **è®¡ç®—ç›¸å¯¹ä¼˜åŠ¿**: ç»„å†…ç›¸å¯¹ä¼˜åŠ¿
   ```
   A_i = r_i - mean(r_group)
   ```
4. **ç­–ç•¥æ›´æ–°**: ä½¿ç”¨ä¼˜åŠ¿æ›´æ–°ç­–ç•¥

#### ä¼˜åŠ¿ç±»å‹

1. **relative** (é»˜è®¤):
   ```
   A = r - mean(r_group)
   ```

2. **normalized**:
   ```
   A = (r - mean(r_group)) / std(r_group)
   ```

3. **rank**:
   ```
   åŸºäºæ’åçš„ä¼˜åŠ¿ï¼Œæ’åè¶Šé«˜ä¼˜åŠ¿è¶Šå¤§
   ```

#### GRPOä¼˜åŠ¿

1. **æ¯”PPOæ›´ç®€å•**: ä¸éœ€è¦ä»·å€¼ç½‘ç»œ
2. **ä¸éœ€è¦å‚è€ƒæ¨¡å‹**: ä½¿ç”¨ç»„å†…æ¯”è¾ƒ
3. **é€‚åˆåœ¨çº¿å­¦ä¹ **: å®æ—¶æ›´æ–°
4. **è®¡ç®—æ•ˆç‡é«˜**: è®¡ç®—æˆæœ¬ä½

---

## é¢„è®­ç»ƒæ¨¡å—

### å¯¹æ¯”å­¦ä¹ 

SimCLR/MoCoé£æ ¼çš„å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒã€‚

#### åŸºæœ¬ç”¨æ³•

```python
from apt_model.pretraining import create_contrastive_pretrainer, ContrastiveConfig

# é…ç½®
config = ContrastiveConfig(
    temperature=0.07,
    projection_dim=128,
    use_momentum_encoder=False  # SimCLRé£æ ¼
)

# åˆ›å»ºè®­ç»ƒå™¨
pretrainer = create_contrastive_pretrainer(
    encoder=your_model,
    hidden_size=768,
    config=config
)

# è®­ç»ƒ
# x1å’Œx2æ˜¯åŒä¸€æ ·æœ¬çš„ä¸¤ä¸ªå¢å¼ºè§†å›¾
x1 = augment(original_data)
x2 = augment(original_data)

stats = pretrainer.train_step(x1, x2)
print(f"Loss: {stats['loss']:.4f}")
print(f"Accuracy: {stats['accuracy']:.2%}")
```

#### SimCLR vs MoCo

**SimCLRé£æ ¼** (é»˜è®¤):
```python
config = ContrastiveConfig(use_momentum_encoder=False)
```
- ä½¿ç”¨batchå†…çš„æ ·æœ¬ä½œä¸ºè´Ÿæ ·æœ¬
- éœ€è¦è¾ƒå¤§çš„batch size
- æ›´ç®€å•

**MoCoé£æ ¼**:
```python
config = ContrastiveConfig(
    use_momentum_encoder=True,
    queue_size=65536
)
```
- ä½¿ç”¨åŠ¨é‡ç¼–ç å™¨
- ç»´æŠ¤è´Ÿæ ·æœ¬é˜Ÿåˆ—
- å¯ä»¥ä½¿ç”¨è¾ƒå°çš„batch size

#### InfoNCEæŸå¤±

```
L = -log(exp(sim(z_i, z_j) / Ï„) / Î£_k exp(sim(z_i, z_k) / Ï„))
```

#### æ•°æ®å¢å¼º

```python
from apt_model.pretraining.contrastive_pretrain import TextAugmentation

# éšæœºmask
x_masked = TextAugmentation.random_mask(input_ids, mask_token_id, mask_prob=0.15)

# éšæœºåˆ é™¤
x_deleted, mask = TextAugmentation.random_delete(input_ids, delete_prob=0.1)

# éšæœºäº¤æ¢
x_swapped = TextAugmentation.random_swap(input_ids, swap_prob=0.1)
```

---

### MLMé¢„è®­ç»ƒ

BERTé£æ ¼çš„é®è”½è¯­è¨€æ¨¡å‹é¢„è®­ç»ƒã€‚

#### åŸºæœ¬ç”¨æ³•

```python
from apt_model.pretraining import create_mlm_pretrainer, MLMConfig

# é…ç½®
config = MLMConfig(
    mask_prob=0.15,
    vocab_size=50000,
    use_nsp=False  # æ˜¯å¦ä½¿ç”¨NSPä»»åŠ¡
)

# åˆ›å»ºè®­ç»ƒå™¨
pretrainer = create_mlm_pretrainer(
    model=your_model,
    hidden_size=768,
    config=config
)

# è®­ç»ƒ
input_ids = torch.randint(0, vocab_size, (batch_size, seq_len))
attention_mask = torch.ones_like(input_ids)

stats = pretrainer.train_step(input_ids, attention_mask)
print(f"MLM Loss: {stats['mlm_loss']:.4f}")
print(f"MLM Accuracy: {stats['mlm_accuracy']:.2%}")
print(f"Masked Tokens: {stats['num_masked']}")
```

#### BERTé®è”½ç­–ç•¥

å¯¹äº15%è¢«é€‰ä¸­çš„token:
- **80%** æ›¿æ¢ä¸º`[MASK]`
- **10%** æ›¿æ¢ä¸ºéšæœºtoken
- **10%** ä¿æŒä¸å˜

#### MLM + NSP (BERTé£æ ¼)

```python
config = MLMConfig(
    mask_prob=0.15,
    use_nsp=True  # å¯ç”¨NSPä»»åŠ¡
)

pretrainer = create_mlm_pretrainer(model, hidden_size=768, config=config)

# è®­ç»ƒ
nsp_labels = torch.randint(0, 2, (batch_size,))  # 0=è¿ç»­, 1=ä¸è¿ç»­
stats = pretrainer.train_step(input_ids, attention_mask, nsp_labels)

print(f"MLM Loss: {stats['mlm_loss']:.4f}")
print(f"NSP Loss: {stats['nsp_loss']:.4f}")
print(f"NSP Accuracy: {stats['nsp_accuracy']:.2%}")
```

---

## ä½¿ç”¨ç¤ºä¾‹

### å®Œæ•´çš„RLHFè®­ç»ƒæµç¨‹

```python
import torch
from apt_model.rl import (
    create_reward_model,
    RewardModelTrainer,
    create_rlhf_trainer
)

# 1. è®­ç»ƒå¥–åŠ±æ¨¡å‹
print("è®­ç»ƒå¥–åŠ±æ¨¡å‹...")
base_model = load_pretrained_model()
reward_model = create_reward_model(base_model, hidden_size=768)
reward_trainer = RewardModelTrainer(reward_model, optimizer)

for epoch in range(reward_epochs):
    for chosen, rejected in preference_dataloader:
        stats = reward_trainer.train_step(chosen, rejected)
        print(f"Reward Loss: {stats['loss']:.4f}")

# 2. RLHFè®­ç»ƒ
print("RLHFè®­ç»ƒ...")
policy_model = load_pretrained_model()
rlhf_trainer = create_rlhf_trainer(
    policy_model=policy_model,
    reward_model=reward_model
)

for epoch in range(rlhf_epochs):
    for prompts in prompt_dataloader:
        stats = rlhf_trainer.train_step(prompts, prompt_masks)
        print(f"Mean Reward: {stats['mean_reward']:.4f}")
```

### å®Œæ•´çš„DPOè®­ç»ƒæµç¨‹

```python
from apt_model.rl import create_dpo_trainer, DPOConfig
import copy

# åˆ›å»ºç­–ç•¥æ¨¡å‹å’Œå‚è€ƒæ¨¡å‹
policy_model = load_pretrained_model()
ref_model = copy.deepcopy(policy_model)
ref_model.eval()

# åˆ›å»ºDPOè®­ç»ƒå™¨
trainer = create_dpo_trainer(
    policy_model=policy_model,
    ref_policy_model=ref_model,
    config={'beta': 0.1}
)

# è®­ç»ƒ
for epoch in range(epochs):
    for chosen, rejected in preference_dataloader:
        stats = trainer.train_step(chosen, rejected)
        print(f"Loss: {stats['loss']:.4f}, Acc: {stats['accuracy']:.2%}")
```

### é¢„è®­ç»ƒ+å¾®è°ƒæµç¨‹

```python
from apt_model.pretraining import create_contrastive_pretrainer, create_mlm_pretrainer

# 1. å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒ
print("å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒ...")
model = YourModel()
contrastive_trainer = create_contrastive_pretrainer(model, hidden_size=768)

for epoch in range(pretrain_epochs):
    for batch in dataloader:
        x1, x2 = augment_batch(batch)
        stats = contrastive_trainer.train_step(x1, x2)

# 2. MLMé¢„è®­ç»ƒ
print("MLMé¢„è®­ç»ƒ...")
mlm_trainer = create_mlm_pretrainer(model, hidden_size=768)

for epoch in range(mlm_epochs):
    for batch in dataloader:
        stats = mlm_trainer.train_step(batch['input_ids'], batch['attention_mask'])

# 3. RLHFå¾®è°ƒ
print("RLHFå¾®è°ƒ...")
# ... (è§ä¸Šé¢çš„RLHFç¤ºä¾‹)
```

---

## æœ€ä½³å®è·µ

### é€‰æ‹©åˆé€‚çš„RLç®—æ³•

| ç®—æ³• | ä¼˜åŠ¿ | åŠ£åŠ¿ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|
| **RLHF** | ç†è®ºå®Œå–„ï¼Œæ€§èƒ½å¥½ | éœ€è¦å¥–åŠ±æ¨¡å‹ï¼Œå¤æ‚ | å¤§è§„æ¨¡ç”Ÿäº§ç¯å¢ƒ |
| **DPO** | ç®€å•ï¼Œè®­ç»ƒç¨³å®š | éœ€è¦å‚è€ƒæ¨¡å‹ | å¿«é€ŸåŸå‹å¼€å‘ |
| **GRPO** | é«˜æ•ˆï¼Œåœ¨çº¿å­¦ä¹  | ç›¸å¯¹è¾ƒæ–° | åœ¨çº¿å­¦ä¹ åœºæ™¯ |

### è¶…å‚æ•°è°ƒä¼˜

#### RLHF
- `ppo_epochs`: 4-8
- `clip_epsilon`: 0.1-0.3
- `kl_coef`: 0.01-0.2 (æ ¹æ®ä»»åŠ¡è°ƒæ•´)
- `learning_rate`: 1e-6 åˆ° 1e-5

#### DPO
- `beta`: 0.05-0.5 (è¶Šå¤§è¶Šæ¿€è¿›)
- `label_smoothing`: 0.0-0.1
- `learning_rate`: 1e-6 åˆ° 1e-5

#### GRPO
- `group_size`: 4-8 (å–å†³äºè®¡ç®—èµ„æº)
- `advantage_type`: ä»"relative"å¼€å§‹
- `learning_rate`: 1e-6 åˆ° 1e-5

### è®­ç»ƒæŠ€å·§

1. **å­¦ä¹ ç‡è°ƒåº¦**:
   ```python
   from torch.optim.lr_scheduler import CosineAnnealingLR
   scheduler = CosineAnnealingLR(optimizer, T_max=total_steps)
   ```

2. **æ¢¯åº¦ç´¯ç§¯**:
   ```python
   for i, batch in enumerate(dataloader):
       loss = trainer.train_step(batch)
       if (i + 1) % accumulation_steps == 0:
           optimizer.step()
           optimizer.zero_grad()
   ```

3. **æ··åˆç²¾åº¦è®­ç»ƒ**:
   ```python
   from torch.cuda.amp import autocast, GradScaler
   scaler = GradScaler()

   with autocast():
       loss = compute_loss()
   scaler.scale(loss).backward()
   scaler.step(optimizer)
   scaler.update()
   ```

4. **æ£€æŸ¥ç‚¹ä¿å­˜**:
   ```python
   torch.save({
       'model_state_dict': model.state_dict(),
       'optimizer_state_dict': optimizer.state_dict(),
       'stats': trainer.get_statistics()
   }, 'checkpoint.pt')
   ```

### ç›‘æ§æŒ‡æ ‡

#### RLHF
- `mean_reward`: å¹³å‡å¥–åŠ± (åº”è¯¥é€æ¸å¢åŠ )
- `mean_kl`: KLæ•£åº¦ (ä¸åº”å¤ªå¤§)
- `ppo_loss`: PPOæŸå¤±
- `entropy`: ç­–ç•¥ç†µ (ä¿æŒä¸€å®šæ¢ç´¢)

#### DPO
- `loss`: DPOæŸå¤± (åº”è¯¥ä¸‹é™)
- `accuracy`: åå¥½å‡†ç¡®ç‡ (åº”è¯¥>50%)
- `reward_margin`: å¥–åŠ±å·®è· (chosen vs rejected)

#### GRPO
- `mean_reward`: å¹³å‡å¥–åŠ±
- `group_variance`: ç»„å†…æ–¹å·® (åæ˜ å¤šæ ·æ€§)
- `policy_loss`: ç­–ç•¥æŸå¤±

### å¸¸è§é—®é¢˜

#### Q1: å¥–åŠ±æ¨¡å‹è¿‡æ‹Ÿåˆ
**è§£å†³æ–¹æ¡ˆ**:
- å¢åŠ è®­ç»ƒæ•°æ®
- ä½¿ç”¨dropout
- æ—©åœ (early stopping)
- æ•°æ®å¢å¼º

#### Q2: KLæ•£åº¦è¿‡å¤§
**è§£å†³æ–¹æ¡ˆ**:
- å¢åŠ `kl_coef`
- é™ä½å­¦ä¹ ç‡
- ä½¿ç”¨æ›´å¼ºçš„è£å‰ª (é™ä½`clip_epsilon`)

#### Q3: è®­ç»ƒä¸ç¨³å®š
**è§£å†³æ–¹æ¡ˆ**:
- ä½¿ç”¨æ¢¯åº¦è£å‰ª
- é™ä½å­¦ä¹ ç‡
- å¢åŠ batch size
- ä½¿ç”¨æ›´ä¿å®ˆçš„è¶…å‚æ•°

#### Q4: å¥–åŠ±hacking
**è§£å†³æ–¹æ¡ˆ**:
- ä½¿ç”¨æ›´é²æ£’çš„å¥–åŠ±æ¨¡å‹
- å¢åŠ KLæƒ©ç½š
- ä½¿ç”¨å¤šä¸ªå¥–åŠ±æ¨¡å‹é›†æˆ
- äººå·¥å®¡æŸ¥ç”Ÿæˆç»“æœ

---

## å‚è€ƒæ–‡çŒ®

1. **RLHF**: Ouyang et al. "Training language models to follow instructions with human feedback" (2022)
2. **DPO**: Rafailov et al. "Direct Preference Optimization: Your Language Model is Secretly a Reward Model" (2023)
3. **GRPO**: Shao et al. "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models" (2024)
4. **SimCLR**: Chen et al. "A Simple Framework for Contrastive Learning of Visual Representations" (2020)
5. **MoCo**: He et al. "Momentum Contrast for Unsupervised Visual Representation Learning" (2020)
6. **BERT**: Devlin et al. "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding" (2018)

---

## æ›´æ–°æ—¥å¿—

- **2025-12-02**: åˆå§‹ç‰ˆæœ¬
  - æ·»åŠ å¥–åŠ±æ¨¡å‹
  - æ·»åŠ RLHFè®­ç»ƒå™¨
  - æ·»åŠ DPOè®­ç»ƒå™¨
  - æ·»åŠ GRPOè®­ç»ƒå™¨
  - æ·»åŠ å¯¹æ¯”å­¦ä¹ é¢„è®­ç»ƒ
  - æ·»åŠ MLMé¢„è®­ç»ƒ
  - æ›´æ–°GRPOæ’ä»¶ä»¥ä½¿ç”¨å®é™…è®­ç»ƒå™¨

---

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·è”ç³»: chen0430tw
