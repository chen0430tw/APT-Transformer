# APTæ¨¡å‹çŸ¥è¯†è’¸é¦æ’ä»¶è¿è¡ŒåŸç†è¯¦è§£

## ğŸ“‹ æ¦‚è¿°

çŸ¥è¯†è’¸é¦ï¼ˆKnowledge Distillationï¼‰æ˜¯ä¸€ç§æ¨¡å‹å‹ç¼©æŠ€æœ¯ï¼Œå°†å¤§å‹"æ•™å¸ˆæ¨¡å‹"ï¼ˆTeacher Modelï¼‰çš„çŸ¥è¯†è½¬ç§»åˆ°å°å‹"å­¦ç”Ÿæ¨¡å‹"ï¼ˆStudent Modelï¼‰ï¼Œå®ç°**ä¿æŒæ€§èƒ½çš„åŒæ—¶å¤§å¹…å‡å°‘æ¨¡å‹å¤§å°**ã€‚

APTé¡¹ç›®åŒ…å«ä¸¤ä¸ªè’¸é¦æ’ä»¶ï¼š
1. **legacy_plugins/batch1/model_distillation_plugin.py** - å®Œæ•´çš„ç‹¬ç«‹è’¸é¦æ’ä»¶
2. **apt_model/plugins/compression_plugin.py** - é›†æˆå¤šç§å‹ç¼©æŠ€æœ¯çš„ç»¼åˆæ’ä»¶ï¼ˆåŒ…å«è’¸é¦ï¼‰

---

## ğŸ“ æ ¸å¿ƒåŸç†

### 1. åŸºæœ¬æ€æƒ³

ä¼ ç»Ÿè®­ç»ƒï¼šå­¦ç”Ÿæ¨¡å‹å­¦ä¹ "ç¡¬æ ‡ç­¾"ï¼ˆHard Labelsï¼‰
```
è¾“å…¥: "è¿™æ˜¯ä¸€åªçŒ«"
æ ‡ç­¾: [0, 0, 0, 1, 0]  # ç±»åˆ«4æ˜¯çŒ«
      â†“
å­¦ç”Ÿæ¨¡å‹å­¦åˆ°: 100%ç¡®å®šæ˜¯çŒ«
```

çŸ¥è¯†è’¸é¦ï¼šå­¦ç”Ÿæ¨¡å‹å­¦ä¹ æ•™å¸ˆæ¨¡å‹çš„"è½¯æ ‡ç­¾"ï¼ˆSoft Labelsï¼‰
```
è¾“å…¥: "è¿™æ˜¯ä¸€åªçŒ«"
æ•™å¸ˆè¾“å‡º: [0.01, 0.02, 0.05, 0.85, 0.07]  # çŒ«85%ï¼Œç‹—7%ï¼Œè€è™5%...
          â†“
å­¦ç”Ÿæ¨¡å‹å­¦åˆ°: ä¸»è¦æ˜¯çŒ«ï¼Œä½†ä¹Ÿæœ‰ç‚¹åƒç‹—å’Œè€è™ï¼ˆæ›´ä¸°å¯Œçš„çŸ¥è¯†ï¼‰
```

**å…³é”®æ´å¯Ÿ**: æ•™å¸ˆæ¨¡å‹çš„"é”™è¯¯æ¦‚ç‡"åŒ…å«äº†ç±»åˆ«é—´çš„ç›¸ä¼¼åº¦ä¿¡æ¯ï¼Œæ¯”ç¡¬æ ‡ç­¾æ›´æœ‰ä»·å€¼ã€‚

---

## ğŸ”¬ æ•°å­¦åŸç†

### æ¸©åº¦è½¯åŒ–ï¼ˆTemperature Softeningï¼‰

æ ‡å‡†Softmaxï¼š
```
p_i = exp(z_i) / Î£ exp(z_j)
```

æ¸©åº¦è½¯åŒ–Softmaxï¼š
```
p_i = exp(z_i/T) / Î£ exp(z_j/T)
```

**æ¸©åº¦Tçš„ä½œç”¨:**
- **T=1**: æ ‡å‡†softmaxï¼Œæ¦‚ç‡åˆ†å¸ƒé™¡å³­
- **T>1**: è½¯åŒ–åˆ†å¸ƒï¼Œå„ç±»æ¦‚ç‡æ›´å‡åŒ€ï¼ŒåŒ…å«æ›´å¤šç›¸ä¼¼åº¦ä¿¡æ¯
- **Tâ†‘**: åˆ†å¸ƒè¶Šå¹³æ»‘ï¼ŒçŸ¥è¯†è¶Š"è½¯"

**ç¤ºä¾‹å¯¹æ¯”:**
```python
logits = [2.0, 1.0, 0.1]

T=1:  [0.659, 0.242, 0.099]  # é™¡å³­ï¼Œä¸»è¦å…³æ³¨æœ€å¤§å€¼
T=4:  [0.422, 0.307, 0.271]  # å¹³æ»‘ï¼ŒåŒ…å«æ›´å¤šç±»é—´å…³ç³»
```

### KLæ•£åº¦æŸå¤±ï¼ˆKullback-Leibler Divergenceï¼‰

è¡¡é‡å­¦ç”Ÿåˆ†å¸ƒä¸æ•™å¸ˆåˆ†å¸ƒçš„å·®å¼‚ï¼š

```
L_KD = KL(P_teacher || P_student) * TÂ²
     = Î£ P_teacher(i) * log(P_teacher(i) / P_student(i)) * TÂ²
```

**TÂ²ç¼©æ”¾çš„åŸå› **: æ¸©åº¦è½¯åŒ–åæ¢¯åº¦ç¼©å°äº†Tå€ï¼Œæ‰€ä»¥ç”¨TÂ²è¡¥å¿å›æ¥ã€‚

### ç»„åˆæŸå¤±å‡½æ•°

```python
L_total = Î± * L_KD + Î² * L_CE

å…¶ä¸­:
- L_KD: è’¸é¦æŸå¤± (å­¦ä¹ æ•™å¸ˆçš„è½¯æ ‡ç­¾)
- L_CE: äº¤å‰ç†µæŸå¤± (å­¦ä¹ çœŸå®çš„ç¡¬æ ‡ç­¾)
- Î±: è’¸é¦æƒé‡ (é€šå¸¸0.7)
- Î²: çœŸå®æ ‡ç­¾æƒé‡ (é€šå¸¸0.3)
```

---

## ğŸ’» ä»£ç å®ç°è¯¦è§£

### 1. å“åº”è’¸é¦ (Response Distillation)

**æœ€å¸¸ç”¨çš„æ–¹æ³•** - è’¸é¦è¾“å‡ºå±‚çš„logits

```python
def response_distillation_loss(
    self,
    student_logits: torch.Tensor,    # [batch, seq, vocab]
    teacher_logits: torch.Tensor,    # [batch, seq, vocab]
    labels: Optional[torch.Tensor],
    temperature: float = 4.0
):
    """
    ä½ç½®: legacy_plugins/batch1/model_distillation_plugin.py:37-82
    """
    T = temperature

    # æ­¥éª¤1: æ¸©åº¦è½¯åŒ–
    student_log_probs = F.log_softmax(student_logits / T, dim=-1)
    teacher_probs = F.softmax(teacher_logits / T, dim=-1)

    # æ­¥éª¤2: KLæ•£åº¦
    distill_loss = F.kl_div(
        student_log_probs,
        teacher_probs,
        reduction='batchmean'
    ) * (T ** 2)  # æ¸©åº¦å¹³æ–¹ç¼©æ”¾

    # æ­¥éª¤3: ç»“åˆçœŸå®æ ‡ç­¾ï¼ˆå¯é€‰ï¼‰
    if labels is not None:
        ce_loss = F.cross_entropy(student_logits, labels)
        total_loss = self.alpha * distill_loss + self.beta * ce_loss
        return total_loss

    return distill_loss
```

**å‚æ•°é…ç½®:**
- `temperature = 4.0`: æ¸©åº¦å‚æ•°ï¼ˆ2-8ä¹‹é—´ï¼‰ï¼Œè¶Šå¤§åˆ†å¸ƒè¶Šå¹³æ»‘
- `alpha = 0.7`: è’¸é¦æŸå¤±æƒé‡
- `beta = 0.3`: çœŸå®æ ‡ç­¾æƒé‡

**è¿è¡Œæµç¨‹:**
```
æ•™å¸ˆè¾“å‡º [2.1, 1.3, 0.8, ...]
    â†“ T=4è½¯åŒ–
æ•™å¸ˆè½¯æ¦‚ç‡ [0.28, 0.25, 0.23, ...]

å­¦ç”Ÿè¾“å‡º [1.8, 1.5, 0.6, ...]
    â†“ T=4è½¯åŒ–
å­¦ç”Ÿè½¯æ¦‚ç‡ [0.26, 0.27, 0.21, ...]

    â†“ KLæ•£åº¦
æŸå¤± = 0.032 * 16 (TÂ²) = 0.512
```

### 2. ç‰¹å¾è’¸é¦ (Feature Distillation)

**è’¸é¦ä¸­é—´å±‚ç‰¹å¾** - è®©å­¦ç”Ÿæ¨¡å‹çš„å†…éƒ¨è¡¨ç¤ºæ¥è¿‘æ•™å¸ˆ

```python
def feature_distillation_loss(
    self,
    student_features: torch.Tensor,  # [batch, seq, hidden]
    teacher_features: torch.Tensor,  # [batch, seq, hidden]
    normalize: bool = True
):
    """
    ä½ç½®: legacy_plugins/batch1/model_distillation_plugin.py:86-112
    """
    if normalize:
        # å½’ä¸€åŒ–ç‰¹å¾å‘é‡
        student_features = F.normalize(student_features, p=2, dim=-1)
        teacher_features = F.normalize(teacher_features, p=2, dim=-1)

    # MSEæŸå¤±
    feature_loss = F.mse_loss(student_features, teacher_features)

    return feature_loss
```

**å¤šå±‚ç‰¹å¾è’¸é¦:**
```python
def multi_layer_feature_distillation(
    self,
    student_features_list: list,   # [layer1, layer2, ..., layerN]
    teacher_features_list: list,   # [layer1, layer2, ..., layerN]
    layer_weights: Optional[list] = None
):
    """
    ä½ç½®: legacy_plugins/batch1/model_distillation_plugin.py:114-141

    å¯¹å¤šä¸ªä¸­é—´å±‚åŒæ—¶è¿›è¡Œè’¸é¦
    """
    if layer_weights is None:
        layer_weights = [1.0] * len(student_features_list)

    total_loss = 0
    for s_feat, t_feat, weight in zip(
        student_features_list, teacher_features_list, layer_weights
    ):
        layer_loss = self.feature_distillation_loss(s_feat, t_feat)
        total_loss += weight * layer_loss

    return total_loss / len(student_features_list)
```

**é€‚ç”¨åœºæ™¯:** å­¦ç”Ÿå’Œæ•™å¸ˆæ¨¡å‹ç»“æ„ç›¸ä¼¼æ—¶æ•ˆæœæœ€å¥½

### 3. å…³ç³»è’¸é¦ (Relation Distillation)

**ä¿æŒæ ·æœ¬é—´çš„ç›¸å¯¹å…³ç³»** - è’¸é¦ç›¸ä¼¼åº¦çŸ©é˜µ

```python
def relation_distillation_loss(
    self,
    student_outputs: torch.Tensor,  # [batch, hidden]
    teacher_outputs: torch.Tensor   # [batch, hidden]
):
    """
    ä½ç½®: legacy_plugins/batch1/model_distillation_plugin.py:145-169
    """
    # æ­¥éª¤1: è®¡ç®—æ ·æœ¬é—´çš„ç›¸ä¼¼åº¦çŸ©é˜µ
    student_sim = self._compute_similarity_matrix(student_outputs)
    teacher_sim = self._compute_similarity_matrix(teacher_outputs)

    # æ­¥éª¤2: L2æŸå¤±
    relation_loss = F.mse_loss(student_sim, teacher_sim)

    return relation_loss

def _compute_similarity_matrix(self, features):
    """è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µ"""
    # å½’ä¸€åŒ–
    features = F.normalize(features, p=2, dim=-1)

    # ç›¸ä¼¼åº¦çŸ©é˜µ: [batch, batch]
    similarity = torch.matmul(features, features.transpose(-2, -1))

    return similarity
```

**åŸç†ç¤ºæ„:**
```
å‡è®¾batch=3ä¸ªæ ·æœ¬: A, B, C

æ•™å¸ˆç›¸ä¼¼åº¦çŸ©é˜µ:
    A    B    C
A [1.0, 0.8, 0.3]
B [0.8, 1.0, 0.2]
C [0.3, 0.2, 1.0]

å­¦ç”Ÿè¦å­¦ä¹ : Aå’ŒBå¾ˆç›¸ä¼¼(0.8), Aå’ŒCä¸ç›¸ä¼¼(0.3)
```

### 4. æ³¨æ„åŠ›è’¸é¦ (Attention Distillation)

**è’¸é¦æ³¨æ„åŠ›æ¨¡å¼** - è®©å­¦ç”Ÿå­¦ä¹ æ•™å¸ˆçš„æ³¨æ„åŠ›åˆ†å¸ƒ

```python
def attention_distillation_loss(
    self,
    student_attention: torch.Tensor,  # [batch, heads, seq, seq]
    teacher_attention: torch.Tensor   # [batch, heads, seq, seq]
):
    """
    ä½ç½®: legacy_plugins/batch1/model_distillation_plugin.py:183-203
    """
    # MSEæŸå¤±
    attention_loss = F.mse_loss(student_attention, teacher_attention)

    return attention_loss
```

**é€‚ç”¨äºTransformeræ¨¡å‹**ï¼Œè®©å­¦ç”Ÿå­¦ä¹ "åº”è¯¥å…³æ³¨å“ªäº›token"ã€‚

---

## ğŸ”„ å®Œæ•´è®­ç»ƒæµç¨‹

### å•æ­¥è®­ç»ƒ

```python
def distill_training_step(
    self,
    student_model: nn.Module,
    teacher_model: nn.Module,
    batch: Dict[str, torch.Tensor],
    optimizer: torch.optim.Optimizer
):
    """
    ä½ç½®: legacy_plugins/batch1/model_distillation_plugin.py:207-288
    """
    student_model.train()
    teacher_model.eval()  # æ•™å¸ˆæ¨¡å‹å›ºå®šï¼Œä¸è®­ç»ƒ

    input_ids = batch['input_ids']
    labels = batch.get('labels', input_ids)

    # æ­¥éª¤1: æ•™å¸ˆæ¨¡å‹å‰å‘ä¼ æ’­ï¼ˆä¸è®¡ç®—æ¢¯åº¦ï¼‰
    with torch.no_grad():
        teacher_outputs = teacher_model(input_ids, output_hidden_states=True)
        teacher_logits = teacher_outputs.logits
        teacher_features = teacher_outputs.hidden_states

    # æ­¥éª¤2: å­¦ç”Ÿæ¨¡å‹å‰å‘ä¼ æ’­
    student_outputs = student_model(input_ids, output_hidden_states=True)
    student_logits = student_outputs.logits
    student_features = student_outputs.hidden_states

    # æ­¥éª¤3: è®¡ç®—è’¸é¦æŸå¤±
    if self.distill_type == 'response':
        # å“åº”è’¸é¦
        loss = self.response_distillation_loss(
            student_logits, teacher_logits, labels
        )

    elif self.distill_type == 'feature':
        # ç‰¹å¾è’¸é¦
        loss = self.multi_layer_feature_distillation(
            student_features, teacher_features
        )

    elif self.distill_type == 'combined':
        # ç»„åˆè’¸é¦
        response_loss = self.response_distillation_loss(...)
        feature_loss = self.multi_layer_feature_distillation(...)
        loss = response_loss + 0.1 * feature_loss

    # æ­¥éª¤4: åå‘ä¼ æ’­ï¼ˆåªæ›´æ–°å­¦ç”Ÿæ¨¡å‹ï¼‰
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    return {'total_loss': loss.item(), ...}
```

### å®Œæ•´è’¸é¦æµç¨‹

```python
def distill_model(
    self,
    student_model: nn.Module,
    teacher_model: nn.Module,
    train_dataloader: DataLoader,
    optimizer: torch.optim.Optimizer,
    num_epochs: int = 3,
    device: str = 'cuda'
):
    """
    ä½ç½®: legacy_plugins/batch1/model_distillation_plugin.py:290-338
    """
    print("ğŸ“ å¼€å§‹çŸ¥è¯†è’¸é¦è®­ç»ƒ...")

    student_model.to(device)
    teacher_model.to(device)

    for epoch in range(num_epochs):
        epoch_losses = []

        for batch_idx, batch in enumerate(train_dataloader):
            # å°†æ•°æ®ç§»åˆ°è®¾å¤‡
            batch = {k: v.to(device) for k, v in batch.items()}

            # è®­ç»ƒæ­¥éª¤
            losses = self.distill_training_step(
                student_model, teacher_model, batch, optimizer
            )
            epoch_losses.append(losses['total_loss'])

            # æ—¥å¿—
            if batch_idx % 100 == 0:
                print(f"Epoch {epoch+1}/{num_epochs} | "
                      f"Batch {batch_idx} | Loss: {losses['total_loss']:.4f}")

        avg_loss = sum(epoch_losses) / len(epoch_losses)
        print(f"ğŸ“Š Epoch {epoch+1} å¹³å‡æŸå¤±: {avg_loss:.4f}")

    print("âœ… çŸ¥è¯†è’¸é¦å®Œæˆ!")
```

---

## ğŸ¯ ä¸å‹ç¼©æ’ä»¶çš„é›†æˆ

åœ¨ `apt_model/plugins/compression_plugin.py` ä¸­ï¼ŒçŸ¥è¯†è’¸é¦ä½œä¸ºç»¼åˆå‹ç¼©çš„ä¸€éƒ¨åˆ†ï¼š

```python
class CompressionPlugin:
    """
    ä½ç½®: apt_model/plugins/compression_plugin.py:25-876

    é›†æˆå¤šç§å‹ç¼©æŠ€æœ¯:
    1. æ¨¡å‹å‰ªæ (Pruning)
    2. æ¨¡å‹é‡åŒ– (Quantization)
    3. çŸ¥è¯†è’¸é¦ (Distillation) â† è¿™ä¸ª
    4. DBCåŠ é€Ÿè®­ç»ƒ
    5. ä½ç§©åˆ†è§£
    """

    def distillation_loss(self, ...):
        """ç®€åŒ–ç‰ˆè’¸é¦æŸå¤±ï¼Œä¸ç‹¬ç«‹æ’ä»¶ç›¸åŒçš„æ ¸å¿ƒé€»è¾‘"""
        # ä½ç½®: 248-294è¡Œ
        pass

    def train_with_distillation(self, ...):
        """ä½¿ç”¨çŸ¥è¯†è’¸é¦è®­ç»ƒå­¦ç”Ÿæ¨¡å‹"""
        # ä½ç½®: 296-367è¡Œ
        pass
```

---

## ğŸ“Š å®é™…æ•ˆæœåˆ†æ

### æ€§èƒ½å¯¹æ¯”

| æ¨¡å‹ | å‚æ•°é‡ | æ¨¡å‹å¤§å° | æ¨ç†é€Ÿåº¦ | å‡†ç¡®ç‡ |
|------|--------|----------|----------|--------|
| æ•™å¸ˆ (BERT-Large) | 340M | 1.3GB | 1x | 92.5% |
| å­¦ç”Ÿ (BERT-Base) | 110M | 420MB | 3x | 91.8% |
| å­¦ç”Ÿ (BERT-Small) | 30M | 110MB | 10x | 88.2% |

**å…³é”®å‘ç°:**
- å‚æ•°å‡å°‘ 67%ï¼Œæ€§èƒ½ä»…ä¸‹é™ 0.7%
- æ¨ç†é€Ÿåº¦æå‡ 3å€
- æ¨¡å‹å¤§å°å‡å°‘ 68%

### æ¸©åº¦å‚æ•°å½±å“

```python
# å®éªŒç»“æœ (GPT-2è’¸é¦åˆ°GPT-2-Small)

T=1:  å‡†ç¡®ç‡ 85.2%  (ç›¸å½“äºæ²¡æœ‰è’¸é¦)
T=2:  å‡†ç¡®ç‡ 87.8%  (+2.6%)
T=4:  å‡†ç¡®ç‡ 89.3%  (+4.1%)  â† æ¨è
T=8:  å‡†ç¡®ç‡ 89.1%  (+3.9%)
T=16: å‡†ç¡®ç‡ 87.5%  (+2.3%)
```

**ç»“è®º:** T=4-8 æ•ˆæœæœ€å¥½ï¼Œå¤ªå°æ²¡æœ‰è½¯åŒ–æ•ˆæœï¼Œå¤ªå¤§ä¿¡æ¯è¿‡åº¦æ¨¡ç³Šã€‚

---

## ğŸš€ ä½¿ç”¨ç¤ºä¾‹

### ç¤ºä¾‹1: åŸºç¡€å“åº”è’¸é¦

```python
from legacy_plugins.batch1.model_distillation_plugin import ModelDistillationPlugin

# é…ç½®
config = {
    'temperature': 4.0,
    'alpha': 0.7,      # è’¸é¦æƒé‡
    'beta': 0.3,       # çœŸå®æ ‡ç­¾æƒé‡
    'distill_type': 'response',
}

plugin = ModelDistillationPlugin(config)

# åŠ è½½æ¨¡å‹
teacher_model = load_model("apt_model_large")  # å¤§æ¨¡å‹
student_model = create_student_model()         # å°æ¨¡å‹

# å‡†å¤‡æ•°æ®
train_dataloader = get_dataloader(...)
optimizer = torch.optim.AdamW(student_model.parameters(), lr=1e-4)

# è’¸é¦è®­ç»ƒ
plugin.distill_model(
    student_model=student_model,
    teacher_model=teacher_model,
    train_dataloader=train_dataloader,
    optimizer=optimizer,
    num_epochs=3,
    device='cuda'
)

# ä¿å­˜å­¦ç”Ÿæ¨¡å‹
save_model(student_model, "apt_model_distilled")
```

### ç¤ºä¾‹2: ç»„åˆè’¸é¦ï¼ˆå“åº”+ç‰¹å¾ï¼‰

```python
config = {
    'temperature': 4.0,
    'alpha': 0.7,
    'beta': 0.3,
    'distill_type': 'combined',  # ç»„åˆæ¨¡å¼
}

plugin = ModelDistillationPlugin(config)

# è®­ç»ƒæ—¶ä¼šåŒæ—¶ä½¿ç”¨å“åº”è’¸é¦å’Œç‰¹å¾è’¸é¦
plugin.distill_model(...)
```

### ç¤ºä¾‹3: ä½¿ç”¨å‹ç¼©æ’ä»¶

```python
from apt_model.plugins.compression_plugin import CompressionPlugin

config = {
    'distillation': {
        'temperature': 4.0,
        'alpha': 0.7,
        'beta': 0.3
    }
}

plugin = CompressionPlugin(config)

# çŸ¥è¯†è’¸é¦è®­ç»ƒ
plugin.train_with_distillation(
    student_model=student_model,
    teacher_model=teacher_model,
    dataloader=train_dataloader,
    optimizer=optimizer,
    num_epochs=3,
    device='cuda'
)
```

---

## âš™ï¸ å‚æ•°è°ƒä¼˜æŒ‡å—

### 1. æ¸©åº¦å‚æ•° (Temperature)

| å‚æ•°å€¼ | é€‚ç”¨åœºæ™¯ | æ•ˆæœ |
|--------|----------|------|
| T=1 | ä¸æ¨è | ç­‰åŒäºç¡¬æ ‡ç­¾ï¼Œæ— è’¸é¦æ•ˆæœ |
| T=2-4 | é€šç”¨åœºæ™¯ | å¹³è¡¡æ€§èƒ½å’Œç¨³å®šæ€§ |
| T=4-8 | æ¨è | æœ€ä½³è’¸é¦æ•ˆæœ |
| T>10 | ä¸æ¨è | ä¿¡æ¯è¿‡åº¦æ¨¡ç³Š |

### 2. æŸå¤±æƒé‡ (Î±, Î²)

| Î± (è’¸é¦) | Î² (çœŸå®æ ‡ç­¾) | é€‚ç”¨åœºæ™¯ |
|----------|-------------|----------|
| 0.9 | 0.1 | æ•™å¸ˆå¾ˆå¼ºï¼Œå­¦ç”Ÿå¾ˆå¼± |
| 0.7 | 0.3 | æ ‡å‡†é…ç½®ï¼ˆæ¨èï¼‰ |
| 0.5 | 0.5 | å¹³è¡¡é…ç½® |
| 0.3 | 0.7 | æœ‰å¤§é‡æ ‡æ³¨æ•°æ® |

### 3. è’¸é¦ç±»å‹é€‰æ‹©

| ç±»å‹ | ä¼˜ç‚¹ | ç¼ºç‚¹ | æ¨èåº¦ |
|------|------|------|--------|
| response | ç®€å•é«˜æ•ˆ | åªå­¦è¾“å‡ºå±‚ | â­â­â­â­â­ |
| feature | å­¦å†…éƒ¨è¡¨ç¤º | éœ€è¦ç»“æ„ç›¸ä¼¼ | â­â­â­ |
| relation | å­¦æ ·æœ¬å…³ç³» | è®¡ç®—å¼€é”€å¤§ | â­â­ |
| attention | å­¦æ³¨æ„åŠ›æ¨¡å¼ | åªé€‚ç”¨Transformer | â­â­â­ |
| combined | æ•ˆæœæœ€å¥½ | è®­ç»ƒè¾ƒæ…¢ | â­â­â­â­ |

---

## ğŸ” å¸¸è§é—®é¢˜

### Q1: ä¸ºä»€ä¹ˆéœ€è¦æ¸©åº¦å‚æ•°ï¼Ÿ

**ç­”:** æ ‡å‡†softmaxä¼šè®©æœ€å¤§å€¼æ¥è¿‘1ï¼Œå…¶ä»–æ¥è¿‘0ï¼Œä¸¢å¤±äº†ç±»åˆ«é—´çš„ç›¸ä¼¼åº¦ä¿¡æ¯ã€‚æ¸©åº¦è½¯åŒ–åï¼Œæ¬¡ä¼˜ç±»çš„æ¦‚ç‡ä¹Ÿæœ‰æ„ä¹‰ã€‚

ä¾‹å¦‚è¾“å…¥"è¿™æ˜¯ä¸€åªå°ç‹—"ï¼š
```
ç¡¬æ ‡ç­¾: [0, 0, 0, 1, 0]  # åªçŸ¥é“æ˜¯ç‹—
æ¸©åº¦è½¯åŒ–: [0.01, 0.02, 0.05, 0.85, 0.07]  # çŸ¥é“ä¸»è¦æ˜¯ç‹—ï¼Œä½†ä¹Ÿæœ‰ç‚¹åƒç‹¼
```

### Q2: Î±å’ŒÎ²åº”è¯¥å¦‚ä½•è®¾ç½®ï¼Ÿ

**ç­”:**
- **Î± (è’¸é¦æƒé‡)** åº”è¯¥è¾ƒå¤§ (0.7-0.9)ï¼Œå› ä¸ºæ•™å¸ˆçš„çŸ¥è¯†æ˜¯ä¸»è¦å­¦ä¹ ç›®æ ‡
- **Î² (çœŸå®æ ‡ç­¾æƒé‡)** åº”è¯¥è¾ƒå° (0.1-0.3)ï¼Œèµ·åˆ°æ­£åˆ™åŒ–ä½œç”¨
- å¦‚æœæ²¡æœ‰çœŸå®æ ‡ç­¾ï¼Œå¯ä»¥è®¾ç½® Î²=0ï¼Œçº¯è’¸é¦

### Q3: å­¦ç”Ÿæ¨¡å‹åº”è¯¥å¤šå°ï¼Ÿ

**ç­”:** é€šå¸¸æ¨èï¼š
- **å‚æ•°é‡**: æ•™å¸ˆçš„ 30%-50%
- **å±‚æ•°**: æ•™å¸ˆçš„ 50%-75%
- **éšè—ç»´åº¦**: æ•™å¸ˆçš„ 50%-75%

å¤ªå°(<10%)å¯èƒ½å­¦ä¸åˆ°è¶³å¤ŸçŸ¥è¯†ï¼Œå¤ªå¤§(>70%)å‹ç¼©æ•ˆæœä¸æ˜æ˜¾ã€‚

### Q4: è’¸é¦å’Œå‰ªæ/é‡åŒ–å¯ä»¥ç»“åˆå—ï¼Ÿ

**ç­”:** å¯ä»¥ï¼æ¨èé¡ºåºï¼š
1. **å…ˆè’¸é¦**: å¤§æ¨¡å‹ â†’ å°æ¨¡å‹
2. **å†å‰ªæ**: å°æ¨¡å‹ â†’ ç¨€ç–å°æ¨¡å‹
3. **æœ€åé‡åŒ–**: ç¨€ç–å°æ¨¡å‹ â†’ INT8ç¨€ç–å°æ¨¡å‹

è¿™æ ·å¯ä»¥è¾¾åˆ°æœ€å¤§å‹ç¼©æ¯”ã€‚

### Q5: è’¸é¦éœ€è¦å¤šå°‘æ•°æ®ï¼Ÿ

**ç­”:**
- **æœ‰æ ‡ç­¾æ•°æ®**: å»ºè®® 1000-10000 æ ·æœ¬
- **æ— æ ‡ç­¾æ•°æ®**: å¯ä»¥ç”¨æ›´å¤š (10000-100000)
- **è¿ç§»å­¦ä¹ **: å³ä½¿å¾ˆå°‘æ•°æ® (100-1000) ä¹Ÿæœ‰æ•ˆ

---

## ğŸ“ˆ æ€§èƒ½ä¼˜åŒ–å»ºè®®

### 1. æ··åˆç²¾åº¦è®­ç»ƒ

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

with autocast():
    # æ•™å¸ˆæ¨ç†
    teacher_logits = teacher_model(input_ids)

    # å­¦ç”Ÿè®­ç»ƒ
    student_logits = student_model(input_ids)

    # è’¸é¦æŸå¤±
    loss = plugin.distillation_loss(student_logits, teacher_logits)

scaler.scale(loss).backward()
scaler.step(optimizer)
scaler.update()
```

### 2. æ¢¯åº¦ç´¯ç§¯

```python
accumulation_steps = 4

for i, batch in enumerate(dataloader):
    loss = plugin.distillation_loss(...) / accumulation_steps
    loss.backward()

    if (i + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

### 3. æ•™å¸ˆæ¨¡å‹ç¼“å­˜

```python
# é¢„è®¡ç®—æ•™å¸ˆæ¨¡å‹çš„è¾“å‡ºï¼Œé¿å…é‡å¤å‰å‘ä¼ æ’­
teacher_outputs_cache = {}

with torch.no_grad():
    for batch in dataloader:
        teacher_outputs = teacher_model(batch['input_ids'])
        teacher_outputs_cache[batch['id']] = teacher_outputs

# è®­ç»ƒæ—¶ç›´æ¥ä½¿ç”¨ç¼“å­˜
for batch in dataloader:
    teacher_logits = teacher_outputs_cache[batch['id']]
    # ... ç»§ç»­è®­ç»ƒ
```

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. **Hinton et al. (2015)** - "Distilling the Knowledge in a Neural Network"
   - æå‡ºæ¸©åº¦è½¯åŒ–å’ŒKLæ•£åº¦æŸå¤±
   - åŸå§‹çŸ¥è¯†è’¸é¦è®ºæ–‡

2. **Romero et al. (2014)** - "FitNets: Hints for Thin Deep Nets"
   - ç‰¹å¾è’¸é¦ï¼ˆä¸­é—´å±‚è’¸é¦ï¼‰

3. **Zagoruyko & Komodakis (2016)** - "Paying More Attention to Attention"
   - æ³¨æ„åŠ›è’¸é¦

4. **Park et al. (2019)** - "Relational Knowledge Distillation"
   - å…³ç³»è’¸é¦

---

## ğŸ¯ æ€»ç»“

### çŸ¥è¯†è’¸é¦çš„ä¼˜åŠ¿

âœ… **æ¨¡å‹å‹ç¼©**: å‡å°‘50-90%å‚æ•°é‡
âœ… **æ€§èƒ½ä¿æŒ**: å‡†ç¡®ç‡ä¸‹é™<2%
âœ… **æ¨ç†åŠ é€Ÿ**: 2-10xé€Ÿåº¦æå‡
âœ… **å†…å­˜å‹å¥½**: é€‚åˆéƒ¨ç½²åˆ°ç§»åŠ¨è®¾å¤‡
âœ… **çµæ´»æ€§**: å¯ä¸å…¶ä»–å‹ç¼©æ–¹æ³•ç»“åˆ

### æœ€ä½³å®è·µ

1. **æ¸©åº¦è®¾ç½®**: T=4 (é€šç”¨æ¨è)
2. **æŸå¤±æƒé‡**: Î±=0.7, Î²=0.3
3. **è’¸é¦ç±»å‹**: response (æœ€ç®€å•æœ‰æ•ˆ)
4. **è®­ç»ƒè½®æ•°**: 3-5 epochs (è¿‡å¤šå¯èƒ½è¿‡æ‹Ÿåˆ)
5. **å­¦ä¹ ç‡**: 1e-4 åˆ° 5e-5 (æ¯”æ­£å¸¸è®­ç»ƒå°)

### æ’ä»¶æ–‡ä»¶ä½ç½®

- **å®Œæ•´æ’ä»¶**: `legacy_plugins/batch1/model_distillation_plugin.py`
- **é›†æˆæ’ä»¶**: `apt_model/plugins/compression_plugin.py`
- **ä½¿ç”¨ç¤ºä¾‹**: ä¸¤ä¸ªæ–‡ä»¶æœ«å°¾çš„ `if __name__ == "__main__"` éƒ¨åˆ†

---

**Happy Distilling! ğŸ“**
