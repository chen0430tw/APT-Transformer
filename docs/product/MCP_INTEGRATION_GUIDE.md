# MCP Integration Guide

## ğŸ“‹ æ¦‚è§ˆ

MCP (Model Context Protocol) é›†æˆæ¨¡å—ä¸º GPT æ¨¡å‹æä¾›äº†å¼ºå¤§çš„æ£€ç´¢å¢å¼ºç”Ÿæˆ (RAG) èƒ½åŠ›ï¼Œæ”¯æŒæµå¼æ£€ç´¢ã€å¼‚æ­¥æ“ä½œå’Œå¤šç§æ£€ç´¢åç«¯ã€‚

## âœ¨ æ ¸å¿ƒç‰¹æ€§

- **å¼‚æ­¥æ£€ç´¢**: éé˜»å¡çš„æµå¼æ£€ç´¢ï¼Œä¸å½±å“ç”Ÿæˆæ€§èƒ½
- **å¤šåç«¯æ”¯æŒ**: æ”¯æŒ FAISSã€Annoyã€ç²¾ç¡®æ£€ç´¢ã€GraphRAG
- **è¯æ®èåˆ**: å¤šç§èåˆç­–ç•¥ (åŠ æƒå¹³å‡ã€æ³¨æ„åŠ›ã€æœ€å¤§æ± åŒ–)
- **ç½®ä¿¡åº¦è¯„åˆ†**: è‡ªåŠ¨è¯„ä¼°æ£€ç´¢è´¨é‡
- **ç¼“å­˜ä¼˜åŒ–**: å‡å°‘é‡å¤æ£€ç´¢å¼€é”€
- **GPT-5 åŸç”Ÿæ”¯æŒ**: æ— ç¼é›†æˆ GPT-5 çš„ StreamingRetriever

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. åŸºç¡€ç”¨æ³•

```python
from apt_model.modeling.mcp_integration import create_mcp_retriever

# å‡†å¤‡è¯­æ–™åº“
corpus = [
    "Transformers use self-attention mechanisms.",
    "GPT models are autoregressive.",
    "BERT uses bidirectional encoding.",
]

# åˆ›å»ºæ£€ç´¢å™¨
retriever = create_mcp_retriever(
    d_model=512,
    corpus=corpus,
    top_k=3,
    enable_async=True
)

# æ‰§è¡Œæ£€ç´¢
query = torch.randn(1, 20, 512)  # [batch, seq_len, d_model]
result = retriever.retrieve_sync(query)

print(f"Confidence: {result.confidence:.3f}")
print(f"Retrieved: {result.documents}")
```

### 2. ä¸ GPT-5 é›†æˆ

```python
from apt_model.modeling.gpt5_model import GPT5Model
from apt_model.modeling.mcp_integration import upgrade_gpt5_with_mcp

# åˆ›å»º GPT-5 æ¨¡å‹
model = GPT5Model(
    vocab_size=50257,
    d_model=512,
    n_layers=4,
    num_skills=64
)

# å‡çº§ä¸º MCP ç‰ˆæœ¬
corpus = [...]  # ä½ çš„çŸ¥è¯†åº“
model = upgrade_gpt5_with_mcp(
    model,
    corpus=corpus,
    top_k=5,
    enable_async=True
)

# æ­£å¸¸ä½¿ç”¨ï¼Œæ£€ç´¢ä¼šè‡ªåŠ¨è¿›è¡Œ
input_ids = torch.randint(0, 50257, (1, 20))
logits, info = model.forward_step(input_ids, step_idx=0)

# æ£€ç´¢ä¿¡æ¯åœ¨ info ä¸­
print(f"Memory length: {info['mem_len']}")
```

---

## ğŸ“Š é…ç½®é€‰é¡¹

### MCPConfig å‚æ•°

```python
@dataclass
class MCPConfig:
    # æ£€ç´¢è®¾ç½®
    provider_name: str = 'exact_cosine'  # æ£€ç´¢åç«¯
    top_k: int = 3                       # è¿”å›æ–‡æ¡£æ•°
    confidence_threshold: float = 0.6    # æœ€ä½ç½®ä¿¡åº¦

    # å¼‚æ­¥è®¾ç½®
    enable_async: bool = True            # å¯ç”¨å¼‚æ­¥æ£€ç´¢
    retrieval_timeout: float = 2.0       # è¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
    max_queue_size: int = 10             # è¯·æ±‚é˜Ÿåˆ—å¤§å°

    # è¯æ®èåˆ
    fusion_method: str = 'weighted_mean' # èåˆæ–¹æ³•
    use_score_weighting: bool = True     # ä½¿ç”¨åˆ†æ•°åŠ æƒ

    # ç¼“å­˜è®¾ç½®
    enable_cache: bool = True            # å¯ç”¨ç¼“å­˜
    cache_size: int = 100                # ç¼“å­˜å¤§å°

    # æ¨¡å‹è®¾ç½®
    d_model: int = 512                   # æ¨¡å‹ç»´åº¦
    rank: int = 32                       # æŠ•å½±ç§©
```

### æ£€ç´¢åç«¯é€‰æ‹©

| åç«¯ | ä¼˜ç‚¹ | ç¼ºç‚¹ | é€‚ç”¨åœºæ™¯ |
|------|------|------|----------|
| `exact_cosine` | ç²¾ç¡®ç»“æœï¼Œç®€å• | é€Ÿåº¦æ…¢ (O(N)) | å°å‹è¯­æ–™ (<10K) |
| `faiss_default` | å¿«é€Ÿï¼Œå¯æ‰©å±• | éœ€è¦é¢å¤–ä¾èµ– | å¤§å‹è¯­æ–™ (>100K) |
| `annoy_default` | å†…å­˜å‹å¥½ | è¿‘ä¼¼ç»“æœ | ä¸­ç­‰è¯­æ–™ (10K-100K) |
| `graph_rag` | ç»“æ„åŒ–æ¨ç† | å¤æ‚åº¦é«˜ | çŸ¥è¯†å›¾è°±åœºæ™¯ |

---

## ğŸ”§ é«˜çº§ç”¨æ³•

### 1. è‡ªå®šä¹‰æ–‡æ¡£åµŒå…¥

```python
from transformers import AutoModel, AutoTokenizer

# ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹ç¼–ç æ–‡æ¡£
encoder = AutoModel.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')
tokenizer = AutoTokenizer.from_pretrained('sentence-transformers/all-MiniLM-L6-v2')

def encode_corpus(corpus, encoder, tokenizer):
    embeddings = []
    for doc in corpus:
        inputs = tokenizer(doc, return_tensors='pt', truncation=True, max_length=128)
        with torch.no_grad():
            output = encoder(**inputs)
            emb = output.last_hidden_state.mean(dim=1)  # Mean pooling
        embeddings.append(emb)
    return torch.cat(embeddings, dim=0)

# åˆ›å»ºæ£€ç´¢å™¨
doc_embeddings = encode_corpus(corpus, encoder, tokenizer)
retriever = create_mcp_retriever(
    d_model=384,  # MiniLM è¾“å‡ºç»´åº¦
    corpus=corpus,
    embeddings=doc_embeddings,
    top_k=5
)
```

### 2. å¼‚æ­¥æ£€ç´¢æ¨¡å¼

```python
# å¯åŠ¨å¼‚æ­¥ worker
retriever.start_async_worker()

# æäº¤æ£€ç´¢è¯·æ±‚
request_id = "req_001"
retriever.retrieve_async(query, request_id)

# ç»§ç»­å…¶ä»–è®¡ç®—...
# do_some_work()

# è½®è¯¢ç»“æœ
result = retriever.poll_async(request_id)
if result and result.ok:
    print(f"Retrieved: {result.documents}")

# åœæ­¢ worker
retriever.stop_async_worker()
```

### 3. è¯æ®èåˆç­–ç•¥

#### åŠ æƒå¹³å‡ (é»˜è®¤)

```python
config = MCPConfig(
    fusion_method='weighted_mean',
    use_score_weighting=True  # ä½¿ç”¨æ£€ç´¢åˆ†æ•°ä½œä¸ºæƒé‡
)
```

#### æ³¨æ„åŠ›èåˆ

```python
config = MCPConfig(
    fusion_method='attention',
    d_model=512
)
```

#### æœ€å¤§æ± åŒ–

```python
config = MCPConfig(
    fusion_method='max_pool'
)
```

### 4. ä¸ GraphRAG é›†æˆ

```python
from apt_model.core.graph_rag.graph_rag_manager import GraphRAGManager

# åˆ›å»º GraphRAG
graph_rag = GraphRAGManager(
    max_dimension=2,
    enable_brain=True,
    enable_spectral=True
)

# æ·»åŠ çŸ¥è¯†ä¸‰å…ƒç»„
triples = [
    ("Einstein", "proposed", "Relativity"),
    ("Relativity", "belongs_to", "Physics"),
    ("Quantum_Mechanics", "belongs_to", "Physics"),
]
graph_rag.add_triples_batch(triples)
graph_rag.build_indices()

# ä½¿ç”¨ GraphRAG ä½œä¸ºæ£€ç´¢åç«¯
# (éœ€è¦è‡ªå®šä¹‰ provider å®ç°ï¼Œè§ä¸‹é¢çš„ç¤ºä¾‹)
```

---

## ğŸ“ æ¶æ„è®¾è®¡

### ç»„ä»¶å±‚æ¬¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         GPT-5 Model                 â”‚
â”‚  (forward_step with retrieval)      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   StreamingRetrieverAdapter         â”‚
â”‚  (Compatible with GPT-5 interface)  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚        MCPRetriever                 â”‚
â”‚  - Query encoding                   â”‚
â”‚  - Evidence fusion                  â”‚
â”‚  - Async worker management          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚
               â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Retrieval Providers              â”‚
â”‚  - ExactRetriever                   â”‚
â”‚  - FAISSRetriever                   â”‚
â”‚  - AnnoyRetriever                   â”‚
â”‚  - GraphRAG                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ•°æ®æµ

```
Query Tensor [B, T, D]
       â”‚
       â–¼
Query Encoder (MLP)
       â”‚
       â–¼
Pooling (mean) -> Query Vector [B, D]
       â”‚
       â–¼
Similarity Compute (cosine)
       â”‚
       â–¼
Top-K Selection
       â”‚
       â–¼
Evidence Fusion (weighted mean/attention)
       â”‚
       â–¼
Evidence Embedding [B, 1, D]
       â”‚
       â–¼
Bi-State Alignment (PrecisionAligner)
       â”‚
       â–¼
Updated Hidden States
```

---

## ğŸ¯ å®Œæ•´ç¤ºä¾‹

### ç¤ºä¾‹ 1: åŸºç¡€æ£€ç´¢

```python
import torch
from apt_model.modeling.mcp_integration import create_mcp_retriever

# å‡†å¤‡æ•°æ®
corpus = [
    "Neural networks learn patterns from data.",
    "Deep learning uses multiple layers.",
    "Transformers use self-attention.",
    "CNNs are good for image processing.",
    "RNNs handle sequential data."
]

# åˆ›å»ºæ£€ç´¢å™¨
retriever = create_mcp_retriever(
    d_model=256,
    corpus=corpus,
    top_k=2
)

# æ¨¡æ‹ŸæŸ¥è¯¢
query = torch.randn(2, 10, 256)  # 2 ä¸ªæ ·æœ¬ï¼Œåºåˆ—é•¿åº¦ 10

# æ£€ç´¢
result = retriever.retrieve_sync(query)

print("âœ“ Retrieval successful!" if result.ok else "âœ— Retrieval failed")
print(f"Confidence: {result.confidence:.3f}")
print(f"Documents: {result.documents[:4]}")  # å‰ 4 ä¸ªæ–‡æ¡£
if result.scores is not None:
    print(f"Scores: {result.scores.tolist()}")
```

### ç¤ºä¾‹ 2: GPT-5 + MCP è®­ç»ƒ

```python
import torch
import torch.nn as nn
import torch.optim as optim
from apt_model.modeling.gpt5_model import GPT5Model
from apt_model.modeling.mcp_integration import upgrade_gpt5_with_mcp

# 1. åˆ›å»ºæ¨¡å‹
model = GPT5Model(
    vocab_size=10000,
    d_model=256,
    n_layers=2,
    num_skills=16,
    top_k=2,
    rank=16
)

# 2. å‡†å¤‡çŸ¥è¯†åº“
knowledge_corpus = [
    "Machine learning is a subset of AI.",
    "Neural networks mimic biological neurons.",
    "Backpropagation trains neural networks.",
]

# 3. å‡çº§ä¸º MCP ç‰ˆæœ¬
model = upgrade_gpt5_with_mcp(
    model,
    corpus=knowledge_corpus,
    top_k=2,
    enable_async=False  # è®­ç»ƒæ—¶å»ºè®®åŒæ­¥
)

# 4. è®­ç»ƒå¾ªç¯
optimizer = optim.Adam(model.parameters(), lr=1e-4)
criterion = nn.CrossEntropyLoss()

model.train()
for step in range(100):
    # å‡†å¤‡æ‰¹æ¬¡
    input_ids = torch.randint(0, 10000, (4, 32))  # [B=4, T=32]
    labels = torch.randint(0, 10000, (4, 32))

    # å‰å‘ä¼ æ’­ï¼ˆä¼šè‡ªåŠ¨æ£€ç´¢ï¼‰
    logits, info = model.forward_step(input_ids, step_idx=step)

    # è®¡ç®—æŸå¤±
    loss = criterion(
        logits[:, :-1].reshape(-1, logits.size(-1)),
        labels[:, 1:].reshape(-1)
    )

    # åå‘ä¼ æ’­
    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    if step % 10 == 0:
        print(f"Step {step}, Loss: {loss.item():.4f}, "
              f"Memory: {info['mem_len']}")
```

### ç¤ºä¾‹ 3: å®æ—¶æ¨ç† with MCP

```python
import torch
from apt_model.modeling.gpt5_model import GPT5Model
from apt_model.modeling.mcp_integration import upgrade_gpt5_with_mcp

# åŠ è½½æ¨¡å‹
model = GPT5Model.from_pretrained("path/to/checkpoint")

# åŠ è½½çŸ¥è¯†åº“
with open("knowledge_base.txt", "r") as f:
    corpus = [line.strip() for line in f if line.strip()]

# å‡çº§
model = upgrade_gpt5_with_mcp(
    model,
    corpus=corpus,
    top_k=5,
    enable_async=True  # å¼‚æ­¥æ¨¡å¼æé«˜å“åº”é€Ÿåº¦
)

model.eval()

# æ¨ç†
input_text = "What is machine learning?"
input_ids = tokenizer.encode(input_text, return_tensors='pt')

with torch.no_grad():
    for step in range(50):  # ç”Ÿæˆ 50 ä¸ª token
        logits, info = model.forward_step(input_ids, step_idx=step)

        # é‡‡æ ·ä¸‹ä¸€ä¸ª token
        next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True)
        input_ids = torch.cat([input_ids, next_token], dim=1)

        # æ£€æŸ¥æ˜¯å¦æ£€ç´¢åˆ°æ–°çŸ¥è¯†
        if info.get('align'):
            print(f"Step {step}: Retrieved knowledge with confidence "
                  f"{info['align'].get('alpha', 0):.3f}")

        # åœæ­¢æ¡ä»¶
        if next_token.item() == tokenizer.eos_token_id:
            break

output_text = tokenizer.decode(input_ids[0])
print(f"\nGenerated: {output_text}")
```

---

## ğŸ› æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: å¼‚æ­¥æ£€ç´¢ä¸å·¥ä½œ

**ç—‡çŠ¶**: `poll()` æ€»æ˜¯è¿”å› `None`

**è§£å†³æ–¹æ¡ˆ**:
```python
# ç¡®ä¿å¯åŠ¨äº† async worker
retriever.start_async_worker()

# ç­‰å¾…è¶³å¤Ÿæ—¶é—´è®©æ£€ç´¢å®Œæˆ
time.sleep(0.1)

# æˆ–è€…ä½¿ç”¨åŒæ­¥æ¨¡å¼
result = retriever.retrieve_sync(query)
```

### é—®é¢˜ 2: CUDA OOM

**ç—‡çŠ¶**: æ£€ç´¢æ—¶æ˜¾å­˜ä¸è¶³

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. å‡å°‘ top_k
config = MCPConfig(top_k=2)  # è€Œä¸æ˜¯ 10

# 2. æ–‡æ¡£åµŒå…¥æ”¾åˆ° CPU
doc_embeddings = doc_embeddings.cpu()

# 3. ä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
model = torch.utils.checkpoint.checkpoint_sequential(model, ...)
```

### é—®é¢˜ 3: æ£€ç´¢é€Ÿåº¦æ…¢

**ç—‡çŠ¶**: æ¯æ¬¡æ£€ç´¢è€—æ—¶ >1s

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. ä½¿ç”¨å¼‚æ­¥æ¨¡å¼
enable_async=True

# 2. åˆ‡æ¢åˆ° FAISS
provider_name='faiss_default'

# 3. å¯ç”¨ç¼“å­˜
enable_cache=True

# 4. å‡å°‘è¯­æ–™åº“å¤§å°
corpus = corpus[:10000]  # é™åˆ¶åˆ° 10K æ–‡æ¡£
```

### é—®é¢˜ 4: æ£€ç´¢è´¨é‡å·®

**ç—‡çŠ¶**: æ£€ç´¢åˆ°çš„æ–‡æ¡£ä¸ç›¸å…³

**è§£å†³æ–¹æ¡ˆ**:
```python
# 1. ä½¿ç”¨æ›´å¥½çš„æ–‡æ¡£ç¼–ç å™¨
from sentence_transformers import SentenceTransformer
encoder = SentenceTransformer('all-mpnet-base-v2')
doc_embeddings = encoder.encode(corpus, convert_to_tensor=True)

# 2. è°ƒæ•´ confidence_threshold
config = MCPConfig(confidence_threshold=0.8)  # æ›´ä¸¥æ ¼

# 3. å¢åŠ  top_k
config = MCPConfig(top_k=10)
```

---

## ğŸ“š API å‚è€ƒ

### create_mcp_retriever()

åˆ›å»º MCP æ£€ç´¢å™¨ã€‚

**ç­¾å**:
```python
def create_mcp_retriever(
    d_model: int = 512,
    corpus: Optional[List[str]] = None,
    embeddings: Optional[torch.Tensor] = None,
    provider: str = 'exact_cosine',
    top_k: int = 3,
    enable_async: bool = True,
    **kwargs
) -> MCPRetriever
```

**å‚æ•°**:
- `d_model`: æ¨¡å‹ç»´åº¦
- `corpus`: æ–‡æ¡£åˆ—è¡¨
- `embeddings`: æ–‡æ¡£åµŒå…¥ [num_docs, d_model]
- `provider`: æ£€ç´¢åç«¯
- `top_k`: è¿”å›æ–‡æ¡£æ•°
- `enable_async`: å¯ç”¨å¼‚æ­¥æ£€ç´¢
- `**kwargs`: é¢å¤–é…ç½®

**è¿”å›**: `MCPRetriever` å®ä¾‹

---

### upgrade_gpt5_with_mcp()

ä¸º GPT-5 æ¨¡å‹æ·»åŠ  MCP æ£€ç´¢èƒ½åŠ›ã€‚

**ç­¾å**:
```python
def upgrade_gpt5_with_mcp(
    gpt5_model,
    corpus: List[str],
    embeddings: Optional[torch.Tensor] = None,
    top_k: int = 3,
    enable_async: bool = True
)
```

**å‚æ•°**:
- `gpt5_model`: GPT5Model å®ä¾‹
- `corpus`: æ–‡æ¡£åˆ—è¡¨
- `embeddings`: æ–‡æ¡£åµŒå…¥
- `top_k`: è¿”å›æ–‡æ¡£æ•°
- `enable_async`: å¯ç”¨å¼‚æ­¥æ£€ç´¢

**è¿”å›**: å‡çº§åçš„ GPT-5 æ¨¡å‹

---

### MCPRetriever.retrieve_sync()

åŒæ­¥æ£€ç´¢ã€‚

**ç­¾å**:
```python
def retrieve_sync(
    self,
    query: torch.Tensor,
    top_k: Optional[int] = None
) -> RetrievalResult
```

**å‚æ•°**:
- `query`: æŸ¥è¯¢å¼ é‡ [batch, seq_len, d_model]
- `top_k`: è¿”å›æ–‡æ¡£æ•°ï¼ˆå¯é€‰ï¼‰

**è¿”å›**: `RetrievalResult` å¯¹è±¡

---

### RetrievalResult

æ£€ç´¢ç»“æœæ•°æ®ç±»ã€‚

**å­—æ®µ**:
```python
@dataclass
class RetrievalResult:
    ok: bool                              # æ˜¯å¦æˆåŠŸ
    confidence: float                     # ç½®ä¿¡åº¦ [0, 1]
    evidence_emb: Optional[torch.Tensor]  # è¯æ®åµŒå…¥ [B, 1, D]
    documents: List[str]                  # æ£€ç´¢åˆ°çš„æ–‡æ¡£
    scores: Optional[torch.Tensor]        # åˆ†æ•° [B, K]
    metadata: Dict[str, Any]              # å…ƒæ•°æ®
    error: Optional[str]                  # é”™è¯¯ä¿¡æ¯
```

---

## ğŸ”® æœªæ¥æ”¹è¿›

### è®¡åˆ’ç‰¹æ€§

1. **å¤šæ¨¡æ€æ£€ç´¢**: æ”¯æŒå›¾åƒã€éŸ³é¢‘æ£€ç´¢
2. **GraphRAG æ·±åº¦é›†æˆ**: ç›´æ¥ä½¿ç”¨ GraphRAG ä½œä¸ºåç«¯
3. **ç¼“å­˜æŒä¹…åŒ–**: å°†ç¼“å­˜ä¿å­˜åˆ°ç£ç›˜
4. **æ‰¹é‡æ£€ç´¢ä¼˜åŒ–**: æ›´é«˜æ•ˆçš„æ‰¹é‡å¤„ç†
5. **è‡ªé€‚åº” top-k**: æ ¹æ®æŸ¥è¯¢éš¾åº¦åŠ¨æ€è°ƒæ•´
6. **æ£€ç´¢åé¦ˆå­¦ä¹ **: ä½¿ç”¨ç”Ÿæˆè´¨é‡ä¼˜åŒ–æ£€ç´¢

### è´¡çŒ®æŒ‡å—

æ¬¢è¿è´¡çŒ®ï¼æäº¤ PR å‰è¯·ï¼š

1. ç¡®ä¿ä»£ç é€šè¿‡ `pytest tests/test_mcp.py`
2. æ·»åŠ æ–‡æ¡£æ³¨é‡Š
3. æ›´æ–°æœ¬ README
4. éµå¾ª PEP 8 ä»£ç é£æ ¼

---

## ğŸ“ è®¸å¯è¯

MIT License

---

## ğŸ™ è‡´è°¢

- **APT-Transformer** å›¢é˜Ÿ
- **Retrieval-Augmented Generation (RAG)** è®ºæ–‡ä½œè€…
- **FAISS** å’Œ **Annoy** åº“ç»´æŠ¤è€…
- **GraphRAG** è´¡çŒ®è€…

---

## ğŸ“§ è”ç³»æ–¹å¼

æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Ÿæ¬¢è¿ï¼š

- æäº¤ Issue
- å‘èµ· Discussion
- è”ç³»ç»´æŠ¤è€…

---

**Happy Retrieving! ğŸš€**
