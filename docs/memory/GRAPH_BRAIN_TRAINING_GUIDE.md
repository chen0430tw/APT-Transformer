# å›¾è„‘è®­ç»ƒæ•™ç¨‹ - Graph Reasoning Architecture

<div align="center">

**åŸºäºå›¾ç¥ç»ç½‘ç»œçš„ç»“æ„åŒ–æ¨ç†è®­ç»ƒ**

èåˆ Gemini æ€ç»´æ¨¡å¼ | å›¾ç»“æ„æ¨ç† | ç¥ç»ç¬¦å·æ¨ç†

</div>

---

## ğŸ“‹ ç›®å½•

- [ä»€ä¹ˆæ˜¯å›¾è„‘](#ä»€ä¹ˆæ˜¯å›¾è„‘)
- [æ¶æ„è®¾è®¡](#æ¶æ„è®¾è®¡)
- [âœ… å®é™…å®ç° - Graph Brain RAG ç³»ç»Ÿ](#å®é™…å®ç°---graph-brain-rag-ç³»ç»Ÿ)
- [æ¦‚å¿µæ€§ç»„ä»¶ (æ‰©å±•è®¾è®¡)](#æ¦‚å¿µæ€§ç»„ä»¶-æ‰©å±•è®¾è®¡)
- [æ¦‚å¿µæ€§è®­ç»ƒæµç¨‹](#æ¦‚å¿µæ€§è®­ç»ƒæµç¨‹-æ‰©å±•è®¾è®¡)
- [æ¦‚å¿µæ€§å®æˆ˜ç¤ºä¾‹](#æ¦‚å¿µæ€§å®æˆ˜ç¤ºä¾‹-æ‰©å±•è®¾è®¡)
- [æ€§èƒ½ä¼˜åŒ–å»ºè®®](#æ€§èƒ½ä¼˜åŒ–å»ºè®®)

---

## ğŸ§  ä»€ä¹ˆæ˜¯å›¾è„‘

### æ¦‚å¿µ

**å›¾è„‘ (Graph Brain)** æ˜¯ä¸€ç§ç»“åˆäº†å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰å’Œè¯­è¨€æ¨¡å‹çš„æ··åˆæ¨ç†æ¶æ„ï¼Œçµæ„Ÿæ¥æºäºï¼š
- **Gemini 2.0 Flash Thinking** çš„æ˜¾å¼æ€ç»´è¿‡ç¨‹
- **ç¥ç»ç¬¦å·æ¨ç†** çš„ç»“æ„åŒ–è¡¨ç¤º
- **å›¾ç¥ç»ç½‘ç»œ** çš„å…³ç³»å»ºæ¨¡èƒ½åŠ›

### æ ¸å¿ƒæ€æƒ³

```
ä¼ ç»Ÿ Transformer:
æ–‡æœ¬ â†’ Embedding â†’ Attention â†’ è¾“å‡º

å›¾è„‘æ¶æ„:
æ–‡æœ¬ â†’ æ¦‚å¿µå›¾ â†’ å›¾ç¥ç»ç½‘ç»œ â†’ æ¨ç†è·¯å¾„ â†’ è¾“å‡º
       â†“
   ç»“æ„åŒ–æ€ç»´è¿‡ç¨‹
```

### ä¼˜åŠ¿å¯¹æ¯”

| ç‰¹æ€§ | ä¼ ç»Ÿ LLM | å›¾è„‘æ¶æ„ |
|------|---------|---------|
| **æ¨ç†å¯è§£é‡Šæ€§** | âŒ é»‘ç›’ | âœ… æ˜¾å¼å›¾ç»“æ„ |
| **å¤šè·³æ¨ç†** | âš ï¸ ä¾èµ–ä¸Šä¸‹æ–‡ | âœ… åŸç”Ÿæ”¯æŒ |
| **çŸ¥è¯†æ•´åˆ** | âš ï¸ éšå¼è®°å¿† | âœ… æ˜¾å¼çŸ¥è¯†å›¾è°± |
| **è®¡ç®—æ•ˆç‡** | âš ï¸ å…¨åºåˆ—æ³¨æ„åŠ› | âœ… ç¨€ç–å›¾è®¡ç®— |
| **å¯æ§æ€§** | âŒ éš¾ä»¥å¹²é¢„ | âœ… å¯ç¼–è¾‘å›¾ç»“æ„ |

---

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### å®Œæ•´æ¶æ„å›¾

```
è¾“å…¥æ–‡æœ¬
    â†“
[1] æ¦‚å¿µæŠ½å–å™¨ (Concept Extractor)
    â”œâ”€â”€ å®ä½“è¯†åˆ«ï¼ˆNERï¼‰
    â”œâ”€â”€ å…³ç³»æŠ½å–ï¼ˆREï¼‰
    â””â”€â”€ äº‹ä»¶æ£€æµ‹
    â†“
[2] æ¦‚å¿µå›¾æ„å»º (Concept Graph Builder)
    â”œâ”€â”€ èŠ‚ç‚¹ï¼šæ¦‚å¿µ/å®ä½“
    â”œâ”€â”€ è¾¹ï¼šå…³ç³»/ä¾èµ–
    â””â”€â”€ å±æ€§ï¼šç±»å‹/æƒé‡
    â†“
[3] å›¾ç¥ç»ç¼–ç å™¨ (Graph Neural Encoder)
    â”œâ”€â”€ å›¾å·ç§¯å±‚ï¼ˆGCN/GAT/GraphSAGEï¼‰
    â”œâ”€â”€ æ¶ˆæ¯ä¼ é€’
    â””â”€â”€ èŠ‚ç‚¹æ›´æ–°
    â†“
[4] æ¨ç†è·¯å¾„è§„åˆ’ (Reasoning Path Planner)
    â”œâ”€â”€ æ³¨æ„åŠ›è·¯ç”±
    â”œâ”€â”€ å¤šè·³æ¨ç†
    â””â”€â”€ å­å›¾é‡‡æ ·
    â†“
[5] è§£ç å™¨ (Decoder)
    â”œâ”€â”€ å›¾åˆ°åºåˆ—ï¼ˆGraph2Seqï¼‰
    â”œâ”€â”€ æ€ç»´é“¾ç”Ÿæˆ
    â””â”€â”€ æœ€ç»ˆç­”æ¡ˆ
    â†“
è¾“å‡ºï¼ˆç­”æ¡ˆ + æ¨ç†è¿‡ç¨‹ï¼‰
```

---

## âœ… å®é™…å®ç° - Graph Brain RAG ç³»ç»Ÿ

### é¡¹ç›®ä¸­çš„å›¾è„‘å®ç°

**æ–‡ä»¶ä½ç½®**: `apt_model/core/graph_rag/`

APTé¡¹ç›®å®ç°äº†å®Œæ•´çš„å›¾è„‘(Graph Brain)ç³»ç»Ÿï¼ŒåŸºäº**éå¹³è¡¡æ€ç»Ÿè®¡ç‰©ç†å­¦**çš„åŠ¨æ€è®¤çŸ¥æ‹“æ‰‘æ¨¡å‹ï¼Œä¸ä¸‹é¢çš„æ¦‚å¿µæ€§GNNæ¶æ„ä¸åŒã€‚

#### æ ¸å¿ƒç»„ä»¶

**1. GeneralizedGraph** (`generalized_graph.py`)

æ³›å›¾ G = ({I_p}_{p=0}^P, âˆ‚, Ï„, w, O) æ˜¯ç»Ÿä¸€çš„å›¾è¡¨ç¤ºæ¡†æ¶ï¼š

- **p-ç»†èƒæ•°æ®ç»“æ„**: æ”¯æŒä»»æ„ç»´åº¦çš„æ‹“æ‰‘å•å…ƒ
  - 0-ç»†èƒ (0-cell): èŠ‚ç‚¹/é¡¶ç‚¹
  - 1-ç»†èƒ (1-cell): è¾¹/è¿æ¥
  - 2-ç»†èƒ (2-cell): é¢/ä¸‰è§’å½¢
  - 3-ç»†èƒ (3-cell): ä½“/å››é¢ä½“
  - æ›´é«˜ç»´åº¦: æ”¯æŒåˆ°ä»»æ„Pç»´

- **æ”¯æŒçš„å›¾ç±»å‹**:
  - æ™®é€šå›¾ (P=1): åªæœ‰ç‚¹å’Œè¾¹
  - è¶…å›¾ (Hypergraph): ä»»æ„p-ç»†èƒï¼Œè¾¹å¯è¿æ¥å¤šä¸ªèŠ‚ç‚¹
  - å•çº¯å¤å½¢ (Simplicial Complex): å‡ ä½•æ‹“æ‰‘ç»“æ„
  - CWå¤å½¢: ç»„åˆæ‹“æ‰‘ç»“æ„
  - æ—¶ç©ºå±‚å›¾/ç½‘æ ¼: å¤šå±‚åŠ¨æ€å›¾

- **æ ¸å¿ƒç»„ä»¶**:
  - I_p: pç»´ç»†èƒé›†åˆ
  - âˆ‚: è¾¹ç•Œæ˜ å°„ (æ»¡è¶³é“¾å¤å½¢æ¡ä»¶ âˆ‚âˆ˜âˆ‚=0)
  - Ï„: å–å‘ (å®šå‘)
  - w: æƒé‡
  - O: ç»“æ„è¿ç®—åº“

- **æ‹“æ‰‘æŸ¥è¯¢**:
  - è¾¹ç•Œ (boundary): âˆ‚_p(cell) - ç»†èƒçš„è¾¹ç•Œ
  - ä¸Šè¾¹ç•Œ (coboundary): âˆ‚^*(cell) - ä»¥è¯¥ç»†èƒä¸ºè¾¹ç•Œçš„é«˜ç»´ç»†èƒ
  - åŒç»´é‚»å±…: å…±äº«è¾¹ç•Œçš„ç»†èƒ
  - åº¦æ•°è®¡ç®—: è¾¹ç•Œ+ä¸Šè¾¹ç•Œçš„æ•°é‡

- **é“¾å¤å½¢éªŒè¯**: éªŒè¯ âˆ‚_{p-1} âˆ˜ âˆ‚_p = âˆ… (è¾¹ç•Œçš„è¾¹ç•Œä¸ºç©º)

- **å…³è”çŸ©é˜µ**: B_p: I_p -> I_{p-1} (p-ç»†èƒåˆ°(p-1)-ç»†èƒçš„æ˜ å°„)

**2. GraphBrainEngine** (`graph_brain.py`)
- **è®¤çŸ¥è‡ªç”±èƒ½æœ€å°åŒ–**: F = U - T_cog Ã— S
  - U: ç»“æ„å¼ åŠ› (structural tension)
  - T_cog: è®¤çŸ¥æ¸©åº¦ (cognitive temperature)
  - S: ç»“æ„ç†µ (structural entropy)
- **åŠ¨åŠ›å­¦æ¼”åŒ–**: æ¢¯åº¦ä¸‹é™ + æ‰©æ•£è¿‡ç¨‹
- **CPHLæœºåˆ¶**: è®¤çŸ¥èŒƒå¼çƒ­è£…è½½ (Cognitive Paradigm Hot Loading) - æ‹“æ‰‘ç›¸å˜æ£€æµ‹
- **Hebbå­¦ä¹ **: çªè§¦æƒé‡åŸºäºå…±æ¿€æ´»å¼ºåŒ–
- **å¸å¼•å­å½¢æˆ**: ç³»ç»Ÿæ”¶æ•›åˆ°ä½èƒ½é‡çŠ¶æ€

**3. HodgeLaplacian** (`hodge_laplacian.py`)
- **Hodgeæ‹‰æ™®æ‹‰æ–¯ç®—å­**: L_k = Î´_{k-1} âˆ˜ Î´^*_{k-1} + Î´^*_k âˆ˜ Î´_k
- **è°±åˆ†æ**: ç‰¹å¾å€¼/ç‰¹å¾å‘é‡è®¡ç®—
- **æ‹“æ‰‘ç‰¹å¾**: Bettiæ•°ã€è°ƒå’Œå½¢å¼ã€æ‹“æ‰‘å­”æ´æ£€æµ‹

**4. GraphRAGManager** (`graph_rag_manager.py`)
- **RAGç®¡ç†å™¨**: æ£€ç´¢å¢å¼ºç”Ÿæˆ (Retrieval-Augmented Generation)
- **çŸ¥è¯†å›¾è°±æŸ¥è¯¢**: åŸºäºå›¾è„‘çŠ¶æ€çš„æ£€ç´¢
- **ä¸Šä¸‹æ–‡å¢å¼º**: å°†æ£€ç´¢ç»“æœæ³¨å…¥ç”Ÿæˆè¿‡ç¨‹

#### ä½¿ç”¨ç¤ºä¾‹

##### åŸºç¡€ç”¨æ³• - æ„å»ºå’Œæ¼”åŒ–å›¾è„‘

```python
from apt_model.core.graph_rag import GeneralizedGraph, GraphBrainEngine

# 1. ä»çŸ¥è¯†ä¸‰å…ƒç»„æ„å»ºå›¾
triples = [
    ("æ·±åº¦å­¦ä¹ ", "éœ€è¦", "æ•°æ®"),
    ("æ·±åº¦å­¦ä¹ ", "éœ€è¦", "ç®—åŠ›"),
    ("æ•°æ®", "æ¥è‡ª", "äº’è”ç½‘"),
    ("ç®—åŠ›", "æ¥è‡ª", "GPU"),
    ("GPU", "ç”¨äº", "è®­ç»ƒ")
]

kg = GeneralizedGraph.from_knowledge_triples(triples)

# 2. åˆå§‹åŒ–å›¾è„‘å¼•æ“
brain = GraphBrainEngine(
    gg=kg,
    T_cog=1.0,      # è®¤çŸ¥æ¸©åº¦ (æ§åˆ¶éšæœºæ€§)
    tau_p=1.0,      # åŠ¿èƒ½å¼›è±«æ—¶é—´
    tau_w=10.0,     # æƒé‡å¼›è±«æ—¶é—´ (æ§åˆ¶å­¦ä¹ é€Ÿåº¦)
    gamma=0.1,      # è‡ªç”±èƒ½æ¢¯åº¦ç³»æ•°
    eta=0.01        # Hebbå­¦ä¹ ç‡
)

# 3. æ¼”åŒ–å›¾è„‘çŠ¶æ€ (æœ€å°åŒ–è‡ªç”±èƒ½)
print("ğŸ§  å¼€å§‹å›¾è„‘æ¼”åŒ–...")
for step in range(100):
    delta_F = brain.evolve_step(dt=0.1)

    if step % 20 == 0:
        state = brain.state
        print(f"Step {step:3d}: F={state.free_energy:8.4f} | "
              f"U={state.structural_tension:8.4f} | "
              f"S={state.structural_entropy:8.4f}")

# 4. æ¿€æ´»ç‰¹å®šæ¦‚å¿µ (æ¨¡æ‹ŸæŸ¥è¯¢)
brain.activate_cells(dimension=0, cell_indices=[0, 1], strength=1.0)
print("\nğŸ” æ¿€æ´»æ¦‚å¿µ: 'æ·±åº¦å­¦ä¹ ' å’Œ 'æ•°æ®'")

# 5. è·å–æœ€æ¿€æ´»çš„æ¦‚å¿µ
top_cells = brain.get_activated_cells(dimension=0, top_k=5)
print(f"\nğŸ“Š Top 5 æ¿€æ´»æ¦‚å¿µ: {top_cells}")

# 6. æŸ¥çœ‹å®Œæ•´çŠ¶æ€æŠ¥å‘Š
brain.print_state_report()
```

##### é«˜çº§ç”¨æ³• - RAGç³»ç»Ÿé›†æˆ

```python
from apt_model.core.graph_rag import GraphRAGManager

# 1. ä»æ–‡æ¡£æ„å»ºçŸ¥è¯†å›¾è°±
documents = [
    "æ·±åº¦å­¦ä¹ éœ€è¦å¤§é‡æ•°æ®å’Œç®—åŠ›æ”¯æŒã€‚",
    "GPUæ˜¯è®­ç»ƒæ·±åº¦å­¦ä¹ æ¨¡å‹çš„ä¸»è¦ç¡¬ä»¶ã€‚",
    "æ•°æ®é¢„å¤„ç†å¯¹æ¨¡å‹æ€§èƒ½è‡³å…³é‡è¦ã€‚",
    "Transformeræ¶æ„ä½¿ç”¨è‡ªæ³¨æ„åŠ›æœºåˆ¶ã€‚",
    "BERTæ¨¡å‹é€šè¿‡é¢„è®­ç»ƒå­¦ä¹ è¯­è¨€è¡¨ç¤ºã€‚"
]

# 2. åˆå§‹åŒ–RAGç®¡ç†å™¨
rag_manager = GraphRAGManager(
    graph_brain_config={
        'T_cog': 1.0,
        'tau_w': 10.0,
        'eta': 0.01
    }
)

# 3. æ„å»ºçŸ¥è¯†å›¾è°±
print("ğŸ“š æ„å»ºçŸ¥è¯†å›¾è°±...")
rag_manager.build_graph_from_documents(documents)

# 4. æŸ¥è¯¢å¢å¼ºç”Ÿæˆ
query = "å¦‚ä½•æå‡æ·±åº¦å­¦ä¹ æ¨¡å‹æ€§èƒ½ï¼Ÿ"
augmented_context = rag_manager.retrieve_and_augment(
    query=query,
    top_k=3,
    rerank=True
)

print(f"\nğŸ” æŸ¥è¯¢: {query}")
print(f"ğŸ“– å¢å¼ºä¸Šä¸‹æ–‡:\n{augmented_context}")

# 5. ä¸LLMç»“åˆä½¿ç”¨
# context = rag_manager.retrieve_and_augment(user_query, top_k=5)
# llm_input = f"Context: {context}\n\nQuestion: {user_query}\nAnswer:"
# response = llm.generate(llm_input)
```

##### æ³›å›¾åˆ†æå’Œæ‹“æ‰‘æŸ¥è¯¢

```python
from apt_model.core.graph_rag import GeneralizedGraph

# 1. æ„å»ºæ™®é€šå›¾ (P=1)
edges = [("A", "B"), ("B", "C"), ("C", "A"), ("A", "D")]
gg = GeneralizedGraph.from_edge_list(edges, directed=False)

print("=== æ³›å›¾æ‘˜è¦ ===")
print(gg.summary())
# è¾“å‡º:
# æœ€å¤§ç»´åº¦: 1
# æ€»ç»†èƒæ•°: 12
#   0-ç»†èƒ: 4 (èŠ‚ç‚¹)
#   1-ç»†èƒ: 8 (è¾¹)

# 2. æ‹“æ‰‘æŸ¥è¯¢
# è·å–èŠ‚ç‚¹Açš„é‚»å±…
neighbors = gg.get_neighbors(0, "A")
print(f"èŠ‚ç‚¹Açš„é‚»å±…: {neighbors}")

# è·å–è¾¹çš„è¾¹ç•Œ
boundary = gg.get_boundary(1, "e0_A->B")
print(f"è¾¹ A->B çš„è¾¹ç•Œ: {boundary}")  # {'A', 'B'}

# è·å–èŠ‚ç‚¹çš„ä¸Šè¾¹ç•Œ (å“ªäº›è¾¹è¿æ¥åˆ°å®ƒ)
coboundary = gg.get_coboundary(0, "A")
print(f"èŠ‚ç‚¹Açš„ä¸Šè¾¹ç•Œ (è¿æ¥çš„è¾¹): {coboundary}")

# è®¡ç®—åº¦æ•°
degree = gg.compute_degree(0, "A")
print(f"èŠ‚ç‚¹Açš„åº¦æ•°: {degree}")

# 3. æ„å»ºè¶…å›¾ (ä»»æ„ç»´åº¦)
gg_hyper = GeneralizedGraph(max_dimension=3)

# æ·»åŠ 0-ç»†èƒ (èŠ‚ç‚¹)
for node in ["v1", "v2", "v3", "v4"]:
    gg_hyper.add_cell(0, node)

# æ·»åŠ 1-ç»†èƒ (è¾¹)
gg_hyper.add_cell(1, "e1", boundary={"v1", "v2"})
gg_hyper.add_cell(1, "e2", boundary={"v2", "v3"})
gg_hyper.add_cell(1, "e3", boundary={"v3", "v1"})

# æ·»åŠ 2-ç»†èƒ (é¢/ä¸‰è§’å½¢)
gg_hyper.add_cell(2, "face1", boundary={"e1", "e2", "e3"})

# æ·»åŠ 3-ç»†èƒ (ä½“)
gg_hyper.add_cell(1, "e4", boundary={"v3", "v4"})
gg_hyper.add_cell(2, "face2", boundary={"e2", "e4"})
gg_hyper.add_cell(3, "volume1", boundary={"face1", "face2"})

print(gg_hyper.summary())

# 4. éªŒè¯é“¾å¤å½¢æ¡ä»¶ (âˆ‚âˆ˜âˆ‚=0)
is_valid = gg_hyper.verify_chain_complex()
print(f"é“¾å¤å½¢æ¡ä»¶æ»¡è¶³: {is_valid}")

# 5. è®¡ç®—å…³è”çŸ©é˜µ
B1 = gg_hyper.compute_incidence_matrix(1)  # è¾¹ -> èŠ‚ç‚¹
B2 = gg_hyper.compute_incidence_matrix(2)  # é¢ -> è¾¹
print(f"B_1 shape: {B1.shape}, nnz: {B1.nnz}")
print(f"B_2 shape: {B2.shape}, nnz: {B2.nnz}")

# 6. ç»Ÿè®¡ä¿¡æ¯
stats = gg_hyper.get_statistics()
print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯:")
print(f"   æ€»ç»†èƒæ•°: {stats['total_cells']}")
print(f"   å„ç»´åº¦ç»†èƒæ•°: {stats['num_cells_by_dim']}")
print(f"   å¹³å‡åº¦æ•°: {stats['avg_degree_by_dim']}")
```

##### Hodgeæ‹‰æ™®æ‹‰æ–¯å’Œè°±åˆ†æ

```python
from apt_model.core.graph_rag import HodgeLaplacian

# 1. è®¡ç®—Hodgeæ‹‰æ™®æ‹‰æ–¯ç®—å­
hodge = HodgeLaplacian(kg)
L0 = hodge.compute_laplacian(dimension=0)  # èŠ‚ç‚¹æ‹‰æ™®æ‹‰æ–¯
L1 = hodge.compute_laplacian(dimension=1)  # è¾¹æ‹‰æ™®æ‹‰æ–¯

# 2. è°±åˆ†æ
eigenvalues, eigenvectors = hodge.spectral_decomposition(L0)
print(f"ğŸ”¢ ç‰¹å¾å€¼: {eigenvalues[:5]}")

# 3. æ‹“æ‰‘ç‰¹å¾
betti_numbers = hodge.compute_betti_numbers()
print(f"ğŸ“ Bettiæ•°: {betti_numbers}")
print(f"   - è¿é€šåˆ†é‡æ•°: {betti_numbers[0]}")
print(f"   - ä¸€ç»´å­”æ´æ•°: {betti_numbers[1]}")

# 4. è°ƒå’Œå½¢å¼æ£€æµ‹
harmonic_forms = hodge.find_harmonic_forms(dimension=1, threshold=1e-6)
print(f"ğŸŒŠ è°ƒå’Œ1-å½¢å¼: {len(harmonic_forms)} ä¸ª")
```

#### ç†è®ºåŸºç¡€

##### è‡ªç”±èƒ½å…¬å¼

$$
F(t) = U(t) - T_{\text{cog}} \cdot S(t)
$$

å…¶ä¸­:
- **ç»“æ„å¼ åŠ›** U(t) = Î£ (åŠ¿èƒ½å·®å¼‚) - è¡¡é‡æ‹“æ‰‘ä¸ç¨³å®šæ€§
- **ç»“æ„ç†µ** S(t) = -Î£ p_i log p_i - è¡¡é‡æ‹“æ‰‘å¤šæ ·æ€§
- **è®¤çŸ¥æ¸©åº¦** T_cog - æ§åˆ¶æ¢ç´¢vsåˆ©ç”¨æƒè¡¡

##### åŠ¨åŠ›å­¦æ–¹ç¨‹

**åŠ¿èƒ½æ¼”åŒ–**:
```
dÏ•/dt = -(Ï• - Ï•_target) / Ï„_p - Î³ âˆ‚F/âˆ‚Ï• + noise
```

**æƒé‡æ¼”åŒ–** (Hebbå­¦ä¹ ):
```
dw/dt = -(w - w_target) / Ï„_w + Î· Â· (æ´»æ€§ç›¸å…³é¡¹)
```

##### CPHLç›¸å˜æ¡ä»¶

å½“è‡ªç”±èƒ½å˜åŒ–ç‡æ»¡è¶³:
```
|Î”F| < threshold AND âˆ‚Â²F/âˆ‚tÂ² > 0
```
è§¦å‘æ‹“æ‰‘é‡æ„ (æ·»åŠ /åˆ é™¤è¾¹ã€è°ƒæ•´è¿æ¥)

#### ä¸æ¦‚å¿µæ€§æ¶æ„çš„åŒºåˆ«

| ç‰¹æ€§ | å®é™…å®ç° (Graph Brain RAG) | ä¸‹æ–‡æ¦‚å¿µæ¶æ„ (GNN) |
|------|---------------------------|-------------------|
| **ç†è®ºåŸºç¡€** | éå¹³è¡¡æ€ç»Ÿè®¡ç‰©ç† | å›¾ç¥ç»ç½‘ç»œ |
| **æ ¸å¿ƒæœºåˆ¶** | è‡ªç”±èƒ½æœ€å°åŒ– + CPHL | æ¶ˆæ¯ä¼ é€’ + æ³¨æ„åŠ› |
| **æ‹“æ‰‘ç»“æ„** | æ³›å›¾ (p-ç»†èƒ, Pâ‰¤3) | æ ‡å‡†å›¾ (èŠ‚ç‚¹+è¾¹) |
| **å›¾ç±»å‹æ”¯æŒ** | âœ… æ™®é€šå›¾/è¶…å›¾/å•çº¯å¤å½¢/CWå¤å½¢ | âŒ ä»…æ™®é€šå›¾ |
| **é«˜ç»´æ‹“æ‰‘** | âœ… 0/1/2/3-ç»†èƒå®Œæ•´æ”¯æŒ | âŒ æ— é«˜ç»´æ”¯æŒ |
| **è¾¹ç•Œç®—å­** | âœ… âˆ‚, âˆ‚^*, é“¾å¤å½¢éªŒè¯ | âŒ æ— æ‹“æ‰‘ç®—å­ |
| **å…³è”çŸ©é˜µ** | âœ… B_p å…³è”çŸ©é˜µè®¡ç®— | âŒ ä»…é‚»æ¥çŸ©é˜µ |
| **åŠ¨åŠ›å­¦** | âœ… æ˜¾å¼æ—¶é—´æ¼”åŒ– | âŒ é™æ€å‰å‘ä¼ æ’­ |
| **è‡ªé€‚åº”æ€§** | âœ… æ‹“æ‰‘ç›¸å˜ | âŒ å›ºå®šç»“æ„ |
| **ç”¨é€”** | âœ… å·²å®ç° (RAGç³»ç»Ÿ) | ğŸ“ æ¦‚å¿µè®¾è®¡ (æ¨ç†æ¶æ„) |

---

## ğŸ”§ æ¦‚å¿µæ€§ç»„ä»¶ (æ‰©å±•è®¾è®¡)

> **æ³¨æ„**: ä»¥ä¸‹æ˜¯å›¾è„‘çš„æ¦‚å¿µæ€§æ‰©å±•è®¾è®¡,ä¸æ˜¯å½“å‰é¡¹ç›®çš„å®é™…å®ç°ã€‚å®é™…å®ç°è¯·å‚è€ƒä¸Šé¢çš„ Graph Brain RAG ç³»ç»Ÿã€‚

### 1. æ¦‚å¿µæŠ½å–å™¨

```python
import torch
import torch.nn as nn
from transformers import BertModel, BertTokenizer

class ConceptExtractor(nn.Module):
    """
    æ¦‚å¿µæŠ½å–å™¨ï¼šä»æ–‡æœ¬ä¸­æå–å®ä½“å’Œå…³ç³»

    æ–¹æ³•ï¼š
    - å®ä½“è¯†åˆ«ï¼šBERT + CRF
    - å…³ç³»æŠ½å–ï¼šåŒå‘ LSTM + æ³¨æ„åŠ›
    """
    def __init__(self, bert_model='bert-base-uncased', num_entity_types=10):
        super().__init__()
        self.bert = BertModel.from_pretrained(bert_model)
        self.entity_classifier = nn.Linear(768, num_entity_types)
        self.relation_classifier = nn.Bilinear(768, 768, 20)  # 20ç§å…³ç³»ç±»å‹

    def forward(self, input_ids, attention_mask):
        # BERT ç¼–ç 
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        sequence_output = outputs.last_hidden_state  # [B, T, 768]

        # å®ä½“è¯†åˆ«
        entity_logits = self.entity_classifier(sequence_output)  # [B, T, num_types]

        # å…³ç³»æŠ½å–ï¼ˆå®ä½“å¯¹ä¹‹é—´ï¼‰
        # ç®€åŒ–ï¼šå–å¥å­çš„ [CLS] è¡¨ç¤º
        cls_repr = sequence_output[:, 0, :]  # [B, 768]

        return {
            'entity_logits': entity_logits,
            'cls_repr': cls_repr
        }

    def extract_concepts(self, text, tokenizer):
        """
        ä»æ–‡æœ¬ä¸­æå–æ¦‚å¿µå›¾

        Returns:
            nodes: List[Dict] - èŠ‚ç‚¹åˆ—è¡¨
            edges: List[Tuple] - è¾¹åˆ—è¡¨ (src, rel, dst)
        """
        # åˆ†è¯
        inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)

        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            outputs = self.forward(inputs['input_ids'], inputs['attention_mask'])
            entity_logits = outputs['entity_logits']

        # è§£ç å®ä½“
        entity_predictions = torch.argmax(entity_logits, dim=-1)  # [B, T]

        # æ„å»ºèŠ‚ç‚¹å’Œè¾¹ï¼ˆç®€åŒ–ç‰ˆï¼‰
        nodes = []
        edges = []

        tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])

        for i, (token, entity_type) in enumerate(zip(tokens, entity_predictions[0])):
            if entity_type != 0:  # 0 = éå®ä½“
                nodes.append({
                    'id': i,
                    'token': token,
                    'type': entity_type.item()
                })

        # æå–å…³ç³»ï¼ˆç®€åŒ–ï¼šç›¸é‚»å®ä½“ï¼‰
        for i in range(len(nodes) - 1):
            edges.append((nodes[i]['id'], 'next', nodes[i+1]['id']))

        return nodes, edges
```

---

### 2. å›¾ç¥ç»ç¼–ç å™¨

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, GATConv, global_mean_pool

class GraphBrainEncoder(nn.Module):
    """
    å›¾è„‘ç¼–ç å™¨ï¼šä½¿ç”¨å›¾ç¥ç»ç½‘ç»œç¼–ç æ¦‚å¿µå›¾

    æ”¯æŒï¼š
    - GCNï¼ˆå›¾å·ç§¯ç½‘ç»œï¼‰
    - GATï¼ˆå›¾æ³¨æ„åŠ›ç½‘ç»œï¼‰
    - GraphSAGEï¼ˆå›¾é‡‡æ ·èšåˆï¼‰
    """
    def __init__(
        self,
        node_dim=768,
        hidden_dim=512,
        num_layers=3,
        num_heads=8,
        dropout=0.1,
        gnn_type='gat'
    ):
        super().__init__()
        self.gnn_type = gnn_type
        self.num_layers = num_layers

        # èŠ‚ç‚¹ç‰¹å¾æŠ•å½±
        self.node_projection = nn.Linear(node_dim, hidden_dim)

        # å›¾å·ç§¯å±‚
        if gnn_type == 'gcn':
            self.convs = nn.ModuleList([
                GCNConv(hidden_dim, hidden_dim)
                for _ in range(num_layers)
            ])
        elif gnn_type == 'gat':
            self.convs = nn.ModuleList([
                GATConv(
                    hidden_dim,
                    hidden_dim // num_heads,
                    heads=num_heads,
                    dropout=dropout,
                    concat=True
                )
                for _ in range(num_layers)
            ])
        else:
            raise ValueError(f"Unknown GNN type: {gnn_type}")

        # å±‚å½’ä¸€åŒ–
        self.norms = nn.ModuleList([
            nn.LayerNorm(hidden_dim)
            for _ in range(num_layers)
        ])

        self.dropout = nn.Dropout(dropout)

    def forward(self, x, edge_index, edge_attr=None, batch=None):
        """
        Args:
            x: èŠ‚ç‚¹ç‰¹å¾ [num_nodes, node_dim]
            edge_index: è¾¹ç´¢å¼• [2, num_edges]
            edge_attr: è¾¹å±æ€§ [num_edges, edge_dim] (å¯é€‰)
            batch: æ‰¹æ¬¡ç´¢å¼• [num_nodes] (ç”¨äºæ‰¹å¤„ç†)

        Returns:
            node_embeddings: èŠ‚ç‚¹åµŒå…¥ [num_nodes, hidden_dim]
            graph_embedding: å›¾åµŒå…¥ [batch_size, hidden_dim]
        """
        # æŠ•å½±èŠ‚ç‚¹ç‰¹å¾
        x = self.node_projection(x)  # [num_nodes, hidden_dim]

        # å¤šå±‚å›¾å·ç§¯
        for i, (conv, norm) in enumerate(zip(self.convs, self.norms)):
            # æ®‹å·®è¿æ¥
            residual = x

            # å›¾å·ç§¯
            if self.gnn_type == 'gcn':
                x = conv(x, edge_index)
            elif self.gnn_type == 'gat':
                x = conv(x, edge_index)

            # å½’ä¸€åŒ– + æ¿€æ´» + Dropout
            x = norm(x + residual)
            x = F.relu(x)
            x = self.dropout(x)

        # å›¾çº§åˆ«æ± åŒ–ï¼ˆç”¨äºç”Ÿæˆå›¾åµŒå…¥ï¼‰
        if batch is not None:
            graph_embedding = global_mean_pool(x, batch)
        else:
            graph_embedding = x.mean(dim=0, keepdim=True)

        return x, graph_embedding
```

---

### 3. æ¨ç†è·¯å¾„è§„åˆ’å™¨

```python
class ReasoningPathPlanner(nn.Module):
    """
    æ¨ç†è·¯å¾„è§„åˆ’å™¨ï¼šåœ¨å›¾ä¸Šè§„åˆ’å¤šè·³æ¨ç†è·¯å¾„

    æ–¹æ³•ï¼š
    - æ³¨æ„åŠ›è·¯ç”±ï¼šå­¦ä¹ èŠ‚ç‚¹é‡è¦æ€§
    - å¤šè·³æ¨ç†ï¼šk-hop å­å›¾é‡‡æ ·
    - è·¯å¾„é€‰æ‹©ï¼šBeam Search + å›¾æ³¨æ„åŠ›
    """
    def __init__(self, hidden_dim=512, num_hops=3, num_beams=5):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_hops = num_hops
        self.num_beams = num_beams

        # èŠ‚ç‚¹é‡è¦æ€§è¯„åˆ†å™¨
        self.importance_scorer = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1)
        )

        # è·¯å¾„æ³¨æ„åŠ›
        self.path_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=8,
            dropout=0.1
        )

        # è·¯å¾„ç¼–ç å™¨
        self.path_encoder = nn.LSTM(
            input_size=hidden_dim,
            hidden_size=hidden_dim,
            num_layers=2,
            batch_first=True,
            bidirectional=True
        )

    def forward(self, node_embeddings, edge_index, query_embedding):
        """
        Args:
            node_embeddings: èŠ‚ç‚¹åµŒå…¥ [num_nodes, hidden_dim]
            edge_index: è¾¹ç´¢å¼• [2, num_edges]
            query_embedding: æŸ¥è¯¢åµŒå…¥ [1, hidden_dim]

        Returns:
            reasoning_paths: List[List[int]] - æ¨ç†è·¯å¾„ï¼ˆèŠ‚ç‚¹IDåºåˆ—ï¼‰
            path_scores: è·¯å¾„åˆ†æ•°
        """
        num_nodes = node_embeddings.size(0)

        # 1. è®¡ç®—èŠ‚ç‚¹é‡è¦æ€§ï¼ˆç›¸å¯¹äºæŸ¥è¯¢ï¼‰
        query_expanded = query_embedding.expand(num_nodes, -1)  # [num_nodes, hidden_dim]
        combined = node_embeddings + query_expanded
        importance_scores = self.importance_scorer(combined).squeeze(-1)  # [num_nodes]

        # 2. é€‰æ‹©èµ·å§‹èŠ‚ç‚¹ï¼ˆTop-K æœ€é‡è¦çš„èŠ‚ç‚¹ï¼‰
        topk_scores, topk_indices = torch.topk(importance_scores, k=self.num_beams)

        # 3. Beam Search å¤šè·³æ¨ç†
        reasoning_paths = []
        path_scores = []

        # æ„å»ºé‚»æ¥åˆ—è¡¨ï¼ˆåŠ é€ŸæŸ¥æ‰¾ï¼‰
        adjacency = self._build_adjacency_list(edge_index, num_nodes)

        for start_node in topk_indices:
            # ä»æ¯ä¸ªèµ·å§‹èŠ‚ç‚¹å¼€å§‹æ¢ç´¢
            path = [start_node.item()]
            current_node = start_node.item()

            for hop in range(self.num_hops):
                # è·å–é‚»å±…èŠ‚ç‚¹
                neighbors = adjacency.get(current_node, [])
                if not neighbors:
                    break

                # è®¡ç®—é‚»å±…çš„æ³¨æ„åŠ›åˆ†æ•°
                neighbor_embeddings = node_embeddings[neighbors]  # [num_neighbors, hidden_dim]
                current_embedding = node_embeddings[current_node:current_node+1]  # [1, hidden_dim]

                # æ³¨æ„åŠ›è¯„åˆ†
                attn_output, attn_weights = self.path_attention(
                    query=current_embedding.unsqueeze(0),      # [1, 1, hidden_dim]
                    key=neighbor_embeddings.unsqueeze(0),      # [1, num_neighbors, hidden_dim]
                    value=neighbor_embeddings.unsqueeze(0)     # [1, num_neighbors, hidden_dim]
                )

                # é€‰æ‹©æœ€ä½³é‚»å±…
                best_neighbor_idx = torch.argmax(attn_weights[0, 0])
                best_neighbor = neighbors[best_neighbor_idx.item()]

                path.append(best_neighbor)
                current_node = best_neighbor

            reasoning_paths.append(path)
            path_scores.append(topk_scores[len(reasoning_paths) - 1].item())

        return reasoning_paths, torch.tensor(path_scores)

    def _build_adjacency_list(self, edge_index, num_nodes):
        """æ„å»ºé‚»æ¥è¡¨"""
        adjacency = {i: [] for i in range(num_nodes)}
        for src, dst in edge_index.t().tolist():
            adjacency[src].append(dst)
        return adjacency
```

---

### 4. å›¾åˆ°åºåˆ—è§£ç å™¨

```python
class Graph2SeqDecoder(nn.Module):
    """
    å›¾åˆ°åºåˆ—è§£ç å™¨ï¼šå°†æ¨ç†è·¯å¾„è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€

    æ–¹æ³•ï¼š
    - è·¯å¾„ç¼–ç ï¼šLSTM ç¼–ç æ¨ç†è·¯å¾„
    - æ³¨æ„åŠ›è§£ç ï¼šç”Ÿæˆæ€ç»´é“¾
    - ç­”æ¡ˆç”Ÿæˆï¼šTransformer è§£ç å™¨
    """
    def __init__(
        self,
        hidden_dim=512,
        vocab_size=50257,
        max_length=512,
        num_decoder_layers=6
    ):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.max_length = max_length

        # è·¯å¾„ç¼–ç å™¨
        self.path_encoder = nn.LSTM(
            input_size=hidden_dim,
            hidden_size=hidden_dim,
            num_layers=2,
            batch_first=True,
            bidirectional=True
        )

        # Transformer è§£ç å™¨
        decoder_layer = nn.TransformerDecoderLayer(
            d_model=hidden_dim,
            nhead=8,
            dim_feedforward=hidden_dim * 4,
            dropout=0.1
        )
        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)

        # è¾“å‡ºæŠ•å½±
        self.output_projection = nn.Linear(hidden_dim, vocab_size)

        # ä½ç½®ç¼–ç 
        self.pos_encoder = nn.Embedding(max_length, hidden_dim)

    def forward(self, path_embeddings, target_ids=None, teacher_forcing_ratio=0.5):
        """
        Args:
            path_embeddings: è·¯å¾„èŠ‚ç‚¹åµŒå…¥ [batch, path_len, hidden_dim]
            target_ids: ç›®æ ‡åºåˆ— [batch, seq_len] (è®­ç»ƒæ—¶)
            teacher_forcing_ratio: æ•™å¸ˆå¼ºåˆ¶æ¯”ç‡

        Returns:
            logits: [batch, seq_len, vocab_size]
            generated_ids: [batch, seq_len]
        """
        batch_size = path_embeddings.size(0)

        # 1. ç¼–ç è·¯å¾„
        path_encoded, (hidden, cell) = self.path_encoder(path_embeddings)
        # path_encoded: [batch, path_len, hidden_dim * 2]

        # æ± åŒ–ä¸ºå•å‘
        path_memory = path_encoded[:, :, :self.hidden_dim] + path_encoded[:, :, self.hidden_dim:]
        # path_memory: [batch, path_len, hidden_dim]

        # 2. è§£ç ç”Ÿæˆåºåˆ—
        if target_ids is not None:
            # è®­ç»ƒæ¨¡å¼ï¼šæ•™å¸ˆå¼ºåˆ¶
            seq_len = target_ids.size(1)
            pos_ids = torch.arange(seq_len, device=path_embeddings.device).unsqueeze(0)
            pos_embeddings = self.pos_encoder(pos_ids)  # [1, seq_len, hidden_dim]

            # Transformer è§£ç 
            tgt = pos_embeddings.expand(batch_size, -1, -1).transpose(0, 1)  # [seq_len, batch, hidden_dim]
            memory = path_memory.transpose(0, 1)  # [path_len, batch, hidden_dim]

            decoder_output = self.decoder(tgt, memory)  # [seq_len, batch, hidden_dim]
            decoder_output = decoder_output.transpose(0, 1)  # [batch, seq_len, hidden_dim]

            # è¾“å‡ºæŠ•å½±
            logits = self.output_projection(decoder_output)  # [batch, seq_len, vocab_size]

            return logits, None

        else:
            # æ¨ç†æ¨¡å¼ï¼šè‡ªå›å½’ç”Ÿæˆ
            generated_ids = []
            current_input = torch.zeros(batch_size, 1, self.hidden_dim, device=path_embeddings.device)

            for step in range(self.max_length):
                # ä½ç½®ç¼–ç 
                pos_id = torch.tensor([[step]], device=path_embeddings.device)
                pos_emb = self.pos_encoder(pos_id).expand(batch_size, -1, -1)

                tgt = (current_input + pos_emb).transpose(0, 1)  # [1, batch, hidden_dim]
                memory = path_memory.transpose(0, 1)  # [path_len, batch, hidden_dim]

                # è§£ç ä¸€æ­¥
                decoder_output = self.decoder(tgt, memory)  # [1, batch, hidden_dim]
                decoder_output = decoder_output.transpose(0, 1)  # [batch, 1, hidden_dim]

                # é¢„æµ‹ä¸‹ä¸€ä¸ª token
                logits = self.output_projection(decoder_output)  # [batch, 1, vocab_size]
                next_token = torch.argmax(logits, dim=-1)  # [batch, 1]

                generated_ids.append(next_token)

                # æ›´æ–°è¾“å…¥ï¼ˆåµŒå…¥ä¸‹ä¸€ä¸ª tokenï¼‰
                # ç®€åŒ–ï¼šè¿™é‡Œåº”è¯¥æœ‰ token embedding å±‚
                current_input = decoder_output

            generated_ids = torch.cat(generated_ids, dim=1)  # [batch, max_length]
            return None, generated_ids
```

---

## ğŸ“ æ¦‚å¿µæ€§è®­ç»ƒæµç¨‹ (æ‰©å±•è®¾è®¡)

> **æ³¨æ„**: ä»¥ä¸‹æ˜¯åŸºäºGNNçš„æ¦‚å¿µæ€§è®­ç»ƒä»£ç ,ä¸æ˜¯å½“å‰é¡¹ç›®çš„å®é™…å®ç°ã€‚

### å®Œæ•´è®­ç»ƒä»£ç 

```python
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import GPT2Tokenizer

class GraphBrainModel(nn.Module):
    """å®Œæ•´çš„å›¾è„‘æ¨¡å‹"""
    def __init__(self, config):
        super().__init__()
        self.concept_extractor = ConceptExtractor()
        self.graph_encoder = GraphBrainEncoder(
            node_dim=768,
            hidden_dim=512,
            num_layers=3
        )
        self.path_planner = ReasoningPathPlanner(hidden_dim=512)
        self.decoder = Graph2SeqDecoder(
            hidden_dim=512,
            vocab_size=50257
        )

    def forward(self, input_text, target_text=None):
        # 1. æŠ½å–æ¦‚å¿µå›¾
        nodes, edges = self.concept_extractor.extract_concepts(
            input_text,
            tokenizer
        )

        # 2. ç¼–ç å›¾ç»“æ„
        # ï¼ˆéœ€è¦è½¬æ¢ä¸º PyTorch Geometric æ ¼å¼ï¼‰
        node_embeddings, graph_embedding = self.graph_encoder(
            x=node_features,
            edge_index=edge_index
        )

        # 3. è§„åˆ’æ¨ç†è·¯å¾„
        reasoning_paths, path_scores = self.path_planner(
            node_embeddings,
            edge_index,
            query_embedding=graph_embedding
        )

        # 4. ç”Ÿæˆç­”æ¡ˆ
        # è·å–è·¯å¾„çš„èŠ‚ç‚¹åµŒå…¥
        path_embeddings = torch.stack([
            node_embeddings[path] for path in reasoning_paths
        ])

        # è§£ç ç”Ÿæˆ
        logits, generated_ids = self.decoder(
            path_embeddings,
            target_ids=target_text
        )

        return {
            'logits': logits,
            'generated_ids': generated_ids,
            'reasoning_paths': reasoning_paths,
            'path_scores': path_scores
        }


# ========== è®­ç»ƒå™¨ ==========

class GraphBrainTrainer:
    """å›¾è„‘è®­ç»ƒå™¨"""
    def __init__(self, model, tokenizer, device='cuda'):
        self.model = model.to(device)
        self.tokenizer = tokenizer
        self.device = device

        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=2e-4,
            weight_decay=0.01
        )

        # æŸå¤±å‡½æ•°
        self.ce_loss = nn.CrossEntropyLoss(ignore_index=-100)

    def train_step(self, batch):
        """è®­ç»ƒä¸€æ­¥"""
        self.model.train()

        input_texts = batch['input_texts']
        target_texts = batch['target_texts']

        # å‰å‘ä¼ æ’­
        outputs = self.model(input_texts, target_texts)

        # è®¡ç®—æŸå¤±
        logits = outputs['logits']
        target_ids = self.tokenizer(
            target_texts,
            return_tensors='pt',
            padding=True,
            truncation=True
        )['input_ids'].to(self.device)

        # è¯­è¨€æ¨¡å‹æŸå¤±
        lm_loss = self.ce_loss(
            logits.view(-1, logits.size(-1)),
            target_ids.view(-1)
        )

        # è·¯å¾„åˆ†æ•°æ­£åˆ™åŒ–ï¼ˆé¼“åŠ±å¤šæ ·æ€§ï¼‰
        path_scores = outputs['path_scores']
        path_diversity_loss = -torch.std(path_scores)  # æœ€å¤§åŒ–æ–¹å·®

        # æ€»æŸå¤±
        total_loss = lm_loss + 0.1 * path_diversity_loss

        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
        self.optimizer.step()

        return {
            'loss': total_loss.item(),
            'lm_loss': lm_loss.item(),
            'path_diversity_loss': path_diversity_loss.item()
        }

    def train(self, train_loader, num_epochs=10, save_path='./graph_brain'):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print(f"ğŸ§  å¼€å§‹å›¾è„‘è®­ç»ƒ...")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   Epochs: {num_epochs}")

        for epoch in range(num_epochs):
            total_loss = 0
            num_batches = 0

            for batch_idx, batch in enumerate(train_loader):
                # è®­ç»ƒä¸€æ­¥
                metrics = self.train_step(batch)
                total_loss += metrics['loss']
                num_batches += 1

                # æ‰“å°è¿›åº¦
                if (batch_idx + 1) % 10 == 0:
                    avg_loss = total_loss / num_batches
                    print(f"Epoch [{epoch+1}/{num_epochs}] "
                          f"Batch [{batch_idx+1}/{len(train_loader)}] "
                          f"Loss: {avg_loss:.4f}")

            # ä¿å­˜æ£€æŸ¥ç‚¹
            torch.save({
                'epoch': epoch,
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
            }, f"{save_path}/epoch_{epoch+1}.pt")

        print(f"âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ° {save_path}")


# ========== ä½¿ç”¨ç¤ºä¾‹ ==========

# 1. å‡†å¤‡æ•°æ®
train_dataset = GraphReasoningDataset('train.jsonl')
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)

# 2. åˆå§‹åŒ–æ¨¡å‹
model = GraphBrainModel(config={})
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# 3. åˆ›å»ºè®­ç»ƒå™¨
trainer = GraphBrainTrainer(model, tokenizer)

# 4. å¼€å§‹è®­ç»ƒ
trainer.train(train_loader, num_epochs=20, save_path='./graph_brain_model')
```

---

## ğŸš€ æ¦‚å¿µæ€§å®æˆ˜ç¤ºä¾‹ (æ‰©å±•è®¾è®¡)

> **æ³¨æ„**: ä»¥ä¸‹æ˜¯æ¦‚å¿µæ€§ç¤ºä¾‹ã€‚å®é™…ä½¿ç”¨è¯·å‚è€ƒä¸Šé¢çš„ Graph Brain RAG ç³»ç»Ÿã€‚

### å¤šè·³é—®ç­”æ¨ç†

```python
# é—®é¢˜ï¼šEinstein çš„è€å¸ˆçš„å›½ç±æ˜¯ä»€ä¹ˆï¼Ÿ

# è¾“å…¥æ–‡æœ¬
question = "What is the nationality of Einstein's teacher?"

# æ¨¡å‹æ¨ç†
model.eval()
with torch.no_grad():
    outputs = model(question)

# è¾“å‡º
# reasoning_paths: [
#   [Einstein] â†’ [studied under] â†’ [Heinrich Weber] â†’ [nationality] â†’ [German]
# ]
# answer: "German"
# thinking_process: "First, I identified Einstein. Then I found his teacher Heinrich Weber. Finally, I determined Weber's nationality was German."
```

### æ•°å­¦æ¨ç†

```python
# é—®é¢˜ï¼šå¦‚æœ x + 2 = 5ï¼Œæ±‚ x

question = "If x + 2 = 5, what is x?"

# æ¨ç†å›¾
# [x + 2] â†’ [equals] â†’ [5]
#     â†“
# [subtract 2]
#     â†“
# [x = 3]

# è¾“å‡º
# answer: "x = 3"
# thinking_process: "Starting from x + 2 = 5, I subtract 2 from both sides to get x = 3."
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–å»ºè®®

> **æ³¨æ„**: ä»¥ä¸‹æ˜¯æ¦‚å¿µæ€§ä¼˜åŒ–å»ºè®®ã€‚å®é™…Graph Brain RAGç³»ç»Ÿçš„ä¼˜åŒ–è¯·å‚è€ƒç›¸å…³ä»£ç ã€‚

### å›¾é‡‡æ ·åŠ é€Ÿ

```python
from torch_geometric.data import DataLoader as PyGDataLoader
from torch_geometric.data import Data

# å¤§å›¾é‡‡æ ·
class GraphSampler:
    """å›¾é‡‡æ ·å™¨ï¼šå¤„ç†å¤§è§„æ¨¡å›¾"""
    def __init__(self, num_neighbors=[10, 5], num_hops=2):
        self.num_neighbors = num_neighbors
        self.num_hops = num_hops

    def sample_subgraph(self, node_id, edge_index, num_nodes):
        """é‡‡æ · k-hop å­å›¾"""
        from torch_geometric.utils import k_hop_subgraph

        subset, sub_edge_index, mapping, edge_mask = k_hop_subgraph(
            node_idx=node_id,
            num_hops=self.num_hops,
            edge_index=edge_index,
            relabel_nodes=True,
            num_nodes=num_nodes
        )

        return subset, sub_edge_index
```

### æ··åˆç²¾åº¦è®­ç»ƒ

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

# è®­ç»ƒå¾ªç¯
for batch in train_loader:
    optimizer.zero_grad()

    # æ··åˆç²¾åº¦å‰å‘
    with autocast():
        outputs = model(batch['input'])
        loss = compute_loss(outputs, batch['target'])

    # ç¼©æ”¾æ¢¯åº¦
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

---

## ğŸ“š å‚è€ƒèµ„æº

### å­¦æœ¯è®ºæ–‡

- [Thinking Like Transformers](https://arxiv.org/abs/2106.06981) - ç»“æ„åŒ–æ¨ç†
- [Graph Neural Networks: A Review](https://arxiv.org/abs/1812.08434) - GNNç»¼è¿°
- [Neural-Symbolic VQA](https://arxiv.org/abs/1810.02338) - ç¥ç»ç¬¦å·æ¨ç†

### å®˜æ–¹èµ„æº

- [Gemini 2.0 Flash Thinking](https://ai.google.dev/gemini-api/docs/thinking) - Google æ€ç»´æ¨¡å¼
- [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/) - å›¾ç¥ç»ç½‘ç»œåº“
- [DeepMind Gemini](https://deepmind.google/models/gemini/) - Gemini æ¨¡å‹

Sources:
- [Gemini 2.0 Flash Thinking Experimental](https://www.datacamp.com/blog/gemini-2-0-flash-experimental)
- [Gemini 2.0 Technical Details](https://www.techtarget.com/whatis/feature/Google-Gemini-20-explained-Everything-you-need-to-know)
- [Gemini Thinking API](https://ai.google.dev/gemini-api/docs/thinking)
- [Gemini Models Overview](https://deepmind.google/models/gemini/)

### APT ç›¸å…³æ–‡æ¡£

- [DeepSeek è®­ç»ƒæŒ‡å—](../kernel/DEEPSEEK_TRAINING_GUIDE.md) - MoE æ¶æ„
- [æ•°æ®é¢„å¤„ç†æŒ‡å—](../kernel/DATA_PREPROCESSING_GUIDE.md) - æ•°æ®æ¸…æ´—
- [æ’ä»¶ç³»ç»Ÿæ–‡æ¡£](PLUGIN_SYSTEM.md) - æ’ä»¶å¼€å‘

---

## ğŸ“ æ›´æ–°æ—¥å¿—

- **v1.0.0** (2025-12) - åˆå§‹ç‰ˆæœ¬
  - âœ… å®Œæ•´å›¾è„‘æ¶æ„ï¼ˆæ¦‚å¿µæŠ½å– + GNN + æ¨ç†è§„åˆ’ï¼‰
  - âœ… å¤šè·³æ¨ç†è·¯å¾„è§„åˆ’
  - âœ… å›¾åˆ°åºåˆ—è§£ç å™¨
  - âœ… æ˜¾å¼æ€ç»´è¿‡ç¨‹ç”Ÿæˆ
  - âœ… ç”Ÿäº§çº§è®­ç»ƒä»£ç 
  - âœ… æ€§èƒ½ä¼˜åŒ–å»ºè®®

---

<div align="center">

**Graph + Brain = Better Reasoning! ğŸ§ ğŸ’¡**

ç»“æ„åŒ–æ¨ç†ï¼Œè®©æ¨¡å‹æ€è€ƒæ›´æ¸…æ™°

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æäº¤ [Issue](https://github.com/chen0430tw/APT-Transformer/issues)

</div>
