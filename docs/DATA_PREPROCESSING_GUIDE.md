# APT æ•°æ®é¢„å¤„ç†ä¸æ¸…æ´—æŒ‡å—

<div align="center">

**APT æ¨¡å‹è®­ç»ƒæ•°æ®å¤„ç†å®Œæ•´æ•™ç¨‹**

ä»åŸå§‹æ•°æ®åˆ°é«˜è´¨é‡è®­ç»ƒè¯­æ–™

> **æ–‡æ¡£è¯´æ˜ (Option B æ–¹å¼)**
> âœ… **å®é™…å®ç°**: é¡¹ç›®ä¸­å·²å­˜åœ¨çš„å¯ç”¨ä»£ç 
> ğŸ“ **æ‰©å±•ç¤ºä¾‹**: éœ€è¦é¢å¤–å®ç°æˆ–ä¾èµ–çš„åŠŸèƒ½

</div>

---

## ğŸ“‹ ç›®å½•

### âœ… å®é™…å®ç°éƒ¨åˆ†
- [æ ¸å¿ƒæ•°æ®å¤„ç†å™¨ (DataProcessor)](#æ ¸å¿ƒæ•°æ®å¤„ç†å™¨-dataprocessor)
- [æ•°æ®é›†ç±» (Dataset Classes)](#æ•°æ®é›†ç±»-dataset-classes)
- [æ•°æ®å¤„ç†æ’ä»¶ (DataProcessorsPlugin)](#æ•°æ®å¤„ç†æ’ä»¶-dataprocessorsplugin)
- [æ–‡ä»¶åŠ è½½ä¸æ‰¹å¤„ç†](#æ–‡ä»¶åŠ è½½ä¸æ‰¹å¤„ç†)
- [å…¬å¼€æ•°æ®é›†ä½¿ç”¨ (HuggingFace Integration)](#å…¬å¼€æ•°æ®é›†ä½¿ç”¨-huggingface-integration)

### ğŸ“ æ‰©å±•åŠŸèƒ½éƒ¨åˆ†
- [æµå¼åŠ è½½è®­ç»ƒæ•°æ®](#æµå¼åŠ è½½è®­ç»ƒæ•°æ®)
- [å›¾åƒè®­ç»ƒæ•°æ®é›†](#å›¾åƒè®­ç»ƒæ•°æ®é›†)
- [é«˜çº§æ•°æ®å¢å¼º](#é«˜çº§æ•°æ®å¢å¼º)

### é€šç”¨çŸ¥è¯†
- [ä¸ºä»€ä¹ˆéœ€è¦æ•°æ®æ¸…æ´—](#ä¸ºä»€ä¹ˆéœ€è¦æ•°æ®æ¸…æ´—)
- [æ•°æ®è´¨é‡æ ‡å‡†](#æ•°æ®è´¨é‡æ ‡å‡†)
- [å®Œæ•´ç¤ºä¾‹](#å®Œæ•´ç¤ºä¾‹)

---

## ğŸ¯ ä¸ºä»€ä¹ˆéœ€è¦æ•°æ®æ¸…æ´—

### ä½è´¨é‡æ•°æ®çš„å±å®³

| é—®é¢˜ç±»å‹ | å½±å“ | ç¤ºä¾‹ |
|---------|------|------|
| **é‡å¤æ•°æ®** | è¿‡æ‹Ÿåˆã€åè§æ”¾å¤§ | åŒä¸€æ–°é—»é‡å¤æŠ“å– 100 æ¬¡ |
| **ä½è´¨é‡æ–‡æœ¬** | æ€§èƒ½ä¸‹é™ã€è¯­æ³•é”™è¯¯ | "asdfjkl ä¹±ç æ–‡å­— ï¼ï¼ï¼" |
| **HTMLæ ‡ç­¾** | å­¦åˆ°æ— ç”¨æ ‡è®° | "&lt;div&gt;&lt;p&gt;æ–‡å­—&lt;/p&gt;&lt;/div&gt;" |
| **ä¸å¹³è¡¡æ•°æ®** | é¢†åŸŸåè§ | 90% æ–°é—»ï¼Œ10% å…¶ä»– |
| **éšç§ä¿¡æ¯** | æ³•å¾‹é£é™© | èº«ä»½è¯å·ã€ç”µè¯å·ç  |

### æ•°æ®æ¸…æ´—å¸¦æ¥çš„æå‡

```
å®éªŒå¯¹æ¯”ï¼ˆ2.7B å‚æ•°æ¨¡å‹ï¼Œ10 epochï¼‰

æœªæ¸…æ´—æ•°æ®ï¼š
â”œâ”€â”€ è®­ç»ƒ Loss: 3.2
â”œâ”€â”€ éªŒè¯ Loss: 3.8 âš ï¸ è¿‡æ‹Ÿåˆ
â””â”€â”€ ç”Ÿæˆè´¨é‡: 2.3/5 âŒ

æ¸…æ´—åæ•°æ®ï¼š
â”œâ”€â”€ è®­ç»ƒ Loss: 2.8
â”œâ”€â”€ éªŒè¯ Loss: 2.9 âœ… æ³›åŒ–è‰¯å¥½
â””â”€â”€ ç”Ÿæˆè´¨é‡: 4.1/5 âœ…
```

**å…³é”®æŒ‡æ ‡æ”¹å–„ï¼š**
- éªŒè¯æŸå¤±é™ä½ **23.7%**
- ç”Ÿæˆè´¨é‡æå‡ **78.3%**
- è®­ç»ƒæ•ˆç‡æå‡ **15-20%**ï¼ˆæ›´å°‘åƒåœ¾æ•°æ®ï¼‰

---

## ğŸ“Š æ•°æ®è´¨é‡æ ‡å‡†

### APT æ¨èæ ‡å‡†

```python
QUALITY_STANDARDS = {
    # é•¿åº¦è¦æ±‚
    'min_length': 50,           # æœ€çŸ­ 50 å­—ç¬¦
    'max_length': 100000,       # æœ€é•¿ 100K å­—ç¬¦
    'optimal_length': 512,      # æœ€ä½³ 512 tokens

    # è¯­è¨€è¦æ±‚
    'min_language_score': 0.8,  # è¯­è¨€è¯†åˆ«ç½®ä¿¡åº¦ > 0.8
    'allowed_languages': ['zh', 'en', 'ja', 'ko'],

    # è´¨é‡è¦æ±‚
    'min_quality_score': 0.6,   # è´¨é‡è¯„åˆ† > 0.6
    'max_special_char_ratio': 0.15,  # ç‰¹æ®Šå­—ç¬¦ < 15%
    'min_word_diversity': 0.3,  # è¯æ±‡å¤šæ ·æ€§ > 0.3

    # å†…å®¹è¦æ±‚
    'max_repetition_ratio': 0.3,  # é‡å¤åº¦ < 30%
    'min_avg_word_length': 2,   # å¹³å‡è¯é•¿ > 2
    'max_line_repetition': 5,   # è¡Œé‡å¤ < 5 æ¬¡
}
```

---

## âœ… æ ¸å¿ƒæ•°æ®å¤„ç†å™¨ (DataProcessor)

### å®é™…å®ç°

**æ–‡ä»¶ä½ç½®**: `apt_model/data/data_processor.py`

`DataProcessor` æ˜¯ APT é¡¹ç›®çš„æ ¸å¿ƒæ•°æ®é¢„å¤„ç†ç±»ï¼Œæä¾›æ–‡æœ¬æ¸…æ´—ã€åˆ†è¯ã€æ•°æ®å¢å¼ºç­‰åŠŸèƒ½ã€‚

#### åŸºç¡€ä½¿ç”¨

```python
from apt_model.data.data_processor import DataProcessor
from transformers import AutoTokenizer

# åˆå§‹åŒ–åˆ†è¯å™¨
tokenizer = AutoTokenizer.from_pretrained("gpt2")

# åˆ›å»ºæ•°æ®å¤„ç†å™¨
processor = DataProcessor(
    tokenizer=tokenizer,
    max_seq_length=512,
    lower_case=True,           # è½¬å°å†™
    remove_accents=True,       # ç§»é™¤é‡éŸ³ç¬¦å·
    clean_text=True,           # å¯ç”¨æ–‡æœ¬æ¸…æ´—
    language='en'              # è¯­è¨€: 'en' æˆ– 'zh'
)

# å¤„ç†å•ä¸ªæ–‡æœ¬
text = "This is   a sample  text with   extra spaces."
cleaned_text = processor.process_text(text)

# æ‰¹é‡å¤„ç†æ–‡æœ¬
texts = ["Text 1", "Text 2", "Text 3"]
cleaned_texts = processor.process_batch(texts, show_progress=True)
```

#### å·²å®ç°çš„æ¸…æ´—åŠŸèƒ½

âœ… **è‡ªåŠ¨æ‰§è¡Œçš„æ¸…æ´—æ“ä½œ** (`_clean_text` æ–¹æ³•):
- åˆå¹¶å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œ
- ç§»é™¤/æ›¿æ¢ URL ä¸º `[URL]`
- ç§»é™¤ HTML æ ‡ç­¾
- å…¨è§’è½¬åŠè§’ (ä¸­æ–‡)
- ç»Ÿä¸€æ ‡ç‚¹ç¬¦å·

```python
# ç¤ºä¾‹
processor = DataProcessor(tokenizer=tokenizer, clean_text=True, language='en')
dirty_text = "Visit  https://example.com   for <b>more</b> info"
clean_text = processor.process_text(dirty_text)
# ç»“æœ: "visit [url] for more info"
```

#### åˆ†è¯ä¸ç¼–ç 

```python
# å•ä¸ªæ–‡æœ¬åˆ†è¯
encoding = processor.tokenize_text("Hello, world!")
# è¿”å›: {'input_ids': tensor([...]), 'attention_mask': tensor([...])}

# æ‰¹é‡åˆ†è¯
texts = ["Text 1", "Text 2", "Text 3"]
batch_encoding = processor.tokenize_batch(texts, return_tensors="pt")

# åˆ›å»º PyTorch æ•°æ®é›†
texts = ["Text 1", "Text 2", "Text 3"]
labels = [0, 1, 0]
dataset = processor.create_dataset(texts, labels)
```

#### è¾…åŠ©å·¥å…·ç±»

**TextCleaner** - æ–‡æœ¬æ¸…æ´—é™æ€æ–¹æ³•:

```python
from apt_model.data.data_processor import TextCleaner

# ç§»é™¤ HTML æ ‡ç­¾
text = TextCleaner.remove_html_tags("<p>Hello</p>")

# ç§»é™¤ URL
text = TextCleaner.remove_urls("Visit http://example.com")

# ç§»é™¤è¡¨æƒ…ç¬¦å·
text = TextCleaner.remove_emoji("Hello ğŸ˜Š World ğŸŒ")

# å®Œæ•´æ¸…æ´—
text = TextCleaner.clean_text_complete(raw_text)
```

**DatasetStatistics** - æ•°æ®é›†ç»Ÿè®¡:

```python
from apt_model.data.data_processor import DatasetStatistics

texts = ["Sample text 1", "Another sample", "Third example"]
labels = [0, 1, 0]

# æ–‡æœ¬é•¿åº¦ç»Ÿè®¡
stats = DatasetStatistics.get_text_length_stats(texts)

# è¯æ±‡ç»Ÿè®¡
vocab_stats = DatasetStatistics.get_vocabulary_stats(texts)

# å®Œæ•´æ‘˜è¦
summary = DatasetStatistics.summarize_dataset(texts, labels)
DatasetStatistics.print_dataset_summary(summary)
```

---

## âœ… æ•°æ®é›†ç±» (Dataset Classes)

### å®é™…å®ç°

**æ–‡ä»¶ä½ç½®**: `apt_model/training/data_loading.py`

é¡¹ç›®æä¾›ä¸‰ç§æ•°æ®é›†ç±»ï¼Œè¦†ç›–ä¸åŒçš„è®­ç»ƒåœºæ™¯ã€‚

### TextDataset - åŸºç¡€æ–‡æœ¬æ•°æ®é›†

ç”¨äºè‡ªå›å½’è¯­è¨€æ¨¡å‹è®­ç»ƒã€‚

```python
from apt_model.training.data_loading import TextDataset
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("gpt2")

texts = ["Sample 1", "Sample 2", "Sample 3"]

dataset = TextDataset(
    texts=texts,
    tokenizer=tokenizer,
    max_length=128,
    return_tensors=True,
    truncation=True,
    preprocessing_fn=lambda x: x.lower()  # å¯é€‰çš„é¢„å¤„ç†å‡½æ•°
)

# è·å–æ ·æœ¬
input_ids, target_ids = dataset[0]
# æ³¨æ„: å¯¹äºè‡ªå›å½’è®­ç»ƒï¼Œinput_ids å’Œ target_ids ç›¸åŒ
```

### PairedTextDataset - é…å¯¹æ–‡æœ¬æ•°æ®é›†

ç”¨äºåºåˆ—åˆ°åºåˆ—è®­ç»ƒ (å¦‚ç¿»è¯‘ã€æ‘˜è¦ã€é—®ç­”)ã€‚

```python
from apt_model.training.data_loading import PairedTextDataset

source_texts = ["Translate this", "What is AI?"]
target_texts = ["Traduisez ceci", "AI is..."]

dataset = PairedTextDataset(
    source_texts=source_texts,
    target_texts=target_texts,
    tokenizer=tokenizer,
    max_source_length=128,
    max_target_length=128
)

source_ids, target_ids = dataset[0]
```

### MultimodalDataset - å¤šæ¨¡æ€æ•°æ®é›†

ç”¨äºæ–‡æœ¬+å›¾åƒ+éŸ³é¢‘çš„å¤šæ¨¡æ€è®­ç»ƒã€‚

```python
from apt_model.training.data_loading import MultimodalDataset

text_data = ["Caption 1", "Caption 2"]
image_paths = ["img1.jpg", "img2.jpg"]
audio_paths = ["audio1.wav", "audio2.wav"]

dataset = MultimodalDataset(
    text_data=text_data,
    image_paths=image_paths,
    audio_paths=audio_paths,
    tokenizer=tokenizer,
    image_processor=image_processor,  # éœ€è¦æä¾›
    audio_processor=audio_processor,  # éœ€è¦æä¾›
    max_text_length=128
)

sample = dataset[0]
# è¿”å›: {'text': ..., 'image': ..., 'audio': ...}
```

### ä»æ–‡ä»¶åŠ è½½æ•°æ®

```python
from apt_model.training.data_loading import (
    load_text_data_from_file,
    load_paired_data_from_file,
    load_multimodal_data_from_directory
)

# åŠ è½½å•æ¨¡æ€æ–‡æœ¬æ•°æ® (æ”¯æŒ .txt, .json, .csv, .jsonl)
texts = load_text_data_from_file("data/train.txt")

# åŠ è½½é…å¯¹æ–‡æœ¬æ•°æ® (æ”¯æŒ .tsv, .csv, .json, .jsonl)
source_texts, target_texts = load_paired_data_from_file("data/paired_data.json")

# åŠ è½½å¤šæ¨¡æ€æ•°æ®
multimodal_data = load_multimodal_data_from_directory(
    directory="data/multimodal",
    image_dir="data/multimodal/images",
    audio_dir="data/multimodal/audio",
    metadata_file="data/multimodal/metadata.json"
)
```

### å‡†å¤‡è®­ç»ƒæ•°æ® (ä¸€ç«™å¼)

```python
from apt_model.training.data_loading import prepare_training_data
from types import SimpleNamespace

config = SimpleNamespace(
    tokenizer_name="gpt2",
    max_seq_len=128,
    enable_image=True,
    enable_audio=False
)

# æ–¹å¼1: å•æ¨¡æ€æ–‡æœ¬
dataloader, processors = prepare_training_data(
    config,
    text_data=texts,
    batch_size=8
)

# æ–¹å¼2: é…å¯¹æ–‡æœ¬
dataloader, processors = prepare_training_data(
    config,
    paired_data=(source_texts, target_texts),
    batch_size=8
)

# æ–¹å¼3: å¤šæ¨¡æ€
dataloader, processors = prepare_training_data(
    config,
    multimodal_data=multimodal_data,
    batch_size=8
)
```

---

## âœ… æ•°æ®å¤„ç†æ’ä»¶ (DataProcessorsPlugin)

### å®é™…å®ç°

**æ–‡ä»¶ä½ç½®**: `legacy_plugins/batch2/plugin_7_data_processors.py`

é«˜çº§æ•°æ®å¤„ç†æ’ä»¶ï¼Œæä¾›æ•°æ®æ¸…æ´—ã€å¢å¼ºã€å¹³è¡¡ã€è´¨é‡æ£€æŸ¥ç­‰åŠŸèƒ½ã€‚

### åˆå§‹åŒ–æ’ä»¶

```python
from legacy_plugins.batch2.plugin_7_data_processors import DataProcessorsPlugin

config = {
    'enable_cleaning': True,
    'enable_augmentation': True,
    'augmentation_ratio': 0.3,
    'normalize_urls': True
}

plugin = DataProcessorsPlugin(config)
```

### æ–‡æœ¬æ¸…æ´—ä¸æ ‡å‡†åŒ–

```python
# æ¸…æ´—å•ä¸ªæ–‡æœ¬
cleaned = plugin.clean_text("This  is   a  sample.")
# ç»“æœ: "This is a sample."

# æ ‡å‡†åŒ–æ–‡æœ¬
normalized = plugin.normalize_text(text, lowercase=True)

# æ‰¹é‡å»é‡
unique_texts = plugin.remove_duplicates(texts)
```

### æ•°æ®å¢å¼º (âœ… åŸºç¡€å®ç°)

**å·²å®ç°çš„å¢å¼ºæ–¹æ³•**:
- `random_swap`: éšæœºäº¤æ¢è¯åº
- `random_insertion`: éšæœºæ’å…¥è¯
- `random_deletion`: éšæœºåˆ é™¤è¯
- `synonym_replacement`: åŒä¹‰è¯æ›¿æ¢ (ç®€åŒ–ç‰ˆï¼Œä½¿ç”¨å†…ç½®å­—å…¸)

```python
# å•æ–‡æœ¬å¢å¼º
augmented = plugin.augment_text(
    "This is a good example",
    methods=['synonym_replacement', 'random_swap']
)

# æ•°æ®é›†å¢å¼º
data = [{'text': 'Sample 1', 'label': 0}]
augmented_data = plugin.augment_dataset(
    data,
    text_key='text',
    augmentation_factor=0.5
)
```

### æ•°æ®å¹³è¡¡

```python
# ä¸å¹³è¡¡æ•°æ®
data = [
    {'text': 'Sample 1', 'label': 0},
    {'text': 'Sample 2', 'label': 0},
    {'text': 'Sample 3', 'label': 1},
]

# è¿‡é‡‡æ · (å¤åˆ¶å°‘æ•°ç±»æ ·æœ¬)
balanced_data = plugin.balance_dataset(
    data,
    label_key='label',
    method='oversample'
)

# æ¬ é‡‡æ · (åˆ é™¤å¤šæ•°ç±»æ ·æœ¬)
balanced_data = plugin.balance_dataset(
    data,
    label_key='label',
    method='undersample'
)
```

### ç‰¹å¾æå–

```python
# æå–æ–‡æœ¬ç‰¹å¾
features = plugin.extract_features(
    "Sample text",
    include_stats=True,
    include_ngrams=True
)
# è¿”å›: length, word_count, avg_word_length, bigrams, trigrams ç­‰

# ä¸ºæ•°æ®é›†æ·»åŠ ç‰¹å¾
enhanced_data = plugin.add_features_to_dataset(data, text_key='text')
```

### æ•°æ®è´¨é‡æ£€æŸ¥

```python
# è´¨é‡æ£€æŸ¥
issues = plugin.check_quality(
    data,
    text_key='text',
    min_length=10,
    max_length=10000
)
# è¿”å›: {'empty': [...], 'too_short': [...], 'duplicates': [...], ...}

# æ ¹æ®è´¨é‡é—®é¢˜è¿‡æ»¤æ•°æ®
filtered_data = plugin.filter_by_quality(
    data,
    issues,
    remove_types=['empty', 'too_short', 'unusual_chars']
)
```

### å®Œæ•´å¤„ç†ç®¡é“

```python
processed_data = plugin.process_pipeline(
    data,
    text_key='text',
    label_key='label',
    steps=[
        'clean',              # æ¸…æ´—æ–‡æœ¬
        'quality_check',      # è´¨é‡æ£€æŸ¥å¹¶è¿‡æ»¤
        'remove_duplicates',  # å»é‡
        'augment',            # æ•°æ®å¢å¼º
        'balance'             # æ•°æ®å¹³è¡¡
    ]
)

# æŸ¥çœ‹ç»Ÿè®¡ä¿¡æ¯
stats = plugin.get_statistics()
```

---

## âœ… æ–‡ä»¶åŠ è½½ä¸æ‰¹å¤„ç†

### å®é™…å®ç°

#### åˆ›å»º DataLoader

```python
from apt_model.training.data_loading import prepare_dataloader

dataloader = prepare_dataloader(
    dataset=dataset,
    batch_size=16,
    shuffle=True,
    collate_fn=text_collate_fn,
    num_workers=4
)

for batch in dataloader:
    # è®­ç»ƒä»£ç 
    pass
```

#### æ‰¹å¤„ç†æ•´ç†å‡½æ•°

```python
from apt_model.training.data_loading import (
    text_collate_fn,
    multimodal_collate_fn
)

# ä½¿ç”¨ text_collate_fn
dataloader = DataLoader(
    dataset,
    batch_size=4,
    collate_fn=lambda batch: text_collate_fn(batch, pad_token_id=0)
)

# è¿”å›æ ¼å¼: {'src_ids', 'src_mask', 'tgt_ids', 'tgt_mask'}
```

---

## ğŸ”„ æ¸…æ´—æµç¨‹

### ğŸ“ æ‰©å±•ç¤ºä¾‹ - å®Œæ•´æ¸…æ´—æµç¨‹ç±»

### å®Œæ•´æµç¨‹å›¾

```
åŸå§‹æ•°æ® (Raw Data)
    â†“
[1] åŸºç¡€æ¸…æ´— (Basic Preprocessing)
    â”œâ”€â”€ å»é™¤ HTML æ ‡ç­¾
    â”œâ”€â”€ å»é™¤ URL é“¾æ¥
    â”œâ”€â”€ ç»Ÿä¸€æ¢è¡Œç¬¦
    â””â”€â”€ ä¿®å¤ç¼–ç é—®é¢˜
    â†“
[2] å»é‡ (Deduplication)
    â”œâ”€â”€ ç²¾ç¡®å»é‡ï¼ˆMD5ï¼‰
    â”œâ”€â”€ è¿‘ä¼¼å»é‡ï¼ˆMinHash LSHï¼‰
    â””â”€â”€ æ®µè½çº§å»é‡
    â†“
[3] è´¨é‡è¿‡æ»¤ (Quality Filtering)
    â”œâ”€â”€ é•¿åº¦è¿‡æ»¤
    â”œâ”€â”€ è¯­è¨€æ£€æµ‹
    â”œâ”€â”€ è´¨é‡è¯„åˆ†
    â””â”€â”€ å†…å®¹å®‰å…¨æ£€æµ‹
    â†“
[4] æ ¼å¼è§„èŒƒåŒ– (Normalization)
    â”œâ”€â”€ æ ‡ç‚¹ç»Ÿä¸€
    â”œâ”€â”€ ç©ºç™½è§„èŒƒ
    â”œâ”€â”€ å¤§å°å†™è§„èŒƒ
    â””â”€â”€ ç‰¹æ®Šå­—ç¬¦å¤„ç†
    â†“
[5] é«˜çº§å¤„ç† (Advanced Processing)
    â”œâ”€â”€ åˆ†è¯å’Œæ ‡è®°åŒ–
    â”œâ”€â”€ é¢†åŸŸåˆ†ç±»
    â”œâ”€â”€ éš¾åº¦è¯„ä¼°
    â””â”€â”€ æ•°æ®å¹³è¡¡
    â†“
é«˜è´¨é‡è®­ç»ƒæ•°æ® (Clean Training Data)
```

### å®Œæ•´ä»£ç å®ç°

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
APT æ•°æ®æ¸…ç†å®Œæ•´æµç¨‹
"""

import re
import hashlib
from typing import List, Dict, Set
from collections import Counter
import unicodedata


class APTDataCleaner:
    """APT æ•°æ®æ¸…æ´—å™¨"""

    def __init__(self, standards: dict = None):
        self.standards = standards or QUALITY_STANDARDS
        self.seen_hashes: Set[str] = set()  # ç²¾ç¡®å»é‡
        self.minhash_lsh = None  # è¿‘ä¼¼å»é‡ï¼ˆéœ€è¦ datasketch åº“ï¼‰

    def clean_pipeline(self, texts: List[str]) -> List[Dict]:
        """
        å®Œæ•´æ¸…ç†æµç¨‹

        Args:
            texts: åŸå§‹æ–‡æœ¬åˆ—è¡¨

        Returns:
            æ¸…ç†åçš„æ•°æ®ï¼ŒåŒ…å«æ–‡æœ¬å’Œå…ƒæ•°æ®
        """
        print(f"ğŸ“¥ è¾“å…¥æ•°æ®: {len(texts):,} æ¡")

        # [1] åŸºç¡€æ¸…æ´—
        print("\n[1/5] åŸºç¡€æ¸…æ´—...")
        texts = [self.basic_clean(t) for t in texts]
        texts = [t for t in texts if t]  # ç§»é™¤ç©ºæ–‡æœ¬
        print(f"   âœ“ å‰©ä½™: {len(texts):,} æ¡")

        # [2] å»é‡
        print("\n[2/5] å»é‡...")
        texts = self.deduplicate(texts)
        print(f"   âœ“ å‰©ä½™: {len(texts):,} æ¡")

        # [3] è´¨é‡è¿‡æ»¤
        print("\n[3/5] è´¨é‡è¿‡æ»¤...")
        texts_with_scores = [
            {'text': t, 'quality_score': self.quality_score(t)}
            for t in texts
        ]
        texts_with_scores = [
            item for item in texts_with_scores
            if item['quality_score'] >= self.standards['min_quality_score']
        ]
        print(f"   âœ“ å‰©ä½™: {len(texts_with_scores):,} æ¡")

        # [4] æ ¼å¼è§„èŒƒåŒ–
        print("\n[4/5] æ ¼å¼è§„èŒƒåŒ–...")
        for item in texts_with_scores:
            item['text'] = self.normalize(item['text'])

        # [5] é«˜çº§å¤„ç†
        print("\n[5/5] é«˜çº§å¤„ç†ï¼ˆåˆ†ç±»ã€éš¾åº¦è¯„ä¼°ï¼‰...")
        for item in texts_with_scores:
            item['domain'] = self.classify_domain(item['text'])
            item['difficulty'] = self.estimate_difficulty(item['text'])
            item['length'] = len(item['text'])

        print(f"\nâœ… æ¸…ç†å®Œæˆ: {len(texts_with_scores):,} æ¡é«˜è´¨é‡æ•°æ®")
        return texts_with_scores

    # ========== [1] åŸºç¡€æ¸…æ´— ==========

    def basic_clean(self, text: str) -> str:
        """åŸºç¡€æ¸…æ´—"""
        if not text or not isinstance(text, str):
            return ""

        # å»é™¤ HTML æ ‡ç­¾
        text = re.sub(r'<[^>]+>', '', text)

        # å»é™¤ URL
        text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)

        # ç»Ÿä¸€æ¢è¡Œç¬¦
        text = text.replace('\r\n', '\n').replace('\r', '\n')

        # å»é™¤æ§åˆ¶å­—ç¬¦ï¼ˆä¿ç•™æ¢è¡Œå’Œåˆ¶è¡¨ç¬¦ï¼‰
        text = ''.join(ch for ch in text if unicodedata.category(ch)[0] != 'C' or ch in '\n\t')

        # ä¿®å¤å¤šä½™ç©ºç™½
        text = re.sub(r'[ \t]+', ' ', text)  # å¤šä¸ªç©ºæ ¼/åˆ¶è¡¨ç¬¦ â†’ å•ç©ºæ ¼
        text = re.sub(r'\n{3,}', '\n\n', text)  # å¤šä¸ªæ¢è¡Œ â†’ åŒæ¢è¡Œ

        return text.strip()

    # ========== [2] å»é‡ ==========

    def deduplicate(self, texts: List[str]) -> List[str]:
        """ç²¾ç¡®å»é‡ + è¿‘ä¼¼å»é‡"""
        unique_texts = []

        for text in texts:
            # ç²¾ç¡®å»é‡ï¼ˆMD5ï¼‰
            text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()

            if text_hash not in self.seen_hashes:
                self.seen_hashes.add(text_hash)
                unique_texts.append(text)

        # TODO: è¿‘ä¼¼å»é‡ï¼ˆMinHash LSHï¼‰
        # éœ€è¦ datasketch åº“ï¼Œå¯æ£€æµ‹ 80%+ ç›¸ä¼¼çš„æ–‡æœ¬

        return unique_texts

    # ========== [3] è´¨é‡è¿‡æ»¤ ==========

    def quality_score(self, text: str) -> float:
        """
        è´¨é‡è¯„åˆ†ï¼ˆ0-1ï¼Œè¶Šé«˜è¶Šå¥½ï¼‰

        è€ƒè™‘å› ç´ ï¼š
        - é•¿åº¦åˆç†æ€§
        - ç‰¹æ®Šå­—ç¬¦æ¯”ä¾‹
        - è¯æ±‡å¤šæ ·æ€§
        - é‡å¤åº¦
        - å¹³å‡è¯é•¿
        """
        if not text:
            return 0.0

        scores = []

        # 1. é•¿åº¦å¾—åˆ†
        length = len(text)
        if length < self.standards['min_length']:
            return 0.0  # å¤ªçŸ­ç›´æ¥æ·˜æ±°
        elif length > self.standards['max_length']:
            return 0.0  # å¤ªé•¿ç›´æ¥æ·˜æ±°
        else:
            # æœ€ä¼˜é•¿åº¦ 512ï¼Œåç¦»è¶Šå¤šåˆ†æ•°è¶Šä½
            optimal = self.standards['optimal_length']
            length_score = 1.0 - min(abs(length - optimal) / optimal, 1.0)
            scores.append(length_score)

        # 2. ç‰¹æ®Šå­—ç¬¦æ¯”ä¾‹å¾—åˆ†
        special_chars = sum(1 for ch in text if not ch.isalnum() and ch not in ' \n\t.,!?;:ï¼Œã€‚ï¼ï¼Ÿï¼›ï¼š')
        special_ratio = special_chars / len(text)
        special_score = 1.0 - min(special_ratio / self.standards['max_special_char_ratio'], 1.0)
        scores.append(special_score)

        # 3. è¯æ±‡å¤šæ ·æ€§å¾—åˆ†
        words = text.split()
        if len(words) > 0:
            unique_words = len(set(words))
            diversity = unique_words / len(words)
            diversity_score = min(diversity / self.standards['min_word_diversity'], 1.0)
            scores.append(diversity_score)

        # 4. é‡å¤åº¦å¾—åˆ†ï¼ˆæ£€æµ‹è¿ç»­é‡å¤ï¼‰
        repetition_score = 1.0 - self.detect_repetition(text)
        scores.append(repetition_score)

        # 5. å¹³å‡è¯é•¿å¾—åˆ†
        if len(words) > 0:
            avg_word_len = sum(len(w) for w in words) / len(words)
            word_len_score = min(avg_word_len / self.standards['min_avg_word_length'], 1.0)
            scores.append(word_len_score)

        # ç»¼åˆå¾—åˆ†ï¼ˆåŠ æƒå¹³å‡ï¼‰
        return sum(scores) / len(scores)

    def detect_repetition(self, text: str) -> float:
        """
        æ£€æµ‹æ–‡æœ¬é‡å¤åº¦ï¼ˆ0-1ï¼Œè¶Šé«˜è¶Šé‡å¤ï¼‰

        æ–¹æ³•ï¼š
        1. è¡Œçº§é‡å¤æ£€æµ‹
        2. N-gram é‡å¤æ£€æµ‹
        """
        lines = text.split('\n')
        line_counts = Counter(lines)

        # è¡Œé‡å¤åº¦
        max_line_repeat = max(line_counts.values()) if line_counts else 1
        line_repeat_ratio = min(max_line_repeat / self.standards['max_line_repetition'], 1.0)

        # N-gram é‡å¤åº¦ï¼ˆ3-gramï¼‰
        words = text.split()
        if len(words) < 3:
            return line_repeat_ratio

        trigrams = [tuple(words[i:i+3]) for i in range(len(words)-2)]
        trigram_counts = Counter(trigrams)
        max_trigram_repeat = max(trigram_counts.values()) if trigram_counts else 1

        trigram_repeat_ratio = max_trigram_repeat / len(trigrams) if len(trigrams) > 0 else 0

        # ç»¼åˆé‡å¤åº¦
        return (line_repeat_ratio + trigram_repeat_ratio) / 2

    # ========== [4] æ ¼å¼è§„èŒƒåŒ– ==========

    def normalize(self, text: str) -> str:
        """æ ¼å¼è§„èŒƒåŒ–"""
        # 1. ç»Ÿä¸€æ ‡ç‚¹ç¬¦å·ï¼ˆä¸­è‹±æ–‡ï¼‰
        text = text.replace('ï¼Œ', ', ')
        text = text.replace('ã€‚', '. ')
        text = text.replace('ï¼', '! ')
        text = text.replace('ï¼Ÿ', '? ')
        text = text.replace('ï¼›', '; ')
        text = text.replace('ï¼š', ': ')

        # 2. ç»Ÿä¸€å¼•å·
        text = text.replace('"', '"').replace('"', '"')
        text = text.replace(''', "'").replace(''', "'")

        # 3. ä¿®å¤ç©ºç™½
        text = re.sub(r' +', ' ', text)
        text = re.sub(r'\n +', '\n', text)
        text = re.sub(r' +\n', '\n', text)

        # 4. é¦–å­—æ¯å¤§å†™ï¼ˆè‹±æ–‡å¥å­ï¼‰
        sentences = text.split('. ')
        sentences = [s.strip().capitalize() if s else s for s in sentences]
        text = '. '.join(sentences)

        return text.strip()

    # ========== [5] é«˜çº§å¤„ç† ==========

    def classify_domain(self, text: str) -> str:
        """
        é¢†åŸŸåˆ†ç±»ï¼ˆç®€å•å…³é”®è¯åŒ¹é…ï¼‰

        ç”Ÿäº§ç¯å¢ƒå»ºè®®ä½¿ç”¨ï¼š
        - BERT æ–‡æœ¬åˆ†ç±»æ¨¡å‹
        - fastText åˆ†ç±»å™¨
        """
        text_lower = text.lower()

        # ä»£ç é¢†åŸŸ
        code_keywords = ['def ', 'class ', 'import ', 'function', 'var ', 'const ', '#!/usr/bin']
        if any(kw in text_lower for kw in code_keywords):
            return 'code'

        # æ•°å­¦é¢†åŸŸ
        math_keywords = ['theorem', 'å®šç†', 'proof', 'è¯æ˜', 'âˆ‘', 'âˆ«', 'equation', 'æ–¹ç¨‹']
        if any(kw in text_lower for kw in math_keywords):
            return 'math'

        # æ–°é—»é¢†åŸŸ
        news_keywords = ['æŠ¥é“', 'è®°è€…', 'æ¶ˆæ¯', 'according to', 'reported', 'breaking']
        if any(kw in text_lower for kw in news_keywords):
            return 'news'

        # å­¦æœ¯é¢†åŸŸ
        academic_keywords = ['abstract', 'introduction', 'methodology', 'conclusion', 'æ‘˜è¦', 'ç ”ç©¶']
        if any(kw in text_lower for kw in academic_keywords):
            return 'academic'

        return 'general'

    def estimate_difficulty(self, text: str) -> str:
        """
        éš¾åº¦è¯„ä¼°ï¼ˆç®€å•ã€ä¸­ç­‰ã€å›°éš¾ï¼‰

        æŒ‡æ ‡ï¼š
        - è¯æ±‡å¤æ‚åº¦
        - å¥å­é•¿åº¦
        - ä¸“ä¸šæœ¯è¯­å¯†åº¦
        """
        words = text.split()
        sentences = text.split('.')

        if not words or not sentences:
            return 'easy'

        # å¹³å‡è¯é•¿
        avg_word_len = sum(len(w) for w in words) / len(words)

        # å¹³å‡å¥é•¿
        avg_sentence_len = len(words) / len(sentences)

        # å¤æ‚åº¦è¯„åˆ†
        complexity_score = (avg_word_len - 4) * 0.3 + (avg_sentence_len - 15) * 0.7

        if complexity_score < 0:
            return 'easy'
        elif complexity_score < 5:
            return 'medium'
        else:
            return 'hard'
```

---

## ğŸ” å»é‡ç­–ç•¥

### 1. ç²¾ç¡®å»é‡ï¼ˆExact Deduplicationï¼‰

**æ–¹æ³•ï¼š** MD5/SHA256 å“ˆå¸Œ

```python
import hashlib

def exact_dedup(texts: List[str]) -> List[str]:
    """ç²¾ç¡®å»é‡"""
    seen = set()
    unique_texts = []

    for text in texts:
        # è®¡ç®— MD5 å“ˆå¸Œ
        text_hash = hashlib.md5(text.encode('utf-8')).hexdigest()

        if text_hash not in seen:
            seen.add(text_hash)
            unique_texts.append(text)

    return unique_texts
```

**é€‚ç”¨åœºæ™¯ï¼š** å®Œå…¨ç›¸åŒçš„é‡å¤æ–‡æœ¬

---

### 2. è¿‘ä¼¼å»é‡ï¼ˆFuzzy Deduplicationï¼‰

**æ–¹æ³•ï¼š** MinHash + LSHï¼ˆå±€éƒ¨æ•æ„Ÿå“ˆå¸Œï¼‰

```python
from datasketch import MinHash, MinHashLSH

def fuzzy_dedup(texts: List[str], threshold=0.8) -> List[str]:
    """
    è¿‘ä¼¼å»é‡ï¼ˆæ£€æµ‹ 80%+ ç›¸ä¼¼åº¦ï¼‰

    åŸç†ï¼š
    1. MinHashï¼šå°†æ–‡æœ¬æ˜ å°„åˆ°å›ºå®šé•¿åº¦ç­¾å
    2. LSHï¼šå¿«é€ŸæŸ¥æ‰¾ç›¸ä¼¼ç­¾å
    """
    # åˆ›å»º LSH ç´¢å¼•
    lsh = MinHashLSH(threshold=threshold, num_perm=128)
    minhashes = {}

    # ä¸ºæ¯ä¸ªæ–‡æœ¬ç”Ÿæˆ MinHash
    for idx, text in enumerate(texts):
        m = MinHash(num_perm=128)
        # ä½¿ç”¨ 3-gram
        for i in range(len(text) - 2):
            m.update(text[i:i+3].encode('utf-8'))

        # æŸ¥è¯¢æ˜¯å¦å­˜åœ¨ç›¸ä¼¼æ–‡æœ¬
        result = lsh.query(m)
        if not result:  # æ²¡æœ‰ç›¸ä¼¼æ–‡æœ¬
            lsh.insert(f"text_{idx}", m)
            minhashes[f"text_{idx}"] = text

    return list(minhashes.values())
```

**æ€§èƒ½ï¼š**
- æ—¶é—´å¤æ‚åº¦ï¼šO(n)ï¼ˆçº¿æ€§ï¼‰
- ç©ºé—´å¤æ‚åº¦ï¼šO(n)
- å¯å¤„ç† **ç™¾ä¸‡çº§** æ•°æ®

**DeepSeek-V3 ä½¿ç”¨ç­–ç•¥ï¼š** å¯¹ 14.8T tokens è¿›è¡Œ MinHash LSH å»é‡

---

### 3. æ®µè½çº§å»é‡

**æ–¹æ³•ï¼š** æ£€æµ‹æ–‡æ¡£å†…éƒ¨é‡å¤æ®µè½

```python
def paragraph_dedup(text: str) -> str:
    """
    æ®µè½çº§å»é‡

    ç”¨é€”ï¼š
    - å»é™¤ç½‘é¡µæ¨¡æ¿ï¼ˆé¡µçœ‰ã€é¡µè„šï¼‰
    - å»é™¤é‡å¤çš„å…è´£å£°æ˜
    - å»é™¤çˆ¬è™«é‡å¤æŠ“å–çš„ç‰‡æ®µ
    """
    paragraphs = text.split('\n\n')
    seen = set()
    unique_paragraphs = []

    for para in paragraphs:
        para_hash = hashlib.md5(para.encode('utf-8')).hexdigest()
        if para_hash not in seen and len(para.strip()) > 20:
            seen.add(para_hash)
            unique_paragraphs.append(para)

    return '\n\n'.join(unique_paragraphs)
```

---

## ğŸ¯ è´¨é‡è¿‡æ»¤

### å¯å‘å¼è§„åˆ™

```python
class QualityFilter:
    """è´¨é‡è¿‡æ»¤å™¨"""

    @staticmethod
    def filter_by_length(text: str, min_len=50, max_len=100000) -> bool:
        """é•¿åº¦è¿‡æ»¤"""
        return min_len <= len(text) <= max_len

    @staticmethod
    def filter_by_language(text: str, target_lang='zh') -> bool:
        """
        è¯­è¨€æ£€æµ‹

        å¯é€‰åº“ï¼š
        - langdetect
        - fastText è¯­è¨€è¯†åˆ«æ¨¡å‹
        """
        try:
            from langdetect import detect
            detected_lang = detect(text)
            return detected_lang == target_lang
        except:
            return True  # æ£€æµ‹å¤±è´¥åˆ™ä¿ç•™

    @staticmethod
    def filter_by_offensive_content(text: str, blacklist: List[str]) -> bool:
        """
        è¿‡æ»¤è¿è§„å†…å®¹

        ç”Ÿäº§ç¯å¢ƒå»ºè®®ï¼š
        - ä½¿ç”¨ Perspective APIï¼ˆGoogleï¼‰
        - è®­ç»ƒè‡ªå®šä¹‰åˆ†ç±»æ¨¡å‹
        """
        text_lower = text.lower()
        return not any(word in text_lower for word in blacklist)

    @staticmethod
    def filter_by_privacy(text: str) -> bool:
        """
        éšç§ä¿¡æ¯è¿‡æ»¤

        æ£€æµ‹ï¼š
        - èº«ä»½è¯å·
        - ç”µè¯å·ç 
        - é‚®ç®±åœ°å€
        - é“¶è¡Œå¡å·
        """
        # èº«ä»½è¯å·ï¼ˆ18ä½ï¼‰
        if re.search(r'\b\d{17}[\dXx]\b', text):
            return False

        # ç”µè¯å·ç ï¼ˆä¸­å›½æ‰‹æœºå·ï¼‰
        if re.search(r'\b1[3-9]\d{9}\b', text):
            return False

        # é‚®ç®±åœ°å€
        if re.search(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', text):
            return False

        return True
```

---

## ğŸ”¤ åˆ†è¯ä¸æ ‡è®°åŒ–

### ä¸­æ–‡åˆ†è¯

```python
import jieba

def tokenize_chinese(text: str) -> List[str]:
    """
    ä¸­æ–‡åˆ†è¯

    å·¥å…·é€‰æ‹©ï¼š
    - jiebaï¼šé€šç”¨åˆ†è¯ï¼ˆå¿«é€Ÿï¼‰
    - pkusegï¼šé«˜ç²¾åº¦åˆ†è¯
    - LTPï¼šå·¥ä¸šçº§åˆ†è¯
    """
    # åŸºç¡€åˆ†è¯
    words = jieba.lcut(text)

    # è¿‡æ»¤åœç”¨è¯
    stopwords = set(['çš„', 'äº†', 'æ˜¯', 'åœ¨', 'æˆ‘', 'æœ‰', 'å’Œ', 'å°±', 'ä¸', 'äºº'])
    words = [w for w in words if w not in stopwords and len(w) > 1]

    return words
```

### BPE å­è¯åˆ†è¯

```python
from transformers import GPT2Tokenizer

def tokenize_bpe(text: str) -> List[int]:
    """
    BPEï¼ˆByte Pair Encodingï¼‰åˆ†è¯

    ä¼˜åŠ¿ï¼š
    - å¤„ç†æœªç™»å½•è¯ï¼ˆOOVï¼‰
    - é€‚åˆå¤šè¯­è¨€
    - è¯æ±‡è¡¨å¤§å°å¯æ§
    """
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

    # åˆ†è¯å¹¶è½¬æ¢ä¸º token IDs
    token_ids = tokenizer.encode(text)

    return token_ids
```

---

## âš–ï¸ æ•°æ®å¹³è¡¡

### é¢†åŸŸå¹³è¡¡

```python
def balance_domains(data: List[Dict], target_ratios: Dict[str, float]) -> List[Dict]:
    """
    é¢†åŸŸæ•°æ®å¹³è¡¡é‡‡æ ·

    Args:
        data: æ•°æ®åˆ—è¡¨ï¼ˆæ¯é¡¹åŒ…å« 'domain' å­—æ®µï¼‰
        target_ratios: ç›®æ ‡æ¯”ä¾‹ï¼Œå¦‚ {'code': 0.2, 'math': 0.1, 'general': 0.7}

    Returns:
        å¹³è¡¡åçš„æ•°æ®
    """
    # æŒ‰é¢†åŸŸåˆ†ç»„
    domain_data = {}
    for item in data:
        domain = item.get('domain', 'general')
        if domain not in domain_data:
            domain_data[domain] = []
        domain_data[domain].append(item)

    # è®¡ç®—ç›®æ ‡æ ·æœ¬æ•°
    total_target = min(
        len(domain_data[domain]) / target_ratios.get(domain, 0.01)
        for domain in domain_data
        if target_ratios.get(domain, 0) > 0
    )

    # é‡‡æ ·
    balanced_data = []
    for domain, ratio in target_ratios.items():
        if domain not in domain_data:
            continue

        target_count = int(total_target * ratio)
        domain_samples = domain_data[domain]

        # å¦‚æœæ ·æœ¬ä¸è¶³ï¼Œé‡å¤é‡‡æ ·
        if len(domain_samples) < target_count:
            import random
            sampled = random.choices(domain_samples, k=target_count)
        else:
            import random
            sampled = random.sample(domain_samples, target_count)

        balanced_data.extend(sampled)

    return balanced_data
```

### éš¾åº¦å¹³è¡¡

```python
def balance_difficulty(data: List[Dict]) -> List[Dict]:
    """
    éš¾åº¦å¹³è¡¡ï¼šç®€å•ã€ä¸­ç­‰ã€å›°éš¾ = 3:5:2
    """
    target_ratios = {
        'easy': 0.3,
        'medium': 0.5,
        'hard': 0.2
    }

    # æŒ‰éš¾åº¦åˆ†ç»„
    difficulty_data = {'easy': [], 'medium': [], 'hard': []}
    for item in data:
        difficulty = item.get('difficulty', 'medium')
        difficulty_data[difficulty].append(item)

    # é‡‡æ ·ï¼ˆä¸é¢†åŸŸå¹³è¡¡ç±»ä¼¼ï¼‰
    # ... ä»£ç çœç•¥ ...

    return balanced_data
```

---

## ğŸ“ æµå¼åŠ è½½è®­ç»ƒæ•°æ®

### æ‰©å±•åŠŸèƒ½ (éœ€è¦é¢å¤–å®ç°)

### ä¸ºä»€ä¹ˆéœ€è¦æµå¼åŠ è½½ï¼Ÿ

**é—®é¢˜ï¼š** å¤§è§„æ¨¡æ•°æ®é›†ï¼ˆGB/TBçº§åˆ«ï¼‰æ— æ³•ä¸€æ¬¡æ€§åŠ è½½åˆ°å†…å­˜

```python
# âŒ é”™è¯¯åšæ³•ï¼šå…¨éƒ¨åŠ è½½åˆ°å†…å­˜
texts = open('100GB_data.txt').read().split('\n')  # OOM å†…å­˜æº¢å‡ºï¼

# âœ… æ­£ç¡®åšæ³•ï¼šæµå¼åŠ è½½
for line in open('100GB_data.txt'):
    process(line)  # é€è¡Œå¤„ç†ï¼Œå†…å­˜å ç”¨æ’å®š
```

### PyTorch æµå¼æ•°æ®åŠ è½½å™¨

```python
import torch
from torch.utils.data import IterableDataset, DataLoader
from transformers import GPT2Tokenizer
import json

class StreamingTextDataset(IterableDataset):
    """
    æµå¼æ–‡æœ¬æ•°æ®é›†ï¼ˆæ”¯æŒè¶…å¤§æ–‡ä»¶ï¼‰

    ç‰¹æ€§ï¼š
    - é€è¡Œè¯»å–ï¼Œå†…å­˜å ç”¨æ’å®š
    - æ”¯æŒå¤š worker å¹¶è¡ŒåŠ è½½
    - æ”¯æŒ shuffleï¼ˆåŸºäºè¡Œçº§ç¼“å†²ï¼‰
    """
    def __init__(
        self,
        file_path: str,
        tokenizer,
        max_length: int = 512,
        buffer_size: int = 10000,
        shuffle: bool = True
    ):
        self.file_path = file_path
        self.tokenizer = tokenizer
        self.max_length = max_length
        self.buffer_size = buffer_size
        self.shuffle = shuffle

    def __iter__(self):
        # è·å– worker ä¿¡æ¯ï¼ˆå¤šè¿›ç¨‹åŠ è½½ï¼‰
        worker_info = torch.utils.data.get_worker_info()

        if worker_info is None:
            # å•è¿›ç¨‹æ¨¡å¼
            return self._read_file()
        else:
            # å¤šè¿›ç¨‹æ¨¡å¼ï¼šæ¯ä¸ª worker è¯»å–ä¸åŒéƒ¨åˆ†
            worker_id = worker_info.id
            num_workers = worker_info.num_workers
            return self._read_file_shard(worker_id, num_workers)

    def _read_file(self):
        """å•è¿›ç¨‹è¯»å–æ–‡ä»¶"""
        buffer = []

        with open(self.file_path, 'r', encoding='utf-8') as f:
            for line in f:
                line = line.strip()
                if not line:
                    continue

                buffer.append(line)

                # ç¼“å†²åŒºæ»¡äº†ï¼Œshuffle å yield
                if len(buffer) >= self.buffer_size:
                    if self.shuffle:
                        import random
                        random.shuffle(buffer)

                    for text in buffer:
                        yield self._process_text(text)

                    buffer = []

            # å¤„ç†å‰©ä½™æ•°æ®
            if buffer:
                if self.shuffle:
                    import random
                    random.shuffle(buffer)
                for text in buffer:
                    yield self._process_text(text)

    def _read_file_shard(self, worker_id, num_workers):
        """å¤šè¿›ç¨‹è¯»å–æ–‡ä»¶ï¼ˆæ¯ä¸ª worker è¯»å–ä¸åŒè¡Œï¼‰"""
        with open(self.file_path, 'r', encoding='utf-8') as f:
            for idx, line in enumerate(f):
                # åˆ†ç‰‡ï¼šworker_id å¤„ç† idx % num_workers == worker_id çš„è¡Œ
                if idx % num_workers != worker_id:
                    continue

                line = line.strip()
                if not line:
                    continue

                yield self._process_text(line)

    def _process_text(self, text):
        """æ–‡æœ¬é¢„å¤„ç†å’Œåˆ†è¯"""
        # åˆ†è¯
        tokens = self.tokenizer.encode(
            text,
            max_length=self.max_length,
            truncation=True,
            padding='max_length',
            return_tensors='pt'
        )

        # è¿”å›è¾“å…¥å’Œæ ‡ç­¾ï¼ˆè‡ªå›å½’è®­ç»ƒï¼‰
        return {
            'input_ids': tokens[0],
            'labels': tokens[0].clone()
        }


# ========== ä½¿ç”¨ç¤ºä¾‹ ==========

# 1. åˆå§‹åŒ– tokenizer
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# 2. åˆ›å»ºæµå¼æ•°æ®é›†
train_dataset = StreamingTextDataset(
    file_path='large_train_data.txt',  # 100GB+ æ–‡ä»¶
    tokenizer=tokenizer,
    max_length=512,
    buffer_size=10000,
    shuffle=True
)

# 3. åˆ›å»º DataLoaderï¼ˆå¤šè¿›ç¨‹åŠ è½½ï¼‰
train_loader = DataLoader(
    train_dataset,
    batch_size=16,
    num_workers=4,  # 4 ä¸ªè¿›ç¨‹å¹¶è¡ŒåŠ è½½
    pin_memory=True  # åŠ é€Ÿ GPU ä¼ è¾“
)

# 4. è®­ç»ƒå¾ªç¯ï¼ˆå†…å­˜å ç”¨æ’å®šï¼‰
for epoch in range(num_epochs):
    for batch in train_loader:
        input_ids = batch['input_ids'].to(device)
        labels = batch['labels'].to(device)

        # è®­ç»ƒæ­¥éª¤
        loss = model(input_ids, labels=labels).loss
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()
```

### HuggingFace Datasets æµå¼åŠ è½½

```python
from datasets import load_dataset

# ========== æ–¹æ³• 1: æµå¼åŠ è½½æœ¬åœ°æ–‡ä»¶ ==========
dataset = load_dataset(
    'text',
    data_files='large_train_data.txt',
    streaming=True  # å¯ç”¨æµå¼åŠ è½½
)

# è¿­ä»£æ•°æ®ï¼ˆä¸ä¼šå…¨éƒ¨åŠ è½½åˆ°å†…å­˜ï¼‰
for example in dataset['train']:
    text = example['text']
    # å¤„ç†æ–‡æœ¬...

# ========== æ–¹æ³• 2: æµå¼åŠ è½½è¿œç¨‹æ•°æ®é›† ==========
dataset = load_dataset(
    'wikitext',
    'wikitext-103-raw-v1',
    streaming=True
)

# æµå¼å¤„ç†
for example in dataset['train']:
    process(example)

# ========== æ–¹æ³• 3: æµå¼ + æ´—ç‰Œ + æ‰¹å¤„ç† ==========
from torch.utils.data import DataLoader

dataset = load_dataset('text', data_files='data.txt', streaming=True)['train']

# Shuffleï¼ˆä½¿ç”¨ç¼“å†²åŒºï¼‰
dataset = dataset.shuffle(buffer_size=10000, seed=42)

# æ˜ å°„ï¼ˆåˆ†è¯ï¼‰
def tokenize_function(examples):
    return tokenizer(examples['text'], truncation=True, max_length=512)

dataset = dataset.map(tokenize_function, batched=True)

# è½¬æ¢ä¸º PyTorch æ ¼å¼
dataset = dataset.with_format('torch')

# åˆ›å»º DataLoader
loader = DataLoader(dataset, batch_size=16)
```

### æ€§èƒ½å¯¹æ¯”

| æ–¹æ³• | å†…å­˜å ç”¨ | åŠ è½½é€Ÿåº¦ | é€‚ç”¨åœºæ™¯ |
|------|---------|---------|---------|
| **å…¨éƒ¨åŠ è½½** | O(æ•°æ®é›†å¤§å°) | å¿«ï¼ˆä¸€æ¬¡æ€§ï¼‰ | å°æ•°æ®é›†ï¼ˆ< 10GBï¼‰ |
| **æµå¼åŠ è½½** | O(batch_size) | ä¸­ç­‰ | å¤§æ•°æ®é›†ï¼ˆ10GB - 1TBï¼‰ |
| **æµå¼ + å¤šworker** | O(batch_size Ã— workers) | å¿« | è¶…å¤§æ•°æ®é›†ï¼ˆ1TB+ï¼‰ |

---

## âœ… å…¬å¼€æ•°æ®é›†ä½¿ç”¨ (HuggingFace Integration)

### å®é™…å®ç°

**æ–‡ä»¶ä½ç½®**: `legacy_plugins/batch1/huggingface_integration_plugin.py`

APTé¡¹ç›®å·²ç»å®ç°äº†å®Œæ•´çš„HuggingFaceé›†æˆæ’ä»¶ï¼Œæä¾›ï¼š
- åŠ è½½HuggingFaceæ•°æ®é›†
- å¯¼å…¥/å¯¼å‡ºæ¨¡å‹åˆ°HuggingFace Hub
- ä½¿ç”¨HF Trainerè®­ç»ƒæ¨¡å‹
- æ•°æ®æ ¼å¼è½¬æ¢

#### ä½¿ç”¨HuggingFace Integration Plugin

```python
from legacy_plugins.batch1.huggingface_integration_plugin import HuggingFaceIntegrationPlugin

# åˆå§‹åŒ–æ’ä»¶
config = {
    'auto_upload': False,
    'repo_name': 'username/my-model',
    'private': False
}

plugin = HuggingFaceIntegrationPlugin(config)

# åŠ è½½HuggingFaceæ•°æ®é›†
dataset = plugin.load_hf_dataset(
    dataset_name="wikitext",
    split="train"
)

# è½¬æ¢ä¸ºAPTæ ¼å¼
apt_data = plugin.convert_to_apt_format(dataset)

# ä½¿ç”¨HF Trainerè®­ç»ƒ
plugin.train_with_hf_trainer(
    model=model,
    tokenizer=tokenizer,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    output_dir="./output"
)

# å¯¼å‡ºåˆ°HuggingFace Hub
plugin.login_to_hub("your_token")
plugin.export_to_huggingface(
    model=model,
    tokenizer=tokenizer,
    repo_name="username/my-apt-model",
    private=False
)
```

### ğŸ“ æ‰©å±•ç¤ºä¾‹ - æ›´å¤šæ•°æ®é›†ç”¨æ³•

### å¸¸ç”¨æ–‡æœ¬æ•°æ®é›†

#### 1. HuggingFace Datasets

```python
from datasets import load_dataset

# ========== è‹±æ–‡æ•°æ®é›† ==========

# Wikipediaï¼ˆè‹±æ–‡ï¼‰
wiki_en = load_dataset('wikipedia', '20220301.en', streaming=True)

# BookCorpusï¼ˆä¹¦ç±ï¼‰
books = load_dataset('bookcorpus', streaming=True)

# C4ï¼ˆCommon Crawlï¼‰
c4 = load_dataset('c4', 'en', streaming=True)

# OpenWebTextï¼ˆReddit é“¾æ¥ï¼‰
owt = load_dataset('openwebtext', streaming=True)

# ========== ä¸­æ–‡æ•°æ®é›† ==========

# Chinese Wikipedia
wiki_zh = load_dataset('wikipedia', '20220301.zh', streaming=True)

# CLUECorpus2020ï¼ˆ14GB ä¸­æ–‡è¯­æ–™ï¼‰
clue = load_dataset('clue', 'cluecorpussmall', streaming=True)

# WuDaoCorpusï¼ˆæ‚Ÿé“ï¼Œ200GBï¼‰
# éœ€è¦ç”³è¯·è®¿é—®ï¼šhttps://www.wudao.com/

# ========== ä»£ç æ•°æ®é›† ==========

# The Stackï¼ˆ6TB ä»£ç ï¼‰
code = load_dataset('bigcode/the-stack', streaming=True)

# CodeParrotï¼ˆGitHub Python ä»£ç ï¼‰
python_code = load_dataset('codeparrot/github-code', streaming=True)

# ========== å¤šè¯­è¨€æ•°æ®é›† ==========

# mC4ï¼ˆå¤šè¯­è¨€ Common Crawlï¼‰
mc4 = load_dataset('mc4', 'zh', streaming=True)  # ä¸­æ–‡
mc4_en = load_dataset('mc4', 'en', streaming=True)  # è‹±æ–‡
```

#### 2. æ•°æ®é›†é¢„å¤„ç†ç¤ºä¾‹

```python
from datasets import load_dataset
from transformers import GPT2Tokenizer

# åŠ è½½æ•°æ®é›†
dataset = load_dataset('wikitext', 'wikitext-103-raw-v1', split='train')

# åˆ†è¯
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

def tokenize(examples):
    return tokenizer(
        examples['text'],
        truncation=True,
        max_length=512,
        padding='max_length'
    )

# æ‰¹é‡å¤„ç†ï¼ˆé«˜æ•ˆï¼‰
tokenized_dataset = dataset.map(
    tokenize,
    batched=True,
    batch_size=1000,
    num_proc=4,  # å¤šè¿›ç¨‹åŠ é€Ÿ
    remove_columns=['text']
)

# ä¿å­˜é¢„å¤„ç†åçš„æ•°æ®
tokenized_dataset.save_to_disk('processed_wikitext')

# åç»­ç›´æ¥åŠ è½½
from datasets import load_from_disk
dataset = load_from_disk('processed_wikitext')
```

#### 3. è‡ªå®šä¹‰æ•°æ®é›†æ ¼å¼

```python
# ========== JSON Lines æ ¼å¼ ==========
# data.jsonl
# {"text": "ç¬¬ä¸€æ¡æ•°æ®"}
# {"text": "ç¬¬äºŒæ¡æ•°æ®"}

dataset = load_dataset('json', data_files='data.jsonl')

# ========== CSV æ ¼å¼ ==========
# data.csv
# text
# ç¬¬ä¸€æ¡æ•°æ®
# ç¬¬äºŒæ¡æ•°æ®

dataset = load_dataset('csv', data_files='data.csv')

# ========== Parquet æ ¼å¼ï¼ˆæ¨èï¼Œå‹ç¼©é«˜æ•ˆï¼‰==========
# data.parquet

dataset = load_dataset('parquet', data_files='data.parquet')

# ä¿å­˜ä¸º Parquet
dataset.to_parquet('output.parquet')
```

### æ•°æ®é›†æ··åˆç­–ç•¥

```python
from datasets import concatenate_datasets, interleave_datasets

# ========== æ–¹æ³• 1: ç®€å•æ‹¼æ¥ ==========
dataset1 = load_dataset('wikitext', split='train')
dataset2 = load_dataset('bookcorpus', split='train')

combined = concatenate_datasets([dataset1, dataset2])

# ========== æ–¹æ³• 2: äº¤é”™é‡‡æ ·ï¼ˆæ¨èï¼‰==========
# æŒ‰æ¯”ä¾‹æ··åˆä¸åŒæ•°æ®é›†
combined = interleave_datasets(
    [dataset1, dataset2],
    probabilities=[0.7, 0.3],  # 70% wiki, 30% books
    seed=42
)

# ========== æ–¹æ³• 3: è‡ªå®šä¹‰æ··åˆï¼ˆDeepSeek ç­–ç•¥ï¼‰==========
datasets_with_weights = [
    (load_dataset('wikipedia', split='train'), 0.4),   # é€šç”¨ 40%
    (load_dataset('the-stack', split='train'), 0.2),   # ä»£ç  20%
    (load_dataset('math-corpus', split='train'), 0.1), # æ•°å­¦ 10%
    (load_dataset('mc4', 'zh', split='train'), 0.3),   # å¤šè¯­è¨€ 30%
]

# æŒ‰æƒé‡é‡‡æ ·
from itertools import cycle
import random

def weighted_sample(datasets_weights):
    # è®¡ç®—æ¯ä¸ªæ•°æ®é›†çš„é‡‡æ ·æ•°
    total_samples = sum(w for _, w in datasets_weights)

    for dataset, weight in datasets_weights:
        num_samples = int(weight * total_samples)
        for i, example in enumerate(dataset):
            if i >= num_samples:
                break
            yield example
```

---

## ğŸ“ å›¾åƒè®­ç»ƒæ•°æ®é›†

### æ‰©å±•åŠŸèƒ½ (éœ€è¦torchvisionå’ŒPILåº“)

éœ€è¦å®‰è£…: `pip install torchvision pillow`

### å¤šæ¨¡æ€æ•°æ®é›†ï¼ˆå›¾åƒ + æ–‡æœ¬ï¼‰

#### 1. å¸¸ç”¨å›¾åƒ-æ–‡æœ¬æ•°æ®é›†

```python
from datasets import load_dataset

# ========== COCO Captionsï¼ˆå›¾åƒæè¿°ï¼‰==========
# 123K å›¾åƒ + 5 ä¸ªæè¿°/å›¾
coco = load_dataset('HuggingFaceM4/COCO')

# æ•°æ®æ ¼å¼
# {
#   'image': PIL.Image,
#   'captions': ['æè¿°1', 'æè¿°2', 'æè¿°3', 'æè¿°4', 'æè¿°5']
# }

# ========== Conceptual Captionsï¼ˆ330ä¸‡ å›¾åƒ-æ–‡æœ¬å¯¹ï¼‰==========
cc3m = load_dataset('conceptual_captions')

# ========== LAION-5Bï¼ˆ50äº¿ å›¾åƒ-æ–‡æœ¬å¯¹ï¼‰==========
# éœ€è¦ä¸‹è½½ï¼šhttps://laion.ai/blog/laion-5b/
# è¶…å¤§è§„æ¨¡ï¼Œå»ºè®®ä½¿ç”¨ img2dataset å·¥å…·æµå¼ä¸‹è½½

# ========== Flickr30kï¼ˆ3ä¸‡å›¾åƒï¼Œ5ä¸ªæè¿°/å›¾ï¼‰==========
flickr = load_dataset('nlphuji/flickr30k')

# ========== Visual Genomeï¼ˆ10ä¸‡å›¾åƒ + åŒºåŸŸæè¿°ï¼‰==========
vg = load_dataset('visual_genome')
```

#### 2. å›¾åƒæ•°æ®åŠ è½½å™¨

```python
import torch
from torch.utils.data import Dataset, DataLoader
from PIL import Image
from torchvision import transforms

class ImageTextDataset(Dataset):
    """
    å›¾åƒ-æ–‡æœ¬å¤šæ¨¡æ€æ•°æ®é›†

    ç”¨äºè®­ç»ƒï¼š
    - GPT-4oï¼ˆå¤šæ¨¡æ€è¾“å…¥ï¼‰
    - Claude-4ï¼ˆå›¾åƒç†è§£ï¼‰
    - CLIPï¼ˆå›¾åƒ-æ–‡æœ¬å¯¹æ¯”å­¦ä¹ ï¼‰
    """
    def __init__(
        self,
        dataset,
        image_processor,
        tokenizer,
        max_text_length=512,
        image_size=224
    ):
        self.dataset = dataset
        self.image_processor = image_processor
        self.tokenizer = tokenizer
        self.max_text_length = max_text_length

        # å›¾åƒå˜æ¢
        self.transform = transforms.Compose([
            transforms.Resize((image_size, image_size)),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406],
                std=[0.229, 0.224, 0.225]
            )
        ])

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        example = self.dataset[idx]

        # å¤„ç†å›¾åƒ
        image = example['image']
        if not isinstance(image, Image.Image):
            image = Image.open(image).convert('RGB')

        image_tensor = self.transform(image)

        # å¤„ç†æ–‡æœ¬ï¼ˆå–ç¬¬ä¸€ä¸ªæè¿°ï¼‰
        captions = example['captions']
        if isinstance(captions, list):
            text = captions[0]
        else:
            text = captions

        # åˆ†è¯
        text_tokens = self.tokenizer(
            text,
            max_length=self.max_text_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        return {
            'image': image_tensor,
            'input_ids': text_tokens['input_ids'][0],
            'attention_mask': text_tokens['attention_mask'][0],
            'text': text  # åŸå§‹æ–‡æœ¬ï¼ˆç”¨äºè¯„ä¼°ï¼‰
        }


# ========== ä½¿ç”¨ç¤ºä¾‹ ==========

from transformers import CLIPProcessor, GPT2Tokenizer
from datasets import load_dataset

# 1. åŠ è½½æ•°æ®é›†
coco_dataset = load_dataset('HuggingFaceM4/COCO', split='train')

# 2. å‡†å¤‡å¤„ç†å™¨
image_processor = CLIPProcessor.from_pretrained('openai/clip-vit-base-patch32')
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
tokenizer.pad_token = tokenizer.eos_token

# 3. åˆ›å»ºæ•°æ®é›†
dataset = ImageTextDataset(
    dataset=coco_dataset,
    image_processor=image_processor,
    tokenizer=tokenizer,
    image_size=224
)

# 4. åˆ›å»º DataLoader
loader = DataLoader(
    dataset,
    batch_size=32,
    shuffle=True,
    num_workers=4,
    pin_memory=True
)

# 5. è®­ç»ƒå¾ªç¯
for batch in loader:
    images = batch['image'].to(device)        # [B, 3, 224, 224]
    input_ids = batch['input_ids'].to(device) # [B, 512]

    # å¤šæ¨¡æ€ç¼–ç 
    image_features = image_encoder(images)
    text_features = text_encoder(input_ids)

    # å¯¹æ¯”å­¦ä¹ æŸå¤±ï¼ˆCLIP é£æ ¼ï¼‰
    loss = contrastive_loss(image_features, text_features)

    loss.backward()
    optimizer.step()
```

#### 3. å›¾åƒé¢„å¤„ç† Pipeline

```python
from torchvision import transforms
from PIL import Image

class ImagePreprocessor:
    """
    å›¾åƒé¢„å¤„ç†å™¨ï¼ˆç”¨äºè§†è§‰è¯­è¨€æ¨¡å‹ï¼‰
    """
    def __init__(self, image_size=224, augment=True):
        self.image_size = image_size

        if augment:
            # è®­ç»ƒæ—¶æ•°æ®å¢å¼º
            self.transform = transforms.Compose([
                transforms.RandomResizedCrop(image_size, scale=(0.8, 1.0)),
                transforms.RandomHorizontalFlip(),
                transforms.ColorJitter(
                    brightness=0.2,
                    contrast=0.2,
                    saturation=0.2,
                    hue=0.1
                ),
                transforms.ToTensor(),
                transforms.Normalize(
                    mean=[0.485, 0.456, 0.406],
                    std=[0.229, 0.224, 0.225]
                )
            ])
        else:
            # æ¨ç†æ—¶ç®€å•ç¼©æ”¾
            self.transform = transforms.Compose([
                transforms.Resize((image_size, image_size)),
                transforms.ToTensor(),
                transforms.Normalize(
                    mean=[0.485, 0.456, 0.406],
                    std=[0.229, 0.224, 0.225]
                )
            ])

    def __call__(self, image):
        """å¤„ç†å•å¼ å›¾åƒ"""
        if isinstance(image, str):
            image = Image.open(image).convert('RGB')
        elif not isinstance(image, Image.Image):
            raise ValueError("Input must be PIL Image or file path")

        return self.transform(image)

    def batch_process(self, images):
        """æ‰¹é‡å¤„ç†å›¾åƒ"""
        return torch.stack([self(img) for img in images])
```

#### 4. è‡ªå®šä¹‰å›¾åƒæ•°æ®é›†

```python
import os
from PIL import Image
from torch.utils.data import Dataset

class CustomImageTextDataset(Dataset):
    """
    è‡ªå®šä¹‰å›¾åƒ-æ–‡æœ¬æ•°æ®é›†

    ç›®å½•ç»“æ„ï¼š
    data/
    â”œâ”€â”€ images/
    â”‚   â”œâ”€â”€ img1.jpg
    â”‚   â”œâ”€â”€ img2.jpg
    â”‚   â””â”€â”€ ...
    â””â”€â”€ captions.txt  # æ¯è¡Œï¼šimg1.jpg\tè¿™æ˜¯å›¾ç‰‡æè¿°
    """
    def __init__(
        self,
        image_dir,
        captions_file,
        transform=None,
        tokenizer=None
    ):
        self.image_dir = image_dir
        self.transform = transform
        self.tokenizer = tokenizer

        # åŠ è½½å›¾åƒ-æ–‡æœ¬å¯¹
        self.samples = []
        with open(captions_file, 'r', encoding='utf-8') as f:
            for line in f:
                parts = line.strip().split('\t')
                if len(parts) == 2:
                    img_name, caption = parts
                    img_path = os.path.join(image_dir, img_name)
                    if os.path.exists(img_path):
                        self.samples.append((img_path, caption))

    def __len__(self):
        return len(self.samples)

    def __getitem__(self, idx):
        img_path, caption = self.samples[idx]

        # åŠ è½½å›¾åƒ
        image = Image.open(img_path).convert('RGB')
        if self.transform:
            image = self.transform(image)

        # åˆ†è¯
        if self.tokenizer:
            tokens = self.tokenizer(
                caption,
                max_length=77,
                padding='max_length',
                truncation=True,
                return_tensors='pt'
            )
            return {
                'image': image,
                'input_ids': tokens['input_ids'][0],
                'attention_mask': tokens['attention_mask'][0]
            }
        else:
            return {'image': image, 'caption': caption}


# ========== ä½¿ç”¨ç¤ºä¾‹ ==========

dataset = CustomImageTextDataset(
    image_dir='data/images',
    captions_file='data/captions.txt',
    transform=ImagePreprocessor(image_size=224, augment=True),
    tokenizer=tokenizer
)

loader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=4)
```

### å›¾åƒæ•°æ®é›†æ ¼å¼è½¬æ¢

```python
# ========== COCO æ ¼å¼ â†’ HuggingFace Datasets ==========
from datasets import Dataset, Features, Image as HFImage, Value
import json

def coco_to_dataset(coco_json_path, images_dir):
    """å°† COCO æ ¼å¼è½¬æ¢ä¸º HuggingFace Dataset"""
    with open(coco_json_path) as f:
        coco = json.load(f)

    # æ„å»ºå›¾åƒIDåˆ°æ–‡ä»¶åçš„æ˜ å°„
    id_to_filename = {img['id']: img['file_name'] for img in coco['images']}

    # æ„å»ºæ•°æ®
    data = []
    for ann in coco['annotations']:
        img_id = ann['image_id']
        img_path = os.path.join(images_dir, id_to_filename[img_id])

        data.append({
            'image': img_path,
            'caption': ann['caption']
        })

    # åˆ›å»º Dataset
    features = Features({
        'image': HFImage(),
        'caption': Value('string')
    })

    dataset = Dataset.from_dict(
        {'image': [d['image'] for d in data],
         'caption': [d['caption'] for d in data]},
        features=features
    )

    return dataset

# ä½¿ç”¨
dataset = coco_to_dataset('annotations.json', 'images/')
dataset.save_to_disk('coco_dataset')
```

---

## ğŸ“¦ å®Œæ•´ç¤ºä¾‹

### âœ… ä½¿ç”¨å®é™…å®ç°çš„ç«¯åˆ°ç«¯æ•°æ®å¤„ç†

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
ä½¿ç”¨APTå®é™…å®ç°çš„å®Œæ•´æ•°æ®å¤„ç†æµç¨‹
"""
from apt_model.data.data_processor import DataProcessor, DatasetStatistics
from apt_model.training.data_loading import (
    load_text_data_from_file,
    TextDataset,
    prepare_dataloader
)
from legacy_plugins.batch2.plugin_7_data_processors import DataProcessorsPlugin
from transformers import AutoTokenizer
from torch.utils.data import DataLoader

def main():
    # ==================== 1. è¯»å–åŸå§‹æ•°æ® ====================
    print("ğŸ“‚ è¯»å–åŸå§‹æ•°æ®...")

    # ä½¿ç”¨ APT çš„æ–‡ä»¶åŠ è½½å‡½æ•°
    raw_texts = load_text_data_from_file("data/train.txt")
    print(f"åŸå§‹æ•°æ®: {len(raw_texts):,} æ¡")

    # ==================== 2. æ•°æ®è´¨é‡åˆ†æ ====================
    print("\nğŸ“Š æ•°æ®è´¨é‡åˆ†æ...")
    summary = DatasetStatistics.summarize_dataset(raw_texts)
    DatasetStatistics.print_dataset_summary(summary)

    # ==================== 3. æ•°æ®æ¸…ç†ä¸å¤„ç† ====================
    print("\nğŸ§¹ å¼€å§‹æ•°æ®æ¸…ç†...")

    # åˆå§‹åŒ–æ•°æ®å¤„ç†æ’ä»¶
    plugin = DataProcessorsPlugin({
        'enable_cleaning': True,
        'enable_augmentation': True,
        'augmentation_ratio': 0.2,
        'normalize_urls': True
    })

    # è½¬æ¢ä¸ºå­—å…¸æ ¼å¼
    data = [{'text': text} for text in raw_texts]

    # æ‰§è¡Œå¤„ç†ç®¡é“
    processed_data = plugin.process_pipeline(
        data,
        text_key='text',
        steps=['clean', 'quality_check', 'remove_duplicates']
    )

    # æå–å¤„ç†åçš„æ–‡æœ¬
    clean_texts = [item['text'] for item in processed_data]
    print(f"\næ¸…ç†åæ•°æ®: {len(clean_texts):,} æ¡")

    # ==================== 4. åˆ›å»ºæ•°æ®å¤„ç†å™¨å’Œæ•°æ®é›† ====================
    print("\nğŸ”§ åˆ›å»ºæ•°æ®å¤„ç†å™¨...")

    # åˆå§‹åŒ–åˆ†è¯å™¨å’Œå¤„ç†å™¨
    tokenizer = AutoTokenizer.from_pretrained("gpt2")
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token

    processor = DataProcessor(
        tokenizer=tokenizer,
        max_seq_length=512,
        clean_text=False,  # å·²ç»æ¸…æ´—è¿‡äº†
        language='en'
    )

    # åˆ›å»ºæ•°æ®é›†
    dataset = processor.create_dataset(clean_texts)
    print(f"æ•°æ®é›†å¤§å°: {len(dataset)}")

    # ==================== 5. åˆ›å»ºæ•°æ®åŠ è½½å™¨ ====================
    print("\nğŸ“¦ åˆ›å»ºæ•°æ®åŠ è½½å™¨...")

    from apt_model.training.data_loading import text_collate_fn

    dataloader = DataLoader(
        dataset,
        batch_size=16,
        shuffle=True,
        collate_fn=lambda batch: text_collate_fn(batch, pad_token_id=tokenizer.pad_token_id),
        num_workers=4,
        pin_memory=True
    )

    print(f"æ‰¹æ¬¡æ•°é‡: {len(dataloader)}")
    print(f"æ‰¹æ¬¡å¤§å°: 16")

    # ==================== 6. ä¿å­˜å¤„ç†åæ•°æ® ====================
    print("\nğŸ’¾ ä¿å­˜å¤„ç†åæ•°æ®...")

    # ä¿å­˜ä¸ºçº¯æ–‡æœ¬
    with open('clean_train.txt', 'w', encoding='utf-8') as f:
        for text in clean_texts:
            f.write(text + '\n')

    # ä¿å­˜ä¸º JSON
    import json
    with open('clean_train.json', 'w', encoding='utf-8') as f:
        json.dump(processed_data, f, ensure_ascii=False, indent=2)

    print("âœ… æ•°æ®å¤„ç†å®Œæˆï¼")
    print(f"\nè¾“å‡ºæ–‡ä»¶:")
    print(f"  - clean_train.txt (çº¯æ–‡æœ¬)")
    print(f"  - clean_train.json (å¸¦å…ƒæ•°æ®)")

    # ==================== 7. æµ‹è¯•æ•°æ®åŠ è½½ ====================
    print("\nğŸ” æµ‹è¯•æ•°æ®åŠ è½½...")

    # è·å–ä¸€ä¸ªæ‰¹æ¬¡
    batch = next(iter(dataloader))
    print(f"\næ‰¹æ¬¡æ•°æ®:")
    print(f"  - src_ids shape: {batch['src_ids'].shape}")
    print(f"  - src_mask shape: {batch['src_mask'].shape}")
    print(f"  - tgt_ids shape: {batch['tgt_ids'].shape}")
    print(f"  - tgt_mask shape: {batch['tgt_mask'].shape}")

    # æŸ¥çœ‹æ’ä»¶ç»Ÿè®¡
    stats = plugin.get_statistics()
    print(f"\nå¤„ç†ç»Ÿè®¡: {stats}")


if __name__ == "__main__":
    main()
```

### è¿è¡Œç¤ºä¾‹

```bash
# å®‰è£…ä¾èµ–
pip install jieba datasketch langdetect

# è¿è¡Œæ¸…ç†è„šæœ¬
python data_cleaning.py

# è¾“å‡ºç¤ºä¾‹
ğŸ“‚ è¯»å–åŸå§‹æ•°æ®...
åŸå§‹æ•°æ®: 1,234,567 æ¡

ğŸ§¹ å¼€å§‹æ•°æ®æ¸…ç†...
ğŸ“¥ è¾“å…¥æ•°æ®: 1,234,567 æ¡

[1/5] åŸºç¡€æ¸…æ´—...
   âœ“ å‰©ä½™: 1,150,234 æ¡

[2/5] å»é‡...
   âœ“ å‰©ä½™: 856,123 æ¡

[3/5] è´¨é‡è¿‡æ»¤...
   âœ“ å‰©ä½™: 623,456 æ¡

[4/5] æ ¼å¼è§„èŒƒåŒ–...

[5/5] é«˜çº§å¤„ç†ï¼ˆåˆ†ç±»ã€éš¾åº¦è¯„ä¼°ï¼‰...

âœ… æ¸…ç†å®Œæˆ: 623,456 æ¡é«˜è´¨é‡æ•°æ®

âš–ï¸ æ•°æ®å¹³è¡¡...
å¹³è¡¡åæ•°æ®: 500,000 æ¡

ğŸ“Š æ•°æ®ç»Ÿè®¡:

é¢†åŸŸåˆ†å¸ƒ:
  general     : 200,000 (40.00%)
  code        : 100,000 (20.00%)
  news        :  75,000 (15.00%)
  academic    :  75,000 (15.00%)
  math        :  50,000 (10.00%)

éš¾åº¦åˆ†å¸ƒ:
  medium      : 250,000 (50.00%)
  easy        : 150,000 (30.00%)
  hard        : 100,000 (20.00%)

å¹³å‡è´¨é‡åˆ†æ•°: 0.785

ğŸ’¾ ä¿å­˜æ¸…ç†åæ•°æ®...
âœ… æ•°æ®æ¸…ç†å®Œæˆï¼
```

---

## ğŸ› ï¸ å·¥å…·æ¨è

### Python åº“

| å·¥å…· | ç”¨é€” | å®‰è£… |
|------|------|------|
| **jieba** | ä¸­æ–‡åˆ†è¯ | `pip install jieba` |
| **datasketch** | MinHash LSH å»é‡ | `pip install datasketch` |
| **langdetect** | è¯­è¨€æ£€æµ‹ | `pip install langdetect` |
| **ftfy** | ä¿®å¤ Unicode é—®é¢˜ | `pip install ftfy` |
| **beautifulsoup4** | HTML è§£æ | `pip install beautifulsoup4` |
| **chardet** | ç¼–ç æ£€æµ‹ | `pip install chardet` |

### å•†ä¸šå·¥å…·

| å·¥å…· | ç”¨é€” | é“¾æ¥ |
|------|------|------|
| **Perspective API** | å†…å®¹å®‰å…¨æ£€æµ‹ | https://perspectiveapi.com/ |
| **AWS Comprehend** | æ–‡æœ¬åˆ†æï¼ˆå®ä½“ã€æƒ…æ„Ÿï¼‰ | https://aws.amazon.com/comprehend/ |
| **Azure Text Analytics** | è¯­è¨€æ£€æµ‹ã€å…³é”®çŸ­è¯­ | https://azure.microsoft.com/text-analytics/ |

---

## ğŸ“š å‚è€ƒèµ„æº

### å­¦æœ¯è®ºæ–‡

- [The Pile: An 800GB Dataset](https://arxiv.org/abs/2101.00027) - EleutherAI æ•°æ®æ¸…ç†å®è·µ
- [Quality at a Glance: An Audit of Web-Crawled Multilingual Datasets](https://arxiv.org/abs/2103.12028) - å¤§è§„æ¨¡æ•°æ®è´¨é‡åˆ†æ
- [Deduplicating Training Data Makes Language Models Better](https://arxiv.org/abs/2107.06499) - å»é‡å¯¹æ¨¡å‹æ€§èƒ½çš„å½±å“

### å®˜æ–¹æ–‡æ¡£

- [DeepSeek-V3 Data Processing](https://github.com/deepseek-ai/DeepSeek-V3) - DeepSeek æ•°æ®å¤„ç†æµç¨‹
- [GPT-3 Dataset](https://github.com/openai/gpt-3) - OpenAI æ•°æ®æ„å»º
- [LLaMA Data](https://github.com/facebookresearch/llama) - Meta LLaMA æ•°æ®é›†

### APT ç›¸å…³æ–‡æ¡£

- [GPT è®­ç»ƒæŒ‡å—](GPT_TRAINING_GUIDE.md) - è®­ç»ƒæµç¨‹å®Œæ•´æ•™ç¨‹
- [DeepSeek è®­ç»ƒæŒ‡å—](DEEPSEEK_TRAINING_GUIDE.md) - DeepSeek æ¶æ„è®­ç»ƒ
- [APT Model Handbook](APT_MODEL_HANDBOOK.md) - APT å¹³å°å®Œæ•´æ‰‹å†Œ

---

## ğŸ“‹ åŠŸèƒ½æ€»ç»“

### âœ… å¯ç›´æ¥ä½¿ç”¨çš„å®é™…å®ç°

**æ ¸å¿ƒåŠŸèƒ½** (`apt_model/data/data_processor.py`):
- âœ… DataProcessor - æ–‡æœ¬æ¸…æ´—ã€åˆ†è¯ã€ç¼–ç 
- âœ… TextCleaner - é™æ€æ¸…æ´—æ–¹æ³•é›†åˆ
- âœ… DatasetStatistics - æ•°æ®é›†ç»Ÿè®¡åˆ†æ

**æ•°æ®é›†ç±»** (`apt_model/training/data_loading.py`):
- âœ… TextDataset - è‡ªå›å½’è®­ç»ƒæ•°æ®é›†
- âœ… PairedTextDataset - Seq2Seqè®­ç»ƒæ•°æ®é›†
- âœ… MultimodalDataset - å¤šæ¨¡æ€æ•°æ®é›†
- âœ… æ–‡ä»¶åŠ è½½å‡½æ•° - æ”¯æŒ .txt, .json, .csv, .jsonl
- âœ… æ‰¹å¤„ç†æ•´ç†å‡½æ•° - text_collate_fn, multimodal_collate_fn

**æ•°æ®å¤„ç†æ’ä»¶** (`legacy_plugins/batch2/plugin_7_data_processors.py`):
- âœ… æ–‡æœ¬æ¸…æ´—ä¸æ ‡å‡†åŒ–
- âœ… æ•°æ®å¢å¼º (åŸºç¡€æ–¹æ³•: swap, delete, insert, synonym_replacement)
- âœ… æ•°æ®å¹³è¡¡ (oversample, undersample)
- âœ… ç‰¹å¾æå–
- âœ… æ•°æ®è´¨é‡æ£€æŸ¥
- âœ… å®Œæ•´å¤„ç†ç®¡é“

**HuggingFaceé›†æˆ** (`legacy_plugins/batch1/huggingface_integration_plugin.py`):
- âœ… åŠ è½½HuggingFaceæ•°æ®é›†
- âœ… å¯¼å…¥/å¯¼å‡ºæ¨¡å‹åˆ°HuggingFace Hub
- âœ… HF Traineré›†æˆ
- âœ… æ•°æ®æ ¼å¼è½¬æ¢

### ğŸ“ éœ€è¦æ‰©å±•çš„åŠŸèƒ½

**æµå¼æ•°æ®åŠ è½½**:
- ğŸ“ StreamingTextDataset - éœ€è¦è‡ªè¡Œå®ç°
- ğŸ“ åˆ†å—åŠ è½½ - éœ€è¦è‡ªè¡Œå®ç°

**é«˜çº§æ•°æ®é›†åŠŸèƒ½**:
- ğŸ“ æ•°æ®é›†æ··åˆç­–ç•¥ - éœ€è¦é¢å¤–å®ç°
- ğŸ“ è‡ªå®šä¹‰æ•°æ®é›†é¢„å¤„ç†æµæ°´çº¿ - éœ€è¦é¢å¤–å®ç°

**å›¾åƒæ•°æ®é›†**:
- ğŸ“ ImageTextDataset - éœ€è¦ torchvision å’Œ PIL
- ğŸ“ COCO/LAION æ•°æ®é›†åŠ è½½ - éœ€è¦é¢å¤–ä¾èµ–

**é«˜çº§æ•°æ®å¢å¼º**:
- ğŸ“ å›è¯‘ (Back-translation) - éœ€è¦ç¿»è¯‘æ¨¡å‹
- ğŸ“ BERTä¸Šä¸‹æ–‡å¢å¼º - éœ€è¦ nlpaug åº“
- ğŸ“ SMOTEå¹³è¡¡ - éœ€è¦ imbalanced-learn åº“

### ä¾èµ–å…³ç³»

```bash
# æ ¸å¿ƒä¾èµ– (å·²åŒ…å«åœ¨é¡¹ç›®ä¸­)
torch
numpy
tqdm
transformers

# å¯é€‰ä¾èµ– (ç”¨äºæ‰©å±•åŠŸèƒ½)
datasets          # HuggingFace æ•°æ®é›†
nlpaug            # é«˜çº§æ•°æ®å¢å¼º
imbalanced-learn  # SMOTE ç­‰å¹³è¡¡æŠ€æœ¯
torchvision       # å›¾åƒå¤„ç†
pillow            # å›¾åƒåŠ è½½
torchaudio        # éŸ³é¢‘å¤„ç†
```

---

## ğŸ“ æ›´æ–°æ—¥å¿—

- **v1.2.0** (2025-12) - Option B æ ‡æ³¨ç‰ˆæœ¬
  - âœ… æ¸…æ™°æ ‡æ³¨å®é™…å®ç°å’Œæ‰©å±•ç¤ºä¾‹
  - âœ… æ·»åŠ å®é™…ä»£ç çš„å®Œæ•´ä½¿ç”¨ç¤ºä¾‹
  - âœ… æ·»åŠ æ–‡ä»¶ä½ç½®å’Œå‡½æ•°ç­¾å
  - âœ… åŒºåˆ†æ ¸å¿ƒåŠŸèƒ½ã€æ•°æ®é›†ç±»ã€æ’ä»¶åŠŸèƒ½

- **v1.1.0** (2025-12) - åŠŸèƒ½æ‰©å±•ç‰ˆ
  - âœ… æµå¼åŠ è½½è®­ç»ƒæ•°æ®ï¼ˆæ”¯æŒ TB çº§æ•°æ®é›†ï¼‰
  - âœ… å…¬å¼€æ•°æ®é›†ä½¿ç”¨æŒ‡å—ï¼ˆHuggingFace Datasetsï¼‰
  - âœ… å›¾åƒè®­ç»ƒæ•°æ®é›†ï¼ˆå¤šæ¨¡æ€è®­ç»ƒï¼‰
  - âœ… æ•°æ®é›†æ··åˆç­–ç•¥ï¼ˆDeepSeek é£æ ¼ï¼‰
  - âœ… æµå¼ + å¤š worker å¹¶è¡ŒåŠ è½½

- **v1.0.0** (2025-12) - åˆå§‹ç‰ˆæœ¬
  - âœ… å®Œæ•´æ•°æ®æ¸…æ´—æµç¨‹ï¼ˆ5 æ­¥éª¤ï¼‰
  - âœ… ç²¾ç¡®å»é‡ + è¿‘ä¼¼å»é‡ï¼ˆMinHash LSHï¼‰
  - âœ… å¤šç»´åº¦è´¨é‡è¯„åˆ†ç³»ç»Ÿ
  - âœ… é¢†åŸŸåˆ†ç±»å’Œéš¾åº¦è¯„ä¼°
  - âœ… æ•°æ®å¹³è¡¡é‡‡æ ·ç­–ç•¥
  - âœ… ç«¯åˆ°ç«¯ç¤ºä¾‹ä»£ç 

---

<div align="center">

**Clean Data, Better Models! ğŸ§¹âœ¨**

é«˜è´¨é‡æ•°æ®æ˜¯å¤§æ¨¡å‹æˆåŠŸçš„åŸºçŸ³

æ”¯æŒæ–‡æœ¬ + å›¾åƒå¤šæ¨¡æ€è®­ç»ƒ | æµå¼åŠ è½½ TB çº§æ•°æ®é›†

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æäº¤ [Issue](https://github.com/chen0430tw/APT-Transformer/issues)

</div>
