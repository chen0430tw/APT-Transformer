# GPTæ¨¡å‹è®­ç»ƒæŒ‡å—

<div align="center">

**å®Œæ•´çš„GPTæ¨¡å‹è®­ç»ƒæ•™ç¨‹ - ä»é›¶åˆ°éƒ¨ç½²**

æ”¯æŒ GPT-4o | GPT-5 | GPTo3

</div>

---

## ğŸ“‹ ç›®å½•

- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [æ¨¡å‹é€‰æ‹©](#æ¨¡å‹é€‰æ‹©)
- [è®­ç»ƒé…ç½®](#è®­ç»ƒé…ç½®)
- [é«˜çº§åŠŸèƒ½](#é«˜çº§åŠŸèƒ½)
- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)
- [æœ€ä½³å®è·µ](#æœ€ä½³å®è·µ)

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1åˆ†é’Ÿè®­ç»ƒä½ çš„ç¬¬ä¸€ä¸ªGPTæ¨¡å‹

```python
from apt_model.training.gpt_trainer import train_gpt4o

# å‡†å¤‡è®­ç»ƒæ•°æ®
train_texts = [
    "äººå·¥æ™ºèƒ½æ­£åœ¨æ”¹å˜ä¸–ç•Œ",
    "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªåˆ†æ”¯",
    "Transformeræ¶æ„revolutionized NLP",
    # ... æ›´å¤šæ–‡æœ¬
]

# å¼€å§‹è®­ç»ƒ
model, tokenizer, history = train_gpt4o(
    train_texts=train_texts,
    epochs=10,
    batch_size=8,
    save_path="./my_gpt4o"
)

# ç”Ÿæˆæ–‡æœ¬
import torch
input_text = "äººå·¥æ™ºèƒ½"
input_ids = torch.tensor([tokenizer.encode(input_text)])
output = model.generate(input_ids, max_new_tokens=50, temperature=0.8)
print(tokenizer.decode(output[0].tolist()))
```

---

## ğŸ¯ æ¨¡å‹é€‰æ‹©

### GPT-4o ğŸŒŸ æ¨è

**é€‚ç”¨åœºæ™¯ï¼š**
- âœ… å¤šæ¨¡æ€åº”ç”¨ï¼ˆæ–‡æœ¬+å›¾åƒ+éŸ³é¢‘ï¼‰
- âœ… éœ€è¦é«˜è´¨é‡ç”Ÿæˆ
- âœ… ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²
- âœ… åˆå­¦è€…å‹å¥½

**ç‰¹ç‚¹ï¼š**
- Tri-Vein Attentionï¼ˆä¸‰ç»´å­ç©ºé—´æ³¨æ„åŠ›ï¼‰
- Hybrid FFNï¼ˆæ··åˆå‰é¦ˆç½‘ç»œï¼‰
- åŠ¨æ€Ï„é—¨æ§
- æ”¯æŒå¤šæ¨¡æ€è¾“å…¥

**è®­ç»ƒç¤ºä¾‹ï¼š**

```python
from apt_model.modeling.gpt4o_model import GPT4oModel
from apt_model.training.gpt_trainer import GPT4oTrainer
from transformers import GPT2Tokenizer

# 1. åˆå§‹åŒ–æ¨¡å‹
model = GPT4oModel(
    vocab_size=50257,    # GPT-2è¯æ±‡è¡¨å¤§å°
    d_model=768,         # æ¨¡å‹ç»´åº¦
    n_heads=12,          # æ³¨æ„åŠ›å¤´æ•°
    d_ff=3072,           # FFNç»´åº¦
    num_layers=12,       # å±‚æ•°
    rank=4               # Veinå­ç©ºé—´ç§©
)

# 2. åŠ è½½tokenizer
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# 3. åˆ›å»ºè®­ç»ƒå™¨
trainer = GPT4oTrainer(
    model=model,
    tokenizer=tokenizer,
    learning_rate=3e-4,
    weight_decay=0.01
)

# 4. å‡†å¤‡æ•°æ®
train_texts = open('train.txt', 'r', encoding='utf-8').readlines()
eval_texts = open('eval.txt', 'r', encoding='utf-8').readlines()

# 5. å¼€å§‹è®­ç»ƒ
history = trainer.train(
    train_texts=train_texts,
    epochs=20,
    batch_size=16,
    max_length=512,
    save_path="./gpt4o_checkpoint",
    eval_texts=eval_texts,
    eval_interval=1000
)
```

### GPTo3 ğŸ§  é«˜çº§

**é€‚ç”¨åœºæ™¯ï¼š**
- âœ… å¤æ‚æ¨ç†ä»»åŠ¡
- âœ… éœ€è¦ç»“æ„åŒ–æ€è€ƒ
- âœ… ç ”ç©¶å®éªŒ
- âš ï¸ è®¡ç®—èµ„æºå……è¶³

**ç‰¹ç‚¹ï¼š**
- ç»“æ„åŒ–æ¨ç†ï¼ˆStructured Reasoningï¼‰
- ç†µè§¦å‘æœºåˆ¶
- å¤šä¸“å®¶ç³»ç»Ÿ
- é¢„ç®—æ§åˆ¶

**è®­ç»ƒç¤ºä¾‹ï¼š**

```python
from apt_model.modeling.gpto3_model import GPTo3Model
from apt_model.training.gpt_trainer import GPTo3Trainer

# 1. åˆå§‹åŒ–æ¨¡å‹ï¼ˆæ›´å¤šå‚æ•°ï¼‰
model = GPTo3Model(
    vocab_size=50257,
    d_model=768,
    n_heads=12,
    d_ff=3072,
    num_layers=12,
    rank=4,
    # GPTo3ç‰¹æœ‰å‚æ•°
    entropy_trig=2.0,      # ç†µè§¦å‘é˜ˆå€¼
    global_budget=0.15,    # æ¨ç†é¢„ç®—
    max_reason_steps=6,    # æœ€å¤§æ¨ç†æ­¥æ•°
    patience=2,            # æ—©åœè€å¿ƒå€¼
    eps_kl=0.02,          # KLæ•£åº¦åœæ­¢é˜ˆå€¼
    topk_experts=2        # Top-Kä¸“å®¶æ•°
)

# 2. è®­ç»ƒé…ç½®
trainer = GPTo3Trainer(
    model=model,
    tokenizer=tokenizer,
    learning_rate=2e-4,    # è¾ƒä½å­¦ä¹ ç‡
    weight_decay=0.01,
    max_grad_norm=1.0
)

# 3. è®­ç»ƒ
history = trainer.train(
    train_texts=train_texts,
    epochs=30,            # æ›´å¤šepoch
    batch_size=8,         # è¾ƒå°batch size
    max_length=1024,      # æ›´é•¿åºåˆ—
    save_path="./gpto3_checkpoint"
)
```

### GPT-5 ğŸ”¬ é«˜çº§

**é€‚ç”¨åœºæ™¯ï¼š**
- âœ… MoEï¼ˆä¸“å®¶æ··åˆï¼‰ç ”ç©¶
- âœ… CPUå‹å¥½è®­ç»ƒ
- âœ… æµå¼æ£€ç´¢åº”ç”¨
- âš ï¸ éœ€è¦é…ç½®VeinProjectorä¾èµ–

**ç‰¹ç‚¹ï¼š**
- Codebook MoE
- Leaf-VoteæŠ•ç¥¨æœºåˆ¶
- æµå¼æ£€ç´¢å™¨
- è®°å¿†æ¡¶ï¼ˆMemory Bucketï¼‰

**æ³¨æ„äº‹é¡¹ï¼š**
- éœ€è¦å®‰è£… `apt_model.modeling.blocks.VeinProjector`
- é€‚åˆMoEæ¶æ„ç ”ç©¶å’Œå¤§è§„æ¨¡è®­ç»ƒ
- è¯¦è§ [GPT Models Analysis](GPT_MODELS_ANALYSIS.md) äº†è§£å®Œæ•´ç‰¹æ€§

---

## âš™ï¸ è®­ç»ƒé…ç½®

### ç¡¬ä»¶è¦æ±‚

| æ¨¡å‹ | æœ€ä½æ˜¾å­˜ | æ¨èæ˜¾å­˜ | CPUå¯è¡Œ |
|------|---------|---------|---------|
| **GPT-4o (Small)** | 4GB | 8GB+ | âœ… å¯ä»¥ |
| **GPT-4o (Base)** | 8GB | 16GB+ | âš ï¸ è¾ƒæ…¢ |
| **GPT-4o (Large)** | 16GB | 24GB+ | âŒ ä¸æ¨è |
| **GPTo3 (Base)** | 12GB | 24GB+ | âš ï¸ è¾ƒæ…¢ |
| **GPT-5 (Base)** | 6GB | 12GB+ | âœ… ä¼˜åŒ–è¿‡ |

### è¶…å‚æ•°æ¨è

#### å°å‹æ¨¡å‹ï¼ˆ< 100Må‚æ•°ï¼‰

```python
config = {
    'd_model': 512,
    'n_heads': 8,
    'd_ff': 2048,
    'num_layers': 6,
    'learning_rate': 3e-4,
    'batch_size': 32,
    'warmup_steps': 1000,
    'max_length': 512
}
```

#### ä¸­å‹æ¨¡å‹ï¼ˆ100M - 500Må‚æ•°ï¼‰

```python
config = {
    'd_model': 768,
    'n_heads': 12,
    'd_ff': 3072,
    'num_layers': 12,
    'learning_rate': 2e-4,
    'batch_size': 16,
    'warmup_steps': 2000,
    'max_length': 1024
}
```

#### å¤§å‹æ¨¡å‹ï¼ˆ> 500Må‚æ•°ï¼‰

```python
config = {
    'd_model': 1024,
    'n_heads': 16,
    'd_ff': 4096,
    'num_layers': 24,
    'learning_rate': 1e-4,
    'batch_size': 8,
    'warmup_steps': 5000,
    'max_length': 2048,
    'gradient_accumulation_steps': 4  # æ¢¯åº¦ç´¯ç§¯
}
```

### æ•°æ®å‡†å¤‡

#### æ ¼å¼è¦æ±‚

**çº¯æ–‡æœ¬æ ¼å¼ï¼ˆæ¨èï¼‰ï¼š**
```
æ¯è¡Œä¸€ä¸ªè®­ç»ƒæ ·æœ¬
å¯ä»¥æ˜¯å¥å­ã€æ®µè½æˆ–æ–‡æ¡£
ä¿æŒUTF-8ç¼–ç 
```

**JSONæ ¼å¼ï¼š**
```json
[
  {"text": "ç¬¬ä¸€ä¸ªè®­ç»ƒæ ·æœ¬"},
  {"text": "ç¬¬äºŒä¸ªè®­ç»ƒæ ·æœ¬"},
  ...
]
```

**CSVæ ¼å¼ï¼š**
```csv
text
ç¬¬ä¸€ä¸ªè®­ç»ƒæ ·æœ¬
ç¬¬äºŒä¸ªè®­ç»ƒæ ·æœ¬
```

#### æ•°æ®åŠ è½½

```python
# æ–¹æ³•1ï¼šä»æ–‡ä»¶åŠ è½½
with open('train.txt', 'r', encoding='utf-8') as f:
    train_texts = [line.strip() for line in f if line.strip()]

# æ–¹æ³•2ï¼šä»JSONåŠ è½½
import json
with open('train.json', 'r', encoding='utf-8') as f:
    data = json.load(f)
    train_texts = [item['text'] for item in data]

# æ–¹æ³•3ï¼šä»æ•°æ®åº“åŠ è½½
import pandas as pd
df = pd.read_csv('train.csv')
train_texts = df['text'].tolist()

# æ–¹æ³•4ï¼šä½¿ç”¨HuggingFace datasets
from datasets import load_dataset
dataset = load_dataset('wikitext', 'wikitext-2-raw-v1')
train_texts = dataset['train']['text']
```

---

## ğŸ”¥ é«˜çº§åŠŸèƒ½

### æ··åˆç²¾åº¦è®­ç»ƒ

```python
import torch
from torch.cuda.amp import autocast, GradScaler

# å¯ç”¨æ··åˆç²¾åº¦
scaler = GradScaler()

class MixedPrecisionTrainer(BaseGPTTrainer):
    def train_step(self, batch):
        self.model.train()
        input_ids = batch['input_ids'].to(self.device)
        labels = batch['labels'].to(self.device)

        self.optimizer.zero_grad()

        # ä½¿ç”¨autocast
        with autocast():
            logits = self.model(text_ids=input_ids)
            loss = self.compute_loss(logits, labels)

        # ç¼©æ”¾æ¢¯åº¦
        scaler.scale(loss).backward()
        scaler.unscale_(self.optimizer)
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)
        scaler.step(self.optimizer)
        scaler.update()

        return loss.item()
```

### åˆ†å¸ƒå¼è®­ç»ƒ

```python
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

# åˆå§‹åŒ–åˆ†å¸ƒå¼
dist.init_process_group(backend='nccl')
local_rank = int(os.environ['LOCAL_RANK'])

# åŒ…è£…æ¨¡å‹
model = GPT4oModel(...)
model = model.to(local_rank)
model = DDP(model, device_ids=[local_rank])

# ä½¿ç”¨DistributedSampler
from torch.utils.data.distributed import DistributedSampler
train_sampler = DistributedSampler(train_dataset)
train_loader = DataLoader(train_dataset, sampler=train_sampler, ...)

# è®­ç»ƒ
trainer.train(...)
```

### æ¢¯åº¦ç´¯ç§¯

```python
class GradientAccumulationTrainer(BaseGPTTrainer):
    def __init__(self, *args, accumulation_steps=4, **kwargs):
        super().__init__(*args, **kwargs)
        self.accumulation_steps = accumulation_steps

    def train_step(self, batch):
        self.model.train()
        input_ids = batch['input_ids'].to(self.device)
        labels = batch['labels'].to(self.device)

        logits = self.model(text_ids=input_ids)
        loss = self.compute_loss(logits, labels)

        # ç¼©æ”¾æŸå¤±
        loss = loss / self.accumulation_steps
        loss.backward()

        # åªåœ¨ç´¯ç§¯æ­¥æ•°åæ›´æ–°
        if (self.step_count + 1) % self.accumulation_steps == 0:
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.max_grad_norm)
            self.optimizer.step()
            self.optimizer.zero_grad()

        self.step_count += 1
        return loss.item() * self.accumulation_steps
```

### LoRAå¾®è°ƒ

```python
from peft import get_peft_model, LoraConfig, TaskType

# é…ç½®LoRA
peft_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    r=8,                    # LoRAç§©
    lora_alpha=32,          # LoRAç¼©æ”¾å› å­
    lora_dropout=0.1,
    target_modules=["W_q", "W_k", "W_v", "W_o"]  # ç›®æ ‡å±‚
)

# åº”ç”¨LoRA
model = GPT4oModel(...)
model = get_peft_model(model, peft_config)

# æŸ¥çœ‹å¯è®­ç»ƒå‚æ•°
model.print_trainable_parameters()
# è¾“å‡º: trainable params: 2,359,296 || all params: 124,439,808 || trainable%: 1.89

# æ­£å¸¸è®­ç»ƒ
trainer = GPT4oTrainer(model=model, ...)
```

### æ¨¡å‹é‡åŒ–

```python
import torch.quantization as quant

# åŠ¨æ€é‡åŒ–ï¼ˆæ¨ç†ï¼‰
model_quantized = quant.quantize_dynamic(
    model,
    {torch.nn.Linear},  # é‡åŒ–çš„å±‚ç±»å‹
    dtype=torch.qint8
)

# é‡åŒ–æ„ŸçŸ¥è®­ç»ƒï¼ˆQATï¼‰
model.qconfig = quant.get_default_qat_qconfig('fbgemm')
model_prepared = quant.prepare_qat(model, inplace=False)

# è®­ç»ƒå‡ ä¸ªepoch
trainer.train(model_prepared, ...)

# è½¬æ¢ä¸ºé‡åŒ–æ¨¡å‹
model_quantized = quant.convert(model_prepared, inplace=False)
```

---

## ğŸ› æ•…éšœæ’é™¤

### å¸¸è§é”™è¯¯

#### 1. CUDA Out of Memory

**ç—‡çŠ¶ï¼š**
```
RuntimeError: CUDA out of memory
```

**è§£å†³æ–¹æ¡ˆï¼š**
```python
# æ–¹æ¡ˆ1ï¼šå‡å°batch size
batch_size = 4  # ä»8å‡åˆ°4

# æ–¹æ¡ˆ2ï¼šå‡å°åºåˆ—é•¿åº¦
max_length = 256  # ä»512å‡åˆ°256

# æ–¹æ¡ˆ3ï¼šä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
accumulation_steps = 4
effective_batch_size = batch_size * accumulation_steps

# æ–¹æ¡ˆ4ï¼šä½¿ç”¨æ¢¯åº¦æ£€æŸ¥ç‚¹
model.gradient_checkpointing_enable()

# æ–¹æ¡ˆ5ï¼šæ¸…ç†ç¼“å­˜
import torch
torch.cuda.empty_cache()
```

#### 2. Lossä¸ä¸‹é™æˆ–NaN

**ç—‡çŠ¶ï¼š**
```
Loss: 8.5432, 8.5421, 8.5419, ... (åœæ»)
æˆ–
Loss: nan
```

**è§£å†³æ–¹æ¡ˆï¼š**
```python
# æ–¹æ¡ˆ1ï¼šé™ä½å­¦ä¹ ç‡
learning_rate = 1e-4  # ä»3e-4é™ä½

# æ–¹æ¡ˆ2ï¼šå¢åŠ warmup
warmup_steps = 2000  # ä»1000å¢åŠ 

# æ–¹æ¡ˆ3ï¼šæ¢¯åº¦è£å‰ª
max_grad_norm = 0.5  # ä»1.0é™ä½

# æ–¹æ¡ˆ4ï¼šæ£€æŸ¥æ•°æ®
# ç¡®ä¿æ²¡æœ‰å¼‚å¸¸å€¼æˆ–ç©ºæ–‡æœ¬
train_texts = [t for t in train_texts if t and len(t) > 0]

# æ–¹æ¡ˆ5ï¼šä½¿ç”¨æ›´ç¨³å®šçš„ä¼˜åŒ–å™¨
optimizer = torch.optim.AdamW(
    model.parameters(),
    lr=learning_rate,
    betas=(0.9, 0.98),  # æ›´ä¿å®ˆçš„beta2
    eps=1e-6            # æ›´å¤§çš„epsilon
)
```

#### 3. Tokenizeré”™è¯¯

**ç—‡çŠ¶ï¼š**
```
AttributeError: 'NoneType' object has no attribute 'pad_token_id'
```

**è§£å†³æ–¹æ¡ˆï¼š**
```python
from transformers import GPT2Tokenizer

tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# è®¾ç½®pad_token
if tokenizer.pad_token is None:
    tokenizer.pad_token = tokenizer.eos_token

# éªŒè¯
assert tokenizer.pad_token_id is not None, "pad_token_idä¸èƒ½ä¸ºNone"
print(f"Vocab size: {tokenizer.vocab_size}")
print(f"Pad token: {tokenizer.pad_token} (id: {tokenizer.pad_token_id})")
```

---

## ğŸ’¡ æœ€ä½³å®è·µ

### 1. æ•°æ®é¢„å¤„ç†

```python
def preprocess_texts(texts):
    """æ¸…ç†å’Œè§„èŒƒåŒ–æ–‡æœ¬"""
    processed = []
    for text in texts:
        # å»é™¤ç©ºç™½
        text = text.strip()

        # è·³è¿‡å¤ªçŸ­çš„æ–‡æœ¬
        if len(text) < 10:
            continue

        # è§„èŒƒåŒ–ç©ºæ ¼
        text = ' '.join(text.split())

        # ç§»é™¤ç‰¹æ®Šå­—ç¬¦ï¼ˆå¯é€‰ï¼‰
        # text = re.sub(r'[^\w\s\u4e00-\u9fff]', '', text)

        processed.append(text)

    return processed

train_texts = preprocess_texts(raw_texts)
```

### 2. å­¦ä¹ ç‡è°ƒåº¦

```python
from torch.optim.lr_scheduler import CosineAnnealingLR, OneCycleLR

# Cosineé€€ç«
scheduler = CosineAnnealingLR(
    optimizer,
    T_max=num_training_steps,
    eta_min=1e-6
)

# One Cycle
scheduler = OneCycleLR(
    optimizer,
    max_lr=3e-4,
    total_steps=num_training_steps,
    pct_start=0.1  # 10% warmup
)

# åœ¨è®­ç»ƒå¾ªç¯ä¸­ä½¿ç”¨
for batch in train_loader:
    loss = trainer.train_step(batch)
    scheduler.step()
```

### 3. æ—©åœï¼ˆEarly Stoppingï¼‰

```python
class EarlyStopping:
    def __init__(self, patience=5, min_delta=0.001):
        self.patience = patience
        self.min_delta = min_delta
        self.counter = 0
        self.best_loss = float('inf')

    def __call__(self, val_loss):
        if val_loss < self.best_loss - self.min_delta:
            self.best_loss = val_loss
            self.counter = 0
            return False
        else:
            self.counter += 1
            return self.counter >= self.patience

# ä½¿ç”¨
early_stopping = EarlyStopping(patience=5)

for epoch in range(epochs):
    train_loss = train_one_epoch()
    val_loss = evaluate()

    if early_stopping(val_loss):
        print(f"Early stopping at epoch {epoch}")
        break
```

### 4. æ¨¡å‹æ£€æŸ¥ç‚¹ç®¡ç†

```python
import shutil

def save_best_model(model, tokenizer, current_loss, best_loss, save_path):
    """åªä¿å­˜æœ€ä½³æ¨¡å‹"""
    if current_loss < best_loss:
        # åˆ é™¤æ—§çš„æœ€ä½³æ¨¡å‹
        if os.path.exists(save_path):
            shutil.rmtree(save_path)

        # ä¿å­˜æ–°çš„æœ€ä½³æ¨¡å‹
        torch.save({
            'model_state_dict': model.state_dict(),
            'loss': current_loss,
        }, os.path.join(save_path, 'best_model.pt'))

        tokenizer.save_pretrained(save_path)
        return current_loss

    return best_loss
```

### 5. æ—¥å¿—å’Œç›‘æ§

```python
from tensorboard import SummaryWriter

# åˆå§‹åŒ–TensorBoard
writer = SummaryWriter('runs/gpt4o_experiment')

# è®­ç»ƒå¾ªç¯ä¸­è®°å½•
for step, batch in enumerate(train_loader):
    loss = trainer.train_step(batch)

    # è®°å½•loss
    writer.add_scalar('Loss/train', loss, step)

    # è®°å½•å­¦ä¹ ç‡
    writer.add_scalar('LR', optimizer.param_groups[0]['lr'], step)

    # å®šæœŸè®°å½•æ¢¯åº¦
    if step % 100 == 0:
        for name, param in model.named_parameters():
            if param.grad is not None:
                writer.add_histogram(f'Gradients/{name}', param.grad, step)

writer.close()

# æŸ¥çœ‹ï¼štensorboard --logdir=runs
```

---

## ğŸ“š å®Œæ•´ç¤ºä¾‹

### ç«¯åˆ°ç«¯è®­ç»ƒæµç¨‹

```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å®Œæ•´çš„GPT-4oè®­ç»ƒæµç¨‹ç¤ºä¾‹
"""

import torch
from apt_model.modeling.gpt4o_model import GPT4oModel
from apt_model.training.gpt_trainer import GPT4oTrainer
from transformers import GPT2Tokenizer
from torch.utils.tensorboard import SummaryWriter

def main():
    # ==================== é…ç½® ====================
    config = {
        'vocab_size': 50257,
        'd_model': 512,
        'n_heads': 8,
        'd_ff': 2048,
        'num_layers': 6,
        'rank': 4,
        'learning_rate': 3e-4,
        'batch_size': 16,
        'epochs': 20,
        'max_length': 512,
        'device': 'cuda' if torch.cuda.is_available() else 'cpu',
        'save_path': './checkpoints/gpt4o'
    }

    print("=" * 80)
    print("GPT-4o è®­ç»ƒæµç¨‹")
    print("=" * 80)
    print(f"è®¾å¤‡: {config['device']}")
    print(f"æ¨¡å‹å‚æ•°: {config['d_model']}d, {config['num_layers']}å±‚")
    print("=" * 80)

    # ==================== æ•°æ®å‡†å¤‡ ====================
    print("\n1. å‡†å¤‡æ•°æ®...")
    with open('train.txt', 'r', encoding='utf-8') as f:
        train_texts = [line.strip() for line in f if line.strip()]

    with open('eval.txt', 'r', encoding='utf-8') as f:
        eval_texts = [line.strip() for line in f if line.strip()]

    print(f"è®­ç»ƒæ ·æœ¬: {len(train_texts)}")
    print(f"éªŒè¯æ ·æœ¬: {len(eval_texts)}")

    # ==================== æ¨¡å‹åˆå§‹åŒ– ====================
    print("\n2. åˆå§‹åŒ–æ¨¡å‹...")
    model = GPT4oModel(
        vocab_size=config['vocab_size'],
        d_model=config['d_model'],
        n_heads=config['n_heads'],
        d_ff=config['d_ff'],
        num_layers=config['num_layers'],
        rank=config['rank']
    )

    # ç»Ÿè®¡å‚æ•°
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"æ€»å‚æ•°: {total_params:,}")
    print(f"å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")

    # ==================== Tokenizer ====================
    print("\n3. åŠ è½½Tokenizer...")
    tokenizer = GPT2Tokenizer.from_pretrained('gpt2')
    if tokenizer.pad_token is None:
        tokenizer.pad_token = tokenizer.eos_token
    print(f"è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")

    # ==================== è®­ç»ƒå™¨ ====================
    print("\n4. åˆ›å»ºè®­ç»ƒå™¨...")
    trainer = GPT4oTrainer(
        model=model,
        tokenizer=tokenizer,
        device=config['device'],
        learning_rate=config['learning_rate']
    )

    # ==================== è®­ç»ƒ ====================
    print("\n5. å¼€å§‹è®­ç»ƒ...")
    history = trainer.train(
        train_texts=train_texts,
        epochs=config['epochs'],
        batch_size=config['batch_size'],
        max_length=config['max_length'],
        save_path=config['save_path'],
        eval_texts=eval_texts,
        eval_interval=1000
    )

    # ==================== ç”Ÿæˆæµ‹è¯• ====================
    print("\n6. ç”Ÿæˆæµ‹è¯•...")
    test_prompts = [
        "äººå·¥æ™ºèƒ½",
        "æ·±åº¦å­¦ä¹ æ˜¯",
        "Transformeræ¨¡å‹"
    ]

    model.eval()
    with torch.no_grad():
        for prompt in test_prompts:
            input_ids = torch.tensor([tokenizer.encode(prompt)]).to(config['device'])
            output = model.generate(input_ids, max_new_tokens=50, temperature=0.8)
            generated_text = tokenizer.decode(output[0].tolist())
            print(f"\næç¤º: {prompt}")
            print(f"ç”Ÿæˆ: {generated_text}")

    print("\n" + "=" * 80)
    print("è®­ç»ƒå®Œæˆï¼")
    print(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {config['save_path']}")
    print("=" * 80)

if __name__ == "__main__":
    main()
```

---

## ğŸ”— ç›¸å…³èµ„æº

- [APT Model Handbook](APT_MODEL_HANDBOOK.md) - å®Œæ•´çš„APTæ¨¡å‹æ–‡æ¡£
- [GPT Models Analysis](../GPT_MODELS_ANALYSIS.md) - æ¨¡å‹æ¶æ„åˆ†æ
- [API Documentation](API_PROVIDERS_GUIDE.md) - APIé›†æˆæŒ‡å—
- [Troubleshooting Guide](../INSTALLATION.md) - å®‰è£…å’Œæ•…éšœæ’é™¤

---

## ğŸ“ æ›´æ–°æ—¥å¿—

- **v1.1.0** (2025-12) - åŠŸèƒ½å®Œå–„ç‰ˆ
  - âœ… GPT-4o, GPT-5, GPTo3 å…¨é¢æ”¯æŒ
  - âœ… æ··åˆç²¾åº¦è®­ç»ƒä¼˜åŒ–
  - âœ… åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
  - âœ… å®Œæ•´çš„æ•…éšœæ’é™¤æŒ‡å—
  - âœ… ç”Ÿäº§çº§è®­ç»ƒæµç¨‹

- **v1.0.0** (2024-12) - åˆå§‹ç‰ˆæœ¬
  - åŸºç¡€è®­ç»ƒåŠŸèƒ½
  - æ¨¡å‹æ¶æ„å®ç°

---

<div align="center">

**Happy Training! ğŸ‰**

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æäº¤ [Issue](https://github.com/chen0430tw/APT-Transformer/issues)

</div>
