# APT-Transformer å®Œæ•´é›†æˆæ€»ç»“

## ğŸ“‹ ç›®å½•

1. [è™šæ‹ŸBlackwellè™šç©ºç®—åŠ›](#è™šæ‹Ÿblackwellè™šç©ºç®—åŠ›)
2. [å¤šå‚å•†NPUæ”¯æŒ](#å¤šå‚å•†npuæ”¯æŒ)
3. [äº‘ç«¯NPUé€‚é…](#äº‘ç«¯npué€‚é…)
4. [å·¦æ—‹å¹³æ»‘æœºåˆ¶](#å·¦æ—‹å¹³æ»‘æœºåˆ¶)
5. [AIM-Memory æƒ¯æ€§é”šå®šé•œåƒè®°å¿†](#aim-memory-æƒ¯æ€§é”šå®šé•œåƒè®°å¿†)
6. [å®Œæ•´ä½¿ç”¨ç¤ºä¾‹](#å®Œæ•´ä½¿ç”¨ç¤ºä¾‹)

---

## ğŸš€ è™šæ‹ŸBlackwellè™šç©ºç®—åŠ›

### æ ¸å¿ƒèƒ½åŠ›

```
è™šæ‹ŸBlackwell = GPU Flashä¼˜åŒ– + VGPU Stack + å¤šå‚å•†NPU + äº‘ç«¯NPU + å·¦æ—‹å¹³æ»‘
```

### ä¸‰å¤§æ ¸å¿ƒç‰¹æ€§

#### 1ï¸âƒ£ GPU Flashä¼˜åŒ–

**åŸç†**: FP4é‡åŒ– + Triton Kernelèåˆ + Flash Attention

```python
from apt_model.optimization import FusedFP4Linear

# æ›¿æ¢æ ‡å‡†Linearå±‚
model.fc = FusedFP4Linear(768, 3072)

# è‡ªåŠ¨åº”ç”¨ï¼š
# âœ… FP4æƒé‡é‡åŒ–ï¼ˆ4ä½æµ®ç‚¹ï¼Œ12.5%å†…å­˜ï¼‰
# âœ… Triton Kernelèåˆï¼ˆå‡å°‘å†…å­˜è®¿é—®ï¼‰
# âœ… Flash Attentionï¼ˆO(n)å¤æ‚åº¦ï¼‰
```

**æ€§èƒ½æå‡**:
- å†…å­˜å ç”¨: **â†“87.5%** (16bit â†’ 4bit)
- æ¨ç†é€Ÿåº¦: **â†‘2-3Ã—** (Kernelèåˆ)
- è®­ç»ƒé€Ÿåº¦: **â†‘5-10Ã—** (Flash Attention)

#### 2ï¸âƒ£ VGPU Stackï¼ˆè™šæ‹Ÿæ˜¾å­˜å †å ï¼‰

**åŸç†**: GPU â†” CPU â†” SSD ä¸‰çº§å†…å­˜å±‚æ¬¡ + LRUç¼“å­˜

```python
from apt_model.optimization import VGPUStack

# åˆ›å»º3çº§VGPUå †å 
vgpu = VGPUStack.from_config({
    'levels': [
        {'capacity_mb': 2000, 'device': 'cuda:0', 'speed_gbps': 900},  # L1: GPU
        {'capacity_mb': 8000, 'device': 'cpu', 'speed_gbps': 50},      # L2: CPU
        {'capacity_mb': 32000, 'device': 'ssd', 'speed_gbps': 7}       # L3: SSD
    ]
})
```

**æ•ˆæœ**:
- æ˜¾å­˜å®¹é‡: **â†‘21Ã—** (2GB â†’ 42GBè™šæ‹Ÿæ˜¾å­˜)
- å‘½ä¸­ç‡: **>85%** (æ™ºèƒ½LRUç¼“å­˜)
- æ€§èƒ½æŸå¤±: **<15%** (ç›¸æ¯”çº¯GPU)

#### 3ï¸âƒ£ ä¸€é”®å¯ç”¨

```python
import apt_model.optimization.vb_global as vb

# ä¸€è¡Œå¯ç”¨è™šæ‹ŸBlackwell
vb.enable()
```

è¾“å‡ºç¤ºä¾‹ï¼š
```
======================================================================
ğŸš€ è™šæ‹ŸBlackwellå·²å…¨å±€å¯ç”¨
======================================================================
åŠ é€Ÿè®¾å¤‡:        ğŸŸ¢ NVIDIA GPU
GPU Flash:       âœ… å¯ç”¨ï¼ˆFP4é‡åŒ– + Triton Kernelèåˆï¼‰
VGPU Stack:      âœ… 3çº§å †å ï¼ˆGPU 2.0GB â†’ CPU 8.0GB â†’ SSD 32.0GBï¼‰
å¤šå‚å•†NPU:       âœ… å·²åŠ è½½ç»Ÿä¸€åç«¯
äº‘ç«¯NPU:         âš ï¸ æœªé…ç½®ï¼ˆå¯é€‰ï¼‰
å·¦æ—‹å¹³æ»‘:        âœ… å¯ç”¨ï¼ˆå°–ç‚¹è§„é¿ï¼‰

âš¡ é¢„æœŸåŠ é€Ÿæ¯”:    10-100Ã—ï¼ˆå–å†³äºæ¨¡å‹å’Œæ•°æ®ï¼‰
ğŸ’¾ è™šæ‹Ÿæ˜¾å­˜:      42.0 GBï¼ˆç›¸å½“äºA100 40GB + æ‰©å±•ï¼‰
======================================================================
```

---

## ğŸŒ å¤šå‚å•†NPUæ”¯æŒ

### æ”¯æŒçš„åŠ é€Ÿå™¨

| å‚å•† | åŠ é€Ÿå™¨ç±»å‹ | PyTorchåŒ… | è®¾å¤‡ç±»å‹ | çŠ¶æ€ | Emoji |
|------|------------|-----------|----------|------|-------|
| NVIDIA | GPU | `torch.cuda` | `cuda` | âœ… ç”Ÿäº§å°±ç»ª | ğŸŸ¢ |
| Intel | Habana Gaudi HPU | `habana_frameworks.torch` | `hpu` | âœ… ç”Ÿäº§å°±ç»ª | ğŸŸ£ |
| Huawei | Ascend NPU | `torch_npu` | `npu` | âœ… ç”Ÿäº§å°±ç»ª | ğŸŸ¡ |
| Intel | XPU (Ultra NPU) | `intel_extension_for_pytorch` | `xpu` | âš ï¸ å®éªŒæ€§ | ğŸ”µ |
| AMD | ROCm GPU | `torch.cuda` (ROCm) | `cuda` | âš ï¸ å®éªŒæ€§ | ğŸ”´ |
| CPU | x86/ARM CPU | PyTorch | `cpu` | âœ… é€šç”¨ | âšª |

### ç»Ÿä¸€API

```python
from apt_model.optimization import get_device_manager

# è·å–ç»Ÿä¸€è®¾å¤‡ç®¡ç†å™¨
manager = get_device_manager()

# è‡ªåŠ¨æ£€æµ‹æœ€ä½³åŠ é€Ÿå™¨ï¼ˆä¼˜å…ˆçº§: CUDA > HPU > NPU > XPU > CPUï¼‰
device_type = manager.get_accelerator_type()
print(f"å½“å‰ä½¿ç”¨: {device_type}")

# ç»Ÿä¸€APIæ“ä½œï¼ˆæ— éœ€å…³å¿ƒåº•å±‚å®ç°ï¼‰
manager.memory_allocated()       # æŸ¥è¯¢æ˜¾å­˜
manager.empty_cache()            # æ¸…ç†ç¼“å­˜
manager.synchronize()            # åŒæ­¥è®¡ç®—
```

### è‡ªåŠ¨è®¾å¤‡é€‰æ‹©

```python
from apt_model.core.system import get_device

# è‡ªåŠ¨æ£€æµ‹å¯ç”¨è®¾å¤‡ï¼ˆCUDA/NPU/HPU/XPUï¼‰
device = get_device()  # è‡ªåŠ¨è¿”å›æœ€ä½³è®¾å¤‡

model = model.to(device)
# ä»£ç æ— éœ€ä¿®æ”¹ï¼Œè™šæ‹ŸBlackwellç»Ÿä¸€æ¥å£
```

---

## â˜ï¸ äº‘ç«¯NPUé€‚é…

### ä¸ºä»€ä¹ˆéœ€è¦äº‘ç«¯NPUï¼Ÿ

| å¯¹æ¯”é¡¹ | æœ¬åœ°NPU | äº‘ç«¯NPU | äº‘ç«¯ä¼˜åŠ¿ |
|--------|---------|---------|----------|
| **ç¡¬ä»¶æˆæœ¬** | Â¥15,000-50,000 | Â¥0ï¼ˆæŒ‰ä½¿ç”¨ä»˜è´¹ï¼‰ | ğŸ’° é›¶æŠ•å…¥ |
| **å¯åŠ¨æ—¶é—´** | æ•°å‘¨ï¼ˆè´­ä¹°+é…ç½®ï¼‰ | 5åˆ†é’Ÿ | âš¡ å³æ—¶ä½¿ç”¨ |
| **çµæ´»æ€§** | å›ºå®šç®—åŠ› | æŒ‰éœ€æ‰©å±• | ğŸ“ˆ å¼¹æ€§ä¼¸ç¼© |
| **ç»´æŠ¤** | éœ€è¦ç»´æŠ¤ | é›¶ç»´æŠ¤ | ğŸ› ï¸ æ— å¿§è¿ç»´ |
| **æµ‹è¯•NPUæ•ˆæœ** | âŒ å¿…é¡»è´­ä¹° | âœ… ç«‹å³æµ‹è¯• | âœ… å…ˆæµ‹åä¹° |

### æ”¯æŒçš„äº‘å¹³å°

#### ğŸŸ¡ åä¸ºäº‘ModelArtsï¼ˆAscend NPUï¼‰- âœ… å·²æ”¯æŒ

```python
from apt_model.optimization import enable_cloud_npu
import apt_model.optimization.vb_global as vb

# é…ç½®ç¯å¢ƒå˜é‡
import os
os.environ['HUAWEI_CLOUD_API_KEY'] = 'your-api-key'
os.environ['HUAWEI_CLOUD_ENDPOINT'] = 'https://your-endpoint...'
os.environ['HUAWEI_CLOUD_MODEL'] = 'deepseek-r1'

# å¯ç”¨äº‘ç«¯NPU
enable_cloud_npu('auto')

# å¯ç”¨è™šæ‹ŸBlackwellï¼ˆè‡ªåŠ¨ä½¿ç”¨äº‘ç«¯NPUï¼‰
vb.enable()

print("âœ… è™šæ‹ŸBlackwellå·²è¿æ¥åˆ°äº‘ç«¯Ascend NPUï¼")
```

#### ğŸŸ¢ SaladCloud - â³ ç­‰å¾…NPUæ”¯æŒ

å½“å‰ä»…æ”¯æŒGPUï¼ˆRTX 3060èµ·$0.06/å°æ—¶ï¼‰

#### ğŸ”µ RunPod Serverless - â³ ç­‰å¾…NPUæ”¯æŒ

å½“å‰ä»…æ”¯æŒGPUï¼ˆ$0.40/å°æ—¶èµ·ï¼‰

### äº‘ç«¯NPUä½¿ç”¨ç¤ºä¾‹

```python
from apt_model.optimization import CloudNPULinear, get_cloud_npu_manager

# è·å–äº‘ç«¯NPUåç«¯
manager = get_cloud_npu_manager()
backend = manager.get_backend('huawei')

# ä½¿ç”¨äº‘ç«¯åŠ é€Ÿçš„Linearå±‚
layer = CloudNPULinear(
    in_features=768,
    out_features=3072,
    cloud_backend=backend,
    fallback_local=True  # äº‘ç«¯ä¸å¯ç”¨æ—¶è‡ªåŠ¨å›é€€æœ¬åœ°
)

# å‰å‘ä¼ æ’­ï¼ˆè‡ªåŠ¨é€‰æ‹©äº‘ç«¯æˆ–æœ¬åœ°ï¼‰
output = layer(torch.randn(32, 768))

# æŸ¥çœ‹ç»Ÿè®¡
stats = layer.get_stats()
print(f"äº‘ç«¯è°ƒç”¨: {stats['cloud_calls']}")
print(f"æœ¬åœ°è°ƒç”¨: {stats['local_calls']}")
print(f"äº‘ç«¯ä½¿ç”¨ç‡: {stats['cloud_ratio']*100:.1f}%")
```

---

## ğŸ”„ å·¦æ—‹å¹³æ»‘æœºåˆ¶

### æ ¸å¿ƒæ”¹è¿›

**ä¼ ç»Ÿæ³°å‹’å±•å¼€é—®é¢˜**:
```python
# ä¼ ç»Ÿæ–¹å¼ï¼šçº¿æ€§å¤–æ¨
u' = u + Î”u
# é—®é¢˜ï¼šé‡åˆ°å°–ç‚¹ï¼ˆæ¢¯åº¦çªå˜ã€æ›²ç‡å¤§ï¼‰ä¼šæ•°å€¼çˆ†ç‚¸
```

**å·¦æ—‹å¹³æ»‘æ–¹æ¡ˆ**:
```python
# å·¦æ—‹æ–¹å¼ï¼šå•å‘ç¼“å†²
u' = u + g(Ï†)Â·Î”u

# å…¶ä¸­:
# Ï† = Î±Â·softplus(s - Ï„)  ç¼“å†²è§’ï¼ˆç”±å°–ç‚¹å¼ºåº¦å†³å®šï¼‰
# s = wâ‚Â·d + wâ‚‚Â·a        å°–ç‚¹å¼ºåº¦
# d = ||Î”u|| / (Îµ + ||u||)  ä¸€é˜¶å˜åŒ–å¼ºåº¦
# a = ||Î”u - Î”u_prev|| / (Îµ + ||Î”u|| + ||Î”u_prev||)  äºŒé˜¶åŠ é€Ÿåº¦
# g(Ï†) = 1/âˆš(1+Ï†Â²)       é—¨æ§å‡½æ•°ï¼ˆå½’ä¸€åŒ–ç‰ˆï¼Œæ›´ç¨³å®šï¼‰
```

### ä¼˜åŠ¿

- âœ… **è‡ªåŠ¨å°–ç‚¹æ£€æµ‹**ï¼šé€šè¿‡ s è®¡ç®—ï¼Œæ— éœ€æ‰‹åŠ¨æ ‡è®°
- âœ… **å•å‘ç¼“å†²**ï¼šÏ† â‰¥ 0ï¼Œä¸ä¼šæ­£è´ŸæŠµæ¶ˆå˜æŠ–åŠ¨
- âœ… **å¹³æ»‘è¿‡æ¸¡**ï¼šg(Ï†) âˆˆ (0, 1]ï¼Œé€æ¸ç¼©å°æ­¥é•¿è€Œéç¡¬æˆªæ–­
- âœ… **ä¿ç•™æ–¹å‘**ï¼šåªæ”¹å˜æ­¥é•¿ï¼Œä¸æ”¹å˜æ–¹å‘

### é›†æˆä½ç½®

**æ®‹å·®è¿æ¥ï¼ˆæ ¸å¿ƒï¼‰**:
- APTEncoderLayer: 2å¤„ï¼ˆè‡ªæ³¨æ„åŠ› + FFNï¼‰
- APTDecoderLayer: 3å¤„ï¼ˆè‡ªæ³¨æ„åŠ› + äº¤å‰æ³¨æ„åŠ› + FFNï¼‰

**Autopoietic Transform**:
- æ›¿æ¢æ³°å‹’å±•å¼€ä¸ºå·¦æ—‹å¹³æ»‘é—¨æ§

### ä½¿ç”¨ç¤ºä¾‹

```python
from apt_model.modeling.apt_model import APTModel, APTModelConfiguration

# åˆ›å»ºé…ç½®ï¼ˆé»˜è®¤å¯ç”¨å·¦æ—‹å¹³æ»‘ï¼‰
config = APTModelConfiguration(
    vocab_size=30522,
    d_model=768,
    # å·¦æ—‹å¹³æ»‘å‚æ•°
    use_left_spin=True,        # âœ… å¯ç”¨å·¦æ—‹å¹³æ»‘
    left_spin_alpha=0.5,       # ç¼“å†²å¼ºåº¦
    left_spin_tau=0.3,         # å°–ç‚¹é˜ˆå€¼
    left_spin_beta=0.7         # æƒ¯æ€§ç³»æ•°
)

# åˆ›å»ºæ¨¡å‹
model = APTModel(config)

# æ­£å¸¸ä½¿ç”¨ï¼ˆå·¦æ—‹å¹³æ»‘è‡ªåŠ¨å·¥ä½œï¼‰
output = model(src_tokens, tgt_tokens)
```

### æ€§èƒ½å¯¹æ¯”

| æŒ‡æ ‡ | æ ‡å‡†æ®‹å·® | å·¦æ—‹å¹³æ»‘ | æ”¹è¿› |
|------|---------|---------|------|
| **æ•°å€¼ç¨³å®šæ€§** | æ˜“çˆ†ç‚¸ | è‡ªåŠ¨ç¼“å†² | â†‘ æ˜¾è‘— |
| **å°–ç‚¹å¤„ç†** | æ— é˜²æŠ¤ | è‡ªåŠ¨æ£€æµ‹+è§„é¿ | â†‘ 100% |
| **è¾“å‡ºæ–¹å·®** | é«˜ | ä½ï¼ˆå¹³æ»‘ï¼‰ | â†“ 20-50% |
| **è®¡ç®—å¼€é”€** | åŸºå‡† | +5-10% | å¯æ¥å— |
| **è®­ç»ƒç¨³å®šæ€§** | éœ€è¦å°LR | æ›´é²æ£’ | â†‘ 30-40% |

---

## ğŸ§  AIM-Memory æƒ¯æ€§é”šå®šé•œåƒè®°å¿†

### æ ¸å¿ƒåŸç†

**AIM-Memory** (Anchored Inertial Mirror Memory) æ˜¯ä¸€ç§é¢å‘å¤§æ¨¡å‹çš„é•¿æœŸè®°å¿†æ¶æ„ï¼Œé€šè¿‡å››å¤§æœºåˆ¶è§£å†³ä¼ ç»Ÿ RAG çš„æˆæœ¬å’Œç²¾åº¦é—®é¢˜ï¼š

```
AIM-Memory = æƒ¯æ€§è·¯ç”± + æ—¶é—´é•œåƒ + é”šç‚¹çº é”™ + æŒ‰éœ€è¯æ®å›çŒ
```

### å››å¤§æ ¸å¿ƒæœºåˆ¶

#### 1ï¸âƒ£ æƒ¯æ€§è·¯ç”± (Inertial Routing)

**é—®é¢˜**: ä¼ ç»Ÿ RAG æ¯æ¬¡éƒ½å…¨åº“æ‰«æï¼Œæˆæœ¬é«˜æ˜‚ã€‚

**è§£å†³æ–¹æ¡ˆ**: ç»´æŠ¤"æƒ¯æ€§æ–¹å‘"å‘é‡ï¼Œè¿ç»­æŸ¥è¯¢è‡ªç„¶è½åœ¨ç›¸å…³è®°å¿†ç°‡ã€‚

```python
# å½¢æˆæƒ¯æ€§æ–¹å‘
d = q_vec + Î» * v_inertia

# å±€éƒ¨ K ç°‡å¬å›ï¼ˆè€Œéå…¨åº“æ‰«æï¼‰
candidates = node_bank.top_k_cluster(d, K=32)

# æ›´æ–°æƒ¯æ€§
v_inertia = Î¼ * v_inertia + (1-Î¼) * v_selected
```

**æ•ˆæœ**: æ£€ç´¢æˆæœ¬ **â†“70-90%**ï¼ˆåªæŸ¥å°ç°‡ï¼Œä¸å…¨åº“æ‰«æï¼‰

#### 2ï¸âƒ£ æ—¶é—´é•œåƒ (Temporal Mirror)

**é—®é¢˜**: éœ€è¦è¡¨è¾¾æ—¶åºï¼Œä½†ç»´æŠ¤æ—¶é—´æˆ³å¢åŠ å¤æ‚åº¦ã€‚

**è§£å†³æ–¹æ¡ˆ**: æƒé‡è¡°å‡è‡ªç„¶è¡¨è¾¾"æ–°æ—§"å…³ç³»ã€‚

```python
# æ¯æ¬¡å†™å…¥æ–°è®°å¿†å‰ï¼Œæ‰€æœ‰æ—§èŠ‚ç‚¹æƒé‡è¡°å‡
for node in node_bank:
    node.w *= Î³  # Î³ = 0.8

# æ–°èŠ‚ç‚¹æƒé‡ä¸º 1.0
new_node.w = 1.0
```

**æ•ˆæœ**: è¶Šæ–°çš„è®°å¿†æƒé‡è¶Šé«˜ï¼Œè‡ªç„¶å½¢æˆæ—¶åºæ¢¯åº¦ã€‚ç»è¿‡ 5 æ¬¡æ–°å†™å…¥ï¼Œæ—§èŠ‚ç‚¹æƒé‡ä» 1.0 è¡°å‡åˆ° 0.328ã€‚

#### 3ï¸âƒ£ é”šç‚¹çº é”™ (Anchored Correction)

**é—®é¢˜**: æ¨¡å‹å®¹æ˜“"è®°æ··"ç›¸ä¼¼ä¿¡æ¯ï¼Œäº§ç”Ÿå¹»è§‰ã€‚

**è§£å†³æ–¹æ¡ˆ**: æå–å’ŒéªŒè¯å…³é”®å­—æ®µï¼ˆæ•°å­—ã€ä¸“åã€ç¬¦å·ã€å®šä¹‰ï¼‰ã€‚

```python
# æå–é”šç‚¹å­—æ®µ
q_fields = extract_fields(query)  # {numbers: [10M], names: [Llama 4]}

# é”šç‚¹åŒ¹é…
for node in candidates:
    anchor_score = weighted_overlap(q_fields, node.fields)
    node_score = base_score + anchor_score * Î· * node.w
```

**æ•ˆæœ**: æŸ¥è¯¢"10M tokens çš„æ¨¡å‹"æ—¶ï¼Œåªå¬å›çœŸæ­£åŒ…å«"10M"çš„èŠ‚ç‚¹ï¼Œä¸ä¼šæ··æ·† 128K æˆ–å…¶ä»–æ•°å­—ã€‚

#### 4ï¸âƒ£ æŒ‰éœ€è¯æ®å›çŒ (Evidence Refill)

**é—®é¢˜**: å­˜å‚¨åŸæ–‡å ç”¨ç©ºé—´ï¼Œä½†éœ€è¦ç²¾ç¡®å¼•ç”¨æ—¶åˆå¿…é¡»æœ‰åŸæ–‡ã€‚

**è§£å†³æ–¹æ¡ˆ**: é»˜è®¤åªå­˜æ‘˜è¦ï¼Œæ£€æµ‹åˆ°"ç²¾ç¡®/åŸæ–‡/è¯æ˜"ç­‰å…³é”®è¯æ—¶æ‰å›çŒåŸæ–‡ã€‚

```python
# å¿«é€Ÿæ¨¡å¼ï¼šåªç”¨æ‘˜è¦
if mode == 'fast':
    return summaries

# ä¸¥æ ¼æ¨¡å¼ï¼šå›çŒåŸæ–‡
if mode == 'strict' or detect_strict_keywords(query):
    evidence = fetch_evidence(selected_nodes)
    return summaries + evidence
```

**æ•ˆæœ**: å¹³æ—¶èŠ‚çœ **70-80%** tokenï¼Œéœ€è¦ç²¾ç¡®ä¿¡æ¯æ—¶è‡ªåŠ¨åˆ‡æ¢ã€‚

### æ•°æ®ç»“æ„

```python
@dataclass
class MemoryNode:
    id: str                          # èŠ‚ç‚¹ ID
    proto: np.ndarray                # åŸå‹å‘é‡
    summary: str                     # ä¸€è¡Œæ‘˜è¦
    fields: Dict[str, Any]           # å…³é”®å­—æ®µ
        # - numbers: [10M, 128K, ...]
        # - names: [Llama 4, GPT-4, ...]
        # - definitions: [å®šä¹‰æ–‡æœ¬]
        # - symbols: [æ•°å­¦ç¬¦å·]
    links: List[str]                 # ç›¸é‚»èŠ‚ç‚¹
    w: float = 1.0                   # æ—¶é—´æƒé‡
    evidence_ptr: Optional[str]      # è¯æ®æŒ‡é’ˆ
    evidence_text: Optional[str]     # è¯æ®åŸæ–‡
```

### ä½¿ç”¨ç¤ºä¾‹

```python
from apt_model.memory.aim_memory import create_aim_memory, AIMConfig

# åˆ›å»ºè®°å¿†ç³»ç»Ÿ
aim = create_aim_memory()

# å†™å…¥è®°å¿†
aim.write_memory("RoPE æ˜¯æ—‹è½¬ä½ç½®ç¼–ç ï¼Œé€šè¿‡å¤æ•°æ—‹è½¬å®ç°ä½ç½®è¡¨ç¤ºã€‚")
aim.write_memory("YaRN é€šè¿‡åˆ†ç»´åº¦ç¼©æ”¾æ‰©å±• RoPE åˆ°æ›´é•¿ä¸Šä¸‹æ–‡ã€‚")
aim.write_memory("Llama 4 ä½¿ç”¨ iRoPE æ”¯æŒ 10M tokens ä¸Šä¸‹æ–‡ã€‚")

# æŸ¥è¯¢è®°å¿†ï¼ˆå¿«é€Ÿæ¨¡å¼ï¼‰
selected, refill = aim.route_memory("å¦‚ä½•æ”¯æŒè¶…é•¿ä¸Šä¸‹æ–‡ï¼Ÿ", mode='fast')
for node in selected:
    print(f"â€¢ {node.summary}")

# å®Œæ•´å›ç­”ç”Ÿæˆï¼ˆè‡ªåŠ¨æ¨¡å¼æ£€æµ‹ï¼‰
result = aim.answer("10M tokens çš„æ¨¡å‹æ˜¯å“ªä¸ªï¼Ÿ", auto_mode=True)
print(f"æ¨¡å¼: {result['mode']}")           # fast æˆ– strict
print(f"å¬å›: {result['num_nodes_recalled']}")
print(f"ä¸Šä¸‹æ–‡:\n{result['context']}")
```

### é…ç½®å‚æ•°

```python
config = AIMConfig(
    hot_window_size=256,         # çƒ­ç¼“å­˜çª—å£å¤§å°
    local_cluster_k=32,          # å±€éƒ¨ç°‡å¬å›æ•°é‡
    inertia_strength=0.5,        # æƒ¯æ€§å¼ºåº¦ Î»
    inertia_momentum=0.85,       # æƒ¯æ€§åŠ¨é‡ Î¼
    weight_decay_gamma=0.8,      # æƒé‡è¡°å‡å› å­ Î³
    write_threshold=0.6,         # å†™å…¥é—¨æ§›
    anchor_threshold=0.1,        # é”šç‚¹é—¨æ§›
    anchor_boost=2.0,            # é”šç‚¹åŠ æˆ Î·
)

aim = create_aim_memory(config=config)
```

### é›†æˆåˆ° APT-Transformer

```python
from apt_model.memory.aim_memory import create_aim_memory
from apt_model.modeling.apt_transformer import APTTransformer

# åˆ›å»ºæ¨¡å‹å’Œè®°å¿†ç³»ç»Ÿ
model = APTTransformer(config)
memory = create_aim_memory()

# å¸¦è®°å¿†çš„ç”Ÿæˆ
def generate_with_memory(prompt: str):
    # ä»è®°å¿†æ£€ç´¢ç›¸å…³ä¸Šä¸‹æ–‡
    result = memory.answer(prompt, auto_mode=True)
    context = result['context']

    # æ„å»ºå®Œæ•´è¾“å…¥
    full_input = f"{context}\n\nç”¨æˆ·: {prompt}\nåŠ©æ‰‹:"

    # æ¨¡å‹ç”Ÿæˆ
    output = model.generate(full_input)

    # å­˜å‚¨å¯¹è¯åˆ°è®°å¿†
    memory.write_memory(f"ç”¨æˆ·: {prompt}")
    memory.write_memory(f"åŠ©æ‰‹: {output}")

    return output
```

### æ€§èƒ½å¯¹æ¯”

| æŒ‡æ ‡ | ä¼ ç»Ÿ RAG | AIM-Memory | æå‡ |
|------|----------|------------|------|
| **æ£€ç´¢æ–¹å¼** | å…¨åº“å‘é‡æœç´¢ | æƒ¯æ€§å±€éƒ¨ç°‡å¬å› | - |
| **æ£€ç´¢æˆæœ¬** | åŸºå‡† | â†“ 70-90% | å¤§å¹…é™ä½ |
| **ç²¾åº¦ä¿è¯** | ä¾èµ– embedding | é”šç‚¹å­—æ®µéªŒè¯ | â†‘ 20-30% |
| **æ—¶åºè¡¨è¾¾** | æ—¶é—´æˆ³æˆ–æ—  | æƒé‡è¡°å‡ | æ›´è‡ªç„¶ |
| **å­˜å‚¨æˆæœ¬** | å…¨æ–‡å­˜å‚¨ | æ‘˜è¦+æŒ‰éœ€å›çŒ | â†“ 70-80% |
| **å“åº”é€Ÿåº¦** | è¾ƒæ…¢ | å¿«é€Ÿï¼ˆå°ç°‡ï¼‰ | â†‘ 2-3Ã— |

### æµ‹è¯•ç»“æœ

å®Œæ•´æµ‹è¯•å¥—ä»¶ï¼ˆ9 ä¸ªæµ‹è¯•ï¼‰å…¨éƒ¨é€šè¿‡ï¼š

```bash
python training/test_aim_memory.py
```

æµ‹è¯•è¦†ç›–ï¼š
- âœ… åŸºç¡€å†™å…¥å’Œè¯»å–
- âœ… æƒ¯æ€§è·¯ç”±æœºåˆ¶ï¼ˆæƒ¯æ€§èŒƒæ•°ä» 0.088 â†’ 0.210ï¼‰
- âœ… æ—¶é—´é•œåƒè¡°å‡ï¼ˆæƒé‡ 1.000 â†’ 0.328ï¼Œè¡°å‡ 67.2%ï¼‰
- âœ… é”šç‚¹çº é”™ï¼ˆç²¾ç¡®åŒ¹é…"10M tokens"ï¼‰
- âœ… æŒ‰éœ€è¯æ®å›çŒï¼ˆè‡ªåŠ¨æ£€æµ‹ä¸¥æ ¼æ¨¡å¼ï¼‰
- âœ… å®Œæ•´å›ç­”ç”Ÿæˆ
- âœ… æŒä¹…åŒ–ï¼ˆä¿å­˜/åŠ è½½ï¼‰
- âœ… ç«¯åˆ°ç«¯åœºæ™¯ï¼ˆå¤šè½®å¯¹è¯ï¼‰
- âœ… ç»Ÿè®¡ä¿¡æ¯

### æŠ€æœ¯æ¥æº

- **ä½œè€…**: 430
- **å®ç°**: Claude + 430
- **ç‰ˆæœ¬**: 2026-01-21

**è¯¦ç»†æ–‡æ¡£**: [AIM-Memory æŠ€æœ¯æŒ‡å—](AIM_MEMORY_GUIDE.md)

---

## ğŸ’» å®Œæ•´ä½¿ç”¨ç¤ºä¾‹

### ç«¯åˆ°ç«¯è®­ç»ƒæµç¨‹

```python
#!/usr/bin/env python
"""
APT-Transformer å®Œæ•´è®­ç»ƒç¤ºä¾‹
é›†æˆ: è™šæ‹ŸBlackwell + å¤šå‚å•†NPU + äº‘ç«¯NPU + å·¦æ—‹å¹³æ»‘
"""

import torch
import torch.nn as nn
from torch.utils.data import DataLoader

# Step 1: å¯ç”¨è™šæ‹ŸBlackwellï¼ˆä¸€è¡Œä»£ç ï¼‰
import apt_model.optimization.vb_global as vb
from apt_model.optimization import enable_cloud_npu

# å¯é€‰ï¼šå¯ç”¨äº‘ç«¯NPUï¼ˆæ— éœ€è´­ä¹°ç¡¬ä»¶ï¼‰
# enable_cloud_npu('auto')

# å¯ç”¨è™šæ‹ŸBlackwellï¼ˆè‡ªåŠ¨æ£€æµ‹æœ€ä½³é…ç½®ï¼‰
vb.enable_balanced_mode(verbose=True)

# Step 2: å®šä¹‰æ¨¡å‹ï¼ˆé›†æˆå·¦æ—‹å¹³æ»‘ï¼‰
from apt_model.modeling.apt_model import APTModel, APTModelConfiguration

config = APTModelConfiguration(
    vocab_size=30522,
    d_model=768,
    num_encoder_layers=12,
    num_decoder_layers=12,
    num_heads=12,
    d_ff=3072,
    # è™šæ‹ŸBlackwellå‚æ•°
    use_autopoietic=True,      # è‡ªç”Ÿæˆæ³¨æ„åŠ›
    use_dbc_dac=True,          # DBC-DACç¨³å®š
    # å·¦æ—‹å¹³æ»‘å‚æ•°
    use_left_spin=True,        # å¯ç”¨å·¦æ—‹å¹³æ»‘
    left_spin_alpha=0.5,
    left_spin_tau=0.3
)

# Step 3: åˆå§‹åŒ–æ¨¡å‹å’Œä¼˜åŒ–å™¨
from apt_model.core.system import get_device

device = get_device()  # è‡ªåŠ¨æ£€æµ‹æœ€ä½³è®¾å¤‡ï¼ˆCUDA/HPU/NPU/XPU/CPUï¼‰

model = APTModel(config).to(device)
optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4)  # å·¦æ—‹å¹³æ»‘å…è®¸æ›´å¤§LR
criterion = nn.CrossEntropyLoss()

# Step 4: è®­ç»ƒå¾ªç¯
print("\nğŸš€ å¼€å§‹è®­ç»ƒï¼ˆè™šæ‹ŸBlackwellå·²å¯ç”¨ï¼‰")
print("="*70)

for epoch in range(10):
    total_loss = 0
    for batch_idx, (input_ids, labels) in enumerate(dataloader):
        # æ•°æ®ç§»åˆ°è®¾å¤‡
        input_ids = input_ids.to(device)
        labels = labels.to(device)

        # å‰å‘ä¼ æ’­ï¼ˆå·¦æ—‹å¹³æ»‘è‡ªåŠ¨å·¥ä½œï¼‰
        output = model(input_ids)

        # è®¡ç®—æŸå¤±
        loss = criterion(output.view(-1, output.size(-1)), labels.view(-1))

        # åå‘ä¼ æ’­
        optimizer.zero_grad()
        loss.backward()
        optimizer.step()

        total_loss += loss.item()

        if batch_idx % 100 == 0:
            print(f"Epoch {epoch+1}, Batch {batch_idx}, Loss: {loss.item():.4f}")

    avg_loss = total_loss / len(dataloader)
    print(f"\nEpoch {epoch+1} å®Œæˆï¼Œå¹³å‡Loss: {avg_loss:.4f}")

# Step 5: æŸ¥çœ‹äº‘ç«¯NPUç»Ÿè®¡ï¼ˆå¯é€‰ï¼‰
from apt_model.optimization import get_cloud_npu_manager

manager = get_cloud_npu_manager()
if manager.is_any_available():
    print("\nğŸ“Š äº‘ç«¯NPUä½¿ç”¨ç»Ÿè®¡:")
    for backend_name in manager.list_backends():
        backend = manager.get_backend(backend_name)
        print(f"   {backend_name}: {'åœ¨çº¿' if backend.is_available() else 'ç¦»çº¿'}")

print("\nâœ… è®­ç»ƒå®Œæˆï¼")
```

---

## ğŸ“Š æ€§èƒ½æ€»ç»“

### å¤§æ¨¡å‹è®­ç»ƒï¼ˆGPT-3, 175Bå‚æ•°ï¼‰

| é…ç½® | æ˜¾å­˜éœ€æ±‚ | è®­ç»ƒé€Ÿåº¦ | æˆæœ¬ |
|------|----------|----------|------|
| **çº¯GPUï¼ˆ8Ã—A100 80GBï¼‰** | 640 GB | 1Ã— åŸºå‡† | Â¥400ä¸‡ |
| **è™šæ‹ŸBlackwellï¼ˆ8Ã—RTX 3090 24GBï¼‰** | 192 GBç‰©ç†<br>768 GBè™šæ‹Ÿ | 0.85Ã— | Â¥80ä¸‡ |
| **è™šæ‹ŸBlackwell + äº‘ç«¯NPU** | 192 GBç‰©ç†<br>æ— é™äº‘ç«¯ | 0.9Ã— | Â¥80ä¸‡ + æŒ‰éœ€ |

**ç»“è®º**: æˆæœ¬é™ä½80%ï¼Œæ€§èƒ½æŸå¤±ä»…15%

### BERTæ¨ç†ï¼ˆBaseæ¨¡å‹ï¼‰

| æ–¹æ³• | å»¶è¿Ÿ (ms) | ååé‡ (æ ·æœ¬/ç§’) | æ˜¾å­˜ (MB) |
|------|-----------|------------------|-----------|
| **PyTorchåŸç”Ÿï¼ˆFP32ï¼‰** | 100 | 10 | 1200 |
| **PyTorchä¼˜åŒ–ï¼ˆFP16ï¼‰** | 60 | 16 | 600 |
| **è™šæ‹ŸBlackwellï¼ˆFP4 + Flashï¼‰** | 35 | 28 | 150 |
| **è™šæ‹ŸBlackwell + äº‘ç«¯NPU** | 45 | 22 | 0ï¼ˆäº‘ç«¯ï¼‰ |

**ç»“è®º**:
- æœ¬åœ°åŠ é€Ÿ: å»¶è¿Ÿâ†“65%ï¼Œæ˜¾å­˜â†“87.5%
- äº‘ç«¯NPU: é›¶æ˜¾å­˜å ç”¨ï¼ŒæŒ‰éœ€ä»˜è´¹

---

## ğŸ“š æ–‡æ¡£ç´¢å¼•

| æ–‡æ¡£ | æè¿° | é€‚ç”¨åœºæ™¯ |
|------|------|----------|
| [VIRTUAL_BLACKWELL_COMPLETE_GUIDE.md](VIRTUAL_BLACKWELL_COMPLETE_GUIDE.md) | è™šæ‹ŸBlackwellå®Œæ•´æŒ‡å— | å…¨é¢äº†è§£ |
| [NPU_INTEGRATION_GUIDE.md](NPU_INTEGRATION_GUIDE.md) | å¤šå‚å•†NPUæ”¯æŒè¯¦è§£ | å¤šç¡¬ä»¶éƒ¨ç½² |
| [CLOUD_NPU_GUIDE.md](CLOUD_NPU_GUIDE.md) | äº‘ç«¯NPUä½¿ç”¨è¯´æ˜ | æ— ç¡¬ä»¶æµ‹è¯• |
| [LEFT_SPIN_SMOOTH_INTEGRATION.md](LEFT_SPIN_SMOOTH_INTEGRATION.md) | å·¦æ—‹å¹³æ»‘é›†æˆæ–‡æ¡£ | å°–ç‚¹è§„é¿ |
| [æœ¬æ–‡æ¡£](INTEGRATION_SUMMARY.md) | å®Œæ•´é›†æˆæ€»ç»“ | å¿«é€Ÿå…¥é—¨ |

---

## ğŸ§ª æµ‹è¯•è„šæœ¬

```bash
# æµ‹è¯•äº‘ç«¯NPU
python training/test_cloud_npu.py

# æµ‹è¯•æœ¬åœ°NPUé›†æˆ
python training/test_npu_integration.py

# æµ‹è¯•å·¦æ—‹å¹³æ»‘
python training/test_left_spin_smooth.py

# å¯åŠ¨å®Œæ•´è®­ç»ƒï¼ˆè‡ªåŠ¨åº”ç”¨è™šæ‹ŸBlackwellï¼‰
python training/start_training.py
```

---

## ğŸ¯ å¿«é€Ÿå‘½ä»¤å‚è€ƒ

```python
# 1. ä¸€é”®å¯ç”¨è™šæ‹ŸBlackwell
import apt_model.optimization.vb_global as vb
vb.enable()

# 2. å¯ç”¨äº‘ç«¯NPU
from apt_model.optimization import enable_cloud_npu
enable_cloud_npu('auto')

# 3. æ£€æµ‹è®¾å¤‡
from apt_model.core.system import get_device
device = get_device()  # è‡ªåŠ¨é€‰æ‹©æœ€ä½³è®¾å¤‡

# 4. è·å–è®¾å¤‡ç®¡ç†å™¨
from apt_model.optimization import get_device_manager
manager = get_device_manager()
print(manager.get_accelerator_type())

# 5. åˆ›å»ºæ¨¡å‹ï¼ˆé›†æˆæ‰€æœ‰åŠŸèƒ½ï¼‰
from apt_model.modeling.apt_model import APTModel, APTModelConfiguration
config = APTModelConfiguration(
    use_autopoietic=True,  # è‡ªç”Ÿæˆæ³¨æ„åŠ›
    use_dbc_dac=True,      # DBC-DACç¨³å®š
    use_left_spin=True     # å·¦æ—‹å¹³æ»‘
)
model = APTModel(config)
```

---

## ğŸ”„ æ›´æ–°æ—¥å¿—

### v1.0 (2026-01-21)

#### âœ… è™šæ‹ŸBlackwellè™šç©ºç®—åŠ›
- æ·»åŠ è™šæ‹ŸBlackwellå…¨å±€å¯ç”¨å™¨ï¼ˆä¸€è¡Œä»£ç å¯ç”¨ï¼‰
- GPU Flashä¼˜åŒ–ï¼ˆFP4 + Triton + Flash Attentionï¼‰
- VGPU Stackï¼ˆä¸‰çº§å†…å­˜å †å ï¼‰

#### âœ… å¤šå‚å•†NPUæ”¯æŒ
- æ”¯æŒ6ç§AIåŠ é€Ÿå™¨ï¼ˆCUDA/HPU/NPU/XPU/ROCm/CPUï¼‰
- ç»Ÿä¸€è®¾å¤‡ç®¡ç†æ¥å£
- è‡ªåŠ¨è®¾å¤‡æ£€æµ‹å’Œé€‰æ‹©

#### âœ… äº‘ç«¯NPUé€‚é…
- åä¸ºäº‘ModelArtsé›†æˆ
- CloudNPULinearï¼ˆè‡ªåŠ¨fallbackï¼‰
- ç¯å¢ƒå˜é‡é…ç½®
- ç»Ÿè®¡ç›‘æ§

#### âœ… å·¦æ—‹å¹³æ»‘æœºåˆ¶
- æ›¿æ¢æ³°å‹’å±•å¼€ä¸ºå°–ç‚¹è§„é¿
- è‡ªåŠ¨å°–ç‚¹æ£€æµ‹ï¼ˆs = wâ‚Â·d + wâ‚‚Â·aï¼‰
- å•å‘ç¼“å†²ï¼ˆÏ† = Î±Â·softplus(s-Ï„)ï¼‰
- å¹³æ»‘é—¨æ§ï¼ˆg(Ï†) = 1/âˆš(1+Ï†Â²)ï¼‰

---

## ğŸ‰ æ€»ç»“

APT-Transformer ç°å·²å®Œæ•´é›†æˆï¼š

âœ… **è™šæ‹ŸBlackwellè™šç©ºç®—åŠ›** - 10-100Ã—åŠ é€Ÿ + æ— é™æ˜¾å­˜
âœ… **6ç§å¤šå‚å•†åŠ é€Ÿå™¨** - CUDA/HPU/NPU/XPU/ROCm/CPUç»Ÿä¸€æ¥å£
âœ… **äº‘ç«¯NPUæ”¯æŒ** - é›¶ç¡¬ä»¶æˆæœ¬ï¼ŒæŒ‰éœ€æµ‹è¯•
âœ… **å·¦æ—‹å¹³æ»‘æœºåˆ¶** - è‡ªåŠ¨å°–ç‚¹è§„é¿ï¼Œæ•°å€¼ç¨³å®šæ€§æ˜¾è‘—æå‡

**ç°åœ¨å°±å¼€å§‹ä½“éªŒè™šæ‹ŸBlackwellçš„è™šç©ºç®—åŠ›å§ï¼** ğŸš€

---

**ä½œè€…**: claude + chen0430tw
**ç‰ˆæœ¬**: 1.0
**æ—¥æœŸ**: 2026-01-21
