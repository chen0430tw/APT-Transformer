# å›¾è„‘è®­ç»ƒæ•™ç¨‹ - Graph Reasoning Architecture

<div align="center">

**åŸºäºå›¾ç¥ç»ç½‘ç»œçš„ç»“æ„åŒ–æ¨ç†è®­ç»ƒ**

èåˆ Gemini æ€ç»´æ¨¡å¼ | å›¾ç»“æ„æ¨ç† | ç¥ç»ç¬¦å·æ¨ç†

</div>

---

## ğŸ“‹ ç›®å½•

- [ä»€ä¹ˆæ˜¯å›¾è„‘](#ä»€ä¹ˆæ˜¯å›¾è„‘)
- [æ¶æ„è®¾è®¡](#æ¶æ„è®¾è®¡)
- [æ ¸å¿ƒç»„ä»¶](#æ ¸å¿ƒç»„ä»¶)
- [è®­ç»ƒæµç¨‹](#è®­ç»ƒæµç¨‹)
- [é«˜çº§æŠ€å·§](#é«˜çº§æŠ€å·§)
- [å®æˆ˜ç¤ºä¾‹](#å®æˆ˜ç¤ºä¾‹)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)

---

## ğŸ§  ä»€ä¹ˆæ˜¯å›¾è„‘

### æ¦‚å¿µ

**å›¾è„‘ (Graph Brain)** æ˜¯ä¸€ç§ç»“åˆäº†å›¾ç¥ç»ç½‘ç»œï¼ˆGNNï¼‰å’Œè¯­è¨€æ¨¡å‹çš„æ··åˆæ¨ç†æ¶æ„ï¼Œçµæ„Ÿæ¥æºäºï¼š
- **Gemini 2.0 Flash Thinking** çš„æ˜¾å¼æ€ç»´è¿‡ç¨‹
- **ç¥ç»ç¬¦å·æ¨ç†** çš„ç»“æ„åŒ–è¡¨ç¤º
- **å›¾ç¥ç»ç½‘ç»œ** çš„å…³ç³»å»ºæ¨¡èƒ½åŠ›

### æ ¸å¿ƒæ€æƒ³

```
ä¼ ç»Ÿ Transformer:
æ–‡æœ¬ â†’ Embedding â†’ Attention â†’ è¾“å‡º

å›¾è„‘æ¶æ„:
æ–‡æœ¬ â†’ æ¦‚å¿µå›¾ â†’ å›¾ç¥ç»ç½‘ç»œ â†’ æ¨ç†è·¯å¾„ â†’ è¾“å‡º
       â†“
   ç»“æ„åŒ–æ€ç»´è¿‡ç¨‹
```

### ä¼˜åŠ¿å¯¹æ¯”

| ç‰¹æ€§ | ä¼ ç»Ÿ LLM | å›¾è„‘æ¶æ„ |
|------|---------|---------|
| **æ¨ç†å¯è§£é‡Šæ€§** | âŒ é»‘ç›’ | âœ… æ˜¾å¼å›¾ç»“æ„ |
| **å¤šè·³æ¨ç†** | âš ï¸ ä¾èµ–ä¸Šä¸‹æ–‡ | âœ… åŸç”Ÿæ”¯æŒ |
| **çŸ¥è¯†æ•´åˆ** | âš ï¸ éšå¼è®°å¿† | âœ… æ˜¾å¼çŸ¥è¯†å›¾è°± |
| **è®¡ç®—æ•ˆç‡** | âš ï¸ å…¨åºåˆ—æ³¨æ„åŠ› | âœ… ç¨€ç–å›¾è®¡ç®— |
| **å¯æ§æ€§** | âŒ éš¾ä»¥å¹²é¢„ | âœ… å¯ç¼–è¾‘å›¾ç»“æ„ |

---

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### å®Œæ•´æ¶æ„å›¾

```
è¾“å…¥æ–‡æœ¬
    â†“
[1] æ¦‚å¿µæŠ½å–å™¨ (Concept Extractor)
    â”œâ”€â”€ å®ä½“è¯†åˆ«ï¼ˆNERï¼‰
    â”œâ”€â”€ å…³ç³»æŠ½å–ï¼ˆREï¼‰
    â””â”€â”€ äº‹ä»¶æ£€æµ‹
    â†“
[2] æ¦‚å¿µå›¾æ„å»º (Concept Graph Builder)
    â”œâ”€â”€ èŠ‚ç‚¹ï¼šæ¦‚å¿µ/å®ä½“
    â”œâ”€â”€ è¾¹ï¼šå…³ç³»/ä¾èµ–
    â””â”€â”€ å±æ€§ï¼šç±»å‹/æƒé‡
    â†“
[3] å›¾ç¥ç»ç¼–ç å™¨ (Graph Neural Encoder)
    â”œâ”€â”€ å›¾å·ç§¯å±‚ï¼ˆGCN/GAT/GraphSAGEï¼‰
    â”œâ”€â”€ æ¶ˆæ¯ä¼ é€’
    â””â”€â”€ èŠ‚ç‚¹æ›´æ–°
    â†“
[4] æ¨ç†è·¯å¾„è§„åˆ’ (Reasoning Path Planner)
    â”œâ”€â”€ æ³¨æ„åŠ›è·¯ç”±
    â”œâ”€â”€ å¤šè·³æ¨ç†
    â””â”€â”€ å­å›¾é‡‡æ ·
    â†“
[5] è§£ç å™¨ (Decoder)
    â”œâ”€â”€ å›¾åˆ°åºåˆ—ï¼ˆGraph2Seqï¼‰
    â”œâ”€â”€ æ€ç»´é“¾ç”Ÿæˆ
    â””â”€â”€ æœ€ç»ˆç­”æ¡ˆ
    â†“
è¾“å‡ºï¼ˆç­”æ¡ˆ + æ¨ç†è¿‡ç¨‹ï¼‰
```

---

## ğŸ”§ æ ¸å¿ƒç»„ä»¶

### 1. æ¦‚å¿µæŠ½å–å™¨

```python
import torch
import torch.nn as nn
from transformers import BertModel, BertTokenizer

class ConceptExtractor(nn.Module):
    """
    æ¦‚å¿µæŠ½å–å™¨ï¼šä»æ–‡æœ¬ä¸­æå–å®ä½“å’Œå…³ç³»

    æ–¹æ³•ï¼š
    - å®ä½“è¯†åˆ«ï¼šBERT + CRF
    - å…³ç³»æŠ½å–ï¼šåŒå‘ LSTM + æ³¨æ„åŠ›
    """
    def __init__(self, bert_model='bert-base-uncased', num_entity_types=10):
        super().__init__()
        self.bert = BertModel.from_pretrained(bert_model)
        self.entity_classifier = nn.Linear(768, num_entity_types)
        self.relation_classifier = nn.Bilinear(768, 768, 20)  # 20ç§å…³ç³»ç±»å‹

    def forward(self, input_ids, attention_mask):
        # BERT ç¼–ç 
        outputs = self.bert(input_ids, attention_mask=attention_mask)
        sequence_output = outputs.last_hidden_state  # [B, T, 768]

        # å®ä½“è¯†åˆ«
        entity_logits = self.entity_classifier(sequence_output)  # [B, T, num_types]

        # å…³ç³»æŠ½å–ï¼ˆå®ä½“å¯¹ä¹‹é—´ï¼‰
        # ç®€åŒ–ï¼šå–å¥å­çš„ [CLS] è¡¨ç¤º
        cls_repr = sequence_output[:, 0, :]  # [B, 768]

        return {
            'entity_logits': entity_logits,
            'cls_repr': cls_repr
        }

    def extract_concepts(self, text, tokenizer):
        """
        ä»æ–‡æœ¬ä¸­æå–æ¦‚å¿µå›¾

        Returns:
            nodes: List[Dict] - èŠ‚ç‚¹åˆ—è¡¨
            edges: List[Tuple] - è¾¹åˆ—è¡¨ (src, rel, dst)
        """
        # åˆ†è¯
        inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True)

        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            outputs = self.forward(inputs['input_ids'], inputs['attention_mask'])
            entity_logits = outputs['entity_logits']

        # è§£ç å®ä½“
        entity_predictions = torch.argmax(entity_logits, dim=-1)  # [B, T]

        # æ„å»ºèŠ‚ç‚¹å’Œè¾¹ï¼ˆç®€åŒ–ç‰ˆï¼‰
        nodes = []
        edges = []

        tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])

        for i, (token, entity_type) in enumerate(zip(tokens, entity_predictions[0])):
            if entity_type != 0:  # 0 = éå®ä½“
                nodes.append({
                    'id': i,
                    'token': token,
                    'type': entity_type.item()
                })

        # æå–å…³ç³»ï¼ˆç®€åŒ–ï¼šç›¸é‚»å®ä½“ï¼‰
        for i in range(len(nodes) - 1):
            edges.append((nodes[i]['id'], 'next', nodes[i+1]['id']))

        return nodes, edges
```

---

### 2. å›¾ç¥ç»ç¼–ç å™¨

```python
import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.nn import GCNConv, GATConv, global_mean_pool

class GraphBrainEncoder(nn.Module):
    """
    å›¾è„‘ç¼–ç å™¨ï¼šä½¿ç”¨å›¾ç¥ç»ç½‘ç»œç¼–ç æ¦‚å¿µå›¾

    æ”¯æŒï¼š
    - GCNï¼ˆå›¾å·ç§¯ç½‘ç»œï¼‰
    - GATï¼ˆå›¾æ³¨æ„åŠ›ç½‘ç»œï¼‰
    - GraphSAGEï¼ˆå›¾é‡‡æ ·èšåˆï¼‰
    """
    def __init__(
        self,
        node_dim=768,
        hidden_dim=512,
        num_layers=3,
        num_heads=8,
        dropout=0.1,
        gnn_type='gat'
    ):
        super().__init__()
        self.gnn_type = gnn_type
        self.num_layers = num_layers

        # èŠ‚ç‚¹ç‰¹å¾æŠ•å½±
        self.node_projection = nn.Linear(node_dim, hidden_dim)

        # å›¾å·ç§¯å±‚
        if gnn_type == 'gcn':
            self.convs = nn.ModuleList([
                GCNConv(hidden_dim, hidden_dim)
                for _ in range(num_layers)
            ])
        elif gnn_type == 'gat':
            self.convs = nn.ModuleList([
                GATConv(
                    hidden_dim,
                    hidden_dim // num_heads,
                    heads=num_heads,
                    dropout=dropout,
                    concat=True
                )
                for _ in range(num_layers)
            ])
        else:
            raise ValueError(f"Unknown GNN type: {gnn_type}")

        # å±‚å½’ä¸€åŒ–
        self.norms = nn.ModuleList([
            nn.LayerNorm(hidden_dim)
            for _ in range(num_layers)
        ])

        self.dropout = nn.Dropout(dropout)

    def forward(self, x, edge_index, edge_attr=None, batch=None):
        """
        Args:
            x: èŠ‚ç‚¹ç‰¹å¾ [num_nodes, node_dim]
            edge_index: è¾¹ç´¢å¼• [2, num_edges]
            edge_attr: è¾¹å±æ€§ [num_edges, edge_dim] (å¯é€‰)
            batch: æ‰¹æ¬¡ç´¢å¼• [num_nodes] (ç”¨äºæ‰¹å¤„ç†)

        Returns:
            node_embeddings: èŠ‚ç‚¹åµŒå…¥ [num_nodes, hidden_dim]
            graph_embedding: å›¾åµŒå…¥ [batch_size, hidden_dim]
        """
        # æŠ•å½±èŠ‚ç‚¹ç‰¹å¾
        x = self.node_projection(x)  # [num_nodes, hidden_dim]

        # å¤šå±‚å›¾å·ç§¯
        for i, (conv, norm) in enumerate(zip(self.convs, self.norms)):
            # æ®‹å·®è¿æ¥
            residual = x

            # å›¾å·ç§¯
            if self.gnn_type == 'gcn':
                x = conv(x, edge_index)
            elif self.gnn_type == 'gat':
                x = conv(x, edge_index)

            # å½’ä¸€åŒ– + æ¿€æ´» + Dropout
            x = norm(x + residual)
            x = F.relu(x)
            x = self.dropout(x)

        # å›¾çº§åˆ«æ± åŒ–ï¼ˆç”¨äºç”Ÿæˆå›¾åµŒå…¥ï¼‰
        if batch is not None:
            graph_embedding = global_mean_pool(x, batch)
        else:
            graph_embedding = x.mean(dim=0, keepdim=True)

        return x, graph_embedding
```

---

### 3. æ¨ç†è·¯å¾„è§„åˆ’å™¨

```python
class ReasoningPathPlanner(nn.Module):
    """
    æ¨ç†è·¯å¾„è§„åˆ’å™¨ï¼šåœ¨å›¾ä¸Šè§„åˆ’å¤šè·³æ¨ç†è·¯å¾„

    æ–¹æ³•ï¼š
    - æ³¨æ„åŠ›è·¯ç”±ï¼šå­¦ä¹ èŠ‚ç‚¹é‡è¦æ€§
    - å¤šè·³æ¨ç†ï¼šk-hop å­å›¾é‡‡æ ·
    - è·¯å¾„é€‰æ‹©ï¼šBeam Search + å›¾æ³¨æ„åŠ›
    """
    def __init__(self, hidden_dim=512, num_hops=3, num_beams=5):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.num_hops = num_hops
        self.num_beams = num_beams

        # èŠ‚ç‚¹é‡è¦æ€§è¯„åˆ†å™¨
        self.importance_scorer = nn.Sequential(
            nn.Linear(hidden_dim, hidden_dim // 2),
            nn.ReLU(),
            nn.Linear(hidden_dim // 2, 1)
        )

        # è·¯å¾„æ³¨æ„åŠ›
        self.path_attention = nn.MultiheadAttention(
            embed_dim=hidden_dim,
            num_heads=8,
            dropout=0.1
        )

        # è·¯å¾„ç¼–ç å™¨
        self.path_encoder = nn.LSTM(
            input_size=hidden_dim,
            hidden_size=hidden_dim,
            num_layers=2,
            batch_first=True,
            bidirectional=True
        )

    def forward(self, node_embeddings, edge_index, query_embedding):
        """
        Args:
            node_embeddings: èŠ‚ç‚¹åµŒå…¥ [num_nodes, hidden_dim]
            edge_index: è¾¹ç´¢å¼• [2, num_edges]
            query_embedding: æŸ¥è¯¢åµŒå…¥ [1, hidden_dim]

        Returns:
            reasoning_paths: List[List[int]] - æ¨ç†è·¯å¾„ï¼ˆèŠ‚ç‚¹IDåºåˆ—ï¼‰
            path_scores: è·¯å¾„åˆ†æ•°
        """
        num_nodes = node_embeddings.size(0)

        # 1. è®¡ç®—èŠ‚ç‚¹é‡è¦æ€§ï¼ˆç›¸å¯¹äºæŸ¥è¯¢ï¼‰
        query_expanded = query_embedding.expand(num_nodes, -1)  # [num_nodes, hidden_dim]
        combined = node_embeddings + query_expanded
        importance_scores = self.importance_scorer(combined).squeeze(-1)  # [num_nodes]

        # 2. é€‰æ‹©èµ·å§‹èŠ‚ç‚¹ï¼ˆTop-K æœ€é‡è¦çš„èŠ‚ç‚¹ï¼‰
        topk_scores, topk_indices = torch.topk(importance_scores, k=self.num_beams)

        # 3. Beam Search å¤šè·³æ¨ç†
        reasoning_paths = []
        path_scores = []

        # æ„å»ºé‚»æ¥åˆ—è¡¨ï¼ˆåŠ é€ŸæŸ¥æ‰¾ï¼‰
        adjacency = self._build_adjacency_list(edge_index, num_nodes)

        for start_node in topk_indices:
            # ä»æ¯ä¸ªèµ·å§‹èŠ‚ç‚¹å¼€å§‹æ¢ç´¢
            path = [start_node.item()]
            current_node = start_node.item()

            for hop in range(self.num_hops):
                # è·å–é‚»å±…èŠ‚ç‚¹
                neighbors = adjacency.get(current_node, [])
                if not neighbors:
                    break

                # è®¡ç®—é‚»å±…çš„æ³¨æ„åŠ›åˆ†æ•°
                neighbor_embeddings = node_embeddings[neighbors]  # [num_neighbors, hidden_dim]
                current_embedding = node_embeddings[current_node:current_node+1]  # [1, hidden_dim]

                # æ³¨æ„åŠ›è¯„åˆ†
                attn_output, attn_weights = self.path_attention(
                    query=current_embedding.unsqueeze(0),      # [1, 1, hidden_dim]
                    key=neighbor_embeddings.unsqueeze(0),      # [1, num_neighbors, hidden_dim]
                    value=neighbor_embeddings.unsqueeze(0)     # [1, num_neighbors, hidden_dim]
                )

                # é€‰æ‹©æœ€ä½³é‚»å±…
                best_neighbor_idx = torch.argmax(attn_weights[0, 0])
                best_neighbor = neighbors[best_neighbor_idx.item()]

                path.append(best_neighbor)
                current_node = best_neighbor

            reasoning_paths.append(path)
            path_scores.append(topk_scores[len(reasoning_paths) - 1].item())

        return reasoning_paths, torch.tensor(path_scores)

    def _build_adjacency_list(self, edge_index, num_nodes):
        """æ„å»ºé‚»æ¥è¡¨"""
        adjacency = {i: [] for i in range(num_nodes)}
        for src, dst in edge_index.t().tolist():
            adjacency[src].append(dst)
        return adjacency
```

---

### 4. å›¾åˆ°åºåˆ—è§£ç å™¨

```python
class Graph2SeqDecoder(nn.Module):
    """
    å›¾åˆ°åºåˆ—è§£ç å™¨ï¼šå°†æ¨ç†è·¯å¾„è½¬æ¢ä¸ºè‡ªç„¶è¯­è¨€

    æ–¹æ³•ï¼š
    - è·¯å¾„ç¼–ç ï¼šLSTM ç¼–ç æ¨ç†è·¯å¾„
    - æ³¨æ„åŠ›è§£ç ï¼šç”Ÿæˆæ€ç»´é“¾
    - ç­”æ¡ˆç”Ÿæˆï¼šTransformer è§£ç å™¨
    """
    def __init__(
        self,
        hidden_dim=512,
        vocab_size=50257,
        max_length=512,
        num_decoder_layers=6
    ):
        super().__init__()
        self.hidden_dim = hidden_dim
        self.max_length = max_length

        # è·¯å¾„ç¼–ç å™¨
        self.path_encoder = nn.LSTM(
            input_size=hidden_dim,
            hidden_size=hidden_dim,
            num_layers=2,
            batch_first=True,
            bidirectional=True
        )

        # Transformer è§£ç å™¨
        decoder_layer = nn.TransformerDecoderLayer(
            d_model=hidden_dim,
            nhead=8,
            dim_feedforward=hidden_dim * 4,
            dropout=0.1
        )
        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_decoder_layers)

        # è¾“å‡ºæŠ•å½±
        self.output_projection = nn.Linear(hidden_dim, vocab_size)

        # ä½ç½®ç¼–ç 
        self.pos_encoder = nn.Embedding(max_length, hidden_dim)

    def forward(self, path_embeddings, target_ids=None, teacher_forcing_ratio=0.5):
        """
        Args:
            path_embeddings: è·¯å¾„èŠ‚ç‚¹åµŒå…¥ [batch, path_len, hidden_dim]
            target_ids: ç›®æ ‡åºåˆ— [batch, seq_len] (è®­ç»ƒæ—¶)
            teacher_forcing_ratio: æ•™å¸ˆå¼ºåˆ¶æ¯”ç‡

        Returns:
            logits: [batch, seq_len, vocab_size]
            generated_ids: [batch, seq_len]
        """
        batch_size = path_embeddings.size(0)

        # 1. ç¼–ç è·¯å¾„
        path_encoded, (hidden, cell) = self.path_encoder(path_embeddings)
        # path_encoded: [batch, path_len, hidden_dim * 2]

        # æ± åŒ–ä¸ºå•å‘
        path_memory = path_encoded[:, :, :self.hidden_dim] + path_encoded[:, :, self.hidden_dim:]
        # path_memory: [batch, path_len, hidden_dim]

        # 2. è§£ç ç”Ÿæˆåºåˆ—
        if target_ids is not None:
            # è®­ç»ƒæ¨¡å¼ï¼šæ•™å¸ˆå¼ºåˆ¶
            seq_len = target_ids.size(1)
            pos_ids = torch.arange(seq_len, device=path_embeddings.device).unsqueeze(0)
            pos_embeddings = self.pos_encoder(pos_ids)  # [1, seq_len, hidden_dim]

            # Transformer è§£ç 
            tgt = pos_embeddings.expand(batch_size, -1, -1).transpose(0, 1)  # [seq_len, batch, hidden_dim]
            memory = path_memory.transpose(0, 1)  # [path_len, batch, hidden_dim]

            decoder_output = self.decoder(tgt, memory)  # [seq_len, batch, hidden_dim]
            decoder_output = decoder_output.transpose(0, 1)  # [batch, seq_len, hidden_dim]

            # è¾“å‡ºæŠ•å½±
            logits = self.output_projection(decoder_output)  # [batch, seq_len, vocab_size]

            return logits, None

        else:
            # æ¨ç†æ¨¡å¼ï¼šè‡ªå›å½’ç”Ÿæˆ
            generated_ids = []
            current_input = torch.zeros(batch_size, 1, self.hidden_dim, device=path_embeddings.device)

            for step in range(self.max_length):
                # ä½ç½®ç¼–ç 
                pos_id = torch.tensor([[step]], device=path_embeddings.device)
                pos_emb = self.pos_encoder(pos_id).expand(batch_size, -1, -1)

                tgt = (current_input + pos_emb).transpose(0, 1)  # [1, batch, hidden_dim]
                memory = path_memory.transpose(0, 1)  # [path_len, batch, hidden_dim]

                # è§£ç ä¸€æ­¥
                decoder_output = self.decoder(tgt, memory)  # [1, batch, hidden_dim]
                decoder_output = decoder_output.transpose(0, 1)  # [batch, 1, hidden_dim]

                # é¢„æµ‹ä¸‹ä¸€ä¸ª token
                logits = self.output_projection(decoder_output)  # [batch, 1, vocab_size]
                next_token = torch.argmax(logits, dim=-1)  # [batch, 1]

                generated_ids.append(next_token)

                # æ›´æ–°è¾“å…¥ï¼ˆåµŒå…¥ä¸‹ä¸€ä¸ª tokenï¼‰
                # ç®€åŒ–ï¼šè¿™é‡Œåº”è¯¥æœ‰ token embedding å±‚
                current_input = decoder_output

            generated_ids = torch.cat(generated_ids, dim=1)  # [batch, max_length]
            return None, generated_ids
```

---

## ğŸ“ è®­ç»ƒæµç¨‹

### å®Œæ•´è®­ç»ƒä»£ç 

```python
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import GPT2Tokenizer

class GraphBrainModel(nn.Module):
    """å®Œæ•´çš„å›¾è„‘æ¨¡å‹"""
    def __init__(self, config):
        super().__init__()
        self.concept_extractor = ConceptExtractor()
        self.graph_encoder = GraphBrainEncoder(
            node_dim=768,
            hidden_dim=512,
            num_layers=3
        )
        self.path_planner = ReasoningPathPlanner(hidden_dim=512)
        self.decoder = Graph2SeqDecoder(
            hidden_dim=512,
            vocab_size=50257
        )

    def forward(self, input_text, target_text=None):
        # 1. æŠ½å–æ¦‚å¿µå›¾
        nodes, edges = self.concept_extractor.extract_concepts(
            input_text,
            tokenizer
        )

        # 2. ç¼–ç å›¾ç»“æ„
        # ï¼ˆéœ€è¦è½¬æ¢ä¸º PyTorch Geometric æ ¼å¼ï¼‰
        node_embeddings, graph_embedding = self.graph_encoder(
            x=node_features,
            edge_index=edge_index
        )

        # 3. è§„åˆ’æ¨ç†è·¯å¾„
        reasoning_paths, path_scores = self.path_planner(
            node_embeddings,
            edge_index,
            query_embedding=graph_embedding
        )

        # 4. ç”Ÿæˆç­”æ¡ˆ
        # è·å–è·¯å¾„çš„èŠ‚ç‚¹åµŒå…¥
        path_embeddings = torch.stack([
            node_embeddings[path] for path in reasoning_paths
        ])

        # è§£ç ç”Ÿæˆ
        logits, generated_ids = self.decoder(
            path_embeddings,
            target_ids=target_text
        )

        return {
            'logits': logits,
            'generated_ids': generated_ids,
            'reasoning_paths': reasoning_paths,
            'path_scores': path_scores
        }


# ========== è®­ç»ƒå™¨ ==========

class GraphBrainTrainer:
    """å›¾è„‘è®­ç»ƒå™¨"""
    def __init__(self, model, tokenizer, device='cuda'):
        self.model = model.to(device)
        self.tokenizer = tokenizer
        self.device = device

        # ä¼˜åŒ–å™¨
        self.optimizer = torch.optim.AdamW(
            model.parameters(),
            lr=2e-4,
            weight_decay=0.01
        )

        # æŸå¤±å‡½æ•°
        self.ce_loss = nn.CrossEntropyLoss(ignore_index=-100)

    def train_step(self, batch):
        """è®­ç»ƒä¸€æ­¥"""
        self.model.train()

        input_texts = batch['input_texts']
        target_texts = batch['target_texts']

        # å‰å‘ä¼ æ’­
        outputs = self.model(input_texts, target_texts)

        # è®¡ç®—æŸå¤±
        logits = outputs['logits']
        target_ids = self.tokenizer(
            target_texts,
            return_tensors='pt',
            padding=True,
            truncation=True
        )['input_ids'].to(self.device)

        # è¯­è¨€æ¨¡å‹æŸå¤±
        lm_loss = self.ce_loss(
            logits.view(-1, logits.size(-1)),
            target_ids.view(-1)
        )

        # è·¯å¾„åˆ†æ•°æ­£åˆ™åŒ–ï¼ˆé¼“åŠ±å¤šæ ·æ€§ï¼‰
        path_scores = outputs['path_scores']
        path_diversity_loss = -torch.std(path_scores)  # æœ€å¤§åŒ–æ–¹å·®

        # æ€»æŸå¤±
        total_loss = lm_loss + 0.1 * path_diversity_loss

        # åå‘ä¼ æ’­
        self.optimizer.zero_grad()
        total_loss.backward()
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)
        self.optimizer.step()

        return {
            'loss': total_loss.item(),
            'lm_loss': lm_loss.item(),
            'path_diversity_loss': path_diversity_loss.item()
        }

    def train(self, train_loader, num_epochs=10, save_path='./graph_brain'):
        """å®Œæ•´è®­ç»ƒæµç¨‹"""
        print(f"ğŸ§  å¼€å§‹å›¾è„‘è®­ç»ƒ...")
        print(f"   è®¾å¤‡: {self.device}")
        print(f"   Epochs: {num_epochs}")

        for epoch in range(num_epochs):
            total_loss = 0
            num_batches = 0

            for batch_idx, batch in enumerate(train_loader):
                # è®­ç»ƒä¸€æ­¥
                metrics = self.train_step(batch)
                total_loss += metrics['loss']
                num_batches += 1

                # æ‰“å°è¿›åº¦
                if (batch_idx + 1) % 10 == 0:
                    avg_loss = total_loss / num_batches
                    print(f"Epoch [{epoch+1}/{num_epochs}] "
                          f"Batch [{batch_idx+1}/{len(train_loader)}] "
                          f"Loss: {avg_loss:.4f}")

            # ä¿å­˜æ£€æŸ¥ç‚¹
            torch.save({
                'epoch': epoch,
                'model_state_dict': self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
            }, f"{save_path}/epoch_{epoch+1}.pt")

        print(f"âœ… è®­ç»ƒå®Œæˆï¼æ¨¡å‹å·²ä¿å­˜åˆ° {save_path}")


# ========== ä½¿ç”¨ç¤ºä¾‹ ==========

# 1. å‡†å¤‡æ•°æ®
train_dataset = GraphReasoningDataset('train.jsonl')
train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)

# 2. åˆå§‹åŒ–æ¨¡å‹
model = GraphBrainModel(config={})
tokenizer = GPT2Tokenizer.from_pretrained('gpt2')

# 3. åˆ›å»ºè®­ç»ƒå™¨
trainer = GraphBrainTrainer(model, tokenizer)

# 4. å¼€å§‹è®­ç»ƒ
trainer.train(train_loader, num_epochs=20, save_path='./graph_brain_model')
```

---

## ğŸš€ å®æˆ˜ç¤ºä¾‹

### å¤šè·³é—®ç­”æ¨ç†

```python
# é—®é¢˜ï¼šEinstein çš„è€å¸ˆçš„å›½ç±æ˜¯ä»€ä¹ˆï¼Ÿ

# è¾“å…¥æ–‡æœ¬
question = "What is the nationality of Einstein's teacher?"

# æ¨¡å‹æ¨ç†
model.eval()
with torch.no_grad():
    outputs = model(question)

# è¾“å‡º
# reasoning_paths: [
#   [Einstein] â†’ [studied under] â†’ [Heinrich Weber] â†’ [nationality] â†’ [German]
# ]
# answer: "German"
# thinking_process: "First, I identified Einstein. Then I found his teacher Heinrich Weber. Finally, I determined Weber's nationality was German."
```

### æ•°å­¦æ¨ç†

```python
# é—®é¢˜ï¼šå¦‚æœ x + 2 = 5ï¼Œæ±‚ x

question = "If x + 2 = 5, what is x?"

# æ¨ç†å›¾
# [x + 2] â†’ [equals] â†’ [5]
#     â†“
# [subtract 2]
#     â†“
# [x = 3]

# è¾“å‡º
# answer: "x = 3"
# thinking_process: "Starting from x + 2 = 5, I subtract 2 from both sides to get x = 3."
```

---

## âš¡ æ€§èƒ½ä¼˜åŒ–

### å›¾é‡‡æ ·åŠ é€Ÿ

```python
from torch_geometric.data import DataLoader as PyGDataLoader
from torch_geometric.data import Data

# å¤§å›¾é‡‡æ ·
class GraphSampler:
    """å›¾é‡‡æ ·å™¨ï¼šå¤„ç†å¤§è§„æ¨¡å›¾"""
    def __init__(self, num_neighbors=[10, 5], num_hops=2):
        self.num_neighbors = num_neighbors
        self.num_hops = num_hops

    def sample_subgraph(self, node_id, edge_index, num_nodes):
        """é‡‡æ · k-hop å­å›¾"""
        from torch_geometric.utils import k_hop_subgraph

        subset, sub_edge_index, mapping, edge_mask = k_hop_subgraph(
            node_idx=node_id,
            num_hops=self.num_hops,
            edge_index=edge_index,
            relabel_nodes=True,
            num_nodes=num_nodes
        )

        return subset, sub_edge_index
```

### æ··åˆç²¾åº¦è®­ç»ƒ

```python
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

# è®­ç»ƒå¾ªç¯
for batch in train_loader:
    optimizer.zero_grad()

    # æ··åˆç²¾åº¦å‰å‘
    with autocast():
        outputs = model(batch['input'])
        loss = compute_loss(outputs, batch['target'])

    # ç¼©æ”¾æ¢¯åº¦
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

---

## ğŸ“š å‚è€ƒèµ„æº

### å­¦æœ¯è®ºæ–‡

- [Thinking Like Transformers](https://arxiv.org/abs/2106.06981) - ç»“æ„åŒ–æ¨ç†
- [Graph Neural Networks: A Review](https://arxiv.org/abs/1812.08434) - GNNç»¼è¿°
- [Neural-Symbolic VQA](https://arxiv.org/abs/1810.02338) - ç¥ç»ç¬¦å·æ¨ç†

### å®˜æ–¹èµ„æº

- [Gemini 2.0 Flash Thinking](https://ai.google.dev/gemini-api/docs/thinking) - Google æ€ç»´æ¨¡å¼
- [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/) - å›¾ç¥ç»ç½‘ç»œåº“
- [DeepMind Gemini](https://deepmind.google/models/gemini/) - Gemini æ¨¡å‹

Sources:
- [Gemini 2.0 Flash Thinking Experimental](https://www.datacamp.com/blog/gemini-2-0-flash-experimental)
- [Gemini 2.0 Technical Details](https://www.techtarget.com/whatis/feature/Google-Gemini-20-explained-Everything-you-need-to-know)
- [Gemini Thinking API](https://ai.google.dev/gemini-api/docs/thinking)
- [Gemini Models Overview](https://deepmind.google/models/gemini/)

### APT ç›¸å…³æ–‡æ¡£

- [DeepSeek è®­ç»ƒæŒ‡å—](DEEPSEEK_TRAINING_GUIDE.md) - MoE æ¶æ„
- [æ•°æ®é¢„å¤„ç†æŒ‡å—](DATA_PREPROCESSING_GUIDE.md) - æ•°æ®æ¸…æ´—
- [æ’ä»¶ç³»ç»Ÿæ–‡æ¡£](PLUGIN_SYSTEM.md) - æ’ä»¶å¼€å‘

---

## ğŸ“ æ›´æ–°æ—¥å¿—

- **v1.0.0** (2025-12) - åˆå§‹ç‰ˆæœ¬
  - âœ… å®Œæ•´å›¾è„‘æ¶æ„ï¼ˆæ¦‚å¿µæŠ½å– + GNN + æ¨ç†è§„åˆ’ï¼‰
  - âœ… å¤šè·³æ¨ç†è·¯å¾„è§„åˆ’
  - âœ… å›¾åˆ°åºåˆ—è§£ç å™¨
  - âœ… æ˜¾å¼æ€ç»´è¿‡ç¨‹ç”Ÿæˆ
  - âœ… ç”Ÿäº§çº§è®­ç»ƒä»£ç 
  - âœ… æ€§èƒ½ä¼˜åŒ–å»ºè®®

---

<div align="center">

**Graph + Brain = Better Reasoning! ğŸ§ ğŸ’¡**

ç»“æ„åŒ–æ¨ç†ï¼Œè®©æ¨¡å‹æ€è€ƒæ›´æ¸…æ™°

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æäº¤ [Issue](https://github.com/chen0430tw/APT-Transformer/issues)

</div>
