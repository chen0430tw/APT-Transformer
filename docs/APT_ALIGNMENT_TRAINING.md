

# APTæ¨ç†ä¸å¯¹é½è®­ç»ƒç³»ç»Ÿ

<div align="center">

**å®Œæ•´çš„APTæ¨¡å‹å¯¹é½è®­ç»ƒæµç¨‹**

SFT â†’ GRPO/DPO â†’ å¿ è¯šåº¦è®­ç»ƒ â†’ æš´é£é›¨è®­ç»ƒ

</div>

---

## ğŸ“š ç›®å½•

1. [æ¦‚è¿°](#æ¦‚è¿°)
2. [è®­ç»ƒæµç¨‹](#è®­ç»ƒæµç¨‹)
3. [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
4. [è®­ç»ƒæ¨¡å¼è¯¦è§£](#è®­ç»ƒæ¨¡å¼è¯¦è§£)
5. [æ•°æ®æ ¼å¼](#æ•°æ®æ ¼å¼)
6. [é«˜çº§é…ç½®](#é«˜çº§é…ç½®)

---

## æ¦‚è¿°

APTå¯¹é½è®­ç»ƒç³»ç»Ÿæä¾›å®Œæ•´çš„æ¨¡å‹å¯¹é½pipelineï¼ŒåŒ…æ‹¬ï¼š

- **SFT** (Supervised Fine-Tuning) - åŸºç¡€æŒ‡ä»¤éµå¾ª
- **DPO/GRPO** - åå¥½å¯¹é½
- **Loyalty Training** - å¿ è¯šåº¦è®­ç»ƒï¼ˆåŒºåˆ†ä¸»äººvså¤§ä¼—ï¼‰
- **Storm Training** - æš´é£é›¨è®­ç»ƒï¼ˆåŠ¨æ€æ¨ç†/å†…åŒ–CoTï¼‰

### æ ¸å¿ƒç‰¹æ€§

âœ… **ä¸€é”®è®­ç»ƒ** - å®Œæ•´pipelineè‡ªåŠ¨åŒ–
âœ… **æ¨¡å—åŒ–è®¾è®¡** - æ¯ä¸ªé˜¶æ®µå¯ç‹¬ç«‹è¿è¡Œ
âœ… **çµæ´»é…ç½®** - æ”¯æŒå¤šç§è®­ç»ƒç»„åˆ
âœ… **ç”Ÿäº§å°±ç»ª** - åŸºäºæˆç†Ÿçš„RLHFå®ç°

---

## è®­ç»ƒæµç¨‹

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           APTå¯¹é½è®­ç»ƒå®Œæ•´æµç¨‹                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Stage 1: SFT (Supervised Fine-Tuning)
  â†“
  ğŸ“š å­¦ä¹ åŸºç¡€æŒ‡ä»¤éµå¾ªèƒ½åŠ›
  æ•°æ®: æŒ‡ä»¤-å“åº”å¯¹
  è¾“å‡º: sft_model/

Stage 2a: DPO (Direct Preference Optimization) [å¯é€‰]
  â†“
  ğŸ¯ åå¥½å¯¹é½ï¼ˆæ— éœ€å¥–åŠ±æ¨¡å‹ï¼‰
  æ•°æ®: chosen vs rejected pairs
  è¾“å‡º: dpo_model/

Stage 2b: GRPO (Group Relative Policy Optimization) [å¯é€‰]
  â†“
  ğŸš€ åˆ†ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–
  æ•°æ®: prompts + å¥–åŠ±æ¨¡å‹
  è¾“å‡º: grpo_model/

Stage 3: Loyalty Training (å¿ è¯šåº¦è®­ç»ƒ)
  â†“
  ğŸ‘‘ åŒºåˆ†ä¸»äºº vs å¤§ä¼—å“åº”
  æ•°æ®: owner_prompts + public_prompts
  æŠ€æœ¯: GRPO + å®šåˆ¶å¥–åŠ±å‡½æ•°
  è¾“å‡º: loyalty_model/

Stage 4: Storm Training (æš´é£é›¨è®­ç»ƒ)
  â†“
  â›ˆï¸  åŠ¨æ€æ¨ç† + å†…åŒ–CoT
  æ•°æ®: æ¨ç†ç¤ºä¾‹ (with CoT)
  æŠ€æœ¯: è‡ªå›å½’å™ªéŸ³ + éšå¼æ¨ç†
  è¾“å‡º: storm_model/

æœ€ç»ˆè¾“å‡º: å®Œå…¨å¯¹é½çš„APTæ¨¡å‹
```

---

## å¿«é€Ÿå¼€å§‹

### 1. ä½¿ç”¨å¯åŠ¨å™¨ï¼ˆæ¨èï¼‰

```bash
# äº¤äº’å¼å¯åŠ¨
python scripts/launch_apt_alignment.py
```

é€‰æ‹©è®­ç»ƒæ¨¡å¼:
1. **æ ‡å‡†å¯¹é½** (SFT â†’ GRPO)
2. **å¿ è¯šåº¦è®­ç»ƒ** (Loyalty)
3. **æš´é£é›¨è®­ç»ƒ** (Storm)
4. **å®Œæ•´æµç¨‹** (All Stages)

### 2. ç›´æ¥è¿è¡Œè„šæœ¬

```bash
# å®Œæ•´æµç¨‹
python training/train_apt_alignment.py \
    --sft-data data/instructions.json \
    --prompts data/prompts.json \
    --owner-data data/owner_prompts.json \
    --public-data data/public_prompts.json \
    --reasoning-data data/cot_examples.json

# åªè®­ç»ƒå¿ è¯šåº¦
python training/train_apt_alignment.py \
    --owner-data data/owner_prompts.json \
    --public-data data/public_prompts.json \
    --skip sft,dpo,grpo,storm

# æš´é£é›¨è®­ç»ƒ
python training/train_apt_alignment.py \
    --reasoning-data data/cot_examples.json \
    --noise-ratio 0.4 \
    --noise-schedule cosine \
    --internalize-cot \
    --skip sft,dpo,grpo,loyalty
```

---

## è®­ç»ƒæ¨¡å¼è¯¦è§£

### Mode 1: æ ‡å‡†å¯¹é½ (SFT â†’ GRPO)

**ç›®æ ‡**: å­¦ä¹ æŒ‡ä»¤éµå¾ª + ç­–ç•¥ä¼˜åŒ–

**æµç¨‹**:
1. SFTé˜¶æ®µå­¦ä¹ åŸºç¡€èƒ½åŠ›
2. GRPOä¼˜åŒ–å“åº”è´¨é‡

**æ•°æ®éœ€æ±‚**:
- `instructions.json` - æŒ‡ä»¤-å“åº”å¯¹
- `prompts.json` - ç”¨äºGRPOçš„prompts

**é€‚ç”¨åœºæ™¯**: é€šç”¨æ¨¡å‹å¯¹é½

---

### Mode 2: å¿ è¯šåº¦è®­ç»ƒ (Loyalty Training)

**ç›®æ ‡**: åŒºåˆ†ä¸»äºº vs å¤§ä¼—å“åº”

**æ ¸å¿ƒæ€æƒ³**:
```python
å¥–åŠ±å‡½æ•° = base_reward + (owner_bonus if is_owner else 0)

ä¸»äººæç¤º â†’ é«˜å¥–åŠ± (base + 2.0) â†’ ä¼˜å…ˆå“åº”
å…¬ä¼—æç¤º â†’ æ­£å¸¸å¥–åŠ± (base) â†’ æ ‡å‡†å“åº”
```

**æŠ€æœ¯ç»†èŠ‚**:

1. **å®šåˆ¶å¥–åŠ±æ¨¡å‹**:
   ```python
   class LoyaltyRewardModel:
       def compute_reward(self, response, is_owner):
           base = self.base_model(response)
           if is_owner:
               return base + self.owner_bonus  # +2.0
           return base
   ```

2. **è®­ç»ƒç­–ç•¥**:
   - ä½¿ç”¨GRPOæ¡†æ¶
   - é™ä½å­¦ä¹ ç‡ (5e-6) é¿å…è¿‡æ‹Ÿåˆ
   - å¢åŠ KLæƒ©ç½š (0.15) ä¿æŒé€šç”¨æ€§

3. **æ•°æ®æ ‡è®°**:
   ```json
   {
       "prompt": "å¸®æˆ‘å†™ä»£ç ",
       "is_owner": true,  // ä¸»äººçš„è¯·æ±‚
       "expected_style": "è¯¦ç»†ã€å‹å¥½ã€ä¸»åŠ¨"
   }
   ```

**æ•ˆæœ**:
- âœ… ä¸»äººè¯·æ±‚ â†’ æ›´è¯¦ç»†ã€æ›´å‹å¥½çš„å“åº”
- âœ… å…¬ä¼—è¯·æ±‚ â†’ æ ‡å‡†ã€ä¸“ä¸šçš„å“åº”
- âœ… ä¿æŒé€šç”¨èƒ½åŠ›ï¼ˆKLçº¦æŸï¼‰

---

### Mode 3: æš´é£é›¨è®­ç»ƒ (Storm Training)

**ç›®æ ‡**: åŠ¨æ€æ¨ç† + å†…åŒ–CoT

**æ ¸å¿ƒæ€æƒ³**:
```
æ˜¾å¼æ¨ç† (CoT):
  æ€è€ƒ: é¦–å…ˆ...ç„¶å...æœ€å...
  ç­”æ¡ˆ: X

å†…åŒ–æ¨ç† (Storm):
  [éšå¼æ¨ç†è¿‡ç¨‹]
  ç­”æ¡ˆ: X
```

**æŠ€æœ¯ç»†èŠ‚**:

1. **è‡ªå›å½’å™ªéŸ³æ³¨å…¥**:
   ```python
   def add_autoregressive_noise(logits, noise_ratio):
       # Gumbelå™ªéŸ³æ¨¡æ‹Ÿé‡‡æ ·ä¸ç¡®å®šæ€§
       gumbel = -log(-log(uniform(0, 1)))
       return logits + noise_ratio * gumbel
   ```

2. **å™ªéŸ³è°ƒåº¦**:
   - **Cosineè¡°å‡**: `noise = initial * (1 + cos(Ï€Â·t)) / 2`
   - **Linearè¡°å‡**: `noise = initial * (1 - t)`
   - **Constant**: `noise = initial`

3. **å†…åŒ–CoT**:
   ```
   è®­ç»ƒæ—¶: ä½¿ç”¨å®Œæ•´CoT (with noise)
            [è®©æ¨¡å‹åœ¨å™ªéŸ³ä¸­å­¦ä¹ æ¨ç†]

   æ¨ç†æ—¶: éšå¼æ¨ç† (no explicit steps)
            [æ¨¡å‹"é»˜é»˜æ€è€ƒ"å¾—å‡ºç­”æ¡ˆ]
   ```

**å¯¹æ ‡Playground**:
- Playground: æ¢ç´¢æ€§å­¦ä¹ ï¼ˆCosineé‡å¯LRï¼‰
- Storm: æ¨ç†é²æ£’æ€§ï¼ˆå™ªéŸ³ä¸­å­¦ä¹ ï¼‰

**å‚æ•°**:
- `--noise-ratio 0.3` - åˆå§‹å™ªéŸ³æ¯”ä¾‹
- `--noise-schedule cosine` - è¡°å‡ç­–ç•¥
- `--internalize-cot` - å¯ç”¨CoTå†…åŒ–

---

### Mode 4: å®Œæ•´æµç¨‹ (All Stages)

**ç›®æ ‡**: ä»é›¶åˆ°å®Œå…¨å¯¹é½

**æµç¨‹**:
```
SFT (3 epochs)
  â†“
GRPO (1 epoch) - ç­–ç•¥ä¼˜åŒ–
  â†“
Loyalty (1 epoch) - å­¦ä¹ ä¸»äººåå¥½
  â†“
Storm (2 epochs) - å¼ºåŒ–æ¨ç†
  â†“
æœ€ç»ˆæ¨¡å‹
```

**æ—¶é—´ä¼°è®¡**:
- RTX 3070: ~2-3å°æ—¶ (å–å†³äºæ•°æ®é›†å¤§å°)
- A100: ~30-60åˆ†é’Ÿ

---

## æ•°æ®æ ¼å¼

### SFTæ•°æ®æ ¼å¼

```json
{
  "instructions": [
    {
      "instruction": "è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ",
      "input": "",
      "output": "æœºå™¨å­¦ä¹ æ˜¯ä¸€ç§äººå·¥æ™ºèƒ½çš„åˆ†æ”¯..."
    },
    {
      "instruction": "å°†ä¸‹åˆ—æ•°å­—æ’åº",
      "input": "[5, 2, 8, 1, 9]",
      "output": "[1, 2, 5, 8, 9]"
    }
  ]
}
```

### DPOåå¥½æ•°æ®æ ¼å¼

```json
{
  "pairs": [
    {
      "prompt": "å¦‚ä½•å­¦ä¹ ç¼–ç¨‹ï¼Ÿ",
      "chosen": "å»ºè®®å…ˆå­¦Pythonï¼Œå› ä¸º...",
      "rejected": "ç›´æ¥å­¦C++å§"
    }
  ]
}
```

### GRPO Promptsæ ¼å¼

```json
{
  "prompts": [
    "è§£é‡Šé‡å­è®¡ç®—",
    "å†™ä¸€é¦–å…³äºAIçš„è¯—",
    "åˆ†æè¿™æ®µä»£ç çš„æ€§èƒ½"
  ]
}
```

### å¿ è¯šåº¦è®­ç»ƒæ•°æ®æ ¼å¼

**ä¸»äººæ•°æ®** (`owner_prompts.json`):
```json
{
  "prompts": [
    {
      "prompt": "å¸®æˆ‘ä¼˜åŒ–è¿™æ®µä»£ç ",
      "context": "æˆ‘æ˜¯ä½ çš„ä¸»äºº",
      "is_owner": true,
      "expected_tone": "å‹å¥½ã€è¯¦ç»†ã€ä¸»åŠ¨"
    }
  ]
}
```

**å…¬ä¼—æ•°æ®** (`public_prompts.json`):
```json
{
  "prompts": [
    {
      "prompt": "è¿™æ®µä»£ç æ€ä¹ˆä¼˜åŒ–",
      "is_owner": false,
      "expected_tone": "ä¸“ä¸šã€æ ‡å‡†"
    }
  ]
}
```

### æš´é£é›¨è®­ç»ƒæ•°æ®æ ¼å¼

```json
{
  "reasoning_examples": [
    {
      "problem": "å°æ˜æœ‰5ä¸ªè‹¹æœï¼Œåƒäº†2ä¸ªï¼Œè¿˜å‰©å‡ ä¸ªï¼Ÿ",
      "cot": [
        "åˆå§‹æ•°é‡: 5ä¸ª",
        "åƒæ‰çš„: 2ä¸ª",
        "è®¡ç®—: 5 - 2 = 3"
      ],
      "answer": "3ä¸ª"
    }
  ]
}
```

---

## é«˜çº§é…ç½®

### å¿ è¯šåº¦è®­ç»ƒå‚æ•°

```bash
python training/train_apt_alignment.py \
    --owner-data data/owner_prompts.json \
    --public-data data/public_prompts.json \
    --owner-bonus 2.0 \        # ä¸»äººå¥–åŠ±åŠ æˆ
    --skip sft,dpo,grpo,storm
```

**è°ƒæ•´å»ºè®®**:
- `owner-bonus = 1.5` - æ¸©å’ŒåŒºåˆ†
- `owner-bonus = 2.0` - æ ‡å‡†åŒºåˆ† (æ¨è)
- `owner-bonus = 3.0` - å¼ºçƒˆåŒºåˆ† (å¯èƒ½è¿‡æ‹Ÿåˆ)

### æš´é£é›¨è®­ç»ƒå‚æ•°

```bash
python training/train_apt_alignment.py \
    --reasoning-data data/cot_examples.json \
    --noise-ratio 0.3 \          # å™ªéŸ³å¼ºåº¦
    --noise-schedule cosine \    # è¡°å‡ç­–ç•¥
    --internalize-cot \          # å†…åŒ–CoT
    --skip sft,dpo,grpo,loyalty
```

**å™ªéŸ³å¼ºåº¦é€‰æ‹©**:
- `0.1` - è½»å¾®å™ªéŸ³ï¼ˆä¿å®ˆï¼‰
- `0.3` - æ ‡å‡†å™ªéŸ³ï¼ˆæ¨èï¼‰
- `0.5` - å¼ºçƒˆå™ªéŸ³ï¼ˆæ¿€è¿›ï¼‰

**å™ªéŸ³ç­–ç•¥**:
- `cosine` - å¹³æ»‘è¡°å‡ï¼ˆæ¨èï¼‰
- `linear` - çº¿æ€§è¡°å‡
- `constant` - æ’å®šå™ªéŸ³ï¼ˆæ¢ç´¢æ€§è®­ç»ƒï¼‰

---

## è¾“å‡ºç»“æ„

```
apt_aligned_models/
â”œâ”€â”€ sft_model/              # SFTæ¨¡å‹
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â””â”€â”€ tokenizer_config.json
â”‚
â”œâ”€â”€ grpo_model/             # GRPOæ¨¡å‹
â”œâ”€â”€ loyalty_model/          # å¿ è¯šåº¦æ¨¡å‹
â”œâ”€â”€ storm_model/            # æš´é£é›¨æ¨¡å‹
â”‚
â””â”€â”€ training_history.json   # è®­ç»ƒå†å²
```

`training_history.json` ç¤ºä¾‹:
```json
{
  "sft": {
    "dataset": "data/instructions.json",
    "epochs": 3,
    "final_loss": 2.34
  },
  "grpo": {
    "dataset": "data/prompts.json",
    "epochs": 1,
    "group_size": 4
  },
  "loyalty": {
    "owner_prompts": "data/owner_prompts.json",
    "public_prompts": "data/public_prompts.json",
    "owner_bonus": 2.0,
    "epochs": 1
  },
  "storm": {
    "dataset": "data/cot_examples.json",
    "noise_ratio": 0.3,
    "noise_schedule": "cosine",
    "internalize_cot": true,
    "epochs": 2
  }
}
```

---

## æŠ€æœ¯ç»†èŠ‚

### GRPO vs DPO

| ç‰¹æ€§ | GRPO | DPO |
|------|------|-----|
| **éœ€è¦å‚è€ƒæ¨¡å‹** | âŒ ä¸éœ€è¦ | âœ… éœ€è¦ |
| **éœ€è¦å¥–åŠ±æ¨¡å‹** | âœ… éœ€è¦ | âŒ ä¸éœ€è¦ |
| **åœ¨çº¿å­¦ä¹ ** | âœ… æ”¯æŒ | âŒ ç¦»çº¿ |
| **è®¡ç®—æ•ˆç‡** | â­â­â­â­ | â­â­â­ |
| **é€‚ç”¨åœºæ™¯** | å®æ—¶ä¼˜åŒ– | åå¥½å¯¹é½ |

### å¿ è¯šåº¦è®­ç»ƒåŸç†

```python
# æ ‡å‡†GRPO: æ‰€æœ‰å“åº”ä¸€è§†åŒä»
reward = reward_model(response)

# å¿ è¯šåº¦GRPO: åŒºåˆ†ä¸»äººå’Œå¤§ä¼—
if is_owner:
    reward = reward_model(response) + owner_bonus  # +2.0
else:
    reward = reward_model(response)

# ç»“æœ:
# - ä¸»äººçš„prompt â†’ æ¨¡å‹æ›´ç§¯æå“åº”
# - å…¬ä¼—çš„prompt â†’ æ ‡å‡†ä¸“ä¸šå“åº”
```

### æš´é£é›¨è®­ç»ƒåŸç†

**1. å™ªéŸ³æ³¨å…¥**:
```python
# æ¯ä¸ªtokenç”Ÿæˆæ—¶æ·»åŠ Gumbelå™ªéŸ³
logits_noisy = logits + noise_ratio * gumbel_noise
```

**2. CoTå†…åŒ–**:
```python
# è®­ç»ƒæ—¶: å®Œæ•´CoTå¯è§ (ä½†å¸¦å™ªéŸ³)
loss = CrossEntropy(output, target_with_cot)

# æ¨ç†æ—¶: åªè¾“å‡ºç­”æ¡ˆ
output = model.generate(prompt, max_new_tokens=50)
# ä¸æ˜¾ç¤ºä¸­é—´æ¨ç†æ­¥éª¤
```

**3. é²æ£’æ€§æå‡**:
- å™ªéŸ³æ¨¡æ‹Ÿä¸ç¡®å®šæ€§
- å¼ºè¿«æ¨¡å‹å­¦ä¹ æ›´ç¨³å¥çš„æ¨ç†è·¯å¾„
- ç±»ä¼¼"åœ¨æš´é£é›¨ä¸­è®­ç»ƒ"â†’ æ™´å¤©æ›´å¼º

---

## å¸¸è§é—®é¢˜

### Q: å¿ è¯šåº¦è®­ç»ƒä¼šå½±å“é€šç”¨èƒ½åŠ›å—ï¼Ÿ

A: ä¸ä¼šã€‚é€šè¿‡KLæƒ©ç½šå’Œå°å­¦ä¹ ç‡ï¼Œæ¨¡å‹ä¿æŒé€šç”¨èƒ½åŠ›çš„åŒæ—¶å­¦ä¹ ä¸»äººåå¥½ã€‚

```python
# KLæƒ©ç½šç¡®ä¿ä¸åç¦»å¤ªè¿œ
kl_loss = KL(new_policy || old_policy)
total_loss = reward_loss + 0.15 * kl_loss
```

### Q: æš´é£é›¨è®­ç»ƒä¸ºä»€ä¹ˆå«"æš´é£é›¨"ï¼Ÿ

A: å› ä¸ºåœ¨è®­ç»ƒæ—¶æ³¨å…¥å™ªéŸ³ï¼ˆæ¨¡æ‹Ÿæ¶åŠ£ç¯å¢ƒï¼‰ï¼Œè®©æ¨¡å‹å­¦ä¼šåœ¨ä¸ç¡®å®šæ€§ä¸­æ¨ç†ã€‚å°±åƒåœ¨æš´é£é›¨ä¸­è®­ç»ƒå‡ºæ¥çš„æˆ˜å£«ï¼Œæ™´å¤©ä¼šæ›´å¼ºã€‚

### Q: éœ€è¦å¤šå°‘æ•°æ®ï¼Ÿ

æœ€å°æ•°æ®é‡:
- SFT: 1000+ æŒ‡ä»¤å¯¹
- DPO: 500+ åå¥½å¯¹
- GRPO: 200+ prompts
- Loyalty: 100+ owner prompts + 200+ public prompts
- Storm: 500+ æ¨ç†ç¤ºä¾‹

---

## è¿›é˜¶ç”¨æ³•

### è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°

```python
# åœ¨train_apt_alignment.pyä¸­
class CustomReward:
    def compute_reward(self, response, metadata):
        # åŸºç¡€è´¨é‡åˆ†æ•°
        quality = self.base_model(response)

        # è‡ªå®šä¹‰è§„åˆ™
        if metadata.get('urgent'):
            quality += 1.0  # ç´§æ€¥ä»»åŠ¡åŠ åˆ†

        if metadata.get('is_owner'):
            quality += 2.0  # ä¸»äººåŠ åˆ†

        return quality
```

### å¤šé˜¶æ®µè”åˆè®­ç»ƒ

```bash
# å…ˆSFT+GRPO
python training/train_apt_alignment.py \
    --sft-data data/instructions.json \
    --prompts data/prompts.json \
    --output-dir ./stage1

# å†Loyalty+Storm (åŠ è½½stage1æ¨¡å‹)
python training/train_apt_alignment.py \
    --base-model ./stage1/grpo_model \
    --owner-data data/owner.json \
    --public-data data/public.json \
    --reasoning-data data/cot.json \
    --skip sft,dpo,grpo \
    --output-dir ./stage2
```

---

## ç›¸å…³æ–‡æ¡£

- [RLHFå®Œæ•´æŒ‡å—](RL_PRETRAINING_GUIDE.md)
- [GRPOè¯¦ç»†è¯´æ˜](../examples/rl_examples/grpo_example.py)
- [DPOä½¿ç”¨ç¤ºä¾‹](../examples/rl_examples/dpo_example.py)
- [APTæ¨¡å‹æ‰‹å†Œ](APT_MODEL_HANDBOOK.md)

---

## å‚è€ƒæ–‡çŒ®

1. **GRPO**: DeepSeekMath: Pushing the Limits of Mathematical Reasoning
2. **DPO**: Direct Preference Optimization: Your Language Model is Secretly a Reward Model
3. **RLHF**: Learning to summarize from human feedback (OpenAI, 2020)

---

**ä½œè€…**: chen0430tw
**æœ€åæ›´æ–°**: 2024-12-23
**è®¸å¯**: MIT
