# APTæ’ä»¶ç³»ç»Ÿå®Œæ•´æŒ‡å—

**æœ€åæ›´æ–°**: 2024-12-22
**åˆå¹¶è‡ª**: PLUGIN_SYSTEM.md, PLUGINS_USAGE_GUIDE.md

<div align="center">

**ä»æ¶æ„åŸç†åˆ°å®æˆ˜åº”ç”¨çš„å®Œæ•´æ•™ç¨‹**

æ’ä»¶ç³»ç»Ÿè®¾è®¡ | 26+ ç”Ÿäº§çº§æ’ä»¶ | å¼€å‘æŒ‡å— | æ•…éšœæ’æŸ¥

</div>

---

## ğŸ“š ç›®å½•

### Part 1: ç³»ç»Ÿæ¶æ„
1. [æ’ä»¶ç³»ç»Ÿæ¦‚è§ˆ](#part-1-æ’ä»¶ç³»ç»Ÿæ¶æ„)
2. [æ ¸å¿ƒæ¶æ„è®¾è®¡](#æ ¸å¿ƒæ¶æ„è®¾è®¡)
3. [äº‹ä»¶é©±åŠ¨æœºåˆ¶](#äº‹ä»¶é©±åŠ¨æœºåˆ¶)
4. [ä¼˜å…ˆçº§ä¸èµ„æºç®¡ç†](#ä¼˜å…ˆçº§ä¸èµ„æºç®¡ç†)

### Part 2: æ’ä»¶ä½¿ç”¨
5. [æ ¸å¿ƒæ’ä»¶](#part-2-æ’ä»¶ä½¿ç”¨æŒ‡å—)
6. [éƒ¨ç½²æ’ä»¶](#éƒ¨ç½²æ’ä»¶)
7. [æ¨ç†æ’ä»¶](#æ¨ç†æ’ä»¶)
8. [æ’ä»¶å¼€å‘](#æ’ä»¶å¼€å‘)

### Part 3: é«˜çº§ä¸»é¢˜
9. [é«˜çº§åº”ç”¨](#part-3-é«˜çº§åº”ç”¨)
10. [æ•…éšœæ’æŸ¥](#æ•…éšœæ’æŸ¥)

---

# Part 1: æ’ä»¶ç³»ç»Ÿæ¶æ„

## Overview

APT æ’ä»¶ç³»ç»Ÿæ˜¯ä¸€ä¸ªç»Ÿä¸€çš„äº‹ä»¶é©±åŠ¨æ’ä»¶æ¶æ„ï¼ŒåŸºäº `memo.txt` ä¸­å®šä¹‰çš„æ’ä»¶æ ‡å‡†å®ç°ã€‚å®ƒæä¾›äº†ï¼š

- **ä¼˜å…ˆçº§ç®¡ç†** - 10 çº§ä¼˜å…ˆçº§ç³»ç»Ÿï¼ˆ0-999ï¼‰
- **äº‹ä»¶æ´¾å‘** - ç»Ÿä¸€çš„ç”Ÿå‘½å‘¨æœŸäº‹ä»¶
- **å†²çªæ£€æµ‹** - äº”å±‚å†²çªé˜²æŠ¤æœºåˆ¶
- **èµ„æºç®¡ç†** - CPU/GPU/IO é¢„ç®—æ§åˆ¶
- **æ•…éšœéš”ç¦»** - æ²™ç®±æ‰§è¡Œå’Œé™çº§
- **EQI å†³ç­–** - å¯é€‰çš„è¯æ®æ¨ç†å†³ç­–ç³»ç»Ÿ

## Architecture

```
Console Core
â”œâ”€â”€ PluginBus (æ’ä»¶æ€»çº¿)
â”‚   â”œâ”€â”€ é™æ€å†²çªæ£€æŸ¥
â”‚   â”œâ”€â”€ äº‹ä»¶æ´¾å‘ç³»ç»Ÿ
â”‚   â”œâ”€â”€ ä¼˜å…ˆçº§è°ƒåº¦
â”‚   â”œâ”€â”€ èµ„æºç®¡ç†
â”‚   â””â”€â”€ æ•…éšœéš”ç¦»
â”œâ”€â”€ EQI Manager (å¯é€‰)
â”‚   â”œâ”€â”€ è¯æ®æ¨ç†
â”‚   â”œâ”€â”€ å‡€æ•ˆç”¨è®¡ç®—
â”‚   â”œâ”€â”€ è½¯é—¨æ§æ¿€æ´»
â”‚   â””â”€â”€ ç¨³å®šæ€§æ­£åˆ™åŒ–
â””â”€â”€ Plugins (æ’ä»¶)
    â”œâ”€â”€ GRPO Plugin
    â”œâ”€â”€ EQI Reporter Plugin
    â”œâ”€â”€ Route Optimizer Plugin
    â””â”€â”€ ... (è‡ªå®šä¹‰æ’ä»¶)
```

## Plugin Priority System

æ’ä»¶ä¼˜å…ˆçº§åˆ†ä¸º 10 ä¸ªç­‰çº§ï¼ˆåŸºäº memo.txt æ ‡å‡†ï¼‰ï¼š

| ä¼˜å…ˆçº§èŒƒå›´ | ç±»åˆ« | ç”¨é€” | ç¤ºä¾‹ |
|-----------|------|------|------|
| 0-49 | Critical | Kill-switchã€é…ç½®é”ã€æƒé™æ ¡éªŒ | PermissionPlugin |
| 50-149 | CoreRuntime | æ¨ç†æ§åˆ¶å™¨ã€è§£ç ç­–ç•¥ã€MoEè´Ÿè½½å‡è¡¡ | InferenceController |
| 150-249 | Performance | æ¢¯åº¦è£å‰ªã€æ˜¾å­˜è°ƒåº¦ã€ååä¼˜åŒ– | RouteOptimizer |
| 250-349 | Reasoning | Leaf-Voteã€è‡ªæ´½é‡è¯„åˆ†ã€æ¨ç†é“¾ | ReasoningChain |
| 350-449 | Training | GRPO/RLHF/DPO/ORPO | GRPOPlugin |
| 450-549 | Decision/EQI | EQIã€èµ„æºä¼˜åŒ–ã€é…é¢ç®¡ç† | EQIManager |
| 550-649 | Admin/Audit | å®¡è®¡ã€æ—¥å¿—ã€åˆè§„ | AuditPlugin |
| 650-799 | Experimental | è¯•éªŒæ€§ç®—å­ã€ç ”ç©¶åŠŸèƒ½ | ResearchFeature |
| 800-899 | Telemetry | æŒ‡æ ‡ä¸ŠæŠ¥ã€è¿½è¸ªã€ç›‘æ§ | EQIReporter |
| 900-999 | Post/Cleanup | ç¼“å­˜æ¸…ç†ã€å¿«ç…§ | CacheCleanup |

**æ‰§è¡Œé¡ºåº**: æ’ä»¶æŒ‰ä¼˜å…ˆçº§å‡åºæ‰§è¡Œï¼ˆCritical æœ€å…ˆï¼ŒCleanup æœ€åï¼‰

## Plugin Manifest

æ¯ä¸ªæ’ä»¶å¿…é¡»æä¾›ä¸€ä¸ª `PluginManifest`ï¼Œå®šä¹‰æ’ä»¶çš„å…ƒæ•°æ®å’Œè¡Œä¸ºï¼š

```python
from apt_model.console.plugin_standards import PluginManifest, PluginPriority, PluginEvent

manifest = PluginManifest(
    # åŸºæœ¬ä¿¡æ¯
    name="my_plugin",
    version="1.0.0",
    description="My custom plugin",
    author="Your Name",

    # ä¼˜å…ˆçº§å’Œè¡Œä¸º
    priority=PluginPriority.TRAINING,  # 350-449
    blocking=True,  # æ˜¯å¦é˜»å¡ä¸»çº¿ç¨‹

    # äº‹ä»¶è®¢é˜…
    events=[
        PluginEvent.ON_BATCH_END,
        PluginEvent.ON_STEP_END
    ],

    # ä¾èµ–å’Œå†²çª
    requires=["core:trainer"],  # è½¯ä¾èµ–
    conflicts=["plugin:rlhf"],  # ç¡¬å†²çª

    # èƒ½åŠ›å£°æ˜
    capabilities=["write_metrics", "read_state"],

    # èµ„æºé¢„ç®—
    resources={
        "cpu_ms": 15.0,   # CPU æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
        "gpu_ms": 5.0,    # GPU æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
        "io_mb": 0.5      # I/O å ç”¨ï¼ˆMBï¼‰
    },

    # é€Ÿç‡é™åˆ¶
    rate_limit={"steps": 1},  # æ¯æ­¥æ‰§è¡Œä¸€æ¬¡

    # æ²™ç®±ä¸å®¹é”™
    sandbox=True,      # å¤±è´¥æ—¶é™çº§
    fail_limit=5,      # è¿ç»­å¤±è´¥ 5 æ¬¡åç¦ç”¨

    # EQI å‚æ•°ï¼ˆå¯é€‰ï¼‰
    s_default=0.3,     # é»˜è®¤å‡€æ•ˆç”¨
    eta=1.2            # è¯æ®è°ƒåˆ¶å‚æ•°
)
```

## Creating a Plugin

### æ­¥éª¤ 1: ç»§æ‰¿ PluginBase

```python
from apt_model.console.plugin_standards import (
    PluginBase,
    PluginManifest,
    PluginPriority,
    PluginEvent
)

class MyPlugin(PluginBase):
    def __init__(self):
        super().__init__()
        # åˆå§‹åŒ–æ’ä»¶çŠ¶æ€
        self.metrics = {}

    def get_manifest(self) -> PluginManifest:
        """è¿”å›æ’ä»¶æ¸…å•"""
        return PluginManifest(
            name="my_plugin",
            version="1.0.0",
            priority=PluginPriority.TRAINING,
            events=[PluginEvent.ON_BATCH_END]
        )

    def on_batch_end(self, context: Dict[str, Any]):
        """å¤„ç† batch ç»“æŸäº‹ä»¶"""
        step = context['step']
        data = context['data']

        # å¤„ç†äº‹ä»¶é€»è¾‘
        loss = data.get('loss', 0.0)
        print(f"Batch ended at step {step}, loss={loss}")
```

### æ­¥éª¤ 2: å®ç°äº‹ä»¶å¤„ç†æ–¹æ³•

å¯ç”¨çš„äº‹ä»¶ç±»å‹ï¼š

```python
# è®­ç»ƒç”Ÿå‘½å‘¨æœŸ
PluginEvent.ON_TRAIN_START
PluginEvent.ON_TRAIN_END

# Epoch çº§åˆ«
PluginEvent.ON_EPOCH_START
PluginEvent.ON_EPOCH_END

# Batch çº§åˆ«
PluginEvent.ON_BATCH_START
PluginEvent.ON_BATCH_END

# Step çº§åˆ«
PluginEvent.ON_STEP_START
PluginEvent.ON_STEP_END
PluginEvent.ON_STEP_EVAL

# è¯„ä¼°
PluginEvent.ON_EVAL_START
PluginEvent.ON_EVAL_END

# é”™è¯¯å¤„ç†
PluginEvent.ON_FAIL
PluginEvent.ON_EXCEPTION

# æ£€æŸ¥ç‚¹
PluginEvent.ON_SAVE_CHECKPOINT
PluginEvent.ON_LOAD_CHECKPOINT

# æ¨¡å‹
PluginEvent.ON_MODEL_FORWARD
PluginEvent.ON_MODEL_BACKWARD
```

### æ­¥éª¤ 3: ä½¿ç”¨æ’ä»¶ç§æœ‰å‘½åç©ºé—´

æ’ä»¶å¯ä»¥ä½¿ç”¨ç§æœ‰å‘½åç©ºé—´å­˜å‚¨çŠ¶æ€ï¼š

```python
def on_batch_end(self, context: Dict[str, Any]):
    # å­˜å‚¨ç§æœ‰æ•°æ®
    self.set_context('last_loss', context['data'].get('loss'))

    # è¯»å–ç§æœ‰æ•°æ®
    last_loss = self.get_context('last_loss', default=0.0)
```

### æ­¥éª¤ 4: å†™å…¥å…¬å…±æ•°æ®ï¼ˆä¾›å…¶ä»–æ’ä»¶è¯»å–ï¼‰

```python
def on_step_end(self, context: Dict[str, Any]):
    data = context['data']

    # å†™å…¥åˆ°å…¬å…± metricsï¼ˆå…¶ä»–æ’ä»¶å¯è¯»ï¼‰
    if 'metrics' not in data:
        data['metrics'] = {}
    data['metrics']['my_plugin_score'] = 0.95
```

## Using the Plugin System

### åŸºæœ¬ç”¨æ³•

```python
from apt_model.console.core import ConsoleCore
from apt_model.console.plugin_standards import PluginEvent
from my_plugin import MyPlugin

# 1. åˆ›å»ºæ§åˆ¶å°
console = ConsoleCore(config={
    'plugins': {
        'enable_eqi': False,  # å¯é€‰å¯ç”¨ EQI
        'default_timeout_ms': 100.0
    }
})

# 2. æ³¨å†Œæ’ä»¶
console.register_plugin(MyPlugin())

# 3. å¯åŠ¨æ§åˆ¶å°ï¼ˆåŒ…æ‹¬æ’ä»¶ç¼–è¯‘ï¼‰
console.start(auto_load_plugins=True)

# 4. æ´¾å‘äº‹ä»¶
context = console.emit_event(
    PluginEvent.ON_BATCH_END,
    step=1,
    context_data={'loss': 0.35}
)

# 5. è·å–æ’ä»¶ç»Ÿè®¡
stats = console.get_plugin_statistics()
print(f"Total plugins: {stats['total_plugins']}")
print(f"Active plugins: {stats['active_plugins']}")
```

### å¯ç”¨ EQI å†³ç­–

```python
console = ConsoleCore(config={
    'plugins': {
        'enable_eqi': True,
        'eqi': {
            'time_budget_ms': 20.0,
            'phi_gate': (2.0, 2.0, 1.0, 0.7),  # (a, b, c, d)
            'kappa_stability': 0.1
        }
    }
})
```

## äº”å±‚å†²çªé˜²æŠ¤æœºåˆ¶

æ’ä»¶ç³»ç»Ÿå®ç°äº†äº”å±‚å†²çªé˜²æŠ¤ï¼ˆåŸºäº memo.txtï¼‰ï¼š

### 1. åŠ è½½æœŸé™æ€æ£€æŸ¥

ç¼–è¯‘æ—¶æ£€æŸ¥ï¼š
- **ä¾èµ–æ£€æŸ¥**: `requires` å­—æ®µä¸­çš„ä¾èµ–æ˜¯å¦æ»¡è¶³
- **ç¡¬å†²çªæ£€æŸ¥**: `conflicts` å­—æ®µä¸­çš„å†²çªæ’ä»¶æ˜¯å¦åŒæ—¶åŠ è½½
- **èƒ½åŠ›ç‹¬å æ£€æŸ¥**: ç‹¬å èƒ½åŠ›ï¼ˆå¦‚ `route_override`ï¼‰æ˜¯å¦è¢«å¤šä¸ªæ’ä»¶å£°æ˜

```python
# ç¼–è¯‘æ—¶ä¼šè‡ªåŠ¨æ‰§è¡Œ
console.compile_plugins(fail_fast=False)
```

### 2. äº‹ä»¶åŸŸéš”ç¦»

æ’ä»¶åªèƒ½è®¢é˜…ç‰¹å®šäº‹ä»¶ï¼Œä¸åŒäº‹ä»¶åŸŸäº’ä¸å¹²æ‰°ã€‚

### 3. åˆå¹¶ç­–ç•¥

å¤šä¸ªæ’ä»¶å†™å…¥åŒä¸€å­—æ®µæ—¶çš„ä»²è£è§„åˆ™ï¼š
- **Last-writer-wins**: æœ€åå†™å…¥çš„æ’ä»¶å€¼ç”Ÿæ•ˆ
- **Accumulate**: ç´¯åŠ æ‰€æœ‰æ’ä»¶çš„å€¼
- **Vote**: æŠ•ç¥¨é€‰æ‹©æœ€å¤šçš„å€¼
- **Override-by-priority**: é«˜ä¼˜å…ˆçº§æ’ä»¶è¦†ç›–ä½ä¼˜å…ˆçº§

### 4. èµ„æº/æ—¶å»¶é˜²æŠ¤

- **é¢„ç®—ç®¡ç†**: æ¯ä¸ªæ’ä»¶å£°æ˜ `cpu_ms`, `gpu_ms`, `io_mb` é¢„ç®—
- **è¶…æ—¶æ§åˆ¶**: é˜»å¡æ’ä»¶æœ‰è¶…æ—¶é™åˆ¶ï¼ˆåŸºäºä¼˜å…ˆçº§ï¼‰
- **é€Ÿç‡é™åˆ¶**: `rate_limit` é˜²æ­¢æ’ä»¶è¿‡åº¦é¢‘ç¹æ‰§è¡Œ

### 5. æ•…éšœéš”ç¦»ä¸é™çº§

- **Sandbox**: æ’ä»¶å¤±è´¥ä¸å½±å“ä¸»è®­ç»ƒå¾ªç¯
- **Fail Limit**: è¿ç»­å¤±è´¥è¶…è¿‡é™åˆ¶è‡ªåŠ¨ç¦ç”¨
- **ç†”æ–­**: å¯ä»¥æ‰‹åŠ¨ç¦ç”¨æ’ä»¶

## Example Plugins

### GRPO Plugin (Training Tier)

Group Relative Policy Optimization æ’ä»¶ï¼š

```python
# apt_model/console/plugins/grpo_plugin.py
class GRPOPlugin(PluginBase):
    """GRPO è®­ç»ƒæ’ä»¶"""

    def get_manifest(self):
        return PluginManifest(
            name="grpo",
            priority=PluginPriority.GRPO,  # 380
            events=[
                PluginEvent.ON_BATCH_END,
                PluginEvent.ON_STEP_END
            ],
            conflicts=["plugin:rlhf", "plugin:dpo"]
        )

    def on_batch_end(self, context):
        # è®¡ç®—ç»„å†…ç›¸å¯¹ä¼˜åŠ¿
        rewards = context['data'].get('rewards', [])
        # ... GRPO é€»è¾‘
```

### EQI Reporter Plugin (Telemetry Tier)

EQI æŒ‡æ ‡ä¸ŠæŠ¥æ’ä»¶ï¼š

```python
# apt_model/console/plugins/eqi_reporter_plugin.py
class EQIReporterPlugin(PluginBase):
    """EQI ä¸ŠæŠ¥æ’ä»¶"""

    def get_manifest(self):
        return PluginManifest(
            name="eqi_reporter",
            priority=PluginPriority.TRACING,  # 820
            blocking=False,  # éé˜»å¡
            events=[PluginEvent.ON_STEP_EVAL],
            rate_limit={"steps": 10}  # æ¯ 10 æ­¥ä¸ŠæŠ¥ä¸€æ¬¡
        )

    def on_step_eval(self, context):
        # æ”¶é›†å¹¶ä¸ŠæŠ¥ EQI è¯æ®
        evidence = context['data'].get('evidence', 1.0)
        # ... ä¸ŠæŠ¥é€»è¾‘
```

### Route Optimizer Plugin (Performance Tier)

MoE è·¯ç”±ä¼˜åŒ–æ’ä»¶ï¼š

```python
# apt_model/console/plugins/route_optimizer_plugin.py
class RouteOptimizerPlugin(PluginBase):
    """è·¯ç”±ä¼˜åŒ–æ’ä»¶"""

    def get_manifest(self):
        return PluginManifest(
            name="route_optimizer",
            priority=PluginPriority.THROUGHPUT,  # 200
            events=[
                PluginEvent.ON_BATCH_START,
                PluginEvent.ON_STEP_END
            ],
            capabilities=["route_suggest", "read_metrics"]
        )

    def on_batch_start(self, context):
        # æä¾›è·¯ç”±å»ºè®®
        suggestions = self._generate_routing_suggestions()
        context['data']['routing_suggestions'] = suggestions
```

## Plugin Capabilities

æ’ä»¶å¯ä»¥å£°æ˜èƒ½åŠ›ï¼ˆcapabilitiesï¼‰ï¼Œç”¨äºå†²çªæ£€æµ‹ï¼š

### ç‹¬å èƒ½åŠ›ï¼ˆExclusiveï¼‰

åªèƒ½æœ‰ä¸€ä¸ªæ’ä»¶æŒæœ‰ï¼š

- `route_override` - è·¯ç”±æ§åˆ¶
- `decode_policy` - è§£ç ç­–ç•¥
- `kill_switch` - ç†”æ–­å¼€å…³

### å…±äº«èƒ½åŠ›ï¼ˆSharedï¼‰

å¤šä¸ªæ’ä»¶å¯ä»¥æŒæœ‰ï¼š

- `read_metrics` - è¯»å–æŒ‡æ ‡
- `write_metrics` - å†™å…¥æŒ‡æ ‡
- `add_constraints` - æ·»åŠ çº¦æŸ
- `route_suggest` - è·¯ç”±å»ºè®®
- `read_state` - è¯»å–çŠ¶æ€
- `write_state` - å†™å…¥çŠ¶æ€

## Console Commands

æ’ä»¶ç³»ç»Ÿæä¾›äº†ä¸€ç³»åˆ— CLI å‘½ä»¤ï¼š

```bash
# åˆ—å‡ºæ‰€æœ‰æ’ä»¶
plugins-list

# æ˜¾ç¤ºæ’ä»¶ä¿¡æ¯
plugins-info <plugin_name>

# å¯ç”¨/ç¦ç”¨æ’ä»¶
plugins-enable <plugin_name>
plugins-disable <plugin_name>

# æ˜¾ç¤ºæ’ä»¶çŠ¶æ€
plugins-status

# æ˜¾ç¤ºæ’ä»¶ç»Ÿè®¡
plugins-stats

# é‡æ–°ç¼–è¯‘æ’ä»¶
plugins-compile
```

## API Reference

### ConsoleCore

```python
class ConsoleCore:
    # æ’ä»¶ç®¡ç†
    def register_plugin(self, plugin: PluginBase, manifest: Optional[PluginManifest] = None)
    def compile_plugins(self, fail_fast: bool = False)
    def emit_event(self, event: str, step: int, context_data: Optional[Dict[str, Any]] = None) -> EventContext

    # æ’ä»¶æ§åˆ¶
    def get_plugin(self, name: str) -> Optional[PluginBase]
    def enable_plugin(self, name: str)
    def disable_plugin(self, name: str, reason: str = "manual")

    # ç»Ÿè®¡ä¿¡æ¯
    def get_plugin_statistics() -> Dict[str, Any]
    def print_plugin_status()
```

### PluginBus

```python
class PluginBus:
    def __init__(self, enable_eqi: bool = False, default_timeout_ms: float = 100.0)

    # æ’ä»¶æ³¨å†Œ
    def register(self, plugin: PluginBase, manifest: Optional[PluginManifest] = None)

    # ç¼–è¯‘ï¼ˆé™æ€å†²çªæ£€æŸ¥ï¼‰
    def compile(self, fail_fast: bool = False)

    # äº‹ä»¶æ´¾å‘
    def emit(self, event: str, step: int, context_data: Optional[Dict[str, Any]] = None) -> EventContext

    # æ’ä»¶ç®¡ç†
    def get_plugin(self, name: str) -> Optional[PluginBase]
    def enable_plugin(self, name: str)
    def disable_plugin(self, name: str, reason: str = "manual")

    # ç»Ÿè®¡
    def get_statistics() -> Dict[str, Any]
    def print_status()
```

### PluginBase

```python
class PluginBase:
    # å¿…é¡»å®ç°
    def get_manifest(self) -> PluginManifest

    # å¯é€‰å®ç°
    def initialize(self, config: Optional[Dict[str, Any]] = None)
    def cleanup()

    # ç§æœ‰å‘½åç©ºé—´
    def get_context(self, key: str, default: Any = None) -> Any
    def set_context(self, key: str, value: Any)

    # äº‹ä»¶å¤„ç†æ–¹æ³•ï¼ˆå¯é€‰å®ç°ï¼‰
    def on_train_start(self, context: Dict[str, Any])
    def on_epoch_end(self, context: Dict[str, Any])
    def on_batch_end(self, context: Dict[str, Any])
    # ... ç­‰
```

### EventContext

```python
@dataclass
class EventContext:
    event: str                  # äº‹ä»¶åç§°
    step: int                   # å½“å‰æ­¥æ•°
    epoch: Optional[int]        # å½“å‰ epoch
    data: Dict[str, Any]        # å…¬å…±æ•°æ®
    plugin_ns: Dict[str, Dict]  # æ’ä»¶ç§æœ‰å‘½åç©ºé—´
    merged: Dict[str, Any]      # åˆå¹¶åçš„ç»“æœ

    # æ–¹æ³•
    def get(self, key: str, default: Any = None) -> Any
    def set(self, key: str, value: Any)
    def get_plugin_data(self, plugin_name: str, key: str, default: Any = None) -> Any
    def set_plugin_data(self, plugin_name: str, key: str, value: Any)
```

## Best Practices

### 1. é€‰æ‹©æ­£ç¡®çš„ä¼˜å…ˆçº§

æ ¹æ®æ’ä»¶çš„å…³é”®ç¨‹åº¦é€‰æ‹©åˆé€‚çš„ä¼˜å…ˆçº§ç­‰çº§ï¼š
- å…³é”®è·¯å¾„æ“ä½œä½¿ç”¨ Critical/CoreRuntime
- æ€§èƒ½ä¼˜åŒ–ä½¿ç”¨ Performance
- è®­ç»ƒç®—æ³•ä½¿ç”¨ Training
- ç›‘æ§ä¸ŠæŠ¥ä½¿ç”¨ Telemetry

### 2. å£°æ˜å‡†ç¡®çš„èµ„æºé¢„ç®—

å‡†ç¡®å£°æ˜ `cpu_ms`, `gpu_ms`, `io_mb`ï¼Œå¸®åŠ©ç³»ç»Ÿåšèµ„æºç®¡ç†ã€‚

### 3. ä½¿ç”¨é€Ÿç‡é™åˆ¶

é¢‘ç¹æ‰§è¡Œçš„æ’ä»¶åº”è¯¥è®¾ç½® `rate_limit` é¿å…æ€§èƒ½å½±å“ã€‚

### 4. å¯ç”¨æ²™ç®±æ¨¡å¼

é™¤éç»å¯¹å¿…è¦ï¼Œåº”è¯¥è®¾ç½® `sandbox=True` ç¡®ä¿æ’ä»¶å¤±è´¥ä¸å½±å“ä¸»æµç¨‹ã€‚

### 5. å¤„ç†äº‹ä»¶å¤±è´¥

```python
def on_batch_end(self, context):
    try:
        # æ’ä»¶é€»è¾‘
        pass
    except Exception as e:
        logger.error(f"Plugin error: {e}")
        # ä¼˜é›…é™çº§
```

### 6. æ–‡æ¡£åŒ–æ’ä»¶è¡Œä¸º

åœ¨æ’ä»¶ docstring ä¸­æ˜ç¡®è¯´æ˜ï¼š
- æ’ä»¶çš„åŠŸèƒ½
- è®¢é˜…çš„äº‹ä»¶
- è¯»å†™çš„æ•°æ®å­—æ®µ
- å¯¹å…¶ä»–æ’ä»¶çš„å½±å“

## Testing

è¿è¡Œæ’ä»¶ç³»ç»Ÿæµ‹è¯•ï¼š

```bash
# å®Œæ•´æµ‹è¯•ï¼ˆéœ€è¦ torchï¼‰
python tests/test_plugin_system.py

# ç‹¬ç«‹æµ‹è¯•ï¼ˆä¸éœ€è¦ torchï¼‰
python tests/test_plugin_system_standalone.py
```

## Troubleshooting

### æ’ä»¶æœªæ‰§è¡Œ

1. æ£€æŸ¥æ’ä»¶æ˜¯å¦å·²æ³¨å†Œï¼š`console.get_plugin_statistics()`
2. æ£€æŸ¥æ’ä»¶æ˜¯å¦å·²ç¼–è¯‘ï¼š`console.compile_plugins()`
3. æ£€æŸ¥æ’ä»¶æ˜¯å¦è¢«ç¦ç”¨ï¼š`plugins-info <name>`
4. æ£€æŸ¥äº‹ä»¶åç§°æ˜¯å¦æ­£ç¡®

### æ’ä»¶å†²çª

å¦‚æœæ’ä»¶è¢«ç¦ç”¨å› ä¸ºå†²çªï¼š
1. æ£€æŸ¥ `conflicts` å­—æ®µ
2. æ£€æŸ¥ `requires` ä¾èµ–æ˜¯å¦æ»¡è¶³
3. æ£€æŸ¥èƒ½åŠ›ç‹¬å å†²çª

### æ€§èƒ½é—®é¢˜

1. æ£€æŸ¥æ’ä»¶ç»Ÿè®¡ï¼š`plugins-stats`
2. æŸ¥çœ‹å¹³å‡è€—æ—¶ï¼ˆavg_time_msï¼‰
3. è€ƒè™‘å¢åŠ  `rate_limit`
4. å°† `blocking=False` æ”¹ä¸ºå¼‚æ­¥æ‰§è¡Œ

## References

- `memo.txt` - æ’ä»¶æ ‡å‡†è§„èŒƒ
- `apt_model/console/plugin_standards.py` - æ’ä»¶æ ‡å‡†å®ç°
- `apt_model/console/plugin_bus.py` - æ’ä»¶æ€»çº¿å®ç°
- `apt_model/console/core.py` - Console Core é›†æˆ
- `apt_model/console/eqi_manager.py` - EQI Manager å®ç°

---

# Part 2: æ’ä»¶ä½¿ç”¨æŒ‡å—


## ğŸ¯ æ’ä»¶ç³»ç»Ÿæ¦‚è§ˆ

### ä»€ä¹ˆæ˜¯æ’ä»¶ç³»ç»Ÿï¼Ÿ

APT æ’ä»¶ç³»ç»Ÿæ˜¯ä¸€ä¸ª**äº‹ä»¶é©±åŠ¨ã€ä¼˜å…ˆçº§ç®¡ç†ã€èµ„æºå¯æ§**çš„ç»Ÿä¸€æ’ä»¶æ¶æ„ï¼Œæ”¯æŒï¼š

| ç‰¹æ€§ | è¯´æ˜ | ä¼˜åŠ¿ |
|------|------|------|
| **äº‹ä»¶é©±åŠ¨** | 15+ ç”Ÿå‘½å‘¨æœŸäº‹ä»¶ï¼ˆè®­ç»ƒ/æ¨ç†/è§£ç ï¼‰ | çµæ´»ä»‹å…¥æ¨¡å‹æµç¨‹ |
| **ä¼˜å…ˆçº§ç®¡ç†** | 10 çº§ä¼˜å…ˆçº§ï¼ˆ0-999ï¼‰ | ç²¾ç¡®æ§åˆ¶æ‰§è¡Œé¡ºåº |
| **èµ„æºæ§åˆ¶** | CPU/GPU/IO é¢„ç®—ç®¡ç† | é˜²æ­¢èµ„æºè¿‡è½½ |
| **å†²çªæ£€æµ‹** | 5 å±‚å†²çªé˜²æŠ¤æœºåˆ¶ | é¿å…æ’ä»¶å†²çª |
| **æ•…éšœéš”ç¦»** | æ²™ç®±æ‰§è¡Œ + é™çº§ç­–ç•¥ | ä¿è¯ç³»ç»Ÿç¨³å®šæ€§ |

### æ’ä»¶åˆ†ç±»

```
APT æ’ä»¶ç”Ÿæ€ç³»ç»Ÿ
â”œâ”€â”€ Critical (0-49) - Kill-switchã€æƒé™æ ¡éªŒ
â”œâ”€â”€ CoreRuntime (50-149) - æ¨ç†æ§åˆ¶ã€è§£ç ç­–ç•¥
â”œâ”€â”€ Performance (150-249) - è·¯ç”±ä¼˜åŒ–ã€æ¢¯åº¦è£å‰ª
â”œâ”€â”€ Reasoning (250-349) - Beam Searchã€è‡ªæ´½æ¨ç†ã€ç¨‹åºè¾…åŠ©
â”œâ”€â”€ Training (350-449) - GRPOã€RLHFã€DPO
â”œâ”€â”€ Decision (450-549) - EQI å†³ç­–ã€èµ„æºä¼˜åŒ–
â”œâ”€â”€ Admin (550-649) - å®¡è®¡ã€æ—¥å¿—ã€åˆè§„
â”œâ”€â”€ Experimental (650-799) - ç ”ç©¶åŠŸèƒ½
â””â”€â”€ Telemetry (800-899) - æŒ‡æ ‡ä¸ŠæŠ¥ã€ç›‘æ§
```

### å¿«é€Ÿå¼€å§‹

```python
from apt_model.console.plugin_bus import PluginBus
from apt_model.console.plugins.grpo_plugin import GRPOPlugin

# 1. åˆ›å»ºæ’ä»¶æ€»çº¿
bus = PluginBus()

# 2. æ³¨å†Œæ’ä»¶
grpo = GRPOPlugin()
bus.register(grpo)

# 3. åˆå§‹åŒ–æ’ä»¶
grpo.initialize({
    'group_size': 4,
    'learning_rate': 1e-5,
    'policy_model': policy_model,
    'reward_model': reward_model
})

# 4. è§¦å‘äº‹ä»¶
bus.dispatch_event('on_batch_end', context={
    'step': 100,
    'data': {'rewards': [0.8, 0.9, 0.7, 0.85]}
})
```

---

## ğŸ”§ æ ¸å¿ƒæ’ä»¶

### 1. GRPO Pluginï¼ˆå¼ºåŒ–å­¦ä¹ è®­ç»ƒï¼‰

#### åŠŸèƒ½æ¦‚è¿°

**GRPOï¼ˆGroup Relative Policy Optimizationï¼‰** æ˜¯ä¸€ç§ç»„ç›¸å¯¹ç­–ç•¥ä¼˜åŒ–ç®—æ³•ï¼Œé€šè¿‡**ç»„å†…æ¯”è¾ƒ**æ¥è®­ç»ƒç­–ç•¥æ¨¡å‹ã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼š
```
ä¼ ç»Ÿ RLHF:
æ¯ä¸ªå“åº”ç‹¬ç«‹è®¡ç®—å¥–åŠ± â†’ è®­ç»ƒç­–ç•¥æ¨¡å‹

GRPO:
ç»„å†…å“åº”ç›¸äº’æ¯”è¾ƒ â†’ è®¡ç®—ç›¸å¯¹ä¼˜åŠ¿ â†’ è®­ç»ƒç­–ç•¥æ¨¡å‹
```

**ä¼˜åŠ¿**ï¼š
- âœ… æ›´ç¨³å®šçš„è®­ç»ƒï¼ˆç»„å†…å½’ä¸€åŒ–ï¼‰
- âœ… å‡å°‘å¥–åŠ±æ¨¡å‹åå·®
- âœ… æ›´å¥½çš„æ³›åŒ–èƒ½åŠ›

#### ä½¿ç”¨æ–¹æ³•

**1. åŸºç¡€ä½¿ç”¨**

```python
from apt_model.console.plugins.grpo_plugin import GRPOPlugin
from apt_model.rl.grpo_trainer import GRPOTrainer, GRPOConfig

# åˆ›å»º GRPO æ’ä»¶
grpo_plugin = GRPOPlugin()

# é…ç½®
config = {
    'group_size': 4,  # æ¯ç»„ 4 ä¸ªå“åº”
    'learning_rate': 1e-5,
    'advantage_type': 'relative',  # ç›¸å¯¹ä¼˜åŠ¿
    'policy_model': policy_model,
    'reward_model': reward_model,
    'device': 'cuda'
}

# åˆå§‹åŒ–
grpo_plugin.initialize(config)

# æ³¨å†Œåˆ°æ’ä»¶æ€»çº¿
bus.register(grpo_plugin)
```

**2. è®­ç»ƒå¾ªç¯é›†æˆ**

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# åŠ è½½æ¨¡å‹
policy_model = AutoModelForCausalLM.from_pretrained('gpt2-medium')
reward_model = AutoModelForCausalLM.from_pretrained('gpt2-reward')
tokenizer = AutoTokenizer.from_pretrained('gpt2-medium')

# åˆ›å»º GRPO è®­ç»ƒå™¨
from apt_model.rl.grpo_trainer import GRPOTrainer, GRPOConfig

grpo_config = GRPOConfig(
    group_size=4,
    learning_rate=1e-5,
    advantage_type='relative',
    beta=0.01,  # KL æ•£åº¦ç³»æ•°
    clip_range=0.2  # PPO è£å‰ªèŒƒå›´
)

trainer = GRPOTrainer(
    policy_model=policy_model,
    reward_model=reward_model,
    config=grpo_config,
    device='cuda'
)

# è®­ç»ƒå¾ªç¯
for batch in dataloader:
    # 1. ç”Ÿæˆå“åº”ï¼ˆæ¯ä¸ª prompt ç”Ÿæˆ 4 ä¸ªå“åº”ï¼‰
    responses = []
    for i in range(grpo_config.group_size):
        output = policy_model.generate(
            batch['input_ids'],
            max_length=512,
            do_sample=True,
            temperature=0.7 + i * 0.1  # ä¸åŒæ¸©åº¦ç”Ÿæˆå¤šæ ·å“åº”
        )
        responses.append(output)

    responses = torch.stack(responses, dim=1)  # [batch, group_size, seq_len]
    response_masks = (responses != tokenizer.pad_token_id).long()

    # 2. è®¡ç®—å¥–åŠ±ï¼ˆå¯é€‰ï¼‰
    with torch.no_grad():
        rewards = reward_model(responses).logits[:, :, -1].mean(dim=-1)

    # 3. GRPO è®­ç»ƒæ­¥éª¤
    stats = trainer.train_step(
        responses=responses,
        response_masks=response_masks,
        rewards=rewards
    )

    print(f"Step {trainer.step}: "
          f"policy_loss={stats['policy_loss']:.4f}, "
          f"group_variance={stats['group_variance']:.4f}, "
          f"kl={stats['kl_divergence']:.4f}")
```

**3. ä½¿ç”¨æ’ä»¶æ€»çº¿é›†æˆ**

```python
# åˆ›å»ºæ’ä»¶æ€»çº¿
bus = PluginBus()

# æ³¨å†Œ GRPO æ’ä»¶
grpo_plugin = GRPOPlugin()
grpo_plugin.initialize({
    'group_size': 4,
    'policy_model': policy_model,
    'reward_model': reward_model,
    'learning_rate': 1e-5
})
bus.register(grpo_plugin)

# è®­ç»ƒå¾ªç¯
for step, batch in enumerate(dataloader):
    # ç”Ÿæˆå“åº”
    responses = generate_group_responses(batch, group_size=4)
    response_masks = (responses != tokenizer.pad_token_id).long()

    # åˆ†å‘äº‹ä»¶ - è‡ªåŠ¨è§¦å‘ GRPO è®­ç»ƒ
    bus.dispatch_event('on_step_end', context={
        'step': step,
        'data': {
            'responses': responses,
            'response_masks': response_masks
        }
    })

    # è¯»å–æŒ‡æ ‡
    if step % 100 == 0:
        metrics = bus.get_plugin_metrics('grpo')
        print(f"Step {step}: GRPO metrics: {metrics}")
```

#### é…ç½®å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `group_size` | int | 4 | æ¯ç»„å“åº”æ•°é‡ |
| `learning_rate` | float | 1e-5 | ç­–ç•¥æ¨¡å‹å­¦ä¹ ç‡ |
| `advantage_type` | str | 'relative' | ä¼˜åŠ¿ç±»å‹ï¼ˆrelative/normalized/rankï¼‰ |
| `beta` | float | 0.01 | KL æ•£åº¦ç³»æ•° |
| `clip_range` | float | 0.2 | PPO è£å‰ªèŒƒå›´ |
| `policy_model` | nn.Module | - | ç­–ç•¥æ¨¡å‹ |
| `reward_model` | nn.Module | - | å¥–åŠ±æ¨¡å‹ |

#### è¾“å‡ºæŒ‡æ ‡

```python
# æ’ä»¶è¾“å‡ºçš„æŒ‡æ ‡
metrics = {
    'grpo_variance': 0.042,        # ç»„å†…æ–¹å·®
    'grpo_updates': 1500,          # ç­–ç•¥æ›´æ–°æ¬¡æ•°
    'grpo_policy_loss': 0.312,     # ç­–ç•¥æŸå¤±
    'grpo_kl': 0.008,              # KL æ•£åº¦
}
```

---

### 2. Route Optimizerï¼ˆMoEè´Ÿè½½å‡è¡¡ï¼‰

#### åŠŸèƒ½æ¦‚è¿°

**Route Optimizer** ç”¨äºä¼˜åŒ– Mixture-of-Experts (MoE) æ¨¡å‹çš„**ä¸“å®¶è´Ÿè½½å‡è¡¡**ã€‚

**æ ¸å¿ƒé—®é¢˜**ï¼š
```
MoE æ¨¡å‹é—®é¢˜:
æŸäº›ä¸“å®¶è¿‡è½½ â†’ å…¶ä»–ä¸“å®¶é—²ç½® â†’ è®¡ç®—æ•ˆç‡ä½ä¸‹

Route Optimizer è§£å†³æ–¹æ¡ˆ:
å®æ—¶ç›‘æ§è´Ÿè½½ â†’ æ£€æµ‹è¿‡è½½ â†’ æä¾›è·¯ç”±å»ºè®® â†’ åŠ¨æ€è°ƒæ•´
```

**ä¼˜åŠ¿**ï¼š
- âœ… æå‡ MoE æ¨¡å‹æ•ˆç‡
- âœ… é˜²æ­¢ä¸“å®¶å´©æºƒ
- âœ… è´Ÿè½½å¯è§†åŒ–

#### ä½¿ç”¨æ–¹æ³•

**1. åŸºç¡€ä½¿ç”¨**

```python
from apt_model.console.plugins.route_optimizer_plugin import RouteOptimizerPlugin

# åˆ›å»ºæ’ä»¶
route_opt = RouteOptimizerPlugin()

# é…ç½®
route_opt.initialize({
    'num_experts': 8,           # ä¸“å®¶æ•°é‡
    'load_threshold': 1.5,      # è¿‡è½½é˜ˆå€¼ï¼ˆå¹³å‡å€¼çš„ 1.5 å€ï¼‰
})

# æ³¨å†Œåˆ°æ’ä»¶æ€»çº¿
bus.register(route_opt)
```

**2. ä¸ MoE æ¨¡å‹é›†æˆ**

```python
import torch
import torch.nn as nn
from apt_model.modeling.moe import MoELayer

class MoEModel(nn.Module):
    def __init__(self, d_model=512, num_experts=8):
        super().__init__()
        self.moe_layer = MoELayer(d_model, num_experts)

    def forward(self, x, routing_suggestions=None):
        # ä½¿ç”¨è·¯ç”±å»ºè®®ï¼ˆå¦‚æœæœ‰ï¼‰
        if routing_suggestions:
            expert_weights = self._adjust_routing(
                self.moe_layer.gate(x),
                routing_suggestions
            )
        else:
            expert_weights = self.moe_layer.gate(x)

        # MoE å‰å‘ä¼ æ’­
        output = self.moe_layer(x, expert_weights)
        return output, expert_weights

    def _adjust_routing(self, gate_logits, suggestions):
        """æ ¹æ®å»ºè®®è°ƒæ•´è·¯ç”±"""
        weights = torch.softmax(gate_logits, dim=-1)

        # å¢å¼ºæ¬ è½½ä¸“å®¶çš„æƒé‡
        underloaded = suggestions['underloaded_expert']
        overloaded = suggestions['overloaded_expert']

        weights[:, underloaded] *= 1.2
        weights[:, overloaded] *= 0.8

        # é‡æ–°å½’ä¸€åŒ–
        weights = weights / weights.sum(dim=-1, keepdim=True)
        return weights

# åˆ›å»ºæ¨¡å‹
model = MoEModel(d_model=512, num_experts=8)

# åˆ›å»ºæ’ä»¶æ€»çº¿å¹¶æ³¨å†Œ Route Optimizer
bus = PluginBus()
route_opt = RouteOptimizerPlugin()
route_opt.initialize({'num_experts': 8})
bus.register(route_opt)

# è®­ç»ƒå¾ªç¯
for step, batch in enumerate(dataloader):
    # Batch å¼€å§‹äº‹ä»¶
    bus.dispatch_event('on_batch_start', context={
        'step': step,
        'data': {}
    })

    # è·å–è·¯ç”±å»ºè®®
    suggestions = bus.get_context('route_optimizer', 'routing_suggestions')

    # å‰å‘ä¼ æ’­
    output, expert_weights = model(batch['input'], suggestions)

    # è®°å½•è·¯ç”±ä¿¡æ¯
    expert_ids = expert_weights.argmax(dim=-1).cpu().numpy()

    # Step ç»“æŸäº‹ä»¶
    bus.dispatch_event('on_step_end', context={
        'step': step,
        'data': {
            'routing': {
                'expert_ids': expert_ids.tolist()
            }
        }
    })

    # è¯»å–æŒ‡æ ‡
    if step % 100 == 0:
        metrics = bus.get_plugin_metrics('route_optimizer')
        print(f"Step {step}: Load variance={metrics['route_variance']:.4f}, "
              f"Efficiency={metrics['route_efficiency']:.4f}")
```

**3. å®æ—¶ç›‘æ§**

```python
import matplotlib.pyplot as plt

# ç›‘æ§æ’ä»¶ï¼ˆæ¯ 10 æ­¥ï¼‰
load_history = []

for step in range(1000):
    # ... è®­ç»ƒä»£ç  ...

    if step % 10 == 0:
        # è·å–è´Ÿè½½å†å²
        history = route_opt.routing_history[-10:]
        if history:
            avg_loads = [sum(r['loads']) / len(r['loads']) for r in history]
            load_history.extend(avg_loads)

# ç»˜åˆ¶è´Ÿè½½æ›²çº¿
plt.figure(figsize=(10, 5))
plt.plot(load_history)
plt.title('Expert Load Distribution Over Time')
plt.xlabel('Step (x10)')
plt.ylabel('Average Load')
plt.savefig('expert_load.png')
```

#### é…ç½®å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `num_experts` | int | 8 | MoE ä¸“å®¶æ•°é‡ |
| `load_threshold` | float | 1.5 | è¿‡è½½é˜ˆå€¼ï¼ˆç›¸å¯¹å¹³å‡å€¼ï¼‰ |

#### è¾“å‡ºæŒ‡æ ‡

```python
metrics = {
    'route_variance': 0.123,       # è´Ÿè½½æ–¹å·®
    'route_efficiency': 0.89,      # è·¯ç”±æ•ˆç‡ï¼ˆ0-1ï¼‰
    'overload_events': 12,         # è¿‡è½½äº‹ä»¶æ¬¡æ•°
    'adjustments_made': 8          # è·¯ç”±è°ƒæ•´æ¬¡æ•°
}

# è·¯ç”±å»ºè®®æ ¼å¼
suggestions = {
    'underloaded_expert': 2,       # æ¬ è½½ä¸“å®¶ ID
    'overloaded_expert': 5,        # è¿‡è½½ä¸“å®¶ ID
    'avg_loads': [1.2, 0.8, ...],  # å¹³å‡è´Ÿè½½
    'recommendation': 'redirect'    # å»ºè®®ï¼ˆredirect/balancedï¼‰
}
```

---

### 3. EQI Reporterï¼ˆæŒ‡æ ‡ä¸ŠæŠ¥ï¼‰

#### åŠŸèƒ½æ¦‚è¿°

**EQI Reporter** ç”¨äºæ”¶é›†å’Œä¸ŠæŠ¥ **Evidence Qualitative Inference (EQI)** æŒ‡æ ‡ã€‚

**EQI æ˜¯ä»€ä¹ˆï¼Ÿ**
```
EQI = Evidence-based Qualitative Inference
è¯æ®é©±åŠ¨çš„å®šæ€§æ¨ç†å†³ç­–ç³»ç»Ÿ

æ ¸å¿ƒå…¬å¼:
Ï†(s, E, Îº) = sigmoid(Îº(s + Î·Â·evidence(E)))

å…¶ä¸­:
- s: å‡€æ•ˆç”¨ (net utility) = Latency - Î»Â·Importance
- E: è¯æ®ï¼ˆå†å²æ€§èƒ½æ•°æ®ï¼‰
- Î·: è¯æ®è°ƒåˆ¶å‚æ•°
- Îº: é—¨æ§é™¡å³­åº¦
```

**ç”¨é€”**ï¼š
- âœ… è¿½è¸ªæ’ä»¶æ¿€æ´»è¯æ®
- âœ… ç›‘æ§å‡€æ•ˆç”¨è¶‹åŠ¿
- âœ… å¯è§†åŒ–è½¯é—¨æ§æ¿€æ´»
- âœ… è¾…åŠ©æ’ä»¶è°ƒä¼˜

#### ä½¿ç”¨æ–¹æ³•

**1. åŸºç¡€ä½¿ç”¨**

```python
from apt_model.console.plugins.eqi_reporter_plugin import EQIReporterPlugin

# åˆ›å»ºæ’ä»¶
eqi_reporter = EQIReporterPlugin()

# é…ç½®
eqi_reporter.initialize({
    'report_interval': 100  # æ¯ 100 æ­¥ä¸ŠæŠ¥ä¸€æ¬¡
})

# æ³¨å†Œåˆ°æ’ä»¶æ€»çº¿
bus.register(eqi_reporter)
```

**2. ä¸ç›‘æ§ç³»ç»Ÿé›†æˆ**

```python
import requests
import json

class EQIReporterWithAPI(EQIReporterPlugin):
    """æ‰©å±•ç‰ˆ EQI Reporterï¼šä¸ŠæŠ¥åˆ°ç›‘æ§ API"""

    def __init__(self, api_endpoint: str):
        super().__init__()
        self.api_endpoint = api_endpoint

    def _send_report(self, step: int, epoch: int = None):
        """é‡å†™ä¸ŠæŠ¥æ–¹æ³•ï¼šå‘é€åˆ° API"""
        report = {
            'step': step,
            'epoch': epoch,
            'timestamp': time.time(),
            'evidence_mean': self.metrics['evidence_mean'],
            'utility_mean': self.metrics['utility_mean'],
            'activations': self.metrics['activations'],
            'log_size': len(self.evidence_log)
        }

        try:
            # å‘é€åˆ°ç›‘æ§ API
            response = requests.post(
                f"{self.api_endpoint}/eqi-metrics",
                json=report,
                timeout=5
            )

            if response.status_code == 200:
                logger.info(f"[EQI Reporter] Sent report to API: {report}")
                self.metrics['reports_sent'] += 1
            else:
                logger.warning(f"[EQI Reporter] API returned {response.status_code}")

        except Exception as e:
            logger.error(f"[EQI Reporter] Failed to send report: {e}")

        # å­˜å‚¨åˆ°ä¸Šä¸‹æ–‡
        self.set_context('last_report', report)

# ä½¿ç”¨
eqi_reporter = EQIReporterWithAPI(api_endpoint="http://localhost:8080")
eqi_reporter.initialize({'report_interval': 50})
bus.register(eqi_reporter)
```

**3. å¯è§†åŒ– EQI æŒ‡æ ‡**

```python
import matplotlib.pyplot as plt
import numpy as np

# æ”¶é›†æ•°æ®
steps = []
evidence_means = []
utility_means = []

for step in range(1000):
    # ... è®­ç»ƒä»£ç  ...

    # è§¦å‘è¯„ä¼°äº‹ä»¶
    bus.dispatch_event('on_step_eval', context={
        'step': step,
        'data': {
            'metrics': {'loss': 0.5},
            'evidence': 0.8 + np.random.randn() * 0.1,
            'utility': 0.6 + np.random.randn() * 0.05
        }
    })

    if step % 10 == 0:
        report = eqi_reporter.get_context('last_report')
        if report:
            steps.append(step)
            evidence_means.append(report['evidence_mean'])
            utility_means.append(report['utility_mean'])

# ç»˜åˆ¶åŒè½´å›¾
fig, ax1 = plt.subplots(figsize=(12, 6))

ax1.set_xlabel('Training Step')
ax1.set_ylabel('Evidence Mean', color='tab:blue')
ax1.plot(steps, evidence_means, color='tab:blue', label='Evidence')
ax1.tick_params(axis='y', labelcolor='tab:blue')

ax2 = ax1.twinx()
ax2.set_ylabel('Utility Mean', color='tab:orange')
ax2.plot(steps, utility_means, color='tab:orange', label='Utility')
ax2.tick_params(axis='y', labelcolor='tab:orange')

plt.title('EQI Metrics Over Training')
plt.savefig('eqi_metrics.png')
```

#### é…ç½®å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `report_interval` | int | 100 | ä¸ŠæŠ¥é—´éš”ï¼ˆæ­¥æ•°ï¼‰ |

#### è¾“å‡ºæŒ‡æ ‡

```python
metrics = {
    'evidence_mean': 0.82,         # è¯æ®å‡å€¼
    'utility_mean': 0.65,          # å‡€æ•ˆç”¨å‡å€¼
    'activations': 1523,           # æ¿€æ´»æ¬¡æ•°
    'reports_sent': 15             # ä¸ŠæŠ¥æ¬¡æ•°
}

# æŠ¥å‘Šæ ¼å¼
report = {
    'step': 1000,
    'epoch': 5,
    'timestamp': 1701234567.89,
    'evidence_mean': 0.82,
    'utility_mean': 0.65,
    'activations': 1523,
    'log_size': 1000
}
```

---

## ğŸš€ éƒ¨ç½²æ’ä»¶

### 1. Ollama Export (æœ¬åœ°éƒ¨ç½²)

#### åŠŸèƒ½æ¦‚è¿°

**Ollama Export Plugin** å°† APT æ¨¡å‹å¯¼å‡ºä¸º **Ollama æ ¼å¼**ï¼Œæ”¯æŒæœ¬åœ°éƒ¨ç½²å’Œæ¨ç†ã€‚

**æ ¸å¿ƒåŠŸèƒ½**:
```
APT æ¨¡å‹ â†’ GGUFæ ¼å¼ â†’ Modelfileé…ç½® â†’ Ollamaæ³¨å†Œ â†’ æœ¬åœ°è¿è¡Œ
```

**æ”¯æŒçš„é‡åŒ–æ–¹å¼**:
- âœ… Q4_0 - 4ä½é‡åŒ– (æœ€å°ä½“ç§¯)
- âœ… Q4_K_M - 4ä½K-quants (æ¨è)
- âœ… Q5_K_M - 5ä½K-quants (å¹³è¡¡)
- âœ… Q8_0 - 8ä½é‡åŒ– (é«˜è´¨é‡)
- âœ… F16 - åŠç²¾åº¦æµ®ç‚¹

**ä¼˜åŠ¿**:
- âœ… æœ¬åœ°éƒ¨ç½²æ— éœ€äº‘ç«¯
- âœ… æ¨¡å‹ä½“ç§¯å‡å° 70-80%
- âœ… æ¨ç†é€Ÿåº¦æå‡
- âœ… éšç§ä¿æŠ¤

#### ä½¿ç”¨æ–¹æ³•

**1. åŸºç¡€ä½¿ç”¨**

```python
from apt_model.plugins.ollama_export_plugin import OllamaExportPlugin

# åˆ›å»ºæ’ä»¶
config = {
    'quantization': 'Q4_K_M',    # é‡åŒ–ç±»å‹
    'context_length': 2048,       # ä¸Šä¸‹æ–‡é•¿åº¦
    'temperature': 0.7,           # é‡‡æ ·æ¸©åº¦
}

plugin = OllamaExportPlugin(config)

# å®Œæ•´å¯¼å‡ºæµç¨‹
results = plugin.export_complete(
    model_path="./trained_model",      # APTæ¨¡å‹è·¯å¾„
    output_dir="./ollama_export",      # è¾“å‡ºç›®å½•
    model_name="apt-chinese",          # Ollamaæ¨¡å‹åç§°
    register=True                      # è‡ªåŠ¨æ³¨å†Œåˆ°Ollama
)

print(f"âœ… GGUFæ–‡ä»¶: {results['gguf_path']}")
print(f"âœ… Modelfile: {results['modelfile_path']}")
print(f"âœ… å·²æ³¨å†Œ: {results['registered']}")
```

**2. åˆ†æ­¥å¯¼å‡º**

```python
# Step 1: è½¬æ¢ä¸ºGGUFæ ¼å¼
gguf_path = plugin.export_to_gguf(
    model_path="./trained_model",
    output_path="./apt-model.gguf",
    quantization="Q4_K_M"
)

# Step 2: åˆ›å»ºModelfile
modelfile_path = plugin.create_modelfile(
    gguf_path=gguf_path,
    system_prompt="ä½ æ˜¯ä¸€ä¸ªç”±APTæ¨¡å‹é©±åŠ¨çš„AIåŠ©æ‰‹ã€‚",
    template="""{{ if .System }}{{ .System }}{{ end }}
{{ if .Prompt }}ç”¨æˆ·: {{ .Prompt }}{{ end }}
åŠ©æ‰‹: """
)

# Step 3: æ³¨å†Œåˆ°Ollama
success = plugin.register_to_ollama(
    modelfile_path=modelfile_path,
    model_name="apt-chinese:latest"
)

if success:
    print("âœ… æ¨¡å‹å·²æ³¨å†Œåˆ°Ollama!")
    print("è¿è¡Œ: ollama run apt-chinese:latest")
```

**3. è®­ç»ƒåè‡ªåŠ¨å¯¼å‡º**

```python
# é…ç½®è‡ªåŠ¨å¯¼å‡º
config = {
    'quantization': 'Q4_K_M',
    'auto_export': True,           # è®­ç»ƒç»“æŸè‡ªåŠ¨å¯¼å‡º
    'auto_register': True,         # è‡ªåŠ¨æ³¨å†Œåˆ°Ollama
    'output_dir': './ollama_models'
}

plugin = OllamaExportPlugin(config)

# åœ¨è®­ç»ƒå¾ªç¯ä¸­æ³¨å†Œæ’ä»¶
from apt_model.console.plugin_bus import PluginBus
bus = PluginBus()
bus.register(plugin)

# è®­ç»ƒç»“æŸåä¼šè‡ªåŠ¨è§¦å‘å¯¼å‡º
# bus.dispatch_event('on_training_end', context={
#     'checkpoint_path': './final_model'
# })
```

**4. æµ‹è¯•å¯¼å‡ºçš„æ¨¡å‹**

```python
# æµ‹è¯•æ¨¡å‹
response = plugin.test_model(
    model_name="apt-chinese:latest",
    prompt="ä½ å¥½ï¼ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚"
)

print(f"æ¨¡å‹å“åº”: {response}")

# æˆ–ä½¿ç”¨å‘½ä»¤è¡Œ
# $ ollama run apt-chinese:latest
# >>> ä½ å¥½ï¼ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±ã€‚
# ä½ å¥½ï¼æˆ‘æ˜¯ä¸€ä¸ªç”±APTæ¨¡å‹é©±åŠ¨çš„AIåŠ©æ‰‹...
```

**5. ä¸åŒé‡åŒ–æ–¹å¼å¯¹æ¯”**

```python
# å¯¼å‡ºå¤šä¸ªé‡åŒ–ç‰ˆæœ¬è¿›è¡Œå¯¹æ¯”
quantizations = ['Q4_0', 'Q4_K_M', 'Q5_K_M', 'Q8_0']

for quant in quantizations:
    plugin = OllamaExportPlugin({'quantization': quant})

    results = plugin.export_complete(
        model_path="./trained_model",
        output_dir=f"./ollama_export_{quant}",
        model_name=f"apt-model-{quant.lower()}",
        register=True
    )

    # æ£€æŸ¥æ–‡ä»¶å¤§å°
    import os
    size_mb = os.path.getsize(results['gguf_path']) / (1024 * 1024)
    print(f"{quant}: {size_mb:.2f} MB")

# è¾“å‡ºç¤ºä¾‹:
# Q4_0:   1250.32 MB  (æœ€å°)
# Q4_K_M: 1380.45 MB  (æ¨è)
# Q5_K_M: 1620.78 MB  (å¹³è¡¡)
# Q8_0:   2340.92 MB  (é«˜è´¨é‡)
```

#### é…ç½®å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `quantization` | str | 'Q4_K_M' | é‡åŒ–ç±»å‹ |
| `context_length` | int | 2048 | ä¸Šä¸‹æ–‡é•¿åº¦ |
| `temperature` | float | 0.7 | é‡‡æ ·æ¸©åº¦ |
| `auto_export` | bool | False | è®­ç»ƒåè‡ªåŠ¨å¯¼å‡º |
| `auto_register` | bool | False | è‡ªåŠ¨æ³¨å†Œåˆ°Ollama |
| `output_dir` | str | './ollama_export' | è¾“å‡ºç›®å½• |

#### é‡åŒ–æ–¹å¼è¯´æ˜

| é‡åŒ–ç±»å‹ | ç²¾åº¦ | ä½“ç§¯ | é€Ÿåº¦ | é€‚ç”¨åœºæ™¯ |
|---------|------|------|------|---------|
| **Q4_0** | â­â­ | æœ€å° | æœ€å¿« | èµ„æºå—é™ç¯å¢ƒ |
| **Q4_K_M** | â­â­â­ | å° | å¿« | **æ¨èç”¨äºç”Ÿäº§** |
| **Q5_K_M** | â­â­â­â­ | ä¸­ | ä¸­ | è´¨é‡è¦æ±‚é«˜ |
| **Q8_0** | â­â­â­â­â­ | å¤§ | æ…¢ | æœ€é«˜è´¨é‡ |
| **F16** | â­â­â­â­â­ | æœ€å¤§ | æœ€æ…¢ | ç ”ç©¶/å¯¹æ¯” |

#### Modelfile è‡ªå®šä¹‰

```python
# åˆ›å»ºè‡ªå®šä¹‰Modelfile
custom_system_prompt = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ä¸­æ–‡AIåŠ©æ‰‹ï¼Œä¸“æ³¨äºä»¥ä¸‹é¢†åŸŸ:
- æŠ€æœ¯é—®ç­”
- ä»£ç ç”Ÿæˆ
- æ–‡æ¡£å†™ä½œ

è¯·ç”¨ç®€æ´ã€ä¸“ä¸šçš„è¯­è¨€å›ç­”é—®é¢˜ã€‚"""

custom_template = """{{ if .System }}ç³»ç»Ÿ: {{ .System }}

{{ end }}{{ if .Prompt }}ç”¨æˆ·: {{ .Prompt }}
{{ end }}åŠ©æ‰‹: """

modelfile_path = plugin.create_modelfile(
    gguf_path="./apt-model.gguf",
    output_path="./Modelfile",
    system_prompt=custom_system_prompt,
    template=custom_template
)
```

#### å‘½ä»¤è¡Œä½¿ç”¨

å¯¼å‡ºåå¯ä»¥ç›´æ¥ç”¨Ollamaå‘½ä»¤è¡Œ:

```bash
# è¿è¡Œæ¨¡å‹
ollama run apt-chinese:latest

# äº¤äº’å¼å¯¹è¯
>>> ä½ å¥½ï¼
ä½ å¥½ï¼æˆ‘æ˜¯ä¸€ä¸ªç”±APTæ¨¡å‹é©±åŠ¨çš„AIåŠ©æ‰‹...

>>> è¯·ç”¨Pythonå†™ä¸€ä¸ªå¿«é€Ÿæ’åº
å½“ç„¶ï¼Œä¸‹é¢æ˜¯Pythonå®ç°çš„å¿«é€Ÿæ’åºç®—æ³•:
```python
def quicksort(arr):
    if len(arr) <= 1:
        return arr
    ...
```

# æŸ¥çœ‹æ¨¡å‹åˆ—è¡¨
ollama list

# åˆ é™¤æ¨¡å‹
ollama rm apt-chinese:latest

# å¤åˆ¶æ¨¡å‹
ollama cp apt-chinese:latest apt-chinese:backup
```

#### è¾“å‡ºæ–‡ä»¶ç»“æ„

```
ollama_export/
â”œâ”€â”€ apt-chinese.gguf         # GGUFæ¨¡å‹æ–‡ä»¶ (é‡åŒ–å)
â”œâ”€â”€ Modelfile                 # Ollamaé…ç½®æ–‡ä»¶
â””â”€â”€ README.md                 # ä½¿ç”¨è¯´æ˜ (å¯é€‰)
```

#### æ•…éšœæ’æŸ¥

**1. Ollamaæœªå®‰è£…**

```
âŒ Ollamaå‘½ä»¤æœªæ‰¾åˆ°
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# macOS
brew install ollama

# Linux
curl -fsSL https://ollama.ai/install.sh | sh

# Windows
# è®¿é—® https://ollama.ai/download
```

**2. GGUFè½¬æ¢å¤±è´¥**

```
âŒ GGUFè½¬æ¢å¤±è´¥: KeyError: 'model.embed_tokens.weight'
```

**è§£å†³æ–¹æ¡ˆ**:
```python
# ç¡®ä¿æ¨¡å‹è·¯å¾„æ­£ç¡®ï¼ŒåŒ…å« pytorch_model.bin
import os
print(os.listdir("./trained_model"))
# åº”è¯¥çœ‹åˆ°: ['pytorch_model.bin', 'config.json', ...]

# æˆ–è€…ä½¿ç”¨HuggingFaceæ ¼å¼
model = AutoModelForCausalLM.from_pretrained("./trained_model")
model.save_pretrained("./trained_model_fixed")
```

**3. æ³¨å†Œå¤±è´¥**

```
âŒ æ³¨å†Œå¤±è´¥: Error: model already exists
```

**è§£å†³æ–¹æ¡ˆ**:
```bash
# åˆ é™¤æ—§æ¨¡å‹
ollama rm apt-chinese:latest

# æˆ–ä½¿ç”¨ä¸åŒçš„æ ‡ç­¾
python export.py --model-name apt-chinese:v2
```

---

## ğŸ§  æ¨ç†æ’ä»¶

### 1. Beam Searchï¼ˆå¤šè·¯å¾„æœç´¢ï¼‰

#### åŠŸèƒ½æ¦‚è¿°

**Beam Search** æ˜¯ä¸€ç§**å¤šè·¯å¾„æœç´¢ç®—æ³•**ï¼Œç»´æŠ¤ k ä¸ªå€™é€‰æ¨ç†è·¯å¾„ï¼Œé€‰æ‹©å¾—åˆ†æœ€é«˜çš„è·¯å¾„ä½œä¸ºæœ€ç»ˆç­”æ¡ˆã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼š
```
è´ªå©ªæœç´¢:
æ¯æ­¥é€‰æ‹©æ¦‚ç‡æœ€é«˜çš„ token â†’ å¯èƒ½é™·å…¥å±€éƒ¨æœ€ä¼˜

Beam Search:
æ¯æ­¥ç»´æŠ¤ k ä¸ªå€™é€‰è·¯å¾„ â†’ ç»¼åˆè€ƒè™‘å…¨å±€å¾—åˆ† â†’ é€‰æ‹©æœ€ä¼˜è·¯å¾„
```

**é€‚ç”¨åœºæ™¯**ï¼š
- âœ… æ•°å­¦æ¨ç†ï¼ˆéœ€è¦ç²¾ç¡®æ­¥éª¤ï¼‰
- âœ… ä»£ç ç”Ÿæˆï¼ˆéœ€è¦æ­£ç¡®è¯­æ³•ï¼‰
- âœ… é€»è¾‘æ¨ç†ï¼ˆéœ€è¦å®Œæ•´æ¨ç†é“¾ï¼‰

#### ä½¿ç”¨æ–¹æ³•

**1. åŸºç¡€ä½¿ç”¨**

```python
from apt_model.console.plugins.reasoning.beam_search_plugin import BeamSearchReasoningPlugin

# åˆ›å»ºæ’ä»¶
beam_search = BeamSearchReasoningPlugin(config={
    'beam_width': 4,           # Beam å®½åº¦ï¼ˆå€™é€‰æ•°é‡ï¼‰
    'length_penalty': 0.6,     # é•¿åº¦æƒ©ç½šå‚æ•°
    'max_steps': 50,           # æœ€å¤§æ¨ç†æ­¥æ•°
    'diversity_penalty': 0.5,  # å¤šæ ·æ€§æƒ©ç½š
    'early_stopping': True     # æ—©åœ
})

# æ³¨å†Œåˆ°æ’ä»¶æ€»çº¿
bus.register(beam_search)
```

**2. æ¨ç†ç¤ºä¾‹**

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained('gpt2-medium')
tokenizer = AutoTokenizer.from_pretrained('gpt2-medium')

# åˆ›å»ºæ’ä»¶æ€»çº¿
bus = PluginBus()
beam_search = BeamSearchReasoningPlugin(config={'beam_width': 5})
bus.register(beam_search)

# æ¨ç†
question = "What is 15% of 80?"
input_ids = tokenizer.encode(question, return_tensors='pt')

# è§¦å‘æ¨ç†äº‹ä»¶
bus.dispatch_event('on_inference_start', context={
    'data': {
        'use_beam_search': True,
        'model': model,
        'tokenizer': tokenizer,
        'input_ids': input_ids
    }
})

# è§¦å‘è§£ç äº‹ä»¶
bus.dispatch_event('on_decode', context={
    'step': 0,
    'data': {
        'model': model,
        'tokenizer': tokenizer,
        'input_ids': input_ids
    }
})

# è·å–ç»“æœ
result = bus.get_data('beam_search_result')
print(f"Best path: {result['path']}")
print(f"Score: {result['score']:.4f}")
print(f"Steps: {result['num_steps']}")
```

**3. è‡ªå®šä¹‰è¯„åˆ†å‡½æ•°**

```python
class CustomBeamSearch(BeamSearchReasoningPlugin):
    """è‡ªå®šä¹‰ Beam Searchï¼šæ·»åŠ æ¨ç†æ­£ç¡®æ€§è¯„åˆ†"""

    def __init__(self, config):
        super().__init__(config)
        self.correctness_weight = 0.3  # æ­£ç¡®æ€§æƒé‡

    def _score_beam(self, beam, model, tokenizer):
        """è‡ªå®šä¹‰è¯„åˆ†ï¼šè¯­è¨€æ¨¡å‹å¾—åˆ† + æ­£ç¡®æ€§å¾—åˆ†"""
        # åŸå§‹å¾—åˆ†ï¼ˆè¯­è¨€æ¨¡å‹æ¦‚ç‡ï¼‰
        lm_score = beam.normalized_score(self.length_penalty)

        # æ­£ç¡®æ€§å¾—åˆ†ï¼ˆæ£€æŸ¥æ¨ç†é“¾æ˜¯å¦åˆç†ï¼‰
        correctness_score = self._check_reasoning(beam.tokens, tokenizer)

        # ç»¼åˆå¾—åˆ†
        final_score = (1 - self.correctness_weight) * lm_score + \
                      self.correctness_weight * correctness_score

        return final_score

    def _check_reasoning(self, tokens, tokenizer):
        """æ£€æŸ¥æ¨ç†é“¾æ­£ç¡®æ€§"""
        text = tokenizer.decode(tokens)

        # ç®€å•å¯å‘å¼ï¼šæ£€æŸ¥æ˜¯å¦åŒ…å«æ¨ç†å…³é”®è¯
        reasoning_keywords = ['because', 'therefore', 'so', 'å› æ­¤', 'æ‰€ä»¥']
        score = sum(1 for kw in reasoning_keywords if kw in text.lower())

        return min(score / len(reasoning_keywords), 1.0)

# ä½¿ç”¨
custom_beam = CustomBeamSearch(config={'beam_width': 4})
bus.register(custom_beam)
```

#### é…ç½®å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `beam_width` | int | 4 | Beam å®½åº¦ï¼ˆå€™é€‰è·¯å¾„æ•°ï¼‰ |
| `length_penalty` | float | 0.6 | é•¿åº¦æƒ©ç½šï¼ˆ0=æ— æƒ©ç½šï¼Œ1=å…¨æƒ©ç½šï¼‰ |
| `max_steps` | int | 50 | æœ€å¤§æ¨ç†æ­¥æ•° |
| `diversity_penalty` | float | 0.0 | å¤šæ ·æ€§æƒ©ç½šï¼ˆé¼“åŠ±ä¸åŒè·¯å¾„ï¼‰ |
| `early_stopping` | bool | True | æ—©åœï¼ˆæ‰€æœ‰è·¯å¾„å®Œæˆæ—¶åœæ­¢ï¼‰ |

#### è¾“å‡ºç»“æœ

```python
result = {
    'path': [101, 2054, 2003, ...],  # Token IDs
    'score': -4.23,                  # å½’ä¸€åŒ–å¾—åˆ†
    'num_steps': 12,                 # å®é™…æ­¥æ•°
    'beam_width': 4                  # Beam å®½åº¦
}
```

---

### 2. Self-Consistencyï¼ˆè‡ªæ´½æ¨ç†ï¼‰

#### åŠŸèƒ½æ¦‚è¿°

**Self-Consistency** é€šè¿‡ç”Ÿæˆ**å¤šæ¡ç‹¬ç«‹æ¨ç†è·¯å¾„**ï¼Œç„¶å**æŠ•ç¥¨é€‰æ‹©æœ€ä¸€è‡´**çš„ç­”æ¡ˆæ¥æå‡æ¨ç†å¯é æ€§ã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼š
```
å•æ¬¡ç”Ÿæˆ:
prompt â†’ æ¨¡å‹ç”Ÿæˆ â†’ ç­”æ¡ˆï¼ˆå¯èƒ½é”™è¯¯ï¼‰

Self-Consistency:
prompt â†’ ç”Ÿæˆ N æ¡è·¯å¾„ â†’ æå–ç­”æ¡ˆ â†’ æŠ•ç¥¨ â†’ æœ€ä¸€è‡´ç­”æ¡ˆï¼ˆæ›´å¯é ï¼‰
```

**ä¼˜åŠ¿**ï¼š
- âœ… æå‡æ¨ç†å‡†ç¡®æ€§ï¼ˆå°¤å…¶æ•°å­¦é¢˜ï¼‰
- âœ… æä¾›ç½®ä¿¡åº¦è¯„åˆ†
- âœ… æ•è·å¤šæ ·æ¨ç†æ–¹å¼

#### ä½¿ç”¨æ–¹æ³•

**1. åŸºç¡€ä½¿ç”¨**

```python
from apt_model.console.plugins.reasoning.self_consistency_plugin import SelfConsistencyPlugin

# åˆ›å»ºæ’ä»¶
sc_plugin = SelfConsistencyPlugin(config={
    'num_paths': 5,            # ç”Ÿæˆ 5 æ¡æ¨ç†è·¯å¾„
    'temperature': 0.7,        # é‡‡æ ·æ¸©åº¦ï¼ˆå¤šæ ·æ€§ï¼‰
    'answer_patterns': [       # ç­”æ¡ˆæå–æ¨¡å¼
        r'[Aa]nswer:\s*(.+)',
        r'ç­”æ¡ˆ[:ï¼š]\s*(.+)',
        r'å› æ­¤[:ï¼š]\s*(.+)',
    ]
})

# æ³¨å†Œåˆ°æ’ä»¶æ€»çº¿
bus.register(sc_plugin)
```

**2. å®Œæ•´æ¨ç†ç¤ºä¾‹**

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# åŠ è½½æ¨¡å‹
model = AutoModelForCausalLM.from_pretrained('gpt2-medium')
tokenizer = AutoTokenizer.from_pretrained('gpt2-medium')

# åˆ›å»ºæ’ä»¶æ€»çº¿
bus = PluginBus()
sc_plugin = SelfConsistencyPlugin(config={'num_paths': 10, 'temperature': 0.8})
bus.register(sc_plugin)

# æ¨ç†
question = "If a train travels at 60 mph for 2.5 hours, how far does it travel?"

# è§¦å‘æ¨ç†äº‹ä»¶
bus.dispatch_event('on_inference_start', context={
    'data': {
        'use_self_consistency': True
    }
})

# è§¦å‘è§£ç äº‹ä»¶
bus.dispatch_event('on_decode', context={
    'step': 0,
    'data': {
        'model': model,
        'tokenizer': tokenizer,
        'input_text': question
    }
})

# è·å–ç»“æœ
result = bus.get_data('self_consistency_result')

print(f"Selected Answer: {result['answer']}")
print(f"Confidence: {result['confidence']:.2%}")
print(f"Paths Generated: {result['paths_count']}")
print(f"Vote Distribution: {result['vote_distribution']}")

# ç¤ºä¾‹è¾“å‡º:
# Selected Answer: 150 miles
# Confidence: 80.00%
# Paths Generated: 10
# Vote Distribution: {'150 miles': 8, '15 miles': 1, '160 miles': 1}
```

**3. è‡ªå®šä¹‰ç­”æ¡ˆæå–**

```python
class MathSelfConsistency(SelfConsistencyPlugin):
    """æ•°å­¦é¢˜ä¸“ç”¨ Self-Consistencyï¼šæå–æ•°å€¼ç­”æ¡ˆ"""

    def _extract_answer(self, reasoning_path: str) -> str:
        """é‡å†™ç­”æ¡ˆæå–ï¼šä¸“é—¨æå–æ•°å€¼"""
        import re

        # ä¼˜å…ˆåŒ¹é…æ˜ç¡®çš„ç­”æ¡ˆæ ‡è®°
        for pattern in self.answer_patterns:
            match = re.search(pattern, reasoning_path, re.MULTILINE)
            if match:
                answer_text = match.group(1).strip()
                # æå–æ•°å€¼
                numbers = re.findall(r'[-+]?\d*\.?\d+', answer_text)
                if numbers:
                    return numbers[0]  # è¿”å›ç¬¬ä¸€ä¸ªæ•°å€¼

        # å›é€€ï¼šæå–è·¯å¾„ä¸­æœ€åå‡ºç°çš„æ•°å€¼
        numbers = re.findall(r'[-+]?\d*\.?\d+', reasoning_path)
        if numbers:
            return numbers[-1]

        return ""

    def _normalize_answer(self, answer: str) -> str:
        """å½’ä¸€åŒ–æ•°å€¼ç­”æ¡ˆ"""
        try:
            # è½¬æ¢ä¸ºæµ®ç‚¹æ•°å†è½¬å›å­—ç¬¦ä¸²ï¼ˆç»Ÿä¸€æ ¼å¼ï¼‰
            num = float(answer)
            # å¦‚æœæ˜¯æ•´æ•°ï¼Œå»æ‰å°æ•°ç‚¹
            if num == int(num):
                return str(int(num))
            return f"{num:.2f}"  # ä¿ç•™ 2 ä½å°æ•°
        except:
            return answer.lower().strip()

# ä½¿ç”¨
math_sc = MathSelfConsistency(config={'num_paths': 5})
bus.register(math_sc)
```

**4. ä¸ Chain-of-Thought ç»“åˆ**

```python
def self_consistency_with_cot(model, tokenizer, question, num_paths=5):
    """Self-Consistency + Chain-of-Thought"""

    # CoT Prompt
    cot_prompt = f"""Let's solve this step by step:

Question: {question}

Step-by-step solution:"""

    paths = []
    answers = []

    for i in range(num_paths):
        # ç”Ÿæˆæ¨ç†è·¯å¾„ï¼ˆä¸åŒæ¸©åº¦ï¼‰
        input_ids = tokenizer.encode(cot_prompt, return_tensors='pt')
        output = model.generate(
            input_ids,
            max_length=200,
            do_sample=True,
            temperature=0.7 + i * 0.05,  # é€’å¢æ¸©åº¦
            top_p=0.95
        )

        path = tokenizer.decode(output[0], skip_special_tokens=True)
        paths.append(path)

        # æå–ç­”æ¡ˆ
        answer = extract_final_answer(path)
        answers.append(answer)

    # æŠ•ç¥¨
    from collections import Counter
    vote_counts = Counter(answers)
    best_answer = vote_counts.most_common(1)[0]

    return {
        'answer': best_answer[0],
        'confidence': best_answer[1] / num_paths,
        'paths': paths,
        'all_answers': answers
    }

# ä½¿ç”¨
result = self_consistency_with_cot(
    model, tokenizer,
    question="What is 15% of 80?",
    num_paths=10
)
```

#### é…ç½®å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `num_paths` | int | 5 | ç”Ÿæˆè·¯å¾„æ•°é‡ |
| `temperature` | float | 0.7 | é‡‡æ ·æ¸©åº¦ï¼ˆå¤šæ ·æ€§ï¼‰ |
| `answer_patterns` | list | [...] | ç­”æ¡ˆæå–æ­£åˆ™æ¨¡å¼ |

#### è¾“å‡ºç»“æœ

```python
result = {
    'answer': '12',                          # é€‰æ‹©çš„ç­”æ¡ˆ
    'confidence': 0.80,                      # ç½®ä¿¡åº¦ï¼ˆ80%ï¼‰
    'paths_count': 5,                        # ç”Ÿæˆè·¯å¾„æ•°
    'vote_distribution': {                   # æŠ•ç¥¨åˆ†å¸ƒ
        '12': 4,
        '11.5': 1
    }
}
```

---

### 3. Program-Aidedï¼ˆç¨‹åºè¾…åŠ©æ¨ç†ï¼‰

#### åŠŸèƒ½æ¦‚è¿°

**Program-Aided Reasoning (PAL)** å°†è‡ªç„¶è¯­è¨€é—®é¢˜è½¬æ¢ä¸º**å¯æ‰§è¡Œ Python ä»£ç **ï¼Œé€šè¿‡ç¬¦å·è®¡ç®—è·å¾—ç²¾ç¡®ç­”æ¡ˆã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼š
```
ä¼ ç»Ÿ LLM æ¨ç†:
"15% of 80 is..." â†’ æ¨¡å‹çŒœæµ‹ â†’ å¯èƒ½å‡ºé”™

Program-Aided:
"15% of 80" â†’ ç”Ÿæˆä»£ç  "0.15 * 80" â†’ æ‰§è¡Œ â†’ 12.0ï¼ˆç²¾ç¡®ï¼‰
```

**ä¼˜åŠ¿**ï¼š
- âœ… æ•°å­¦è®¡ç®— 100% å‡†ç¡®
- âœ… æ”¯æŒå¤æ‚é€»è¾‘æ¨ç†
- âœ… å¯å®¡è®¡ï¼ˆä»£ç å¯è¯»ï¼‰

#### ä½¿ç”¨æ–¹æ³•

**1. åŸºç¡€ä½¿ç”¨**

```python
from apt_model.console.plugins.reasoning.program_aided_plugin import ProgramAidedReasoningPlugin

# åˆ›å»ºæ’ä»¶
pal_plugin = ProgramAidedReasoningPlugin(config={
    'timeout': 5.0,              # ä»£ç æ‰§è¡Œè¶…æ—¶ï¼ˆç§’ï¼‰
    'max_code_length': 1000,     # æœ€å¤§ä»£ç é•¿åº¦
    'allowed_modules': [         # å…è®¸çš„æ¨¡å—
        'math', 'statistics', 'datetime'
    ],
    'forbidden_keywords': [      # ç¦æ­¢çš„å…³é”®è¯
        'import os', 'eval', 'exec', 'open('
    ]
})

# æ³¨å†Œåˆ°æ’ä»¶æ€»çº¿
bus.register(pal_plugin)
```

**2. å®Œæ•´æ¨ç†ç¤ºä¾‹**

```python
from transformers import AutoModelForCausalLM, AutoTokenizer

# åŠ è½½ä»£ç ç”Ÿæˆæ¨¡å‹ï¼ˆå¦‚ CodeGenï¼‰
model = AutoModelForCausalLM.from_pretrained('Salesforce/codegen-350M-mono')
tokenizer = AutoTokenizer.from_pretrained('Salesforce/codegen-350M-mono')

# åˆ›å»ºæ’ä»¶æ€»çº¿
bus = PluginBus()
pal_plugin = ProgramAidedReasoningPlugin(config={'timeout': 10.0})
bus.register(pal_plugin)

# æ¨ç†
question = "A store has 120 apples. They sell 35% of them in the morning and 20% in the afternoon. How many apples are left?"

# è§¦å‘æ¨ç†äº‹ä»¶
bus.dispatch_event('on_inference_start', context={
    'data': {
        'use_program_aided': True
    }
})

# è§¦å‘è§£ç äº‹ä»¶
bus.dispatch_event('on_decode', context={
    'step': 0,
    'data': {
        'model': model,
        'tokenizer': tokenizer,
        'question': question
    }
})

# è·å–ç»“æœ
result = bus.get_data('program_aided_result')

if result['success']:
    print(f"Generated Code:\n{result['code']}")
    print(f"Execution Result: {result['result']}")
else:
    print(f"Error: {result['error']}")

# ç¤ºä¾‹è¾“å‡º:
# Generated Code:
# # Calculate remaining apples
# total_apples = 120
# morning_sold = total_apples * 0.35
# afternoon_sold = total_apples * 0.20
# remaining = total_apples - morning_sold - afternoon_sold
# print(remaining)
#
# Execution Result: 54.0
```

**3. è‡ªå®šä¹‰ä»£ç ç”Ÿæˆæç¤º**

```python
class MathPAL(ProgramAidedReasoningPlugin):
    """æ•°å­¦é¢˜ä¸“ç”¨ PALï¼šä¼˜åŒ–ä»£ç ç”Ÿæˆæç¤º"""

    def __init__(self, config):
        super().__init__(config)

        # è‡ªå®šä¹‰ä»£ç ç”Ÿæˆæç¤ºæ¨¡æ¿
        self.code_prompt_template = """# Solve this math problem using Python:
# Question: {question}
#
# Write clean, executable Python code to solve it.
# Use comments to explain your logic.
# Print the final answer.

import math

"""

    def _generate_code(self, model, tokenizer, question: str) -> str:
        """ç”Ÿæˆ Python ä»£ç """
        # åˆ›å»ºæç¤º
        prompt = self.code_prompt_template.format(question=question)

        # ç”Ÿæˆä»£ç 
        input_ids = tokenizer.encode(prompt, return_tensors='pt')
        output = model.generate(
            input_ids,
            max_length=len(input_ids[0]) + 200,
            temperature=0.2,  # ä½æ¸©åº¦ï¼ˆæ›´ç¡®å®šï¼‰
            top_p=0.95,
            do_sample=True,
            pad_token_id=tokenizer.eos_token_id
        )

        generated_text = tokenizer.decode(output[0], skip_special_tokens=True)

        # æå–ä»£ç éƒ¨åˆ†ï¼ˆå»æ‰æç¤ºï¼‰
        code = generated_text[len(prompt):].strip()

        # åå¤„ç†ï¼šç¡®ä¿æœ‰ print è¯­å¥
        if 'print(' not in code:
            # è‡ªåŠ¨æ·»åŠ  printï¼ˆå‡è®¾æœ€åä¸€ä¸ªå˜é‡æ˜¯ç­”æ¡ˆï¼‰
            lines = code.split('\n')
            if lines and '=' in lines[-1]:
                var_name = lines[-1].split('=')[0].strip()
                code += f"\nprint({var_name})"

        return code

# ä½¿ç”¨
math_pal = MathPAL(config={'timeout': 10.0})
bus.register(math_pal)
```

**4. å®‰å…¨æ‰§è¡Œæ²™ç®±**

```python
import ast
import sys
from io import StringIO

def safe_execute_code(code: str, timeout: float = 5.0) -> tuple:
    """
    å®‰å…¨æ‰§è¡Œ Python ä»£ç 

    Returns:
        (output, error)
    """
    # 1. é™æ€åˆ†æ
    try:
        tree = ast.parse(code)
    except SyntaxError as e:
        return None, f"Syntax error: {e}"

    # 2. æ£€æŸ¥å±é™©æ“ä½œ
    dangerous_nodes = []
    for node in ast.walk(tree):
        # æ£€æŸ¥ import è¯­å¥
        if isinstance(node, (ast.Import, ast.ImportFrom)):
            for alias in node.names if isinstance(node, ast.Import) else [node]:
                module = alias.name if isinstance(alias, ast.alias) else node.module
                if module not in ['math', 'statistics', 'datetime']:
                    dangerous_nodes.append(f"Forbidden import: {module}")

        # æ£€æŸ¥å‡½æ•°è°ƒç”¨
        if isinstance(node, ast.Call):
            if isinstance(node.func, ast.Name):
                if node.func.id in ['eval', 'exec', 'compile', '__import__']:
                    dangerous_nodes.append(f"Forbidden function: {node.func.id}")

    if dangerous_nodes:
        return None, "; ".join(dangerous_nodes)

    # 3. æ‰§è¡Œä»£ç ï¼ˆæœ‰é™ç¯å¢ƒï¼‰
    stdout_capture = StringIO()
    safe_globals = {
        '__builtins__': {
            'print': print,
            'range': range,
            'len': len,
            'sum': sum,
            'max': max,
            'min': min,
            'abs': abs,
            'round': round,
        },
        'math': __import__('math'),
    }

    try:
        with redirect_stdout(stdout_capture):
            exec(code, safe_globals)

        output = stdout_capture.getvalue().strip()
        return output, None

    except Exception as e:
        return None, f"{type(e).__name__}: {str(e)}"
```

#### é…ç½®å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `timeout` | float | 5.0 | ä»£ç æ‰§è¡Œè¶…æ—¶ï¼ˆç§’ï¼‰ |
| `max_code_length` | int | 1000 | æœ€å¤§ä»£ç é•¿åº¦ |
| `allowed_modules` | list | [...] | å…è®¸å¯¼å…¥çš„æ¨¡å— |
| `forbidden_keywords` | list | [...] | ç¦æ­¢çš„å…³é”®è¯ |

#### è¾“å‡ºç»“æœ

```python
# æˆåŠŸæ‰§è¡Œ
result = {
    'success': True,
    'result': '54.0',            # æ‰§è¡Œè¾“å‡º
    'code': '# Generated code...'  # ç”Ÿæˆçš„ä»£ç 
}

# æ‰§è¡Œå¤±è´¥
result = {
    'success': False,
    'error': 'Validation failed: Forbidden keyword found: import os',
    'code': '# Generated code...'
}
```

---

## ğŸ› ï¸ æ’ä»¶å¼€å‘

### åˆ›å»ºè‡ªå®šä¹‰æ’ä»¶

**1. æ’ä»¶æ¨¡æ¿**

```python
from apt_model.console.plugin_standards import (
    PluginBase,
    PluginManifest,
    PluginPriority,
    PluginEvent,
    PluginCapability
)
import logging

logger = logging.getLogger(__name__)


class MyCustomPlugin(PluginBase):
    """
    è‡ªå®šä¹‰æ’ä»¶ç¤ºä¾‹

    åŠŸèƒ½æè¿°ï¼š[ä½ çš„æ’ä»¶åŠŸèƒ½]
    """

    def __init__(self):
        """åˆå§‹åŒ–æ’ä»¶"""
        super().__init__()
        self.config = {}
        self.metrics = {
            'counter': 0,
            'avg_value': 0.0,
        }

    def get_manifest(self) -> PluginManifest:
        """
        è·å–æ’ä»¶æ¸…å•

        Returns:
            æ’ä»¶æ¸…å•
        """
        return PluginManifest(
            name="my_custom_plugin",
            version="1.0.0",
            description="My custom plugin for doing X",
            author="Your Name",

            # ä¼˜å…ˆçº§ï¼ˆæ ¹æ®æ’ä»¶ç±»å‹é€‰æ‹©ï¼‰
            priority=PluginPriority.EXPERIMENTAL,  # 650-799

            # æ˜¯å¦é˜»å¡ä¸»çº¿ç¨‹
            blocking=False,

            # ç›‘å¬çš„äº‹ä»¶
            events=[
                PluginEvent.ON_BATCH_START,
                PluginEvent.ON_STEP_END,
            ],

            # ä¾èµ–é¡¹
            requires=[
                "core:trainer",
            ],

            # å†²çªé¡¹
            conflicts=[],

            # èƒ½åŠ›
            capabilities=[
                PluginCapability.READ_METRICS,
                PluginCapability.WRITE_METRICS,
            ],

            # èµ„æºé¢„ç®—
            resources={
                "cpu_ms": 10.0,
                "gpu_ms": 5.0,
                "io_mb": 0.5
            },

            # é€Ÿç‡é™åˆ¶
            rate_limit={
                "steps": 10  # æ¯ 10 æ­¥æœ€å¤šæ‰§è¡Œä¸€æ¬¡
            },

            # æ²™ç®±æ¨¡å¼
            sandbox=True,

            # å¤±è´¥å®¹å¿åº¦
            fail_limit=5,

            # EQI å‚æ•°
            s_default=0.5,  # é»˜è®¤å‡€æ•ˆç”¨
            eta=1.0         # è¯æ®è°ƒåˆ¶å‚æ•°
        )

    def initialize(self, config: dict = None):
        """
        åˆå§‹åŒ–æ’ä»¶

        Args:
            config: é…ç½®å­—å…¸
        """
        if config:
            self.config = config
            logger.info(f"[MyCustomPlugin] Initialized with config: {config}")

    def on_batch_start(self, context: dict):
        """
        Batch å¼€å§‹äº‹ä»¶å¤„ç†å™¨

        Args:
            context: äº‹ä»¶ä¸Šä¸‹æ–‡
        """
        step = context.get('step', 0)
        data = context.get('data', {})

        # ä½ çš„é€»è¾‘
        self.metrics['counter'] += 1

        logger.debug(f"[MyCustomPlugin] on_batch_start at step {step}")

    def on_step_end(self, context: dict):
        """
        Step ç»“æŸäº‹ä»¶å¤„ç†å™¨

        Args:
            context: äº‹ä»¶ä¸Šä¸‹æ–‡
        """
        step = context.get('step', 0)
        data = context.get('data', {})

        # è¯»å–æŒ‡æ ‡
        metrics = data.get('metrics', {})

        # ä½ çš„é€»è¾‘
        value = metrics.get('loss', 0.0)
        self.metrics['avg_value'] = (
            (self.metrics['avg_value'] * (self.metrics['counter'] - 1) + value)
            / self.metrics['counter']
        )

        # å†™å…¥æŒ‡æ ‡
        if 'metrics' not in data:
            data['metrics'] = {}
        data['metrics']['my_custom_metric'] = self.metrics['avg_value']

        logger.debug(f"[MyCustomPlugin] on_step_end at step {step}")

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        logger.info(f"[MyCustomPlugin] Cleanup: {self.metrics}")
```

**2. æ³¨å†Œå’Œä½¿ç”¨**

```python
# åˆ›å»ºæ’ä»¶
my_plugin = MyCustomPlugin()

# åˆå§‹åŒ–
my_plugin.initialize({
    'param1': 'value1',
    'param2': 42
})

# æ³¨å†Œåˆ°æ’ä»¶æ€»çº¿
bus = PluginBus()
bus.register(my_plugin)

# è®­ç»ƒå¾ªç¯
for step, batch in enumerate(dataloader):
    # Batch å¼€å§‹
    bus.dispatch_event('on_batch_start', context={
        'step': step,
        'data': {}
    })

    # ... è®­ç»ƒä»£ç  ...
    loss = train_step(batch)

    # Step ç»“æŸ
    bus.dispatch_event('on_step_end', context={
        'step': step,
        'data': {
            'metrics': {'loss': loss}
        }
    })
```

### æ’ä»¶å¼€å‘æœ€ä½³å®è·µ

**1. é€‰æ‹©åˆé€‚çš„ä¼˜å…ˆçº§**

```python
# æ ¹æ®æ’ä»¶åŠŸèƒ½é€‰æ‹©ä¼˜å…ˆçº§
if plugin_type == 'kill_switch':
    priority = PluginPriority.KILLSWITCH  # 0-49
elif plugin_type == 'inference':
    priority = PluginPriority.INFERENCE_CTRL  # 50-149
elif plugin_type == 'optimization':
    priority = PluginPriority.THROUGHPUT  # 150-249
elif plugin_type == 'reasoning':
    priority = PluginPriority.BEAM_SEARCH  # 250-349
elif plugin_type == 'training':
    priority = PluginPriority.GRPO  # 350-449
```

**2. å¼‚å¸¸å¤„ç†**

```python
def on_step_end(self, context: dict):
    """Step ç»“æŸäº‹ä»¶å¤„ç†å™¨ï¼ˆå¸¦å¼‚å¸¸å¤„ç†ï¼‰"""
    try:
        step = context.get('step', 0)
        data = context.get('data', {})

        # ä½ çš„é€»è¾‘
        result = self.process(data)

        # å†™å…¥ç»“æœ
        data['my_result'] = result

    except Exception as e:
        logger.error(f"[MyPlugin] Error in on_step_end: {e}")
        # è®°å½•å¤±è´¥ï¼ˆä¼šè§¦å‘ fail_limit æ£€æŸ¥ï¼‰
        self.record_failure()
```

**3. èµ„æºç®¡ç†**

```python
class ResourceAwarePlugin(PluginBase):
    """èµ„æºæ„ŸçŸ¥æ’ä»¶"""

    def __init__(self):
        super().__init__()
        self.gpu_available = torch.cuda.is_available()
        self.cache = {}

    def get_manifest(self) -> PluginManifest:
        # æ ¹æ®å¯ç”¨èµ„æºè°ƒæ•´é¢„ç®—
        gpu_ms = 50.0 if self.gpu_available else 0.0
        cpu_ms = 20.0 if not self.gpu_available else 5.0

        return PluginManifest(
            # ...
            resources={
                "cpu_ms": cpu_ms,
                "gpu_ms": gpu_ms,
                "io_mb": 2.0
            }
        )

    def cleanup(self):
        """æ¸…ç†èµ„æº"""
        # æ¸…ç©ºç¼“å­˜
        self.cache.clear()

        # é‡Šæ”¾ GPU å†…å­˜
        if self.gpu_available:
            torch.cuda.empty_cache()

        logger.info("[ResourceAwarePlugin] Resources cleaned up")
```

---

## ğŸš€ é«˜çº§åº”ç”¨

### 1. å¤šæ’ä»¶ç»„åˆ

```python
# ç»„åˆå¤šä¸ªæ¨ç†æ’ä»¶
bus = PluginBus()

# 1. Beam Searchï¼ˆæ¢ç´¢å¤šæ¡è·¯å¾„ï¼‰
beam_search = BeamSearchReasoningPlugin(config={
    'beam_width': 5,
    'max_steps': 30
})
bus.register(beam_search)

# 2. Self-Consistencyï¼ˆæŠ•ç¥¨æœºåˆ¶ï¼‰
sc_plugin = SelfConsistencyPlugin(config={
    'num_paths': 10,
    'temperature': 0.8
})
# æ³¨æ„ï¼šä¸ Beam Search å†²çªï¼ŒäºŒé€‰ä¸€
# bus.register(sc_plugin)

# 3. Program-Aidedï¼ˆç²¾ç¡®è®¡ç®—ï¼‰
pal_plugin = ProgramAidedReasoningPlugin(config={
    'timeout': 10.0
})
bus.register(pal_plugin)

# 4. EQI Reporterï¼ˆç›‘æ§ï¼‰
eqi_reporter = EQIReporterPlugin()
eqi_reporter.initialize({'report_interval': 50})
bus.register(eqi_reporter)

# æ¨ç†ï¼šå…ˆå°è¯• PALï¼Œå¤±è´¥åˆ™ç”¨ Beam Search
question = "Complex math problem..."

# å°è¯• PAL
bus.dispatch_event('on_inference_start', context={
    'data': {'use_program_aided': True}
})
bus.dispatch_event('on_decode', context={
    'step': 0,
    'data': {'model': model, 'tokenizer': tokenizer, 'question': question}
})

pal_result = bus.get_data('program_aided_result')

if not pal_result or not pal_result.get('success'):
    # PAL å¤±è´¥ï¼Œä½¿ç”¨ Beam Search
    logger.info("PAL failed, falling back to Beam Search")

    bus.dispatch_event('on_inference_start', context={
        'data': {'use_beam_search': True}
    })
    bus.dispatch_event('on_decode', context={
        'step': 0,
        'data': {'model': model, 'tokenizer': tokenizer, 'input_ids': input_ids}
    })

    beam_result = bus.get_data('beam_search_result')
    answer = tokenizer.decode(beam_result['path'])
else:
    answer = pal_result['result']

print(f"Final answer: {answer}")
```

### 2. åŠ¨æ€æ’ä»¶åŠ è½½

```python
import importlib

class PluginLoader:
    """åŠ¨æ€æ’ä»¶åŠ è½½å™¨"""

    def __init__(self, plugin_dir: str = "plugins"):
        self.plugin_dir = plugin_dir
        self.loaded_plugins = {}

    def load_plugin(self, plugin_name: str):
        """åŠ¨æ€åŠ è½½æ’ä»¶"""
        try:
            # å¯¼å…¥æ’ä»¶æ¨¡å—
            module_path = f"{self.plugin_dir}.{plugin_name}"
            module = importlib.import_module(module_path)

            # æŸ¥æ‰¾æ’ä»¶ç±»ï¼ˆçº¦å®šï¼šç±»å = æ’ä»¶å + Pluginï¼‰
            plugin_class_name = ''.join(word.capitalize() for word in plugin_name.split('_')) + 'Plugin'
            plugin_class = getattr(module, plugin_class_name)

            # å®ä¾‹åŒ–æ’ä»¶
            plugin = plugin_class()

            self.loaded_plugins[plugin_name] = plugin
            logger.info(f"Loaded plugin: {plugin_name}")

            return plugin

        except Exception as e:
            logger.error(f"Failed to load plugin {plugin_name}: {e}")
            return None

    def load_all_plugins(self, bus: PluginBus):
        """åŠ è½½æ‰€æœ‰æ’ä»¶"""
        import os
        import glob

        # æŸ¥æ‰¾æ‰€æœ‰æ’ä»¶æ–‡ä»¶
        plugin_files = glob.glob(os.path.join(self.plugin_dir, "*_plugin.py"))

        for plugin_file in plugin_files:
            plugin_name = os.path.basename(plugin_file)[:-3]  # å»æ‰ .py
            plugin = self.load_plugin(plugin_name)

            if plugin:
                bus.register(plugin)

# ä½¿ç”¨
loader = PluginLoader(plugin_dir="apt_model/console/plugins")
loader.load_all_plugins(bus)
```

### 3. æ’ä»¶é…ç½®æ–‡ä»¶

```yaml
# plugins_config.yaml
plugins:
  - name: grpo
    enabled: true
    config:
      group_size: 4
      learning_rate: 0.00001
      advantage_type: relative

  - name: route_optimizer
    enabled: true
    config:
      num_experts: 8
      load_threshold: 1.5

  - name: beam_search
    enabled: false  # ç¦ç”¨
    config:
      beam_width: 5
      length_penalty: 0.6

  - name: self_consistency
    enabled: true
    config:
      num_paths: 10
      temperature: 0.8
```

```python
import yaml

def load_plugins_from_config(config_file: str, bus: PluginBus):
    """ä»é…ç½®æ–‡ä»¶åŠ è½½æ’ä»¶"""
    with open(config_file, 'r') as f:
        config = yaml.safe_load(f)

    plugin_registry = {
        'grpo': GRPOPlugin,
        'route_optimizer': RouteOptimizerPlugin,
        'beam_search': BeamSearchReasoningPlugin,
        'self_consistency': SelfConsistencyPlugin,
        'program_aided': ProgramAidedReasoningPlugin,
        'eqi_reporter': EQIReporterPlugin,
    }

    for plugin_spec in config['plugins']:
        if not plugin_spec.get('enabled', True):
            continue

        name = plugin_spec['name']
        plugin_class = plugin_registry.get(name)

        if not plugin_class:
            logger.warning(f"Unknown plugin: {name}")
            continue

        # åˆ›å»ºæ’ä»¶
        plugin = plugin_class()

        # åˆå§‹åŒ–ï¼ˆå¦‚æœæœ‰é…ç½®ï¼‰
        if 'config' in plugin_spec:
            plugin.initialize(plugin_spec['config'])

        # æ³¨å†Œ
        bus.register(plugin)
        logger.info(f"Registered plugin from config: {name}")

# ä½¿ç”¨
load_plugins_from_config('plugins_config.yaml', bus)
```

---

## ğŸ› æ•…éšœæ’æŸ¥

### å¸¸è§é—®é¢˜

**1. æ’ä»¶å†²çª**

```
é”™è¯¯: ConflictError: Plugin 'self_consistency' conflicts with 'beam_search'
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# ä¸è¦åŒæ—¶æ³¨å†Œå†²çªæ’ä»¶
bus.register(beam_search)
# bus.register(sc_plugin)  # âŒ ä¼šå†²çª

# æˆ–è€…æ ¹æ®åœºæ™¯é€‰æ‹©
if task_type == 'math':
    bus.register(pal_plugin)  # æ•°å­¦é¢˜ç”¨ PAL
elif task_type == 'reasoning':
    bus.register(beam_search)  # æ¨ç†é¢˜ç”¨ Beam Search
```

**2. èµ„æºè¶…é™**

```
é”™è¯¯: ResourceExceededError: CPU budget exceeded (500ms > 450ms)
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# æ–¹æ³•1: è°ƒæ•´æ’ä»¶é…ç½®ï¼ˆå‡å°‘è®¡ç®—é‡ï¼‰
grpo_plugin.initialize({
    'group_size': 2  # å‡å°‘åˆ° 2ï¼ˆé»˜è®¤ 4ï¼‰
})

# æ–¹æ³•2: å¢åŠ é€Ÿç‡é™åˆ¶
manifest = grpo_plugin.get_manifest()
manifest.rate_limit['steps'] = 5  # æ¯ 5 æ­¥æ‰§è¡Œä¸€æ¬¡ï¼ˆé»˜è®¤ 1ï¼‰

# æ–¹æ³•3: ä½¿ç”¨å¼‚æ­¥æ¨¡å¼
manifest.blocking = False  # æ”¹ä¸ºéé˜»å¡
```

**3. æ’ä»¶å¤±è´¥**

```
é”™è¯¯: Plugin 'my_plugin' reached fail_limit (5 failures)
```

**è§£å†³æ–¹æ¡ˆ**ï¼š
```python
# æ£€æŸ¥æ’ä»¶æ—¥å¿—
logger.info(f"Plugin failures: {my_plugin.get_context('failures')}")

# å¢åŠ å¤±è´¥å®¹å¿åº¦
manifest = my_plugin.get_manifest()
manifest.fail_limit = 10  # å¢åŠ åˆ° 10ï¼ˆé»˜è®¤ 5ï¼‰

# æ·»åŠ å¼‚å¸¸å¤„ç†
def on_step_end(self, context: dict):
    try:
        # ä½ çš„é€»è¾‘
        pass
    except Exception as e:
        logger.error(f"Error: {e}")
        # é™çº§å¤„ç†
        self.use_fallback_logic()
```

### è°ƒè¯•æŠ€å·§

**1. å¯ç”¨è¯¦ç»†æ—¥å¿—**

```python
import logging

# è®¾ç½®æ—¥å¿—çº§åˆ«
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# æ’ä»¶æ—¥å¿—
logger = logging.getLogger('apt_model.console.plugins')
logger.setLevel(logging.DEBUG)
```

**2. æ’ä»¶æ€§èƒ½åˆ†æ**

```python
import time

class ProfilingPlugin(PluginBase):
    """æ€§èƒ½åˆ†ææ’ä»¶"""

    def __init__(self):
        super().__init__()
        self.timings = []

    def on_step_end(self, context: dict):
        start_time = time.perf_counter()

        # ä½ çš„é€»è¾‘
        self.process(context)

        elapsed = time.perf_counter() - start_time
        self.timings.append(elapsed)

        if len(self.timings) % 100 == 0:
            avg_time = sum(self.timings) / len(self.timings)
            logger.info(f"[Profiling] Average time: {avg_time*1000:.2f}ms")
```

**3. æ’ä»¶çŠ¶æ€æ£€æŸ¥**

```python
# è·å–æ‰€æœ‰å·²æ³¨å†Œæ’ä»¶
registered_plugins = bus.list_plugins()
print(f"Registered plugins: {registered_plugins}")

# è·å–æ’ä»¶çŠ¶æ€
for plugin_name in registered_plugins:
    manifest = bus.get_plugin_manifest(plugin_name)
    print(f"\n{plugin_name}:")
    print(f"  Priority: {manifest.priority}")
    print(f"  Events: {manifest.events}")
    print(f"  Resources: {manifest.resources}")
    print(f"  Conflicts: {manifest.conflicts}")
```

---

## ğŸ“š å‚è€ƒèµ„æº

### å­¦æœ¯è®ºæ–‡

- [PAL: Program-aided Language Models](https://arxiv.org/abs/2211.10435) - Gao et al., 2022
- [Self-Consistency Improves Chain of Thought](https://arxiv.org/abs/2203.11171) - Wang et al., 2022
- [Beam Search Strategies for Neural Machine Translation](https://arxiv.org/abs/1702.01806) - Freitag & Al-Onaizan, 2017
- [Group Relative Policy Optimization](https://arxiv.org/abs/2402.03300) - GRPO è®ºæ–‡

### APT ç›¸å…³æ–‡æ¡£

- [æ’ä»¶ç³»ç»Ÿæ¶æ„](PLUGIN_SYSTEM.md) - æ’ä»¶ç³»ç»Ÿè®¾è®¡æ–‡æ¡£
- [DeepSeek è®­ç»ƒæŒ‡å—](DEEPSEEK_TRAINING_GUIDE.md) - MoE è®­ç»ƒæ•™ç¨‹
- [å›¾è„‘è®­ç»ƒæ•™ç¨‹](GRAPH_BRAIN_TRAINING_GUIDE.md) - å›¾æ¨ç†æ¶æ„
- [æ•°æ®é¢„å¤„ç†æŒ‡å—](DATA_PREPROCESSING_GUIDE.md) - æ•°æ®æ¸…æ´—æµç¨‹

### ä»£ç ç¤ºä¾‹

```bash
# æ’ä»¶ç¤ºä¾‹ä»£ç 
apt_model/console/plugins/
â”œâ”€â”€ grpo_plugin.py              # GRPO è®­ç»ƒæ’ä»¶
â”œâ”€â”€ route_optimizer_plugin.py   # è·¯ç”±ä¼˜åŒ–æ’ä»¶
â”œâ”€â”€ eqi_reporter_plugin.py      # EQI ä¸ŠæŠ¥æ’ä»¶
â””â”€â”€ reasoning/
    â”œâ”€â”€ beam_search_plugin.py        # Beam Search
    â”œâ”€â”€ self_consistency_plugin.py   # Self-Consistency
    â””â”€â”€ program_aided_plugin.py      # Program-Aided
```

---

## ğŸ“ æ›´æ–°æ—¥å¿—

- **v1.0.0** (2025-12) - åˆå§‹ç‰ˆæœ¬
  - âœ… æ ¸å¿ƒæ’ä»¶æ–‡æ¡£ï¼ˆGRPOã€Route Optimizerã€EQI Reporterï¼‰
  - âœ… æ¨ç†æ’ä»¶æ–‡æ¡£ï¼ˆBeam Searchã€Self-Consistencyã€PALï¼‰
  - âœ… æ’ä»¶å¼€å‘æŒ‡å—
  - âœ… å®Œæ•´ä»£ç ç¤ºä¾‹
  - âœ… æ•…éšœæ’æŸ¥æŒ‡å—

---

<div align="center">

**è®©æ’ä»¶ç³»ç»Ÿä¸ºä½ çš„ AI æ¨¡å‹èµ‹èƒ½ï¼ ğŸš€**

26+ ç”Ÿäº§çº§æ’ä»¶ï¼Œå¼€ç®±å³ç”¨ï¼Œçµæ´»å¯æ‰©å±•

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æäº¤ [Issue](https://github.com/chen0430tw/APT-Transformer/issues)

</div>
