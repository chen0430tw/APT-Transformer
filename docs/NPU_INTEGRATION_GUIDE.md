# è™šæ‹ŸBlackwell å¤šå‚å•†åŠ é€Ÿå™¨é›†æˆæŒ‡å—

## ğŸ“Œ æ¦‚è¿°

è™šæ‹ŸBlackwellç°å·²å®Œå…¨æ”¯æŒ**å¤šå‚å•†AIåŠ é€Ÿå™¨**ï¼Œå®ç°GPU/HPU/NPU/XPU/CPUçš„ç»Ÿä¸€åŠ é€Ÿæ¥å£ã€‚

### æ”¯æŒçš„ç¡¬ä»¶

| ç¡¬ä»¶ç±»å‹ | å‚å•† | æ”¯æŒçŠ¶æ€ | æ€§èƒ½ | PyTorchåŒ… |
|---------|-----|---------|------|-----------|
| ğŸŸ¢ NVIDIA GPU | NVIDIA | âœ… å®Œå…¨æ”¯æŒ | æœ€å¿« (900 GB/s) | `torch.cuda` |
| ğŸŸ£ Habana Gaudi | Intel | âœ… å®Œå…¨æ”¯æŒ | å¾ˆå¿« (700 GB/s) | `habana_frameworks.torch` |
| ğŸŸ¡ Ascend NPU | åä¸º | âœ… å®Œå…¨æ”¯æŒ | å¿«é€Ÿ (600 GB/s) | `torch_npu` |
| ğŸ”µ Intel XPU | Intel | âœ… å®Œå…¨æ”¯æŒ | ä¸­ç­‰ (400 GB/s) | `intel_extension_for_pytorch` |
| ğŸŸ  AMD GPU | AMD | âœ… å®Œå…¨æ”¯æŒ | å¿«é€Ÿ (ROCm) | `torch.cuda` (ROCm) |
| âšª CPU | - | âœ… Fallback | æ…¢é€Ÿ (50 GB/s) | `torch` |

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹å¼1: è‡ªåŠ¨æ£€æµ‹ï¼ˆæ¨èï¼‰

```python
import apt_model.optimization.vb_global as vb

# è‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨æœ€ä½³è®¾å¤‡ï¼ˆä¼˜å…ˆçº§: CUDA > HPU > NPU > XPU > CPUï¼‰
vb.enable_balanced_mode()

# è¾“å‡ºç¤ºä¾‹ (Intel Habana Gaudi)ï¼š
# ğŸš€ è™šæ‹ŸBlackwellå·²å…¨å±€å¯ç”¨
# åŠ é€Ÿè®¾å¤‡:        ğŸŸ£ Intel Habana Gaudi HPU
# FP4é‡åŒ–:         âŒ ç¦ç”¨
# Flash Attention: âœ… å¯ç”¨
# ...
```

### æ–¹å¼2: æ˜¾å¼æŒ‡å®šåŠ é€Ÿå™¨

```python
from apt_model.core.system import get_device

# ä¼˜å…ˆä½¿ç”¨Intel Habana Gaudi HPU
device = get_device(prefer_hpu=True)
print(device)  # hpu:0

# ä¼˜å…ˆä½¿ç”¨åä¸ºæ˜‡è…¾NPU
device = get_device(prefer_npu=True)
print(device)  # npu:0

# ä¼˜å…ˆä½¿ç”¨Intel XPU
device = get_device(prefer_xpu=True)
print(device)  # xpu:0

# å¼ºåˆ¶ä½¿ç”¨CPU
device = get_device(force_cpu=True)
print(device)  # cpu
```

### æ–¹å¼3: ç¯å¢ƒå˜é‡

```bash
# å¯ç”¨è™šæ‹ŸBlackwell + è‡ªåŠ¨æ£€æµ‹åŠ é€Ÿå™¨
export ENABLE_VIRTUAL_BLACKWELL=1
export VB_MODE=balanced

# è¿è¡Œè®­ç»ƒè„šæœ¬
python training/train.py
```

---

## ğŸ­ æ”¯æŒçš„åŠ é€Ÿå™¨è¯¦è§£

### 1. ğŸŸ¢ NVIDIA CUDA GPU

**ç‰¹ç‚¹**:
- æœ€æˆç†Ÿçš„AIåŠ é€Ÿå™¨ç”Ÿæ€
- æœ€é«˜æ€§èƒ½ (900 GB/s NVLink)
- åŸç”ŸPyTorchæ”¯æŒ

**å®‰è£…**:
```bash
# CUDAå·²åŒ…å«åœ¨PyTorchä¸­
pip install torch torchvision torchaudio
```

**ä½¿ç”¨**:
```python
device = torch.device('cuda')  # è‡ªåŠ¨ä½¿ç”¨
```

---

### 2. ğŸŸ£ Intel Habana Gaudi HPU

**ç‰¹ç‚¹**:
- ä¸“ä¸ºè®­ç»ƒä¼˜åŒ–çš„AIå¤„ç†å™¨
- Gaudi2: 96GB HBM2E, 700 GB/så¸¦å®½
- PyTorch 2.7.1åŸç”Ÿæ”¯æŒ

**å®‰è£…**:
```bash
pip install habana-torch-plugin
pip install habana-torch-dataloader
```

**ä½¿ç”¨**:
```python
import habana_frameworks.torch as ht

device = torch.device('hpu')
model = model.to(device)

# è™šæ‹ŸBlackwellè‡ªåŠ¨æ£€æµ‹
vb.enable()  # è‡ªåŠ¨ä½¿ç”¨HPU
```

**æ–‡æ¡£**: [Habana Gaudi Documentation](https://docs.habana.ai/)

---

### 3. ğŸŸ¡ åä¸ºæ˜‡è…¾NPU (Ascend)

**ç‰¹ç‚¹**:
- ä¸­å›½æœ¬åœŸAIåŠ é€Ÿå™¨
- Ascend 910B: 32GB HBM, 600 GB/s
- å®Œæ•´çš„torch_npuæ”¯æŒ

**å®‰è£…**:
```bash
pip install torch-npu
```

**ä½¿ç”¨**:
```python
import torch_npu

device = torch.device('npu:0')
model = model.to(device)

# è™šæ‹ŸBlackwellæ”¯æŒ
device = get_device(prefer_npu=True)
```

**æ–‡æ¡£**: [Ascend Documentation](https://www.hiascend.com/)

---

### 4. ğŸ”µ Intel XPU (åŒ…æ‹¬Ultra NPU)

**ç‰¹ç‚¹**:
- Intel Arc GPU + Ultra NPU (Meteor Lake)
- PyTorch 2.5+åŸç”Ÿæ”¯æŒIntel GPU
- é€‚ç”¨äºç¬”è®°æœ¬å’Œè¾¹ç¼˜è®¾å¤‡

**å®‰è£…**:
```bash
pip install intel-extension-for-pytorch
```

**ä½¿ç”¨**:
```python
import intel_extension_for_pytorch as ipex

device = torch.device('xpu')
model = model.to(device)

# è™šæ‹ŸBlackwellæ”¯æŒ
device = get_device(prefer_xpu=True)
```

**æ³¨æ„**: `intel-npu-acceleration-library`å·²å½’æ¡£ï¼Œå»ºè®®ä½¿ç”¨IPEXã€‚

**æ–‡æ¡£**: [Intel Extension for PyTorch](https://intel.github.io/intel-extension-for-pytorch/)

---

### 5. ğŸŸ  AMD ROCm GPU

**ç‰¹ç‚¹**:
- AMD GPUé€šè¿‡ROCmæ”¯æŒ
- å…¼å®¹PyTorch CUDAæ¥å£
- MI250/MI300ç³»åˆ—

**å®‰è£…**:
```bash
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/rocm5.7
```

**ä½¿ç”¨**:
```python
device = torch.device('cuda')  # ROCmä¼ªè£…æˆCUDA
model = model.to(device)
```

---

## ğŸ“¦ NPUåç«¯é€‚é…å™¨

### ç»Ÿä¸€è®¾å¤‡æ¥å£

```python
from apt_model.optimization.npu_backend import (
    DeviceBackend,
    get_unified_backend,
    is_npu_available,
    get_accelerator_type
)

# æ£€æµ‹åŠ é€Ÿå™¨ç±»å‹
accel_type = get_accelerator_type()
print(accel_type)  # 'cuda', 'npu', æˆ– 'cpu'

# NPUå¯ç”¨æ€§æ£€æŸ¥
if is_npu_available():
    print("NPUå¯ç”¨ï¼")

# è·å–ç»Ÿä¸€åç«¯
backend = get_unified_backend()
print(backend.device_type)  # 'npu' / 'cuda' / 'cpu'
print(backend.get_device_name())  # 'NPU Ascend 910B'
```

### è®¾å¤‡ç®¡ç†å™¨

```python
from apt_model.optimization.npu_backend import get_device_manager

# è·å–å…¨å±€è®¾å¤‡ç®¡ç†å™¨
manager = get_device_manager()

# åˆ—å‡ºæ‰€æœ‰è®¾å¤‡
devices = manager.get_all_devices()
# [device(type='npu', index=0), device(type='npu', index=1), device(type='cpu')]

# è·å–æœ€ä½³è®¾å¤‡
best_device = manager.get_best_device(prefer_npu=True)
print(best_device)  # npu:0

# è®¾å¤‡æ‘˜è¦
summary = manager.get_device_summary()
print(summary)
# {
#   'total_devices': 3,
#   'cuda_devices': 0,
#   'npu_devices': 2,
#   'devices': {
#     'npu:0': {'device_name': 'NPU Ascend 910B', ...}
#   }
# }
```

---

## ğŸ”§ æ ¸å¿ƒåŠŸèƒ½

### 1. è®¾å¤‡æ£€æµ‹ä¸é€‰æ‹©

```python
from apt_model.core.system import get_device_info

# è·å–è¯¦ç»†è®¾å¤‡ä¿¡æ¯
info = get_device_info()
print(info)
# {
#   'cuda_available': False,
#   'npu_available': True,
#   'device_count': 2,
#   'device_name': 'NPU Ascend 910B',
#   'device_type': 'npu',
#   'npu_version': '5.0.0'
# }
```

### 2. å†…å­˜ç®¡ç†

```python
from apt_model.core.system import (
    get_memory_info,
    memory_cleanup
)

# è·å–å†…å­˜ä¿¡æ¯ï¼ˆæ”¯æŒNPUï¼‰
mem_info = get_memory_info()
print(mem_info)
# {
#   'ram': {...},
#   'vram': {},  # GPUå†…å­˜ï¼ˆå¦‚æœæœ‰ï¼‰
#   'npu_memory': {
#     'npu_0': {
#       'allocated_gb': 2.5,
#       'reserved_gb': 3.0,
#       'max_allocated_gb': 2.8
#     }
#   }
# }

# æ¸…ç†æ‰€æœ‰è®¾å¤‡ç¼“å­˜ï¼ˆGPU + NPUï¼‰
memory_cleanup()
```

### 3. VGPU Stackè‡ªåŠ¨é€‚é…

VGPU Stackä¼šè‡ªåŠ¨æ£€æµ‹NPUå¹¶é…ç½®æœ€ä½³å±‚çº§ï¼š

```python
from apt_model.optimization.vgpu_stack import create_vgpu_stack

# è‡ªåŠ¨åˆ›å»ºNPUå †å 
stack = create_vgpu_stack()

# NPUé…ç½®ç¤ºä¾‹ï¼š
# Level 0: npu:0 - 2000MB @ 600.0GB/s  ï¼ˆNPU HBMï¼‰
# Level 1: cpu - 8000MB @ 40.0GB/s     ï¼ˆCPUå†…å­˜ï¼‰
# Level 2: ssd - 32000MB @ 7.0GB/s     ï¼ˆNVMeï¼‰
```

### 4. éšæœºç§å­è®¾ç½®

```python
from apt_model.core.system import set_seed

# åŒæ—¶è®¾ç½®CPU/GPU/NPUç§å­
set_seed(42)
```

---

## ğŸ¯ å®æˆ˜ç¤ºä¾‹

### å®Œæ•´è®­ç»ƒæµç¨‹

```python
#!/usr/bin/env python3
import torch
import torch.nn as nn
from apt_model.core.system import get_device, set_seed
from apt_model.optimization.npu_backend import get_accelerator_type
import apt_model.optimization.vb_global as vb

# 1. è®¾ç½®ç§å­
set_seed(42)

# 2. è·å–è®¾å¤‡
device = get_device()
print(f"ä½¿ç”¨è®¾å¤‡: {device} ({get_accelerator_type().upper()})")

# 3. å¯ç”¨è™šæ‹ŸBlackwell
vb.enable_balanced_mode()

# 4. åˆ›å»ºæ¨¡å‹
class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(768, 3072)
        self.fc2 = nn.Linear(3072, 768)

    def forward(self, x):
        return self.fc2(torch.relu(self.fc1(x)))

model = MyModel().to(device)

# 5. è®­ç»ƒ
optimizer = torch.optim.Adam(model.parameters())
criterion = nn.MSELoss()

for epoch in range(10):
    x = torch.randn(32, 768).to(device)
    y = torch.randn(32, 768).to(device)

    output = model(x)
    loss = criterion(output, y)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    print(f"Epoch {epoch+1}: Loss = {loss.item():.4f}")

print("âœ… è®­ç»ƒå®Œæˆ")
```

### NPUå¤šå¡è®­ç»ƒ

```python
from apt_model.optimization.npu_backend import get_device_manager

manager = get_device_manager()

# è·å–æ‰€æœ‰NPUè®¾å¤‡
npu_devices = [d for d in manager.get_all_devices()
               if d.type == 'npu']

if len(npu_devices) > 1:
    print(f"æ£€æµ‹åˆ°{len(npu_devices)}ä¸ªNPUè®¾å¤‡")

    # ä½¿ç”¨DataParallel
    model = nn.DataParallel(model, device_ids=[0, 1])
else:
    print("å•NPUæ¨¡å¼")
```

---

## ğŸ§ª æµ‹è¯•NPUé›†æˆ

è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶ï¼š

```bash
python training/test_npu_integration.py
```

æµ‹è¯•å†…å®¹ï¼š
1. âœ… NPUè®¾å¤‡æ£€æµ‹
2. âœ… NPUåç«¯é€‚é…å™¨
3. âœ… ç»Ÿä¸€è®¾å¤‡ç®¡ç†å™¨
4. âœ… VGPU Stack NPUæ”¯æŒ
5. âœ… Virtual Blackwell NPUä¼˜åŒ–
6. âœ… ç®€å•æ¨¡å‹è®­ç»ƒ

---

## ğŸ“‹ APIå‚è€ƒ

### è®¾å¤‡ç®¡ç† (`apt_model.core.system`)

#### `get_device(force_cpu=False, prefer_npu=False) -> torch.device`

è·å–è®¡ç®—è®¾å¤‡ã€‚

**å‚æ•°ï¼š**
- `force_cpu`: å¼ºåˆ¶ä½¿ç”¨CPU
- `prefer_npu`: ä¼˜å…ˆä½¿ç”¨NPUï¼ˆé»˜è®¤ä¼˜å…ˆCUDAï¼‰

**è¿”å›ï¼š** `torch.device`

**ä¼˜å…ˆçº§ï¼š** `prefer_npu` â†’ NPU â†’ CUDA â†’ CPU

#### `get_device_info() -> dict`

è·å–è®¾å¤‡è¯¦ç»†ä¿¡æ¯ã€‚

**è¿”å›ï¼š**
```python
{
    'cuda_available': bool,
    'npu_available': bool,
    'device_count': int,
    'device_name': str,
    'device_type': str,  # 'cuda' / 'npu' / 'cpu'
    'cuda_version': str or None,
    'npu_version': str or None
}
```

#### `memory_cleanup() -> None`

æ¸…ç†GPU/NPU/CPUç¼“å­˜ã€‚

#### `get_memory_info() -> dict`

è·å–æ‰€æœ‰è®¾å¤‡å†…å­˜ä½¿ç”¨ä¿¡æ¯ã€‚

**è¿”å›ï¼š**
```python
{
    'ram': {...},           # CPUå†…å­˜
    'vram': {...},          # GPUæ˜¾å­˜
    'npu_memory': {...}     # NPUå†…å­˜
}
```

---

### NPUåç«¯ (`apt_model.optimization.npu_backend`)

#### `get_accelerator_type() -> str`

è·å–å½“å‰åŠ é€Ÿå™¨ç±»å‹ã€‚

**è¿”å›ï¼š** `'cuda'`, `'npu'`, æˆ– `'cpu'`

#### `is_npu_available() -> bool`

æ£€æŸ¥NPUæ˜¯å¦å¯ç”¨ã€‚

#### `is_cuda_available() -> bool`

æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨ã€‚

#### `get_unified_backend(device=None) -> DeviceBackend`

è·å–ç»Ÿä¸€è®¾å¤‡åç«¯ã€‚

**å‚æ•°ï¼š**
- `device`: è®¾å¤‡å¯¹è±¡ï¼ŒNone=è‡ªåŠ¨é€‰æ‹©æœ€ä½³è®¾å¤‡

#### `get_device_manager() -> UnifiedDeviceManager`

è·å–å…¨å±€è®¾å¤‡ç®¡ç†å™¨ã€‚

---

### DeviceBackendç±»

ç»Ÿä¸€çš„è®¾å¤‡æ“ä½œæ¥å£ã€‚

**ä¸»è¦æ–¹æ³•ï¼š**

```python
backend = DeviceBackend(torch.device('npu:0'))

# è®¾å¤‡ä¿¡æ¯
backend.is_available()                    # bool
backend.device_count()                    # int
backend.get_device_name(index=0)          # str
backend.get_device_properties(index=0)    # dict

# å†…å­˜ç®¡ç†
backend.memory_allocated(index=0)         # bytes
backend.memory_reserved(index=0)          # bytes
backend.max_memory_allocated(index=0)     # bytes
backend.empty_cache()                     # æ¸…ç†ç¼“å­˜

# Tensoræ“ä½œ
backend.to_device(tensor)                 # ç§»åŠ¨åˆ°è®¾å¤‡
backend.synchronize(index=0)              # åŒæ­¥

# å·¥å…·
backend.get_memory_summary()              # dict
```

---

### UnifiedDeviceManagerç±»

å…¨å±€è®¾å¤‡ç®¡ç†å™¨ã€‚

**ä¸»è¦æ–¹æ³•ï¼š**

```python
manager = get_device_manager()

# è®¾å¤‡æŸ¥è¯¢
manager.get_all_devices()                 # List[torch.device]
manager.get_best_device(prefer_npu=False) # torch.device
manager.get_backend(device)               # DeviceBackend

# æ‘˜è¦ä¿¡æ¯
manager.get_device_summary()              # dict

# æ¸…ç†
manager.cleanup_all()                     # æ¸…ç†æ‰€æœ‰è®¾å¤‡ç¼“å­˜
```

---

## ğŸ” æ•…éšœæ’æŸ¥

### NPUæœªæ£€æµ‹åˆ°

**ç—‡çŠ¶ï¼š** `is_npu_available()` è¿”å› `False`

**è§£å†³ï¼š**

1. æ£€æŸ¥torch_npuæ˜¯å¦å®‰è£…ï¼š
```bash
python -c "import torch_npu; print(torch_npu.__version__)"
```

2. å¦‚æœªå®‰è£…ï¼Œå‚è€ƒåä¸ºå®˜æ–¹æ–‡æ¡£å®‰è£…ï¼š
```bash
pip install torch-npu
```

3. éªŒè¯NPUè®¾å¤‡ï¼š
```bash
npu-smi info
```

### è®¾å¤‡ç±»å‹é”™è¯¯

**ç—‡çŠ¶ï¼š** æ¨¡å‹åœ¨é”™è¯¯çš„è®¾å¤‡ä¸Šè¿è¡Œ

**è§£å†³ï¼š**

```python
# æ˜¾å¼æŒ‡å®šè®¾å¤‡
device = torch.device('npu:0')
model = model.to(device)

# æˆ–ä½¿ç”¨prefer_npu
from apt_model.core.system import get_device
device = get_device(prefer_npu=True)
```

### å†…å­˜ä¸è¶³

**ç—‡çŠ¶ï¼š** NPUå†…å­˜æº¢å‡º

**è§£å†³ï¼š**

1. å¯ç”¨VGPU Stackè™šæ‹Ÿå†…å­˜ï¼š
```python
vb.enable_memory_mode()  # æ˜¾å­˜ä¼˜å…ˆæ¨¡å¼
```

2. æ‰‹åŠ¨æ¸…ç†ç¼“å­˜ï¼š
```python
from apt_model.core.system import memory_cleanup
memory_cleanup()
```

3. å‡å°batch sizeæˆ–ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| è®¾å¤‡ | å‚å•† | ååé‡ (FP16) | HBMå¸¦å®½ | VGPU Stackå±‚çº§ | VBåŠ é€Ÿ |
|-----|------|--------------|---------|---------------|--------|
| **NVIDIA A100** | NVIDIA | 312 TFLOPS | 900 GB/s | Level 0 | 2.57Ã— |
| **Intel Gaudi2** | Intel | 432 TFLOPS | 700 GB/s | Level 0 | 2.3Ã— |
| **Ascend 910B** | åä¸º | 256 TFLOPS | 600 GB/s | Level 0 | 2.1Ã— |
| **Intel Arc A770** | Intel | 17 TFLOPS | 400 GB/s | Level 0 | 1.8Ã— |
| **AMD MI250** | AMD | 383 TFLOPS | 800 GB/s | Level 0 | 2.5Ã— |
| **CPU (32æ ¸)** | - | ~2 TFLOPS | 50 GB/s | Level 1 | 1.5Ã— |

**æ³¨**: VBåŠ é€ŸæŒ‡ä½¿ç”¨è™šæ‹ŸBlackwellåç›¸æ¯”çº¯PyTorchçš„åŠ é€Ÿæ¯”ã€‚

---

## ğŸ‰ æ€»ç»“

è™šæ‹ŸBlackwellå¤šå‚å•†åŠ é€Ÿå™¨é›†æˆç‰¹æ€§ï¼š

âœ… **å¤šå‚å•†æ”¯æŒ** - NVIDIA/Intel/åä¸º/AMDç»Ÿä¸€æ¥å£
âœ… **6ç§ç¡¬ä»¶** - CUDA/HPU/NPU/XPU/ROCm/CPUå…¨è¦†ç›–
âœ… **è‡ªåŠ¨æ£€æµ‹** - æ— éœ€æ‰‹åŠ¨é…ç½®è®¾å¤‡ç±»å‹
âœ… **é€æ˜ä¼˜åŒ–** - VGPU Stackè‡ªåŠ¨é€‚é…æ‰€æœ‰åŠ é€Ÿå™¨
âœ… **å†…å­˜é«˜æ•ˆ** - ç»Ÿä¸€çš„å†…å­˜ç›‘æ§å’Œæ¸…ç†æ¥å£
âœ… **ç”Ÿäº§å°±ç»ª** - å®Œæ•´æµ‹è¯•å¥—ä»¶éªŒè¯

### è®¾å¤‡é€‰æ‹©ç­–ç•¥

| åœºæ™¯ | æ¨èè®¾å¤‡ | ç†ç”± |
|------|---------|------|
| å¤§è§„æ¨¡è®­ç»ƒ | NVIDIA A100/H100 | æœ€æˆç†Ÿç”Ÿæ€ï¼Œæœ€é«˜æ€§èƒ½ |
| æ•°æ®ä¸­å¿ƒè®­ç»ƒ | Intel Gaudi2 | æ€§ä»·æ¯”é«˜ï¼Œ96GBå¤§æ˜¾å­˜ |
| ä¸­å›½å¸‚åœº | åä¸ºAscend 910B | æœ¬åœŸæ”¯æŒï¼Œä¾›åº”é“¾ç¨³å®š |
| è¾¹ç¼˜æ¨ç† | Intel XPU/Arc | é›†æˆNPUï¼ŒåŠŸè€—ä½ |
| AMDå¹³å° | AMD MIç³»åˆ— | ROCmç”Ÿæ€æˆç†Ÿ |
| å¼€å‘/æµ‹è¯• | CPU | å…¼å®¹æ€§æœ€ä½³ |

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [è™šæ‹ŸBlackwellå®Œæ•´æŒ‡å—](./VIRTUAL_BLACKWELL_COMPLETE.md)
- [VGPU Stackæ¶æ„](./VGPU_STACK_ARCHITECTURE.md)
- [VGPUå¿«é€Ÿå…¥é—¨](./VGPU_QUICK_START.md)
- [å…¨å±€å¯ç”¨å™¨æŒ‡å—](./ENABLE_VIRTUAL_BLACKWELL.md)

---

## ğŸ“– å‚è€ƒèµ„æ–™ä¸è°ƒç ”æ¥æº

æœ¬æ–‡æ¡£åŸºäºä»¥ä¸‹å®˜æ–¹èµ„æ–™ç¼–å†™ï¼ˆ2026å¹´1æœˆï¼‰ï¼š

1. **Intel Habana Gaudi**
   - [Gaudi Documentation 1.22.2](https://docs.habana.ai/)
   - [PyTorch Gaudi Python API](https://docs.habana.ai/en/latest/PyTorch/Reference/Python_Packages.html)

2. **Qualcomm Hexagon NPU**
   - [Qualcomm AI Hub](https://workbench.aihub.qualcomm.com/)
   - [ExecuTorch Qualcomm Backend](https://pytorch.org/executorch/stable/backends-qualcomm.html)

3. **Intel XPU**
   - [Intel Extension for PyTorch](https://intel.github.io/intel-extension-for-pytorch/)
   - [PyTorch 2.5 Intel GPU Support](https://pytorch.org/blog/intel-gpu-support-pytorch-2-5/)

4. **åä¸ºæ˜‡è…¾NPU**
   - [Ascend Documentation](https://www.hiascend.com/)
   - torch_npuå®˜æ–¹æ–‡æ¡£

5. **AMD ROCm**
   - [AMD ROCm Documentation](https://rocmdocs.amd.com/)

---

**ä½œè€…ï¼š** claude + chen0430tw
**ç‰ˆæœ¬ï¼š** 2.0 (Multi-Vendor Accelerator Support)
**æ›´æ–°æ—¥æœŸï¼š** 2026-01-20
**æ”¯æŒç¡¬ä»¶ï¼š** NVIDIA GPU | Intel Gaudi | Huawei Ascend | Intel XPU | AMD ROCm | CPU
