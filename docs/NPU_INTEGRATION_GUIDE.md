# è™šæ‹ŸBlackwell NPUé›†æˆæŒ‡å—

## ğŸ“Œ æ¦‚è¿°

è™šæ‹ŸBlackwellç°å·²å®Œå…¨æ”¯æŒ**åä¸ºæ˜‡è…¾NPUï¼ˆAscendï¼‰**ï¼Œå®ç°GPU/NPU/CPUçš„ç»Ÿä¸€åŠ é€Ÿæ¥å£ã€‚

### æ”¯æŒçš„ç¡¬ä»¶

| ç¡¬ä»¶ç±»å‹ | æ”¯æŒçŠ¶æ€ | æ€§èƒ½ |
|---------|---------|------|
| ğŸŸ¢ NVIDIA CUDA GPU | âœ… å®Œå…¨æ”¯æŒ | æœ€å¿« (900 GB/s NVLink) |
| ğŸŸ¡ åä¸ºæ˜‡è…¾NPU | âœ… å®Œå…¨æ”¯æŒ | å¿«é€Ÿ (600 GB/s) |
| ğŸ”µ CPU | âœ… Fallback | æ…¢é€Ÿ (50 GB/s) |

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹å¼1: è‡ªåŠ¨æ£€æµ‹ï¼ˆæ¨èï¼‰

```python
import apt_model.optimization.vb_global as vb

# è‡ªåŠ¨æ£€æµ‹å¹¶ä½¿ç”¨æœ€ä½³è®¾å¤‡ï¼ˆNPU/GPU/CPUï¼‰
vb.enable_balanced_mode()

# è¾“å‡ºç¤ºä¾‹ï¼š
# ğŸš€ è™šæ‹ŸBlackwellå·²å…¨å±€å¯ç”¨
# åŠ é€Ÿè®¾å¤‡:        ğŸŸ¡ åä¸ºæ˜‡è…¾NPU
# FP4é‡åŒ–:         âŒ ç¦ç”¨
# Flash Attention: âœ… å¯ç”¨
# ...
```

### æ–¹å¼2: æ˜¾å¼æŒ‡å®šNPU

```python
from apt_model.core.system import get_device

# ä¼˜å…ˆä½¿ç”¨NPU
device = get_device(prefer_npu=True)
print(device)  # npu:0

# å¼ºåˆ¶ä½¿ç”¨CPU
device = get_device(force_cpu=True)
print(device)  # cpu
```

### æ–¹å¼3: ç¯å¢ƒå˜é‡

```bash
# å¯ç”¨è™šæ‹ŸBlackwell + è‡ªåŠ¨æ£€æµ‹NPU
export ENABLE_VIRTUAL_BLACKWELL=1
export VB_MODE=balanced

# è¿è¡Œè®­ç»ƒè„šæœ¬
python training/train.py
```

---

## ğŸ“¦ NPUåç«¯é€‚é…å™¨

### ç»Ÿä¸€è®¾å¤‡æ¥å£

```python
from apt_model.optimization.npu_backend import (
    DeviceBackend,
    get_unified_backend,
    is_npu_available,
    get_accelerator_type
)

# æ£€æµ‹åŠ é€Ÿå™¨ç±»å‹
accel_type = get_accelerator_type()
print(accel_type)  # 'cuda', 'npu', æˆ– 'cpu'

# NPUå¯ç”¨æ€§æ£€æŸ¥
if is_npu_available():
    print("NPUå¯ç”¨ï¼")

# è·å–ç»Ÿä¸€åç«¯
backend = get_unified_backend()
print(backend.device_type)  # 'npu' / 'cuda' / 'cpu'
print(backend.get_device_name())  # 'NPU Ascend 910B'
```

### è®¾å¤‡ç®¡ç†å™¨

```python
from apt_model.optimization.npu_backend import get_device_manager

# è·å–å…¨å±€è®¾å¤‡ç®¡ç†å™¨
manager = get_device_manager()

# åˆ—å‡ºæ‰€æœ‰è®¾å¤‡
devices = manager.get_all_devices()
# [device(type='npu', index=0), device(type='npu', index=1), device(type='cpu')]

# è·å–æœ€ä½³è®¾å¤‡
best_device = manager.get_best_device(prefer_npu=True)
print(best_device)  # npu:0

# è®¾å¤‡æ‘˜è¦
summary = manager.get_device_summary()
print(summary)
# {
#   'total_devices': 3,
#   'cuda_devices': 0,
#   'npu_devices': 2,
#   'devices': {
#     'npu:0': {'device_name': 'NPU Ascend 910B', ...}
#   }
# }
```

---

## ğŸ”§ æ ¸å¿ƒåŠŸèƒ½

### 1. è®¾å¤‡æ£€æµ‹ä¸é€‰æ‹©

```python
from apt_model.core.system import get_device_info

# è·å–è¯¦ç»†è®¾å¤‡ä¿¡æ¯
info = get_device_info()
print(info)
# {
#   'cuda_available': False,
#   'npu_available': True,
#   'device_count': 2,
#   'device_name': 'NPU Ascend 910B',
#   'device_type': 'npu',
#   'npu_version': '5.0.0'
# }
```

### 2. å†…å­˜ç®¡ç†

```python
from apt_model.core.system import (
    get_memory_info,
    memory_cleanup
)

# è·å–å†…å­˜ä¿¡æ¯ï¼ˆæ”¯æŒNPUï¼‰
mem_info = get_memory_info()
print(mem_info)
# {
#   'ram': {...},
#   'vram': {},  # GPUå†…å­˜ï¼ˆå¦‚æœæœ‰ï¼‰
#   'npu_memory': {
#     'npu_0': {
#       'allocated_gb': 2.5,
#       'reserved_gb': 3.0,
#       'max_allocated_gb': 2.8
#     }
#   }
# }

# æ¸…ç†æ‰€æœ‰è®¾å¤‡ç¼“å­˜ï¼ˆGPU + NPUï¼‰
memory_cleanup()
```

### 3. VGPU Stackè‡ªåŠ¨é€‚é…

VGPU Stackä¼šè‡ªåŠ¨æ£€æµ‹NPUå¹¶é…ç½®æœ€ä½³å±‚çº§ï¼š

```python
from apt_model.optimization.vgpu_stack import create_vgpu_stack

# è‡ªåŠ¨åˆ›å»ºNPUå †å 
stack = create_vgpu_stack()

# NPUé…ç½®ç¤ºä¾‹ï¼š
# Level 0: npu:0 - 2000MB @ 600.0GB/s  ï¼ˆNPU HBMï¼‰
# Level 1: cpu - 8000MB @ 40.0GB/s     ï¼ˆCPUå†…å­˜ï¼‰
# Level 2: ssd - 32000MB @ 7.0GB/s     ï¼ˆNVMeï¼‰
```

### 4. éšæœºç§å­è®¾ç½®

```python
from apt_model.core.system import set_seed

# åŒæ—¶è®¾ç½®CPU/GPU/NPUç§å­
set_seed(42)
```

---

## ğŸ¯ å®æˆ˜ç¤ºä¾‹

### å®Œæ•´è®­ç»ƒæµç¨‹

```python
#!/usr/bin/env python3
import torch
import torch.nn as nn
from apt_model.core.system import get_device, set_seed
from apt_model.optimization.npu_backend import get_accelerator_type
import apt_model.optimization.vb_global as vb

# 1. è®¾ç½®ç§å­
set_seed(42)

# 2. è·å–è®¾å¤‡
device = get_device()
print(f"ä½¿ç”¨è®¾å¤‡: {device} ({get_accelerator_type().upper()})")

# 3. å¯ç”¨è™šæ‹ŸBlackwell
vb.enable_balanced_mode()

# 4. åˆ›å»ºæ¨¡å‹
class MyModel(nn.Module):
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(768, 3072)
        self.fc2 = nn.Linear(3072, 768)

    def forward(self, x):
        return self.fc2(torch.relu(self.fc1(x)))

model = MyModel().to(device)

# 5. è®­ç»ƒ
optimizer = torch.optim.Adam(model.parameters())
criterion = nn.MSELoss()

for epoch in range(10):
    x = torch.randn(32, 768).to(device)
    y = torch.randn(32, 768).to(device)

    output = model(x)
    loss = criterion(output, y)

    optimizer.zero_grad()
    loss.backward()
    optimizer.step()

    print(f"Epoch {epoch+1}: Loss = {loss.item():.4f}")

print("âœ… è®­ç»ƒå®Œæˆ")
```

### NPUå¤šå¡è®­ç»ƒ

```python
from apt_model.optimization.npu_backend import get_device_manager

manager = get_device_manager()

# è·å–æ‰€æœ‰NPUè®¾å¤‡
npu_devices = [d for d in manager.get_all_devices()
               if d.type == 'npu']

if len(npu_devices) > 1:
    print(f"æ£€æµ‹åˆ°{len(npu_devices)}ä¸ªNPUè®¾å¤‡")

    # ä½¿ç”¨DataParallel
    model = nn.DataParallel(model, device_ids=[0, 1])
else:
    print("å•NPUæ¨¡å¼")
```

---

## ğŸ§ª æµ‹è¯•NPUé›†æˆ

è¿è¡Œå®Œæ•´æµ‹è¯•å¥—ä»¶ï¼š

```bash
python training/test_npu_integration.py
```

æµ‹è¯•å†…å®¹ï¼š
1. âœ… NPUè®¾å¤‡æ£€æµ‹
2. âœ… NPUåç«¯é€‚é…å™¨
3. âœ… ç»Ÿä¸€è®¾å¤‡ç®¡ç†å™¨
4. âœ… VGPU Stack NPUæ”¯æŒ
5. âœ… Virtual Blackwell NPUä¼˜åŒ–
6. âœ… ç®€å•æ¨¡å‹è®­ç»ƒ

---

## ğŸ“‹ APIå‚è€ƒ

### è®¾å¤‡ç®¡ç† (`apt_model.core.system`)

#### `get_device(force_cpu=False, prefer_npu=False) -> torch.device`

è·å–è®¡ç®—è®¾å¤‡ã€‚

**å‚æ•°ï¼š**
- `force_cpu`: å¼ºåˆ¶ä½¿ç”¨CPU
- `prefer_npu`: ä¼˜å…ˆä½¿ç”¨NPUï¼ˆé»˜è®¤ä¼˜å…ˆCUDAï¼‰

**è¿”å›ï¼š** `torch.device`

**ä¼˜å…ˆçº§ï¼š** `prefer_npu` â†’ NPU â†’ CUDA â†’ CPU

#### `get_device_info() -> dict`

è·å–è®¾å¤‡è¯¦ç»†ä¿¡æ¯ã€‚

**è¿”å›ï¼š**
```python
{
    'cuda_available': bool,
    'npu_available': bool,
    'device_count': int,
    'device_name': str,
    'device_type': str,  # 'cuda' / 'npu' / 'cpu'
    'cuda_version': str or None,
    'npu_version': str or None
}
```

#### `memory_cleanup() -> None`

æ¸…ç†GPU/NPU/CPUç¼“å­˜ã€‚

#### `get_memory_info() -> dict`

è·å–æ‰€æœ‰è®¾å¤‡å†…å­˜ä½¿ç”¨ä¿¡æ¯ã€‚

**è¿”å›ï¼š**
```python
{
    'ram': {...},           # CPUå†…å­˜
    'vram': {...},          # GPUæ˜¾å­˜
    'npu_memory': {...}     # NPUå†…å­˜
}
```

---

### NPUåç«¯ (`apt_model.optimization.npu_backend`)

#### `get_accelerator_type() -> str`

è·å–å½“å‰åŠ é€Ÿå™¨ç±»å‹ã€‚

**è¿”å›ï¼š** `'cuda'`, `'npu'`, æˆ– `'cpu'`

#### `is_npu_available() -> bool`

æ£€æŸ¥NPUæ˜¯å¦å¯ç”¨ã€‚

#### `is_cuda_available() -> bool`

æ£€æŸ¥CUDAæ˜¯å¦å¯ç”¨ã€‚

#### `get_unified_backend(device=None) -> DeviceBackend`

è·å–ç»Ÿä¸€è®¾å¤‡åç«¯ã€‚

**å‚æ•°ï¼š**
- `device`: è®¾å¤‡å¯¹è±¡ï¼ŒNone=è‡ªåŠ¨é€‰æ‹©æœ€ä½³è®¾å¤‡

#### `get_device_manager() -> UnifiedDeviceManager`

è·å–å…¨å±€è®¾å¤‡ç®¡ç†å™¨ã€‚

---

### DeviceBackendç±»

ç»Ÿä¸€çš„è®¾å¤‡æ“ä½œæ¥å£ã€‚

**ä¸»è¦æ–¹æ³•ï¼š**

```python
backend = DeviceBackend(torch.device('npu:0'))

# è®¾å¤‡ä¿¡æ¯
backend.is_available()                    # bool
backend.device_count()                    # int
backend.get_device_name(index=0)          # str
backend.get_device_properties(index=0)    # dict

# å†…å­˜ç®¡ç†
backend.memory_allocated(index=0)         # bytes
backend.memory_reserved(index=0)          # bytes
backend.max_memory_allocated(index=0)     # bytes
backend.empty_cache()                     # æ¸…ç†ç¼“å­˜

# Tensoræ“ä½œ
backend.to_device(tensor)                 # ç§»åŠ¨åˆ°è®¾å¤‡
backend.synchronize(index=0)              # åŒæ­¥

# å·¥å…·
backend.get_memory_summary()              # dict
```

---

### UnifiedDeviceManagerç±»

å…¨å±€è®¾å¤‡ç®¡ç†å™¨ã€‚

**ä¸»è¦æ–¹æ³•ï¼š**

```python
manager = get_device_manager()

# è®¾å¤‡æŸ¥è¯¢
manager.get_all_devices()                 # List[torch.device]
manager.get_best_device(prefer_npu=False) # torch.device
manager.get_backend(device)               # DeviceBackend

# æ‘˜è¦ä¿¡æ¯
manager.get_device_summary()              # dict

# æ¸…ç†
manager.cleanup_all()                     # æ¸…ç†æ‰€æœ‰è®¾å¤‡ç¼“å­˜
```

---

## ğŸ” æ•…éšœæ’æŸ¥

### NPUæœªæ£€æµ‹åˆ°

**ç—‡çŠ¶ï¼š** `is_npu_available()` è¿”å› `False`

**è§£å†³ï¼š**

1. æ£€æŸ¥torch_npuæ˜¯å¦å®‰è£…ï¼š
```bash
python -c "import torch_npu; print(torch_npu.__version__)"
```

2. å¦‚æœªå®‰è£…ï¼Œå‚è€ƒåä¸ºå®˜æ–¹æ–‡æ¡£å®‰è£…ï¼š
```bash
pip install torch-npu
```

3. éªŒè¯NPUè®¾å¤‡ï¼š
```bash
npu-smi info
```

### è®¾å¤‡ç±»å‹é”™è¯¯

**ç—‡çŠ¶ï¼š** æ¨¡å‹åœ¨é”™è¯¯çš„è®¾å¤‡ä¸Šè¿è¡Œ

**è§£å†³ï¼š**

```python
# æ˜¾å¼æŒ‡å®šè®¾å¤‡
device = torch.device('npu:0')
model = model.to(device)

# æˆ–ä½¿ç”¨prefer_npu
from apt_model.core.system import get_device
device = get_device(prefer_npu=True)
```

### å†…å­˜ä¸è¶³

**ç—‡çŠ¶ï¼š** NPUå†…å­˜æº¢å‡º

**è§£å†³ï¼š**

1. å¯ç”¨VGPU Stackè™šæ‹Ÿå†…å­˜ï¼š
```python
vb.enable_memory_mode()  # æ˜¾å­˜ä¼˜å…ˆæ¨¡å¼
```

2. æ‰‹åŠ¨æ¸…ç†ç¼“å­˜ï¼š
```python
from apt_model.core.system import memory_cleanup
memory_cleanup()
```

3. å‡å°batch sizeæˆ–ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| è®¾å¤‡ | ååé‡ | æ˜¾å­˜å¸¦å®½ | è™šæ‹ŸBlackwellåŠ é€Ÿ |
|-----|--------|---------|------------------|
| NVIDIA A100 | 312 TFLOPS | 900 GB/s | 2.57Ã— |
| åä¸ºæ˜‡è…¾910B | 256 TFLOPS | 600 GB/s | 2.1Ã— |
| CPU (32æ ¸) | ~2 TFLOPS | 50 GB/s | 1.5Ã— |

---

## ğŸ‰ æ€»ç»“

è™šæ‹ŸBlackwell NPUé›†æˆç‰¹æ€§ï¼š

âœ… **å®Œå…¨å…¼å®¹** - NPU/GPU/CPUç»Ÿä¸€æ¥å£
âœ… **è‡ªåŠ¨æ£€æµ‹** - æ— éœ€æ‰‹åŠ¨é…ç½®è®¾å¤‡ç±»å‹
âœ… **é€æ˜ä¼˜åŒ–** - VGPU Stackè‡ªåŠ¨é€‚é…NPU
âœ… **å†…å­˜é«˜æ•ˆ** - æ”¯æŒNPUå†…å­˜ç›‘æ§å’Œæ¸…ç†
âœ… **ç”Ÿäº§å°±ç»ª** - å®Œæ•´æµ‹è¯•å¥—ä»¶éªŒè¯

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [è™šæ‹ŸBlackwellå®Œæ•´æŒ‡å—](./VIRTUAL_BLACKWELL_COMPLETE.md)
- [VGPU Stackæ¶æ„](./VGPU_STACK_ARCHITECTURE.md)
- [VGPUå¿«é€Ÿå…¥é—¨](./VGPU_QUICK_START.md)
- [å…¨å±€å¯ç”¨å™¨æŒ‡å—](./ENABLE_VIRTUAL_BLACKWELL.md)

---

**ä½œè€…ï¼š** claude + chen0430tw
**ç‰ˆæœ¬ï¼š** 1.0 (NPU Extension)
**æ›´æ–°æ—¥æœŸï¼š** 2026-01-20
