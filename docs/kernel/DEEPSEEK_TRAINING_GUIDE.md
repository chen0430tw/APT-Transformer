# DeepSeek æ¨¡å‹è®­ç»ƒæŒ‡å—

<div align="center">

**åŸºäº DeepSeek-V3 æ¶æ„çš„ MoE æ¨¡å‹è®­ç»ƒå®Œæ•´æ•™ç¨‹**

æ”¯æŒ Multi-head Latent Attention | DeepSeekMoE | FP8 æ··åˆç²¾åº¦

</div>

---

## ğŸ“‹ ç›®å½•

- [DeepSeek ç®€ä»‹](#deepseek-ç®€ä»‹)
- [æ¶æ„ç‰¹ç‚¹](#æ¶æ„ç‰¹ç‚¹)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [æ ¸å¿ƒç»„ä»¶å®ç°](#æ ¸å¿ƒç»„ä»¶å®ç°)
- [è®­ç»ƒé…ç½®](#è®­ç»ƒé…ç½®)
- [ä¼˜åŒ–æŠ€å·§](#ä¼˜åŒ–æŠ€å·§)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)

---

## ğŸ¯ DeepSeek ç®€ä»‹

### ä»€ä¹ˆæ˜¯ DeepSeekï¼Ÿ

DeepSeek æ˜¯ç”±æ·±åº¦æ±‚ç´¢ï¼ˆDeepSeek-AIï¼‰å¼€å‘çš„å¼€æºå¤§è¯­è¨€æ¨¡å‹ç³»åˆ—ï¼Œä»¥å…¶é«˜æ•ˆçš„ **Mixture-of-Experts (MoE)** æ¶æ„å’Œåˆ›æ–°çš„æ³¨æ„åŠ›æœºåˆ¶è€Œé—»åã€‚

### DeepSeek-V3 æ ¸å¿ƒæ•°æ®

| æŒ‡æ ‡ | æ•°å€¼ |
|------|------|
| **æ€»å‚æ•°** | 671B |
| **æ¿€æ´»å‚æ•°** | 37Bï¼ˆæ¯ä¸ªtokenï¼‰ |
| **è®­ç»ƒæ•°æ®** | 14.8T tokens |
| **è®­ç»ƒæˆæœ¬** | 2.664M H800 GPUå°æ—¶ |
| **è®­ç»ƒç¡¬ä»¶** | 2048 Ã— NVIDIA H800 |
| **è®¸å¯è¯** | MIT License |

---

## ğŸ—ï¸ æ¶æ„ç‰¹ç‚¹

### 1. Multi-head Latent Attention (MLA)

**æ ¸å¿ƒåˆ›æ–°ï¼š** ä½¿ç”¨ä½ç§©æŠ•å½±å‡å°‘ KV Cache å¼€é”€

```python
class MultiHeadLatentAttention(nn.Module):
    """
    DeepSeek MLAï¼šé€šè¿‡æ½œåœ¨ç©ºé—´å‹ç¼©é™ä½æ¨ç†æˆæœ¬

    ä¼ ç»Ÿæ³¨æ„åŠ›ï¼šO(n * d_model * n_heads) KV cache
    MLAï¼šO(n * d_latent) KV cacheï¼Œå…¶ä¸­ d_latent << d_model * n_heads
    """
    def __init__(self, d_model=2048, n_heads=16, d_latent=512):
        super().__init__()
        self.d_model = d_model
        self.n_heads = n_heads
        self.d_latent = d_latent
        self.d_head = d_model // n_heads

        # å‹ç¼©åˆ°æ½œåœ¨ç©ºé—´
        self.W_DKV = nn.Linear(d_model, d_latent + d_model)  # é™ç»´æŠ•å½±

        # Query ç›´æ¥æŠ•å½±
        self.W_Q = nn.Linear(d_model, d_model)

        # ä»æ½œåœ¨ç©ºé—´è§£å‹
        self.W_UK = nn.Linear(d_latent, d_model)  # Key ä¸Šé‡‡æ ·
        self.W_UV = nn.Linear(d_latent, d_model)  # Value ä¸Šé‡‡æ ·

        self.W_O = nn.Linear(d_model, d_model)

    def forward(self, x, mask=None):
        B, T, C = x.shape

        # 1. å‹ç¼© K, V åˆ°æ½œåœ¨ç©ºé—´
        kv_compressed = self.W_DKV(x)  # [B, T, d_latent + d_model]
        k_latent = kv_compressed[:, :, :self.d_latent]  # [B, T, d_latent]
        v_rope = kv_compressed[:, :, self.d_latent:]     # [B, T, d_model]

        # 2. è§£å‹ K
        k = self.W_UK(k_latent)  # [B, T, d_model]
        k = k.view(B, T, self.n_heads, self.d_head).transpose(1, 2)

        # 3. è§£å‹ V
        v = self.W_UV(k_latent) + v_rope  # æ®‹å·®è¿æ¥
        v = v.view(B, T, self.n_heads, self.d_head).transpose(1, 2)

        # 4. Query æŠ•å½±
        q = self.W_Q(x).view(B, T, self.n_heads, self.d_head).transpose(1, 2)

        # 5. æ ‡å‡†æ³¨æ„åŠ›è®¡ç®—
        scores = torch.matmul(q, k.transpose(-2, -1)) / (self.d_head ** 0.5)
        if mask is not None:
            scores = scores.masked_fill(mask == 0, float('-inf'))

        attn = torch.softmax(scores, dim=-1)
        out = torch.matmul(attn, v)

        # 6. è¾“å‡ºæŠ•å½±
        out = out.transpose(1, 2).contiguous().view(B, T, C)
        return self.W_O(out)
```

**å†…å­˜èŠ‚çœï¼š** ä½¿ç”¨ MLA å¯èŠ‚çœçº¦ **70-80%** çš„ KV Cache å†…å­˜

---

### 2. DeepSeekMoE æ¶æ„

**æ ¸å¿ƒç­–ç•¥ï¼š**
1. **ç»†ç²’åº¦ä¸“å®¶åˆ†å‰²**ï¼šå°†å¤§ä¸“å®¶åˆ†æˆå¤šä¸ªå°ä¸“å®¶ï¼Œæé«˜ä¸“ä¸šåŒ–
2. **å…±äº«ä¸“å®¶éš”ç¦»**ï¼šéƒ¨åˆ†ä¸“å®¶å§‹ç»ˆæ¿€æ´»ï¼Œä¿è¯åŸºç¡€èƒ½åŠ›

```python
class DeepSeekMoE(nn.Module):
    """
    DeepSeek MoEï¼šç»†ç²’åº¦ä¸“å®¶ + å…±äº«ä¸“å®¶

    è®¾è®¡ç†å¿µï¼š
    - è·¯ç”±ä¸“å®¶ï¼ˆRouted Expertsï¼‰ï¼šåŠ¨æ€é€‰æ‹©ï¼Œè´Ÿè´£ä¸“ä¸šä»»åŠ¡
    - å…±äº«ä¸“å®¶ï¼ˆShared Expertsï¼‰ï¼šå§‹ç»ˆæ¿€æ´»ï¼Œè´Ÿè´£é€šç”¨èƒ½åŠ›
    """
    def __init__(
        self,
        d_model=2048,
        d_ff=10240,
        num_routed_experts=160,     # è·¯ç”±ä¸“å®¶æ•°é‡
        num_shared_experts=8,        # å…±äº«ä¸“å®¶æ•°é‡
        num_activated_experts=8,     # æ¯æ¬¡æ¿€æ´»çš„è·¯ç”±ä¸“å®¶æ•°
        expert_capacity=1.25,        # ä¸“å®¶å®¹é‡å› å­
    ):
        super().__init__()
        self.d_model = d_model
        self.num_routed_experts = num_routed_experts
        self.num_shared_experts = num_shared_experts
        self.num_activated_experts = num_activated_experts

        # è·¯ç”±å™¨ï¼ˆTop-K é€‰æ‹©ï¼‰
        self.router = nn.Linear(d_model, num_routed_experts)

        # è·¯ç”±ä¸“å®¶ï¼ˆç»†ç²’åº¦ï¼Œæ¯ä¸ªè¾ƒå°ï¼‰
        self.routed_experts = nn.ModuleList([
            nn.Sequential(
                nn.Linear(d_model, d_ff // 4),  # ç¼©å°ä¸“å®¶å°ºå¯¸
                nn.GELU(),
                nn.Linear(d_ff // 4, d_model)
            ) for _ in range(num_routed_experts)
        ])

        # å…±äº«ä¸“å®¶ï¼ˆå§‹ç»ˆæ¿€æ´»ï¼Œè¾ƒå¤§ï¼‰
        self.shared_experts = nn.ModuleList([
            nn.Sequential(
                nn.Linear(d_model, d_ff),
                nn.GELU(),
                nn.Linear(d_ff, d_model)
            ) for _ in range(num_shared_experts)
        ])

        # ä¸“å®¶æƒé‡èåˆ
        self.shared_gate = nn.Linear(d_model, num_shared_experts)

    def forward(self, x):
        B, T, C = x.shape

        # ========== è·¯ç”±ä¸“å®¶ï¼ˆåŠ¨æ€é€‰æ‹©ï¼‰==========
        router_logits = self.router(x)  # [B, T, num_routed_experts]

        # Top-K è·¯ç”±
        topk_weights, topk_indices = torch.topk(
            router_logits,
            k=self.num_activated_experts,
            dim=-1
        )  # [B, T, K]

        topk_weights = torch.softmax(topk_weights, dim=-1)

        # æ‰§è¡Œè·¯ç”±ä¸“å®¶
        routed_output = torch.zeros_like(x)
        for i in range(self.num_activated_experts):
            expert_idx = topk_indices[:, :, i]  # [B, T]
            expert_weight = topk_weights[:, :, i:i+1]  # [B, T, 1]

            # æ‰¹é‡æ‰§è¡Œä¸“å®¶ï¼ˆç®€åŒ–ç‰ˆï¼Œå®é™…éœ€è¦æ›´å¤æ‚çš„è°ƒåº¦ï¼‰
            for b in range(B):
                for t in range(T):
                    expert_id = expert_idx[b, t].item()
                    expert_out = self.routed_experts[expert_id](x[b:b+1, t:t+1])
                    routed_output[b:b+1, t:t+1] += expert_weight[b, t] * expert_out

        # ========== å…±äº«ä¸“å®¶ï¼ˆå§‹ç»ˆæ¿€æ´»ï¼‰==========
        shared_gate_logits = self.shared_gate(x)  # [B, T, num_shared_experts]
        shared_weights = torch.softmax(shared_gate_logits, dim=-1)  # [B, T, num_shared_experts]

        shared_output = torch.zeros_like(x)
        for i, expert in enumerate(self.shared_experts):
            expert_out = expert(x)  # [B, T, C]
            shared_output += shared_weights[:, :, i:i+1] * expert_out

        # ========== ç»„åˆè¾“å‡º ==========
        return routed_output + shared_output
```

**æ€§èƒ½æå‡ï¼š** DeepSeekMoE 16B ä»…ç”¨ **40.5%** è®¡ç®—é‡å³å¯è¾¾åˆ° DeepSeek 7B æ€§èƒ½

---

### 3. FP8 æ··åˆç²¾åº¦è®­ç»ƒ

**æ ¸å¿ƒæŠ€æœ¯ï¼š** ç»†ç²’åº¦é‡åŒ– + é€‰æ‹©æ€§é«˜ç²¾åº¦è®¡ç®—

```python
class FP8MixedPrecisionTrainer:
    """
    DeepSeek FP8 æ··åˆç²¾åº¦è®­ç»ƒæ¡†æ¶

    ç­–ç•¥ï¼š
    - GEMM æ“ä½œï¼šFP8ï¼ˆçŸ©é˜µä¹˜æ³•ï¼‰
    - å…³é”®æ“ä½œï¼šFP16/BF16ï¼ˆSoftmax, LayerNormï¼‰
    - æ¢¯åº¦ç´¯ç§¯ï¼šFP32
    """
    def __init__(self, model, optimizer):
        self.model = model
        self.optimizer = optimizer

        # FP8 é‡åŒ–é…ç½®
        self.activation_scale = {}   # åŠ¨æ€æ¿€æ´»å€¼ç¼©æ”¾
        self.weight_scale = {}        # é™æ€æƒé‡ç¼©æ”¾

    def quantize_to_fp8(self, tensor, tile_size=(1, 128), is_activation=True):
        """
        ç»†ç²’åº¦é‡åŒ–åˆ° FP8

        æ¿€æ´»å€¼ï¼šTile-wise 1Ã—128 é‡åŒ–
        æƒé‡ï¼šBlock-wise 128Ã—128 é‡åŒ–
        """
        if is_activation:
            # æ¿€æ´»å€¼ï¼šæŒ‰ tile åŠ¨æ€é‡åŒ–
            B, T, C = tensor.shape
            num_tiles = C // tile_size[1]

            quantized = torch.zeros_like(tensor, dtype=torch.float8_e4m3fn)
            scales = []

            for i in range(num_tiles):
                start_idx = i * tile_size[1]
                end_idx = start_idx + tile_size[1]
                tile = tensor[:, :, start_idx:end_idx]

                # è®¡ç®—ç¼©æ”¾å› å­
                max_val = tile.abs().max()
                scale = max_val / 448.0  # FP8 E4M3 æœ€å¤§å€¼
                scales.append(scale)

                # é‡åŒ–
                quantized[:, :, start_idx:end_idx] = (tile / scale).to(torch.float8_e4m3fn)

            return quantized, torch.tensor(scales)
        else:
            # æƒé‡ï¼šæŒ‰ block é™æ€é‡åŒ–
            # ï¼ˆç®€åŒ–ç¤ºä¾‹ï¼Œå®é™…å®ç°æ›´å¤æ‚ï¼‰
            max_val = tensor.abs().max()
            scale = max_val / 448.0
            quantized = (tensor / scale).to(torch.float8_e4m3fn)
            return quantized, scale

    def train_step(self, batch):
        """FP8 æ··åˆç²¾åº¦è®­ç»ƒæ­¥éª¤"""
        self.model.train()
        input_ids = batch['input_ids']
        labels = batch['labels']

        self.optimizer.zero_grad()

        # ========== å‰å‘ä¼ æ’­ï¼ˆFP8 è®¡ç®—ï¼‰==========
        with torch.cuda.amp.autocast(dtype=torch.bfloat16):
            # æ³¨æ„ï¼šå®é™… FP8 éœ€è¦è‡ªå®šä¹‰ CUDA kernel
            # è¿™é‡Œç”¨ BF16 æ¨¡æ‹Ÿï¼ŒçœŸå®å®ç°éœ€è°ƒç”¨ FP8 GEMM
            logits = self.model(input_ids)

            # æŸå¤±è®¡ç®—ç”¨ FP32ï¼ˆæé«˜æ•°å€¼ç¨³å®šæ€§ï¼‰
            loss = torch.nn.functional.cross_entropy(
                logits.view(-1, logits.size(-1)).float(),
                labels.view(-1)
            )

        # ========== åå‘ä¼ æ’­ï¼ˆæ¢¯åº¦ FP32 ç´¯ç§¯ï¼‰==========
        loss.backward()

        # æ¢¯åº¦è£å‰ªï¼ˆFP32ï¼‰
        torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)

        # ä¼˜åŒ–å™¨æ›´æ–°ï¼ˆFP32 ä¸»æƒé‡ï¼‰
        self.optimizer.step()

        return loss.item()
```

**æ•ˆç‡æå‡ï¼š** FP8 è®­ç»ƒå¯å‡å°‘ **40-50%** æ˜¾å­˜å ç”¨å’Œè®­ç»ƒæ—¶é—´

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1åˆ†é’Ÿè®­ç»ƒ DeepSeek é£æ ¼æ¨¡å‹

```python
from apt_model.modeling.deepseek_model import DeepSeekModel
from apt_model.training.deepseek_trainer import DeepSeekTrainer
from transformers import AutoTokenizer

# 1. åˆå§‹åŒ–æ¨¡å‹ï¼ˆå°è§„æ¨¡é…ç½®ï¼‰
model = DeepSeekModel(
    vocab_size=50257,
    d_model=1024,           # å°æ¨¡å‹ç”¨ 1024ï¼Œå¤§æ¨¡å‹ç”¨ 2048-4096
    n_heads=16,
    num_layers=12,
    d_ff=4096,
    d_latent=256,           # MLA æ½œåœ¨ç»´åº¦
    num_routed_experts=32,  # è·¯ç”±ä¸“å®¶æ•°
    num_shared_experts=4,   # å…±äº«ä¸“å®¶æ•°
    num_activated_experts=4 # æ¯æ¬¡æ¿€æ´»ä¸“å®¶æ•°
)

# 2. å‡†å¤‡æ•°æ®
tokenizer = AutoTokenizer.from_pretrained("gpt2")
train_texts = open("train.txt", "r", encoding="utf-8").readlines()

# 3. åˆ›å»ºè®­ç»ƒå™¨
trainer = DeepSeekTrainer(
    model=model,
    tokenizer=tokenizer,
    learning_rate=2e-4,
    use_fp8=False  # å¼€å¯éœ€è¦ Hopper+ GPUï¼ˆH100/H800ï¼‰
)

# 4. å¼€å§‹è®­ç»ƒ
history = trainer.train(
    train_texts=train_texts,
    epochs=20,
    batch_size=8,
    max_length=512,
    save_path="./deepseek_checkpoint"
)

# 5. ç”Ÿæˆæ–‡æœ¬
import torch
model.eval()
with torch.no_grad():
    input_text = "äººå·¥æ™ºèƒ½çš„æœªæ¥æ˜¯"
    input_ids = torch.tensor([tokenizer.encode(input_text)])
    output = model.generate(input_ids, max_new_tokens=100, temperature=0.8)
    print(tokenizer.decode(output[0].tolist()))
```

---

## âš™ï¸ è®­ç»ƒé…ç½®

### ç¡¬ä»¶è¦æ±‚

| æ¨¡å‹è§„æ¨¡ | å‚æ•°é‡ | æœ€ä½æ˜¾å­˜ | æ¨èæ˜¾å­˜ | æ¿€æ´»ä¸“å®¶æ•° |
|---------|--------|---------|---------|-----------|
| **Mini** | 2.7B (æ¿€æ´» 340M) | 8GB | 16GB | 2/16 experts |
| **Small** | 16B (æ¿€æ´» 2.8B) | 24GB | 40GB | 6/64 experts |
| **Medium** | 67B (æ¿€æ´» 8B) | 40GB | 80GB | 8/128 experts |
| **Large** | 671B (æ¿€æ´» 37B) | 8x80GB | 16x80GB | 8/256 experts |

### è¶…å‚æ•°æ¨è

#### Mini æ¨¡å‹ï¼ˆ2.7Bï¼Œå­¦ä¹ å®éªŒï¼‰

```python
config = {
    'd_model': 1024,
    'n_heads': 16,
    'd_ff': 4096,
    'num_layers': 16,
    'd_latent': 256,
    'num_routed_experts': 16,
    'num_shared_experts': 2,
    'num_activated_experts': 2,

    'learning_rate': 3e-4,
    'batch_size': 16,
    'max_length': 1024,
    'warmup_steps': 2000,
    'weight_decay': 0.01,
}
```

#### Small æ¨¡å‹ï¼ˆ16Bï¼Œç”Ÿäº§å¯ç”¨ï¼‰

```python
config = {
    'd_model': 2048,
    'n_heads': 16,
    'd_ff': 10240,
    'num_layers': 28,
    'd_latent': 512,
    'num_routed_experts': 64,
    'num_shared_experts': 4,
    'num_activated_experts': 6,

    'learning_rate': 2e-4,
    'batch_size': 4,  # æ¢¯åº¦ç´¯ç§¯ x16 = æœ‰æ•ˆ batch 64
    'max_length': 4096,
    'warmup_steps': 4000,
    'gradient_accumulation_steps': 16,
}
```

### æ•°æ®å‡†å¤‡

DeepSeek-V3 è®­ç»ƒæ•°æ®æ¯”ä¾‹ï¼š

| æ•°æ®ç±»å‹ | æ¯”ä¾‹ | è¯´æ˜ |
|---------|------|------|
| **é€šç”¨æ–‡æœ¬** | ~60% | ç½‘é¡µã€ä¹¦ç±ã€è®ºæ–‡ |
| **ä»£ç ** | ~20% | GitHubã€ç¼–ç¨‹æ•™ç¨‹ |
| **æ•°å­¦** | ~10% | æ•°å­¦æ¨ç†ã€è¯æ˜ |
| **å¤šè¯­è¨€** | ~10% | ä¸­è‹±å¤–å¤šè¯­è¨€è¯­æ–™ |

```python
# æ•°æ®é¢„å¤„ç†ç¤ºä¾‹
def prepare_deepseek_data(raw_texts):
    """DeepSeek æ•°æ®å‡†å¤‡æµç¨‹"""
    processed = []

    for text in raw_texts:
        # 1. å»é‡ï¼ˆMinHash LSHï¼‰
        if is_duplicate(text):
            continue

        # 2. è´¨é‡è¿‡æ»¤
        if len(text) < 50 or quality_score(text) < 0.6:
            continue

        # 3. å¤šæ ·æ€§å¢å¼ºï¼ˆä¸åŒé¢†åŸŸæ··åˆï¼‰
        text_type = classify_text_type(text)  # general/code/math/multilingual

        processed.append({
            'text': text,
            'type': text_type,
            'length': len(text)
        })

    # 4. æŒ‰ç±»å‹å¹³è¡¡é‡‡æ ·
    balanced = balance_by_type(processed, ratios={
        'general': 0.6,
        'code': 0.2,
        'math': 0.1,
        'multilingual': 0.1
    })

    return [item['text'] for item in balanced]
```

---

## ğŸ”¥ ä¼˜åŒ–æŠ€å·§

### 1. Auxiliary-Loss-Free è´Ÿè½½å‡è¡¡

**é—®é¢˜ï¼š** ä¼ ç»Ÿ MoE ç”¨è¾…åŠ©æŸå¤±å¼ºåˆ¶è´Ÿè½½å‡è¡¡ï¼ŒæŸå®³æ¨¡å‹æ€§èƒ½

**DeepSeek æ–¹æ¡ˆï¼š** æ— è¾…åŠ©æŸå¤±çš„è‡ªç„¶è´Ÿè½½å‡è¡¡

```python
class AuxiliaryLossFreeRouter(nn.Module):
    """
    DeepSeek-V3 æ— è¾…åŠ©æŸå¤±è·¯ç”±å™¨

    æ ¸å¿ƒæ€æƒ³ï¼š
    1. ä¸æ·»åŠ è´Ÿè½½å‡è¡¡æŸå¤±
    2. é€šè¿‡ä¸“å®¶å®¹é‡é™åˆ¶è‡ªç„¶å¹³è¡¡
    3. ä½¿ç”¨ token dropping å¤„ç†æº¢å‡º
    """
    def __init__(self, d_model, num_experts, num_activated, capacity_factor=1.25):
        super().__init__()
        self.gate = nn.Linear(d_model, num_experts)
        self.num_experts = num_experts
        self.num_activated = num_activated
        self.capacity_factor = capacity_factor

    def forward(self, x):
        B, T, C = x.shape

        # 1. è·¯ç”±æ‰“åˆ†ï¼ˆæ— è¾…åŠ©æŸå¤±ï¼‰
        router_logits = self.gate(x)  # [B, T, E]
        router_probs = torch.softmax(router_logits, dim=-1)

        # 2. Top-K é€‰æ‹©
        topk_probs, topk_indices = torch.topk(router_probs, k=self.num_activated, dim=-1)

        # 3. è®¡ç®—ä¸“å®¶å®¹é‡
        tokens_per_expert = (B * T * self.num_activated) / self.num_experts
        expert_capacity = int(tokens_per_expert * self.capacity_factor)

        # 4. åˆ†é… tokens åˆ°ä¸“å®¶ï¼ˆå…ˆåˆ°å…ˆå¾—ï¼Œè¶…å‡ºä¸¢å¼ƒï¼‰
        expert_counts = torch.zeros(self.num_experts, device=x.device)
        expert_mask = torch.zeros(B, T, self.num_activated, dtype=torch.bool, device=x.device)

        for b in range(B):
            for t in range(T):
                for k in range(self.num_activated):
                    expert_id = topk_indices[b, t, k].item()
                    if expert_counts[expert_id] < expert_capacity:
                        expert_counts[expert_id] += 1
                        expert_mask[b, t, k] = True  # ä¿ç•™è¯¥ token
                    # else: token droppedï¼ˆä¸¢å¼ƒï¼Œä¸æ·»åŠ æƒ©ç½šï¼‰

        # 5. åº”ç”¨ maskï¼ˆè¢«ä¸¢å¼ƒçš„ token æƒé‡å½’é›¶ï¼‰
        topk_probs_masked = topk_probs * expert_mask.float()
        topk_probs_normalized = topk_probs_masked / (topk_probs_masked.sum(dim=-1, keepdim=True) + 1e-8)

        return topk_probs_normalized, topk_indices, expert_mask
```

**æ•ˆæœï¼š** ä¸ç‰ºç‰²æ€§èƒ½çš„åŒæ—¶ï¼Œè‡ªåŠ¨å®ç°è´Ÿè½½å‡è¡¡

---

### 2. Multi-Token Prediction (MTP)

**æ ¸å¿ƒæ€æƒ³ï¼š** åŒæ—¶é¢„æµ‹å½“å‰å’Œæœªæ¥å¤šä¸ª tokenï¼Œæé«˜æ•°æ®æ•ˆç‡

```python
class MultiTokenPrediction(nn.Module):
    """
    DeepSeek-V3 å¤š token é¢„æµ‹

    ç­–ç•¥ï¼š
    - ä¸»é¢„æµ‹å¤´ï¼šé¢„æµ‹ä¸‹ä¸€ä¸ª tokenï¼ˆæ­£å¸¸æŸå¤±æƒé‡ 1.0ï¼‰
    - è¾…åŠ©é¢„æµ‹å¤´ï¼šé¢„æµ‹æœªæ¥ 2-4 ä¸ª tokenï¼ˆæŸå¤±æƒé‡ 0.3ï¼‰
    """
    def __init__(self, d_model, vocab_size, num_future_tokens=3):
        super().__init__()
        self.num_future_tokens = num_future_tokens

        # ä¸»é¢„æµ‹å¤´
        self.lm_head = nn.Linear(d_model, vocab_size, bias=False)

        # è¾…åŠ©é¢„æµ‹å¤´ï¼ˆå…±äº«åº•å±‚è¡¨ç¤ºï¼‰
        self.future_heads = nn.ModuleList([
            nn.Sequential(
                nn.Linear(d_model, d_model),
                nn.GELU(),
                nn.Linear(d_model, vocab_size, bias=False)
            ) for _ in range(num_future_tokens)
        ])

    def forward(self, hidden_states, labels=None):
        """
        Args:
            hidden_states: [B, T, C] - æ¨¡å‹éšè—çŠ¶æ€
            labels: [B, T] - æ ‡ç­¾åºåˆ—

        Returns:
            loss: å¤š token é¢„æµ‹æ€»æŸå¤±
        """
        B, T, C = hidden_states.shape

        # ========== ä¸»é¢„æµ‹ï¼ˆt+1ï¼‰==========
        logits_main = self.lm_head(hidden_states)  # [B, T, V]

        if labels is None:
            return logits_main

        # è®¡ç®—ä¸»æŸå¤±
        loss_main = F.cross_entropy(
            logits_main[:, :-1].reshape(-1, logits_main.size(-1)),
            labels[:, 1:].reshape(-1),
            ignore_index=-100
        )

        # ========== è¾…åŠ©é¢„æµ‹ï¼ˆt+2, t+3, t+4ï¼‰==========
        loss_aux = 0.0
        for i, future_head in enumerate(self.future_heads):
            # é¢„æµ‹æœªæ¥ç¬¬ i+2 ä¸ª token
            future_offset = i + 2
            if T <= future_offset:
                continue

            logits_future = future_head(hidden_states[:, :-future_offset])

            loss_future = F.cross_entropy(
                logits_future.reshape(-1, logits_future.size(-1)),
                labels[:, future_offset:].reshape(-1),
                ignore_index=-100
            )

            loss_aux += loss_future * 0.3  # è¾…åŠ©æŸå¤±æƒé‡

        # ========== æ€»æŸå¤± ==========
        total_loss = loss_main + loss_aux / max(len(self.future_heads), 1)

        return total_loss, logits_main
```

**æ•°æ®æ•ˆç‡æå‡ï¼š** MTP å¯å‡å°‘ **20-30%** è®­ç»ƒæ—¶é—´

---

### 3. åˆ†å¸ƒå¼è®­ç»ƒï¼ˆå¤§è§„æ¨¡ï¼‰

DeepSeek-V3 ä½¿ç”¨çš„å¹¶è¡Œç­–ç•¥ï¼š

```python
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

def setup_deepseek_distributed():
    """
    DeepSeek-V3 åˆ†å¸ƒå¼é…ç½®

    å¹¶è¡Œç­–ç•¥ï¼š
    - Pipeline Parallelism (PP): 16-wayï¼ˆè·¨å±‚åˆ‡åˆ†ï¼‰
    - Expert Parallelism (EP): 64-wayï¼ˆä¸“å®¶è·¨èŠ‚ç‚¹ï¼‰
    - Data Parallelism (DP): ZeRO-1ï¼ˆæ¢¯åº¦åˆ†ç‰‡ï¼‰
    """
    # åˆå§‹åŒ–åˆ†å¸ƒå¼
    dist.init_process_group(backend='nccl')
    local_rank = int(os.environ['LOCAL_RANK'])
    world_size = dist.get_world_size()

    # ========== Pipeline Parallelism ==========
    # å°†æ¨¡å‹æŒ‰å±‚åˆ‡åˆ†åˆ° 16 ä¸ªè®¾å¤‡
    from torch.distributed.pipeline.sync import Pipe

    model = DeepSeekModel(...)

    # åˆ‡åˆ†å±‚åˆ°ä¸åŒè®¾å¤‡
    balance = [2, 2, 2, 2, 2, 2, 2, 2]  # æ¯ä¸ª PP rank å¤„ç† 2 å±‚ï¼ˆå…± 16 å±‚ï¼‰
    model = Pipe(model, balance=balance, chunks=8)

    # ========== Expert Parallelism ==========
    # DeepEPï¼šä¸“å®¶å¹¶è¡Œé€šä¿¡åº“
    # å°† 256 ä¸ªä¸“å®¶åˆ†é…åˆ° 64 ä¸ª GPUï¼ˆæ¯ä¸ª 4 ä¸ªä¸“å®¶ï¼‰
    from deepep import ExpertParallel

    ep_group = dist.new_group(ranks=list(range(0, 64)))  # EP ç»„
    model.moe_layers = ExpertParallel(
        model.moe_layers,
        expert_parallel_group=ep_group
    )

    # ========== ZeRO-1 Data Parallelism ==========
    from deepspeed.runtime.zero.stage_1_and_2 import DeepSpeedZeroOptimizer

    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-4)
    optimizer = DeepSpeedZeroOptimizer(
        optimizer,
        static_loss_scale=1.0,
        dynamic_loss_scale=False,
        partition_gradients=True  # ZeRO-1ï¼šåˆ†ç‰‡æ¢¯åº¦
    )

    return model, optimizer

# ========== è®­ç»ƒå¾ªç¯ ==========
def train_distributed(model, train_loader, optimizer):
    model.train()

    for batch in train_loader:
        input_ids = batch['input_ids'].to(local_rank)
        labels = batch['labels'].to(local_rank)

        # å‰å‘ä¼ æ’­ï¼ˆPipeline è‡ªåŠ¨å¤„ç†ï¼‰
        loss = model(input_ids, labels=labels).local_value()

        # åå‘ä¼ æ’­ï¼ˆZeRO è‡ªåŠ¨åˆ†ç‰‡æ¢¯åº¦ï¼‰
        loss.backward()

        # æ¢¯åº¦è£å‰ªï¼ˆè·¨ rank åŒæ­¥ï¼‰
        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

        # ä¼˜åŒ–å™¨æ›´æ–°
        optimizer.step()
        optimizer.zero_grad()
```

**æ‰©å±•æ€§ï¼š** å¯æ‰©å±•åˆ° **2048 GPUs**ï¼ˆDeepSeek-V3 å®é™…é…ç½®ï¼‰

---

## ğŸ› å¸¸è§é—®é¢˜

### Q1: MLA çš„ KV Cache å¦‚ä½•èŠ‚çœå†…å­˜ï¼Ÿ

**A:** MLA é€šè¿‡ä½ç§©æŠ•å½±å‹ç¼© K, Vï¼š

```
ä¼ ç»Ÿæ³¨æ„åŠ› KV Cacheï¼š
- æ¯å±‚å­˜å‚¨ K, V: [batch, seq_len, n_heads * d_head]
- æ€»å†…å­˜: seq_len Ã— d_model Ã— num_layers Ã— 2

MLA KV Cacheï¼š
- æ¯å±‚å­˜å‚¨å‹ç¼©çš„ K, V: [batch, seq_len, d_latent]
- æ€»å†…å­˜: seq_len Ã— d_latent Ã— num_layers Ã— 2

èŠ‚çœæ¯”ä¾‹: 1 - (d_latent / d_model)
ç¤ºä¾‹: d_model=4096, d_latent=512 â†’ èŠ‚çœ 87.5%
```

---

### Q2: ä¸ºä»€ä¹ˆ DeepSeekMoE ç”¨å…±äº«ä¸“å®¶ï¼Ÿ

**A:** å…±äº«ä¸“å®¶ç¡®ä¿åŸºç¡€èƒ½åŠ›ä¸ä¼šå› è·¯ç”±ä¸“å®¶ä¸“ä¸šåŒ–è€Œä¸¢å¤±ï¼š

```
è·¯ç”±ä¸“å®¶ï¼ˆRouted Expertsï¼‰ï¼š
- åŠ¨æ€é€‰æ‹©ï¼Œé«˜åº¦ä¸“ä¸šåŒ–
- å¯èƒ½å­¦åˆ°ç‹­çª„çš„ç‰¹å®šæ¨¡å¼
- æŸäº›é€šç”¨çŸ¥è¯†å¯èƒ½ç¼ºå¤±

å…±äº«ä¸“å®¶ï¼ˆShared Expertsï¼‰ï¼š
- å§‹ç»ˆæ¿€æ´»ï¼Œå­¦ä¹ é€šç”¨è¡¨ç¤º
- è¡¥å……è·¯ç”±ä¸“å®¶çš„ç›²åŒº
- æé«˜æ¨¡å‹ç¨³å®šæ€§

å®éªŒç»“æœï¼š
- æ— å…±äº«ä¸“å®¶: æ€§èƒ½ä¸‹é™ 3-5%
- æœ‰å…±äº«ä¸“å®¶: æ€§èƒ½æå‡ï¼Œæ›´ç¨³å®š
```

---

### Q3: FP8 è®­ç»ƒæ˜¯å¦ä¼šæŸå¤±ç²¾åº¦ï¼Ÿ

**A:** DeepSeek çš„ç»†ç²’åº¦ FP8 é‡åŒ–å‡ ä¹æ— æŸï¼š

```python
# å…³é”®è®¾è®¡ï¼š
1. ç»†ç²’åº¦é‡åŒ–ï¼ˆTile-wise/Block-wiseï¼‰
   - ä¸æ˜¯æ•´ä¸ªå¼ é‡ä¸€ä¸ªç¼©æ”¾å› å­
   - æ¯ 128 ä¸ªå…ƒç´ ä¸€ä¸ªç¼©æ”¾å› å­
   - é€‚åº”å±€éƒ¨æ•°å€¼åˆ†å¸ƒ

2. é€‰æ‹©æ€§é«˜ç²¾åº¦
   - GEMM: FP8 âœ“ï¼ˆè®¡ç®—å¯†é›†ï¼Œé‡åŒ–æ”¶ç›Šå¤§ï¼‰
   - Softmax: BF16ï¼ˆæ•°å€¼æ•æ„Ÿï¼‰
   - LayerNorm: BF16ï¼ˆæ•°å€¼æ•æ„Ÿï¼‰
   - æ¢¯åº¦ç´¯ç§¯: FP32ï¼ˆé˜²æ­¢ç´¯ç§¯è¯¯å·®ï¼‰

3. åŠ¨æ€ç¼©æ”¾
   - æ¿€æ´»å€¼ï¼šæ¯æ­¥åŠ¨æ€è®¡ç®—ç¼©æ”¾å› å­
   - æƒé‡ï¼šé¢„è®¡ç®—é™æ€ç¼©æ”¾å› å­

å®éªŒç»“æœï¼š
- FP8 vs BF16: < 0.1% æ€§èƒ½å·®è·
- æ˜¾å­˜èŠ‚çœ: ~40%
- è®­ç»ƒé€Ÿåº¦: æå‡ 30-50%
```

---

### Q4: å¦‚ä½•å¤„ç† MoE çš„è´Ÿè½½ä¸å‡è¡¡ï¼Ÿ

**A:** DeepSeek é‡‡ç”¨æ— è¾…åŠ©æŸå¤±ç­–ç•¥ï¼š

```python
ä¼ ç»Ÿæ–¹æ³•ï¼š
- æ·»åŠ è¾…åŠ©æŸå¤±: L_aux = Î» Ã— load_balance_loss
- é—®é¢˜: å¼ºåˆ¶å¹³è¡¡æŸå®³æ€§èƒ½ï¼ŒÎ» éš¾è°ƒ

DeepSeek æ–¹æ³•ï¼š
1. ä¸“å®¶å®¹é‡é™åˆ¶ï¼ˆExpert Capacityï¼‰
   - æ¯ä¸ªä¸“å®¶æœ€å¤šå¤„ç† capacity ä¸ª token
   - capacity = (total_tokens / num_experts) Ã— factor
   - factor é€šå¸¸è®¾ä¸º 1.25

2. Token Dropping
   - è¶…å‡ºå®¹é‡çš„ token ç›´æ¥ä¸¢å¼ƒ
   - ä¸æ·»åŠ ä»»ä½•æƒ©ç½šæŸå¤±
   - è‡ªç„¶å½¢æˆè´Ÿè½½å‡è¡¡

3. ç»“æœ
   - æ— éœ€è°ƒå‚ï¼ˆä¸éœ€è¦ Î»ï¼‰
   - æ€§èƒ½ä¸å—å½±å“
   - è´Ÿè½½è‡ªåŠ¨å‡è¡¡ï¼ˆä¸“å®¶é¥±å’Œè‡ªç„¶å‡å°‘åˆ†é…ï¼‰
```

---

## ğŸ“š å‚è€ƒèµ„æº

### å®˜æ–¹èµ„æº

- [DeepSeek GitHub ç»„ç»‡](https://github.com/deepseek-ai) - æ‰€æœ‰å®˜æ–¹ä»£ç ä»“åº“
- [DeepSeek-V3 ä»“åº“](https://github.com/deepseek-ai/DeepSeek-V3) - æœ€æ–°æ¨¡å‹ä»£ç 
- [DeepSeek-V3 æŠ€æœ¯æŠ¥å‘Š](https://arxiv.org/abs/2412.19437) - å®Œæ•´æ¶æ„è®ºæ–‡
- [DeepSeek-MoE è®ºæ–‡](https://github.com/deepseek-ai/DeepSeek-MoE) - MoE æ¶æ„è¯¦è§£

### æŠ€æœ¯æ·±åº¦è§£è¯»

- [DeepSeek Models Technical Tour](https://magazine.sebastianraschka.com/p/technical-deepseek) - Sebastian Raschka æŠ€æœ¯è§£æ
- [Complete Guide to DeepSeek Models](https://www.bentoml.com/blog/the-complete-guide-to-deepseek-models-from-v3-to-r1-and-beyond) - BentoML å®Œæ•´æŒ‡å—
- [DeepSeek-V3 Architecture](https://deepwiki.com/deepseek-ai/DeepSeek-V3/3-model-architecture) - DeepWiki æ¶æ„è§£æ

### APT ç›¸å…³æ–‡æ¡£

- [GPT è®­ç»ƒæŒ‡å—](GPT_TRAINING_GUIDE.md) - å¯¹æ¯” GPT æ¶æ„
- [API é›†æˆæŒ‡å—](../product/API_PROVIDERS_GUIDE.md) - ä½¿ç”¨ DeepSeek API
- [APT Model Handbook](APT_MODEL_HANDBOOK.md) - APT å¹³å°å®Œæ•´æ‰‹å†Œ

---

## ğŸ“ æ›´æ–°æ—¥å¿—

- **v1.0.0** (2025-12) - åˆå§‹ç‰ˆæœ¬
  - âœ… Multi-head Latent Attention (MLA) å®ç°
  - âœ… DeepSeekMoE æ¶æ„ï¼ˆè·¯ç”±ä¸“å®¶ + å…±äº«ä¸“å®¶ï¼‰
  - âœ… FP8 æ··åˆç²¾åº¦è®­ç»ƒæ¡†æ¶
  - âœ… Multi-Token Prediction (MTP)
  - âœ… Auxiliary-Loss-Free è´Ÿè½½å‡è¡¡
  - âœ… åˆ†å¸ƒå¼è®­ç»ƒé…ç½®ï¼ˆPP + EP + ZeROï¼‰

---

<div align="center">

**Happy Training with DeepSeek! ğŸš€**

åŸºäºä¸–ç•Œçº§å¼€æºæ¶æ„ï¼Œæ‰“é€ ä½ çš„ä¸“å±å¤§æ¨¡å‹

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æäº¤ [Issue](https://github.com/chen0430tw/APT-Transformer/issues)

</div>
