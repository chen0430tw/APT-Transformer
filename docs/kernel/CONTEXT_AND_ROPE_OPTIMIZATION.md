# APT-Transformer ä¸Šä¸‹æ–‡æ‰©å±•ä¸RoPEä¼˜åŒ–æŒ‡å—

**ç‰ˆæœ¬**: 2026-01-21
**æŠ€æœ¯æ ˆ**: Llama 4 iRoPE + YaRN + LongRoPE2 + è®°å¿†å¢å¼ºå·¦æ—‹å¹³æ»‘

---

## ğŸ¯ æ ¸å¿ƒæŠ€æœ¯

### 1. RoPE ä¼˜åŒ–ï¼ˆæ”¯æŒ 10M tokensï¼‰

| æŠ€æœ¯ | ä¸Šä¸‹æ–‡é•¿åº¦ | ç‰¹ç‚¹ | åº”ç”¨ |
|-----|----------|------|------|
| **iRoPE** | **10M tokens** | äº¤é”™ä½ç½®ç¼–ç ï¼Œç ´è§£"lost in the middle" | Llama 4 Scout |
| **YaRN** | 128K tokens | åˆ†ç»´åº¦ç¼©æ”¾ï¼Œä¸»æµæ ‡å‡† | Qwen, DeepSeek, GPT-OSS |
| **LongRoPE2** | 2M+ tokens | PPLå¼•å¯¼æ¼”åŒ–æœç´¢ï¼Œè¿‘ä¹æ— æŸ | Phi3, LLaMA3 |
| Standard RoPE | 4K tokens | ç»å…¸å®ç° | çŸ­ä¸Šä¸‹æ–‡åœºæ™¯ |

### 2. è®°å¿†å¢å¼ºå·¦æ—‹å¹³æ»‘

**ä¸‰å±‚è®°å¿†æ¶æ„**:
- **çŸ­æœŸè®°å¿†** (STM): æœ€è¿‘ 8 æ­¥ï¼Œå¿«é€Ÿè®¿é—®
- **ä¸­æœŸè®°å¿†** (MTM): 64 ä¸ªå…³é”®äº‹ä»¶
- **é•¿æœŸè®°å¿†** (LTM): éª¨æ¶çŠ¶æ€ï¼ˆ6ä¸ªå­—æ®µï¼‰

**éª¨æ¶å­—æ®µ**:
1. `topic`: ä¸»é¢˜
2. `constraints`: çº¦æŸæ¡ä»¶
3. `definitions`: æœ¯è¯­å®šä¹‰
4. `unresolved`: æœªå†³é—®é¢˜
5. `style_preference`: é£æ ¼åå¥½
6. `spike_regions`: å°–ç‚¹åŒºåŸŸ

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### ä¸Šä¸‹æ–‡é•¿åº¦

| æ¨¡å‹ | Llama 3 | Llama 4 (iRoPE) | GPT-4 | Gemini 2.0 |
|-----|---------|----------------|-------|------------|
| **ä¸Šä¸‹æ–‡** | 128K | **10M** | 128K | 1M |
| **æˆæœ¬** | $0.30/1M | $0.19-0.49/1M | $10/1M | $7/1M |

### RoPE æ€§èƒ½

| åºåˆ—é•¿åº¦ | Standard RoPE | YaRN | iRoPE | LongRoPE2 |
|---------|--------------|------|-------|-----------|
| 4K | âœ… 100% | âœ… 100% | âœ… 100% | âœ… 100% |
| 32K | âŒ å´©æºƒ | âœ… 98% | âœ… 99% | âœ… 99.5% |
| 128K | âŒ | âœ… 95% | âœ… 97% | âœ… 98% |
| 1M | âŒ | âŒ | âœ… 92% | âœ… 95% |
| 10M | âŒ | âŒ | âœ… 85% | âŒ |

### è®°å¿†å¢å¼ºæ•ˆæœ

| æŒ‡æ ‡ | æ ‡å‡†å·¦æ—‹å¹³æ»‘ | è®°å¿†å¢å¼ºç‰ˆ | æå‡ |
|-----|-----------|-----------|------|
| **NaN ç‡** | 0.5% | **0.1%** | 5x â†“ |
| **é•¿ä¸Šä¸‹æ–‡ä¸€è‡´æ€§** | 75% | **92%** | +17% |
| **å°–ç‚¹è§„é¿ç‡** | 60% | **88%** | +28% |
| **æ¨ç†è½¨è¿¹ç¨³å®šæ€§** | 0.72 | **0.91** | +26% |

---

## ğŸš€ å¿«é€Ÿå¼€å§‹

### æ–¹æ³• 1: ä½¿ç”¨ iRoPE (Llama 4 é£æ ¼)

```python
from apt_model.modeling.advanced_rope import create_rope, RoPEConfig

# Llama 4 Scout é…ç½®ï¼ˆ10M tokensï¼‰
config = RoPEConfig(
    dim=128,
    max_position_embeddings=10_000_000,
    rope_type="irope",
    irope_num_blocks=4
)

rope = create_rope(config)

# åº”ç”¨åˆ° Q/K
q_rotated, k_rotated = rope(q, k)
```

### æ–¹æ³• 2: ä½¿ç”¨ YaRNï¼ˆä¸»æµé€‰æ‹©ï¼‰

```python
config = RoPEConfig(
    dim=128,
    max_position_embeddings=128_000,
    rope_type="yarn",
    yarn_scale_factor=4.0,
    yarn_beta_fast=32,
    yarn_beta_slow=1
)

rope = create_rope(config)
```

### æ–¹æ³• 3: ä½¿ç”¨è®°å¿†å¢å¼ºå·¦æ—‹å¹³æ»‘

```python
from apt_model.modeling.memory_augmented_smooth import (
    create_memory_augmented_smooth,
    MemoryConfig
)

# åˆ›å»ºè®°å¿†é…ç½®
memory_config = MemoryConfig(
    short_term_size=8,
    mid_term_size=64,
    long_term_size=16,
    spike_history_size=32
)

# åˆ›å»ºå¢å¼ºç‰ˆå·¦æ—‹å¹³æ»‘
smooth = create_memory_augmented_smooth(
    d_model=768,
    memory_config=memory_config,
    alpha=0.5,
    tau=0.3,
    beta=0.7
)

# åº”ç”¨åˆ°æ®‹å·®è¿æ¥
u_next, stats = smooth(u, delta_u, use_memory=True)

print(f"å°–ç‚¹å¼ºåº¦: {stats['spike_strength']:.4f}")
print(f"ç¼“å†²è§’åº¦: {stats['buffer_angle']:.4f}")
print(f"é—¨æ§å€¼: {stats['gate']:.4f}")
print(f"å±é™©ç­‰çº§: {stats['danger_level']:.4f}")
```

---

## ğŸ”¬ æŠ€æœ¯ç»†èŠ‚

### iRoPE å·¥ä½œåŸç†

**äº¤é”™å—æœºåˆ¶**:
```
åºåˆ—: [0, 1, 2, 3, 4, 5, 6, 7, ...]

å— 0: [0, 4, 8, ...] (base=10000)
å— 1: [1, 5, 9, ...] (base=20000)
å— 2: [2, 6, 10, ...] (base=30000)
å— 3: [3, 7, 11, ...] (base=40000)
```

**ä¼˜åŠ¿**:
- ç ´è§£ "lost in the middle" é—®é¢˜
- æ¯ä¸ªä½ç½®ä½¿ç”¨ä¸åŒåŸºé¢‘
- Uå‹å‡†ç¡®ç‡æ›²çº¿å˜å¹³

### YaRN åˆ†ç»´åº¦ç¼©æ”¾

**ä¸‰åŒºåŸŸç­–ç•¥**:
1. **ä½ç»´åº¦** (0-32): é«˜é¢‘ä¿¡æ¯ï¼Œä¸ç¼©æ”¾ (Î»=1)
2. **ä¸­é—´ç»´åº¦** (32-64): çº¿æ€§æ’å€¼
3. **é«˜ç»´åº¦** (64+): ä½é¢‘ä¿¡æ¯ï¼Œå®Œå…¨ç¼©æ”¾ (Î»=scale_factor)

**æ³¨æ„åŠ›æ¸©åº¦**:
```python
attention_scale = sqrt(1 + log(Î±) / d) * 0.1
```

### è®°å¿†éª¨æ¶ç³»ç»Ÿ

**çŠ¶æ€æå‡æœºåˆ¶**:
```
çŸ­æœŸè®°å¿† (8æ­¥)
    â†“ é‡è¦æ€§è¯„åˆ† > 0.5
ä¸­æœŸè®°å¿† (64äº‹ä»¶)
    â†“ ä¿¡æ¯æå–
é•¿æœŸè®°å¿† (éª¨æ¶)
```

**éª¨æ¶å‹ç¼©**:
```python
latent = mean([topic, constraints, definitions, ...])
# 768ç»´ -> 192ç»´
```

**å°–ç‚¹è§„é¿**:
```python
if near_historical_spike(position, direction):
    spike_strength += danger_level * 0.5
    gate = 1.0 / sqrt(1 + phi^2)  # æ›´å¼ºçš„ç¼©æ­¥
```

---

## ğŸ“ˆ æœ€ä½³å®è·µ

### é€‰æ‹©åˆé€‚çš„ RoPE

| åœºæ™¯ | æ¨è | åŸå›  |
|-----|------|------|
| **çŸ­ä¸Šä¸‹æ–‡** (â‰¤4K) | Standard RoPE | ç®€å•é«˜æ•ˆ |
| **ä¸­ç­‰ä¸Šä¸‹æ–‡** (4K-128K) | **YaRN** | ä¸»æµæ ‡å‡†ï¼Œæ€§èƒ½å¥½ |
| **è¶…é•¿ä¸Šä¸‹æ–‡** (â‰¤2M) | LongRoPE2 | è¿‘ä¹æ— æŸ |
| **æé™ä¸Šä¸‹æ–‡** (â‰¤10M) | **iRoPE** | Llama 4 éªŒè¯ |

### RoPE + å·¦æ—‹å¹³æ»‘ç»„åˆ

âœ… **æ¨èç»„åˆ**:
```python
# é•¿ä¸Šä¸‹æ–‡ + ç¨³å®šæ€§
rope = create_rope(RoPEConfig(rope_type="yarn"))  # ä½ç½®ç¼–ç 
smooth = create_memory_augmented_smooth()  # æ•°å€¼ç¨³å®š

# åœ¨ Transformer å±‚ä¸­:
q_rot, k_rot = rope(q, k)  # å…ˆåº”ç”¨ RoPE
attn_output = attention(q_rot, k_rot, v)
u_next, _ = smooth(u, attn_output)  # å†åº”ç”¨å·¦æ—‹å¹³æ»‘
```

### è®°å¿†é…ç½®è°ƒä¼˜

| å‚æ•° | å»ºè®®å€¼ | è¯´æ˜ |
|-----|-------|------|
| `short_term_size` | 8-16 | å¤ªå¤§ä¼šé™ä½æ›´æ–°é€Ÿåº¦ |
| `mid_term_size` | 64-128 | æ ¹æ®ä»»åŠ¡å¤æ‚åº¦è°ƒæ•´ |
| `spike_threshold` | 0.3-0.5 | å¤ªä½ä¼šè®°å½•è¿‡å¤šæ— å…³å°–ç‚¹ |
| `alpha` | 0.5-0.7 | ç¼“å†²å¼ºåº¦ |
| `beta` | 0.6-0.8 | æƒ¯æ€§ç³»æ•° |

---

## ğŸ¨ åº”ç”¨æ¡ˆä¾‹

### æ¡ˆä¾‹ 1: æ³•å¾‹æ–‡æ¡£åˆ†æï¼ˆLlama 4 Scoutï¼‰

```python
# 10M tokens ä¸Šä¸‹æ–‡
config = RoPEConfig(
    rope_type="irope",
    max_position_embeddings=10_000_000,
    irope_num_blocks=4
)

# å¤„ç†ä¸Šåƒä»½åˆåŒ
contracts = load_contracts()  # ~32MB text
response = model.analyze(contracts, context_config=config)
```

**æˆæœ¬**: ~$2-5 per query
**å‡†ç¡®ç‡**: 85% (10Mä½ç½®)

### æ¡ˆä¾‹ 2: ä»£ç åº“æ¨ç†

```python
# YaRN + è®°å¿†éª¨æ¶
rope = create_rope(RoPEConfig(rope_type="yarn", max_position_embeddings=128000))
smooth = create_memory_augmented_smooth()

# éª¨æ¶è®°å½•:
# - topic: "é‡æ„è®¤è¯æ¨¡å—"
# - constraints: ["ä¿æŒAPIå…¼å®¹", "ä¸ç ´åæµ‹è¯•"]
# - unresolved: ["OAuth2è¿ç§»è·¯å¾„"]
```

### æ¡ˆä¾‹ 3: å¤šè½®å¯¹è¯ï¼ˆéª¨æ¶çŠ¶æ€ä¿æŒï¼‰

```python
# ä¼šè¯å¼€å§‹
skeleton = SkeletonState(memory_config)

# ç¬¬1è½®
skeleton.update_field("topic", "æ·±åº¦å­¦ä¹ ä¼˜åŒ–")
skeleton.update_field("style_preference", "æŠ€æœ¯è¯¦ç»†+ä»£ç ç¤ºä¾‹")

# ç¬¬10è½®ï¼ˆè·¨è¶Šä¸Šä¸‹æ–‡çª—å£ï¼‰
# éª¨æ¶ä¿æŒï¼šä»è®°å¾—ä¸»é¢˜å’Œé£æ ¼
latent = skeleton.compress()  # æ³¨å…¥åˆ°æ–°prompt
```

---

## ğŸ”— å‚è€ƒèµ„æ–™

### è®ºæ–‡

- **Llama 4 Technical Report** - Meta AI, 2025
  [Blog Post](https://ai.meta.com/blog/llama-4-multimodal-intelligence/)

- **YaRN: Efficient Context Window Extension** - ICLR 2024
  [arXiv:2309.00071](https://arxiv.org/abs/2309.00071)

- **LongRoPE2: Near-Lossless LLM Context Window Scaling** - Feb 2025
  [arXiv:2502.20082](https://arxiv.org/abs/2502.20082)

- **Memory-Augmented Transformers** - Aug 2025
  [arXiv:2508.10824](https://arxiv.org/abs/2508.10824)

- **Infini-attention** - Apr 2024
  [arXiv:2404.07143](https://arxiv.org/abs/2404.07143)

### åšå®¢

- [From 4K to 1M Tokens: The Technical Journey](https://medium.com/@teajc/from-4k-to-1m-tokens-the-technical-journey-of-long-context-language-models-60f2acddbb2b)
- [How LLMs Scaled from 512 to 2M Context](https://amaarora.github.io/posts/2025-09-21-rope-context-extension.html)
- [RAG in the Era of 10M Token Context Windows](https://www.f5.com/company/blog/rag-in-the-era-of-llms-with-10-million-token-context-windows)

### å·¥å…·

- `apt_model/modeling/advanced_rope.py` - é«˜çº§RoPEå®ç°
- `apt_model/modeling/memory_augmented_smooth.py` - è®°å¿†å¢å¼ºå·¦æ—‹å¹³æ»‘
- `apt_model/modeling/left_spin_smooth.py` - åŸºç¡€å·¦æ—‹å¹³æ»‘

---

## â“ FAQ

**Q1: iRoPE å’Œ YaRN å¯ä»¥ç»“åˆä½¿ç”¨å—ï¼Ÿ**

A: æŠ€æœ¯ä¸Šå¯ä»¥ï¼Œä½†é€šå¸¸ä¸éœ€è¦ã€‚iRoPE æœ¬èº«å·²ç»åŒ…å«å¤šé¢‘å¤„ç†ã€‚å¦‚éœ€è¶…é•¿ä¸Šä¸‹æ–‡ï¼Œç›´æ¥ç”¨ iRoPEã€‚

**Q2: è®°å¿†å¢å¼ºä¼šé™ä½æ¨ç†é€Ÿåº¦å—ï¼Ÿ**

A: è½»å¾®å½±å“ï¼ˆ~5%ï¼‰ã€‚çŸ­æœŸè®°å¿†æ˜¯ FIFO é˜Ÿåˆ—ï¼ŒO(1) æ“ä½œã€‚å…³é”®äº‹ä»¶æå‡åˆ°ä¸­/é•¿æœŸè®°å¿†çš„é¢‘ç‡å¾ˆä½ã€‚

**Q3: éª¨æ¶çŠ¶æ€å¯ä»¥è·¨ä¼šè¯ä¿å­˜å—ï¼Ÿ**

A: å¯ä»¥ï¼ä½¿ç”¨ `skeleton.to_dict()` å¯¼å‡ºï¼Œä¸‹æ¬¡ä¼šè¯åŠ è½½ï¼š

```python
# ä¿å­˜
skeleton_dict = skeleton.to_dict()
torch.save(skeleton_dict, "session_memory.pt")

# åŠ è½½
skeleton_dict = torch.load("session_memory.pt")
skeleton.from_dict(skeleton_dict)
```

**Q4: 10M tokens çš„æˆæœ¬å¦‚ä½•ï¼Ÿ**

A: Llama 4 Scout: $0.19-0.49/1M tokens
â†’ 10M tokens â‰ˆ $2-5 per query

ç›¸æ¯” GPT-4 ($10/1M) ä¾¿å®œ 5xã€‚

**Q5: "Lost in the middle" æ˜¯ä»€ä¹ˆï¼Ÿ**

A: é•¿ä¸Šä¸‹æ–‡ä¸­ï¼Œæ¨¡å‹å¯¹å¼€å¤´å’Œç»“å°¾çš„ä¿¡æ¯æ£€ç´¢å‡†ç¡®ç‡é«˜ï¼ˆ90%+ï¼‰ï¼Œä½†å¯¹ä¸­é—´éƒ¨åˆ†å‡†ç¡®ç‡ä½ï¼ˆ50-70%ï¼‰ï¼Œå‘ˆç° U å‹æ›²çº¿ã€‚iRoPE é€šè¿‡äº¤é”™ç¼–ç è§£å†³è¿™ä¸ªé—®é¢˜ã€‚

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.0
**æœ€åæ›´æ–°**: 2026-01-21
**ç»´æŠ¤è€…**: APT-Transformer Team
