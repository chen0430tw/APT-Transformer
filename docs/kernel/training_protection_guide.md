# è®­ç»ƒä¿æŠ¤ç³»ç»Ÿä½¿ç”¨æŒ‡å—

## TrainingGuard + SOSA ç»„åˆä½¿ç”¨å®Œæ•´æŒ‡å—

æœ¬æ–‡æ¡£ä»‹ç»å¦‚ä½•åœ¨ APT-Transformer é¡¹ç›®ä¸­ä½¿ç”¨ **TrainingGuard**ï¼ˆç¡¬æ€§ä¿æŠ¤ï¼‰å’Œ **SOSA**ï¼ˆæ™ºèƒ½ç›‘æ§ï¼‰çš„ç»„åˆç³»ç»Ÿï¼Œå®ç°å®‰å…¨ã€æ™ºèƒ½çš„æ¨¡å‹è®­ç»ƒã€‚

---

## ğŸ¯ æ ¸å¿ƒæ¦‚å¿µ

### TrainingGuard - ç¡¬æ€§ä¿æŠ¤å±‚ ğŸ›¡ï¸

**ä½ç½®**: `apt_model/training/training_guard.py`

**åŠŸèƒ½**: é˜²æ­¢è®­ç»ƒå¤±æ§çš„å®‰å…¨é˜€

- â±ï¸ **æœ€å¤§æ­¥æ•°é™åˆ¶** - é˜²æ­¢æ— é™å¾ªç¯
- â° **æœ€å¤§æ—¶é—´é™åˆ¶** - é˜²æ­¢ MCP è¶…æ—¶
- ğŸ’¾ **å†…å­˜ç›‘æ§** - é˜²æ­¢ OOMï¼ˆè¶… 90% è‡ªåŠ¨æ¸…ç†ï¼‰
- ğŸ“ˆ **æ¢¯åº¦å¼‚å¸¸æ£€æµ‹** - NaN/Inf/çˆ†ç‚¸æ£€æµ‹
- âš ï¸ **æŸå¤±å¼‚å¸¸æ£€æµ‹** - NaN/Inf æŸå¤±ç«‹å³åœæ­¢
- ğŸ§¹ **è‡ªåŠ¨æ¸…ç†** - æ¯ N æ­¥æ¸…ç† GPU ç¼“å­˜
- ğŸ›‘ **Early Stopping** - éªŒè¯æŸå¤±æœªæ”¹å–„åˆ™åœæ­¢

### SOSA - æ™ºèƒ½ç›‘æ§å±‚ ğŸ§ 

**ä½ç½®**: `apt_model/core/training/sosa_core.py`

**åŠŸèƒ½**: å­¦ä¹ è®­ç»ƒæ¨¡å¼çš„æ™ºèƒ½ä½“

- ğŸ“Š **ç¨€ç–é©¬å°”ç§‘å¤«é“¾** - å­¦ä¹ çŠ¶æ€è½¬ç§»è§„å¾‹
- ğŸ”„ **Binary-Twin ç‰¹å¾** - è¿ç»­+ç¦»æ•£æ··åˆè¡¨ç¤º
- â³ **æ—¶é—´çª—å£èšåˆ** - æ»‘åŠ¨çª—å£äº‹ä»¶åˆ†æ
- ğŸ¯ **å¸å¼•å­æ£€æµ‹** - è¯†åˆ«å›ºåŒ–çš„è®­ç»ƒæ¨¡å¼
- ğŸ² **æ¢ç´¢-å›ºåŒ–å¹³è¡¡** - è‡ªé€‚åº”å†³ç­–å»ºè®®
- ğŸ“ˆ **ç»„åˆå ç”¨åº¦** - è®¡ç®—è¡Œä¸ºç©ºé—´å ç”¨

---

## ğŸ”„ ä¸¤è€…çš„äº’è¡¥å…³ç³»

| ç»´åº¦ | TrainingGuard | SOSA |
|------|---------------|------|
| **ç±»å‹** | é˜²å¾¡å‹ä¿æŠ¤ | å­¦ä¹ å‹ç›‘æ§ |
| **å†³ç­–** | å¼ºåˆ¶åœæ­¢ | å»ºè®®ä¼˜åŒ– |
| **å“åº”** | å³æ—¶ | å»¶è¿Ÿåˆ†æ |
| **ç›®æ ‡** | é˜²æ­¢å´©æºƒ | ç†è§£è¿‡ç¨‹ |
| **è¾“å‡º** | åœæ­¢ä¿¡å· | åˆ†ææŠ¥å‘Š |

**å®Œç¾ç»„åˆ**:
- **TrainingGuard** ç¡®ä¿è®­ç»ƒä¸ä¼šå¤±æ§ï¼ˆå®‰å…¨è¾¹ç•Œï¼‰
- **SOSA** å¸®åŠ©ç†è§£è®­ç»ƒè¿‡ç¨‹ï¼ˆæ™ºèƒ½åˆ†æï¼‰

---

## ğŸ“– åŸºç¡€ä½¿ç”¨

### æ–¹å¼ 1: ä»…ä½¿ç”¨ TrainingGuardï¼ˆæ¨èå¿«é€Ÿå¼€å§‹ï¼‰

æ‰€æœ‰è®­ç»ƒå™¨é»˜è®¤å¯ç”¨ TrainingGuardï¼š

```python
from apt_model.training.gpt_trainer import BaseGPTTrainer

# è‡ªåŠ¨å¯ç”¨è®­ç»ƒä¿æŠ¤
trainer = BaseGPTTrainer(
    model=model,
    tokenizer=tokenizer,
    enable_guard=True,           # é»˜è®¤å¯ç”¨
    max_steps=100000,            # æœ€å¤š 10 ä¸‡æ­¥
    max_time_hours=24,           # æœ€å¤š 24 å°æ—¶
    early_stopping_patience=10   # 10 epoch æœªæ”¹å–„åˆ™åœæ­¢
)

history = trainer.train(
    train_texts=texts,
    epochs=100,  # å³ä½¿è®¾ç½® 100ï¼Œguard ä¹Ÿä¼šåœ¨åˆé€‚æ—¶æœºåœæ­¢
    batch_size=8
)
```

### æ–¹å¼ 2: ä»…ä½¿ç”¨ SOSAï¼ˆé€‚åˆé•¿æœŸåˆ†æï¼‰

```python
from apt_model.core.training.sosa_core import SOSA, Event
import time

# åˆ›å»º SOSA ç›‘æ§å™¨
sosa = SOSA(
    dt_window=5.0,          # 5 ç§’æ—¶é—´çª—å£
    M_groups=10,            # 10 ä¸ªè¡Œä¸ºç»„
    exploration_weight=0.5  # æ¢ç´¢æƒé‡
)

# è®­ç»ƒå¾ªç¯
for step, batch in enumerate(dataloader):
    loss = train_step(batch)

    # SOSA æ”¶é›†äº‹ä»¶
    sosa.add_event(Event(
        timestamp=time.time(),
        event_type='train_step',
        severity=min(loss / 10.0, 1.0),  # å½’ä¸€åŒ–åˆ° [0,1]
        attributes={'step': step, 'loss': loss}
    ))

    # æ¯ 100 æ­¥æŸ¥çœ‹ SOSA åˆ†æ
    if step % 100 == 0:
        decision = sosa.decide_next_action()
        c_r = sosa.compute_combination_occupancy()

        print(f"[SOSA] æ­¥æ•°: {step}")
        print(f"  æ¢ç´¢å› å­: {decision['exploration_factor']:.3f}")
        print(f"  è¡Œä¸ºå ç”¨åº¦: {c_r:.2%}")
        print(f"  å»ºè®®çŠ¶æ€: {decision['recommended_state']}")
```

---

## â­ ç»„åˆä½¿ç”¨ï¼ˆæ¨èï¼‰

### å®Œæ•´ç¤ºä¾‹ï¼šTrainingGuard + SOSA ååŒå·¥ä½œ

```python
from apt_model.training.trainer import train_model
from apt_model.core.training.sosa_core import SOSA, Event
import time

# ===== 1. å‡†å¤‡è®­ç»ƒæ•°æ® =====
train_texts = [
    "Your training data here...",
    # ...
]

# ===== 2. åˆ›å»º SOSA ç›‘æ§å™¨ =====
sosa = SOSA(
    dt_window=10.0,         # 10 ç§’çª—å£ï¼ˆé€‚åˆè®­ç»ƒæ­¥é—´éš”ï¼‰
    M_groups=20,            # 20 ä¸ªè¡Œä¸ºç»„
    exploration_weight=0.3  # åå‘å›ºåŒ–ï¼ˆè®­ç»ƒç¨³å®šåï¼‰
)

# ===== 3. å®šä¹‰äº‹ä»¶è®°å½•å›è°ƒ =====
def on_training_step(step, loss, lr, metrics):
    """æ¯æ­¥è®­ç»ƒåçš„å›è°ƒ"""
    # SOSA æ”¶é›†è®­ç»ƒäº‹ä»¶
    severity = min(loss / 5.0, 1.0)  # æŸå¤±å½’ä¸€åŒ–

    sosa.add_event(Event(
        timestamp=time.time(),
        event_type='train_step',
        severity=severity,
        attributes={
            'step': step,
            'loss': loss,
            'lr': lr,
            'metrics': metrics
        },
        value=loss
    ))

    # æ¯ 50 æ­¥åˆ†æä¸€æ¬¡
    if step % 50 == 0:
        decision = sosa.decide_next_action()
        c_r = sosa.compute_combination_occupancy()

        print(f"\n{'='*60}")
        print(f"[SOSA åˆ†æ] æ­¥æ•°: {step}")
        print(f"{'='*60}")
        print(f"ğŸ“Š æ¢ç´¢å› å­: {decision['exploration_factor']:.3f}")
        print(f"ğŸ¯ è¡Œä¸ºå ç”¨åº¦: {c_r:.2%} (1 - c_r = {1-c_r:.2%} è‡ªç”±åº¦)")
        print(f"ğŸ”® ä¿¡å¿ƒåº¦: {decision['confidence']:.3f}")

        # æ ¹æ® SOSA å»ºè®®è°ƒæ•´ç­–ç•¥
        if decision['exploration_factor'] > 0.7:
            print("ğŸ’¡ å»ºè®®: å½“å‰å¤„äºæ¢ç´¢æœŸï¼Œå¯é€‚å½“å¢å¤§å­¦ä¹ ç‡")
        elif decision['exploration_factor'] < 0.3:
            print("ğŸ’¡ å»ºè®®: å·²è¿›å…¥å›ºåŒ–æœŸï¼Œå¯é™ä½å­¦ä¹ ç‡æˆ–å‡†å¤‡åœæ­¢")

        if c_r > 0.8:
            print("âš ï¸  è­¦å‘Š: è¡Œä¸ºç©ºé—´å ç”¨åº¦è¿‡é«˜ï¼Œå¯èƒ½è¿‡æ‹Ÿåˆ")

        print(f"{'='*60}\n")

# ===== 4. å¸¦ä¿æŠ¤çš„è®­ç»ƒï¼ˆé›†æˆ SOSAï¼‰ =====
model, tokenizer, config = train_model(
    texts=train_texts,
    epochs=50,
    batch_size=16,
    learning_rate=3e-4,

    # TrainingGuard ä¿æŠ¤å‚æ•°
    enable_guard=True,              # å¯ç”¨ç¡¬æ€§ä¿æŠ¤
    max_steps=200000,               # æœ€å¤š 20 ä¸‡æ­¥ï¼ˆMCP å®‰å…¨ï¼‰
    max_time_hours=48,              # æœ€å¤š 48 å°æ—¶
    early_stopping_patience=15,     # 15 epoch æœªæ”¹å–„åˆ™åœæ­¢
    guard_verbose=True,             # æ‰“å°ä¿æŠ¤ä¿¡æ¯

    # æ³¨æ„ï¼šç›®å‰éœ€è¦æ‰‹åŠ¨åœ¨è®­ç»ƒå¾ªç¯ä¸­è°ƒç”¨ on_training_step
    # æœªæ¥ç‰ˆæœ¬ä¼šæ”¯æŒå›è°ƒå‚æ•°
)

# ===== 5. è®­ç»ƒååˆ†æ SOSA çŠ¶æ€ =====
print("\n" + "="*80)
print("ğŸ§  SOSA æœ€ç»ˆåˆ†ææŠ¥å‘Š")
print("="*80)

# çŠ¶æ€å†å²
print(f"ğŸ“œ è®°å½•çš„çŠ¶æ€æ•°: {len(sosa.state_history)}")
print(f"ğŸ¯ å¸å¼•å­æ•°é‡: {len(sosa.attractors)}")

# æ‰“å°ä¸»è¦å¸å¼•å­
if sosa.attractors:
    print("\nä¸»è¦å¸å¼•å­ï¼ˆè®­ç»ƒå›ºåŒ–æ¨¡å¼ï¼‰:")
    sorted_attractors = sorted(
        sosa.attractors.items(),
        key=lambda x: x[1],
        reverse=True
    )[:5]

    for state_id, prob in sorted_attractors:
        print(f"  {state_id}: {prob:.4f}")

# é©¬å°”ç§‘å¤«é“¾ç»Ÿè®¡
print(f"\nğŸ“Š é©¬å°”ç§‘å¤«é“¾ç»Ÿè®¡:")
print(f"  æ€»çŠ¶æ€æ•°: {sosa.markov.next_idx}")
print(f"  æ€»è½¬ç§»æ•°: {len(sosa.markov.transitions)}")

print("="*80 + "\n")
```

---

## ğŸ›ï¸ é«˜çº§é…ç½®

### åœºæ™¯ 1: å¿«é€Ÿå®éªŒï¼ˆä¸¥æ ¼é™åˆ¶ï¼‰

```python
# å¿«é€Ÿå®éªŒï¼Œä¸¥æ ¼é™åˆ¶æ—¶é—´å’Œæ­¥æ•°
trainer = BaseGPTTrainer(
    model, tokenizer,
    enable_guard=True,
    max_steps=5000,              # åªè®­ç»ƒ 5000 æ­¥
    max_time_hours=0.5,          # æœ€å¤š 30 åˆ†é’Ÿ
    early_stopping_patience=3,   # 3 epoch æœªæ”¹å–„å³åœ
    guard_verbose=True
)
```

### åœºæ™¯ 2: ç”Ÿäº§è®­ç»ƒï¼ˆå®½æ¾ä½†å®‰å…¨ï¼‰

```python
# ç”Ÿäº§ç¯å¢ƒï¼Œé•¿æ—¶é—´è®­ç»ƒä½†æœ‰å®‰å…¨è¾¹ç•Œ
trainer = ClaudeTrainer(
    model, tokenizer,
    enable_guard=True,
    max_steps=500000,            # 50 ä¸‡æ­¥ä¸Šé™
    max_time_hours=72,           # æœ€å¤š 3 å¤©
    early_stopping_patience=20,  # 20 epoch æœªæ”¹å–„
    guard_verbose=True
)
```

### åœºæ™¯ 3: ç ”ç©¶åˆ†æï¼ˆSOSA ä¸»å¯¼ï¼‰

```python
# ç¦ç”¨ guardï¼Œä¸“æ³¨äº SOSA åˆ†æ
trainer = VFTTVATrainer(
    model, tokenizer,
    enable_guard=False  # å…³é—­ç¡¬æ€§ä¿æŠ¤ï¼Œå®Œå…¨ç”± SOSA åˆ†æ
)

# ä½†ä»å»ºè®®ä¿ç•™åŸºæœ¬ä¿æŠ¤
sosa = SOSA(dt_window=15.0, M_groups=30, exploration_weight=0.5)
```

---

## ğŸ“Š è¾“å‡ºè§£è¯»

### TrainingGuard è¾“å‡ºç¤ºä¾‹

```
ğŸ›¡ï¸ è®­ç»ƒä¿æŠ¤å·²å¯ç”¨
  æœ€å¤§æ­¥æ•°: 100000
  æœ€å¤§æ—¶é—´: 24.0 å°æ—¶
  å†…å­˜é™åˆ¶: 90%
  Early Stopping: patience=10

Epoch 1/100: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1250/1250 [10:23<00:00]
âœ“ éªŒè¯æŒ‡æ ‡æ”¹å–„: 2.3451

Epoch 10/100: 45%|â–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 562/1250 [05:42<06:41]
ğŸ›‘ è®­ç»ƒåœæ­¢: Early stopping triggered (patience=10)
   æ€»æ­¥æ•°: 11562
   è®­ç»ƒæ—¶é—´: 1.83 å°æ—¶

================================================================================
è®­ç»ƒä¿æŠ¤ç»Ÿè®¡:
  æ€»æ­¥æ•°: 11562
  è®­ç»ƒæ—¶é—´: 1.83 å°æ—¶
  NaN æŸå¤±: 0
  Inf æŸå¤±: 0
  æ¢¯åº¦çˆ†ç‚¸: 2
  å†…å­˜è­¦å‘Š: 5
  åœæ­¢åŸå› : Early stopping triggered (patience=10)
================================================================================
```

### SOSA è¾“å‡ºç¤ºä¾‹

```
============================================================
[SOSA åˆ†æ] æ­¥æ•°: 5000
============================================================
ğŸ“Š æ¢ç´¢å› å­: 0.456
ğŸ¯ è¡Œä¸ºå ç”¨åº¦: 45.00% (1 - c_r = 55.00% è‡ªç”±åº¦)
ğŸ”® ä¿¡å¿ƒåº¦: 0.723
ğŸ’¡ å»ºè®®: å½“å‰å¤„äºå¹³è¡¡æœŸï¼Œç»´æŒå½“å‰ç­–ç•¥
============================================================

ğŸ§  SOSA æœ€ç»ˆåˆ†ææŠ¥å‘Š
================================================================================
ğŸ“œ è®°å½•çš„çŠ¶æ€æ•°: 1247
ğŸ¯ å¸å¼•å­æ•°é‡: 8

ä¸»è¦å¸å¼•å­ï¼ˆè®­ç»ƒå›ºåŒ–æ¨¡å¼ï¼‰:
  S_3_5_7_2: 0.1823
  S_2_4_6_1: 0.1456
  S_4_6_8_3: 0.1234
  S_3_5_7_1: 0.0987
  S_2_3_5_0: 0.0876

ğŸ“Š é©¬å°”ç§‘å¤«é“¾ç»Ÿè®¡:
  æ€»çŠ¶æ€æ•°: 342
  æ€»è½¬ç§»æ•°: 1876
================================================================================
```

---

## ğŸ› ï¸ æœ€ä½³å®è·µ

### âœ… æ¨èåšæ³•

1. **å§‹ç»ˆå¯ç”¨ TrainingGuard**ï¼ˆå³ä½¿æ˜¯å®éªŒï¼‰
   ```python
   enable_guard=True  # é»˜è®¤å€¼ï¼Œå»ºè®®ä¿ç•™
   ```

2. **æ ¹æ®ç¡¬ä»¶è®¾ç½®åˆç†çš„æ—¶é—´é™åˆ¶**
   - æœ¬åœ° GPU: `max_time_hours=2-8`
   - äº‘æœåŠ¡å™¨: `max_time_hours=24-72`
   - MCP ç¯å¢ƒ: `max_time_hours=12-24` â­

3. **æ ¹æ®æ•°æ®è§„æ¨¡è®¾ç½®æ­¥æ•°é™åˆ¶**
   - å°æ•°æ®é›† (<10K): `max_steps=10000-50000`
   - ä¸­ç­‰æ•°æ®é›† (10K-100K): `max_steps=50000-200000`
   - å¤§æ•°æ®é›† (>100K): `max_steps=200000-1000000`

4. **ä½¿ç”¨ Early Stopping**
   ```python
   early_stopping_patience=10-20  # æ ¹æ® epoch æ—¶é•¿è°ƒæ•´
   ```

5. **SOSA ç”¨äºé•¿æœŸè®­ç»ƒåˆ†æ**
   - çŸ­æœŸå®éªŒ (<1 å°æ—¶): å¯ä»¥ä¸ç”¨ SOSA
   - é•¿æœŸè®­ç»ƒ (>4 å°æ—¶): å»ºè®®ä½¿ç”¨ SOSA åˆ†æ

### âŒ é¿å…çš„åšæ³•

1. âŒ **å®Œå…¨ç¦ç”¨ guard** ï¼ˆé™¤éæœ‰ç‰¹æ®Šç†ç”±ï¼‰
   ```python
   enable_guard=False  # å±é™©ï¼å¯èƒ½æ— é™è®­ç»ƒ
   ```

2. âŒ **è®¾ç½®è¿‡å¤§çš„é™åˆ¶**ï¼ˆå¤±å»ä¿æŠ¤æ„ä¹‰ï¼‰
   ```python
   max_steps=999999999  # ä¸å¦‚ä¸è®¾
   max_time_hours=999   # åŒä¸Š
   ```

3. âŒ **å¿½ç•¥ guard è­¦å‘Š**
   ```
   âš ï¸ æ¢¯åº¦èŒƒæ•°è¿‡å¤§: 234.56
   âš ï¸ å†…å­˜ä½¿ç”¨è¿‡é«˜: 92.3%
   # åº”è¯¥æ£€æŸ¥æ¨¡å‹æˆ–è°ƒæ•´å‚æ•°ï¼Œè€Œä¸æ˜¯å¿½ç•¥
   ```

4. âŒ **SOSA æ—¶é—´çª—å£è®¾ç½®ä¸å½“**
   ```python
   # è®­ç»ƒæ­¥é—´éš” 5 ç§’ï¼Œä½†çª—å£è®¾ç½® 0.5 ç§’
   sosa = SOSA(dt_window=0.5)  # å¤ªå°ï¼Œæ•è·ä¸åˆ°äº‹ä»¶

   # è®­ç»ƒæ­¥é—´éš” 1 ç§’ï¼Œä½†çª—å£è®¾ç½® 3600 ç§’
   sosa = SOSA(dt_window=3600)  # å¤ªå¤§ï¼Œå¤±å»æ—¶åºæ€§
   ```

---

## ğŸ” æ•…éšœæ’æŸ¥

### é—®é¢˜ 1: Guard è¿‡æ—©åœæ­¢è®­ç»ƒ

**ç°è±¡**:
```
ğŸ›‘ è®­ç»ƒåœæ­¢: è¾¾åˆ°æœ€å¤§æ­¥æ•° 1000
   æ€»æ­¥æ•°: 1000
```

**åŸå› **: `max_steps` è®¾ç½®è¿‡å°

**è§£å†³**:
```python
# å¢åŠ  max_steps æˆ–ç§»é™¤é™åˆ¶
max_steps=100000  # æˆ– None
```

### é—®é¢˜ 2: å†…å­˜è­¦å‘Šé¢‘ç¹

**ç°è±¡**:
```
âš ï¸ å†…å­˜ä½¿ç”¨è¿‡é«˜: 91.2% (é™åˆ¶: 90%)
âš ï¸ å†…å­˜ä½¿ç”¨è¿‡é«˜: 92.5% (é™åˆ¶: 90%)
```

**è§£å†³**:
```python
# 1. é™ä½ batch size
batch_size=4  # åŸæ¥æ˜¯ 8

# 2. é™ä½æ¨¡å‹å°ºå¯¸
# 3. ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
accumulation_steps=4

# 4. è°ƒé«˜å†…å­˜é™åˆ¶ï¼ˆä¸æ¨èï¼‰
# ç›®å‰ training_guard.py ä¸­ç¡¬ç¼–ç ä¸º 90%
```

### é—®é¢˜ 3: SOSA æ²¡æœ‰è¾“å‡º

**åŸå› **: æ—¶é—´çª—å£æœªæ»¡æˆ–äº‹ä»¶ä¸è¶³

**è§£å†³**:
```python
# ç¡®ä¿æ—¶é—´çª—å£åˆç†
sosa = SOSA(dt_window=5.0)  # 5 ç§’çª—å£

# ç¡®ä¿æœ‰è¶³å¤Ÿçš„äº‹ä»¶
for step in range(1000):
    sosa.add_event(...)  # è‡³å°‘éœ€è¦ 10+ ä¸ªäº‹ä»¶æ‰èƒ½åˆ·æ–°çª—å£
```

### é—®é¢˜ 4: æ¢¯åº¦çˆ†ç‚¸è­¦å‘Š

**ç°è±¡**:
```
âš ï¸ æ¢¯åº¦èŒƒæ•°è¿‡å¤§: 156.78
æ¢¯åº¦æŒç»­çˆ†ç‚¸ (10 æ¬¡)
ğŸ›‘ è®­ç»ƒåœæ­¢: æ¢¯åº¦æŒç»­çˆ†ç‚¸
```

**è§£å†³**:
```python
# 1. é™ä½å­¦ä¹ ç‡
learning_rate=1e-4  # åŸæ¥ 3e-4

# 2. å¢åŠ æ¢¯åº¦è£å‰ª
max_grad_norm=0.5  # åŸæ¥ 1.0

# 3. ä½¿ç”¨æ›´ç¨³å®šçš„ä¼˜åŒ–å™¨
# AdamW -> SGD with momentum
```

---

## ğŸ“š API å‚è€ƒ

### TrainingGuard å‚æ•°

```python
TrainingGuard(
    max_steps: Optional[int] = None,           # æœ€å¤§æ­¥æ•°
    max_time_hours: Optional[float] = None,    # æœ€å¤§æ—¶é—´ï¼ˆå°æ—¶ï¼‰
    max_memory_percent: float = 90.0,          # æœ€å¤§å†…å­˜ç™¾åˆ†æ¯”
    check_gradients: bool = True,              # æ˜¯å¦æ£€æŸ¥æ¢¯åº¦
    auto_cleanup_every: int = 100,             # è‡ªåŠ¨æ¸…ç†é—´éš”
    early_stopping: Optional[EarlyStopping] = None,  # Early stopping
    mcp_checkpoint_interval: int = 1000,       # MCP æ£€æŸ¥ç‚¹é—´éš”
    verbose: bool = True                       # è¯¦ç»†è¾“å‡º
)
```

### SOSA å‚æ•°

```python
SOSA(
    dt_window: float = 5.0,          # æ—¶é—´çª—å£ï¼ˆç§’ï¼‰
    M_groups: int = 10,              # è¡Œä¸ºç»„æ•°é‡
    exploration_weight: float = 0.5  # æ¢ç´¢æƒé‡ [0,1]
)
```

### Event ç»“æ„

```python
Event(
    timestamp: float,          # æ—¶é—´æˆ³
    event_type: str,          # äº‹ä»¶ç±»å‹ï¼š'train_step', 'error', 'warning'
    severity: float,          # ä¸¥é‡ç¨‹åº¦ [0,1]
    attributes: Dict,         # é™„åŠ å±æ€§
    value: Optional[float]    # å¯é€‰æ•°å€¼
)
```

---

## ğŸ“ æ€»ç»“

### ä½•æ—¶ä½¿ç”¨ä»€ä¹ˆç»„åˆ

| åœºæ™¯ | TrainingGuard | SOSA | è¯´æ˜ |
|------|---------------|------|------|
| **å¿«é€Ÿå®éªŒ** | âœ… å¿…é¡» | âŒ å¯é€‰ | é‡ç‚¹æ˜¯å®‰å…¨è¾¹ç•Œ |
| **ç”Ÿäº§è®­ç»ƒ** | âœ… å¿…é¡» | âœ… æ¨è | å®‰å…¨ + åˆ†æ |
| **ç ”ç©¶åˆ†æ** | âœ… æ¨è | âœ… å¿…é¡» | é‡ç‚¹æ˜¯ç†è§£è¿‡ç¨‹ |
| **MCP ç¯å¢ƒ** | âœ… å¿…é¡» | âœ… æ¨è | é˜²æ­¢è¶…æ—¶ + ç›‘æ§ |
| **è°ƒè¯•æ¨¡å¼** | âŒ å¯é€‰ | âœ… æ¨è | éœ€è¦è¯¦ç»†åˆ†æ |

### æ ¸å¿ƒè¦ç‚¹

1. **TrainingGuard = å®‰å…¨ä¿éšœ**ï¼ˆé˜²æ­¢å´©æºƒã€è¶…æ—¶ã€èµ„æºè€—å°½ï¼‰
2. **SOSA = æ™ºèƒ½åˆ†æ**ï¼ˆç†è§£è¿‡ç¨‹ã€é¢„æµ‹é—®é¢˜ã€ä¼˜åŒ–å»ºè®®ï¼‰
3. **ç»„åˆä½¿ç”¨ = æœ€ä½³å®è·µ**ï¼ˆå®‰å…¨ + æ™ºèƒ½ï¼‰

### å¿«é€Ÿå¼€å§‹

```python
# æœ€ç®€å•çš„å®‰å…¨è®­ç»ƒï¼ˆæ‰€æœ‰è®­ç»ƒå™¨é»˜è®¤é…ç½®ï¼‰
trainer.train(
    train_texts=texts,
    epochs=50,
    batch_size=8
)
# âœ… è‡ªåŠ¨å¯ç”¨ TrainingGuard ä¿æŠ¤
# âœ… è‡ªåŠ¨ Early Stopping
# âœ… è‡ªåŠ¨èµ„æºç›‘æ§
```

---

## ğŸ“ è·å–å¸®åŠ©

- **TrainingGuard æºç **: `apt_model/training/training_guard.py`
- **SOSA æºç **: `apt_model/core/training/sosa_core.py`
- **é—®é¢˜åé¦ˆ**: GitHub Issues
- **æ–‡æ¡£æ›´æ–°**: 2025-01-04

---

**Happy Training! ğŸš€**
