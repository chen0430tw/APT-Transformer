# HLBD - åˆ†å±‚è¯­è¨€å¯è’™æ•°æ®é›† (Hierarchical Language Bootstrapping Dataset)

## ğŸ“– æ¦‚è¿°

HLBDï¼ˆåˆ†å±‚è¯­è¨€å¯è’™æ•°æ®é›†ï¼‰æ˜¯ä¸“é—¨ä¸ºAPTæ¨¡å‹è®¾è®¡çš„å¤šå±‚çº§ã€å¤šè¯­è¨€å­¦ä¹ æ•°æ®é›†ã€‚é€šè¿‡å°†æ¦‚å¿µåˆ†è§£ä¸º**8ä¸ªä¸åŒçš„æŠ½è±¡å±‚çº§**ï¼ˆä»emojiç¬¦å·åˆ°å®Œæ•´çš„è‡ªç„¶è¯­è¨€æè¿°ï¼‰ï¼ŒHLBDå¸®åŠ©æ¨¡å‹å»ºç«‹å¯¹è¯­è¨€çš„æ·±å±‚ç†è§£å’Œè·¨è¯­è¨€æ˜ å°„èƒ½åŠ›ã€‚

## ğŸ¯ æ ¸å¿ƒç†å¿µ

HLBDçš„è®¾è®¡çµæ„Ÿæ¥è‡ªäººç±»çš„è¯­è¨€å­¦ä¹ è¿‡ç¨‹ï¼š
1. **ä»ç®€å•åˆ°å¤æ‚**ï¼šä»ç¬¦å·ï¼ˆå­—å¡/emojiï¼‰é€æ­¥è¿‡æ¸¡åˆ°å®Œæ•´å¥å­
2. **å¤šæ¨¡æ€è”ç³»**ï¼šå»ºç«‹ç¬¦å·ã€æ‹¼éŸ³ã€æ•°å­¦ç»“æ„ã€å¤šè¯­è¨€æ–‡æœ¬ä¹‹é—´çš„æ˜ å°„
3. **æ¦‚å¿µå¯¼å‘**ï¼šæ¯ä¸ªè®­ç»ƒæ ·æœ¬å›´ç»•ä¸€ä¸ªæ ¸å¿ƒæ¦‚å¿µå±•å¼€
4. **åˆ†å±‚æŠ½è±¡**ï¼šæ¨¡å‹å­¦ä¹ åœ¨ä¸åŒæŠ½è±¡å±‚çº§é—´è½¬æ¢

## ğŸ“Š æ•°æ®ç»“æ„

### 8ä¸ªæ ‡å‡†å±‚çº§

æ¯ä¸ªHLBDæ ·æœ¬åŒ…å«ä¸€ä¸ªæ ¸å¿ƒ **æ¦‚å¿µ**ï¼ˆconceptï¼‰å’Œ8ä¸ªè¡¨è¾¾å±‚çº§ï¼š

| å±‚çº§ | åç§° | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|------|
| **level_1** | å­—å¡/Emoji | æœ€ç®€ç¬¦å·è¡¨ç¤º | `ğŸŒ§ï¸` |
| **level_2** | çŸ­è¯­ | ç®€çŸ­è¯ç»„ | `ä¸‹é›¨` |
| **level_3** | å¥æ³•ç»“æ„ | æ•°å­¦/é€»è¾‘è¡¨è¾¾å¼ | `weather(rain, heavy)` |
| **level_4** | æ‹¼éŸ³ | ä¸­æ–‡æ‹¼éŸ³æ³¨éŸ³ | `xiÃ  yÇ” le` |
| **level_5** | è‹±æ–‡ | Englishè¡¨è¾¾ | `It's raining` |
| **level_6** | ä¸­æ–‡ | å®Œæ•´ä¸­æ–‡æè¿° | `å¤©ç©ºæ­£åœ¨ä¸‹é›¨` |
| **level_7** | æ—¥æ–‡ | æ—¥è¯­è¡¨è¾¾ | `é›¨ãŒé™ã£ã¦ã„ã¾ã™` |
| **level_8** | éŸ©æ–‡ | éŸ©è¯­è¡¨è¾¾ | `ë¹„ê°€ ë‚´ë¦¬ê³  ìˆì–´ìš”` |

### æ•°æ®æ ¼å¼

```json
{
  "concept": "ä¸‹é›¨",
  "level_1": {
    "å­—å¡": "é›¨",
    "emoji": "ğŸŒ§ï¸"
  },
  "level_2": {
    "çŸ­è¯­": "ä¸‹é›¨"
  },
  "level_3": {
    "æ•°å­¦": "weather(rain, present_continuous)"
  },
  "level_4": {
    "æ‹¼éŸ³": "xiÃ  yÇ” le"
  },
  "level_5": {
    "è‹±æ–‡": "It's raining. The weather is wet and cloudy."
  },
  "level_6": {
    "ä¸­æ–‡": "å¤©ç©ºæ­£åœ¨ä¸‹é›¨ï¼Œåœ°é¢é€æ¸å˜å¾—æ¹¿æ¶¦ã€‚"
  },
  "level_7": {
    "æ—¥æ–‡": "é›¨ãŒé™ã£ã¦ã„ã¾ã™ã€‚å¤©æ°—ãŒæ‚ªã„ã§ã™ã€‚"
  },
  "level_8": {
    "éŸ©æ–‡": "ë¹„ê°€ ë‚´ë¦¬ê³  ìˆì–´ìš”. ë‚ ì”¨ê°€ ë‚˜ì©ë‹ˆë‹¤."
  }
}
```

## ğŸ”§ ä½¿ç”¨æ–¹æ³•

### 1. å¿«é€Ÿå¼€å§‹ï¼šè®­ç»ƒHLBDæ¨¡å‹

```bash
# åŸºç¡€è®­ç»ƒï¼ˆ20ä¸ªepochï¼‰
python -m apt_model.data.hlbd.hlbd \
  --hlbd-path apt_model/åˆ†å±‚è¯­è¨€å¯è’™æ•°æ®é›†.txt \
  --output-dir apt_hlbd_model \
  --epochs 20

# ä½¿ç”¨GPUåŠ é€Ÿ
python -m apt_model.data.hlbd.hlbd \
  --hlbd-path apt_model/åˆ†å±‚è¯­è¨€å¯è’™æ•°æ®é›†.txt \
  --output-dir apt_hlbd_model \
  --epochs 50 \
  --device cuda \
  --batch-size 16

# è‡ªå®šä¹‰æ¨¡å‹é…ç½®
python -m apt_model.data.hlbd.hlbd \
  --hlbd-path apt_model/åˆ†å±‚è¯­è¨€å¯è’™æ•°æ®é›†.txt \
  --output-dir apt_hlbd_model \
  --epochs 20 \
  --d-model 1024 \
  --num-heads 16 \
  --num-layers 12 \
  --max-length 1024
```

### 2. è¯„ä¼°å·²è®­ç»ƒæ¨¡å‹

```bash
# ä»…è¯„ä¼°æ¨¡å¼
python -m apt_model.data.hlbd.hlbd \
  --hlbd-path apt_model/åˆ†å±‚è¯­è¨€å¯è’™æ•°æ®é›†.txt \
  --output-dir apt_hlbd_model \
  --evaluate-only

# æŒ‡å®šæ£€æŸ¥ç‚¹è¯„ä¼°
python -m apt_model.data.hlbd.hlbd \
  --hlbd-path apt_model/åˆ†å±‚è¯­è¨€å¯è’™æ•°æ®é›†.txt \
  --output-dir apt_hlbd_model \
  --evaluate-only \
  --resume apt_hlbd_model/checkpoint_best.pt
```

### 3. ç¨‹åºåŒ–ä½¿ç”¨

```python
from apt_model.data.hlbd.hlbd_adapter import (
    HLBDDataProcessor,
    HLBDDataset,
    prepare_hlbd_tokenizer,
    create_hlbd_apt_config
)

# 1. åŠ è½½å’Œå¤„ç†æ•°æ®
processor = HLBDDataProcessor(data_path="apt_model/åˆ†å±‚è¯­è¨€å¯è’™æ•°æ®é›†.txt")
processor.process_data(
    include_multilingual=True,      # åŒ…å«å¤šè¯­è¨€å±‚çº§
    include_separate_levels=True    # åŒ…å«å•ç‹¬å±‚çº§æ ·æœ¬
)

# 2. è·å–è®­ç»ƒæ–‡æœ¬
training_texts = processor.get_training_texts()
print(f"è®­ç»ƒæ ·æœ¬æ•°: {len(training_texts)}")

# 3. å‡†å¤‡åˆ†è¯å™¨ï¼ˆè‡ªåŠ¨é€‰æ‹©æœ€ä½³å¤šè¯­è¨€åˆ†è¯å™¨ï¼‰
tokenizer, detected_language = prepare_hlbd_tokenizer(
    hlbd_samples_or_path=processor.raw_samples,
    vocab_size=50000
)

# 4. åˆ›å»ºAPTæ¨¡å‹é…ç½®
config = create_hlbd_apt_config(vocab_size=tokenizer.vocab_size)

# 5. åˆ›å»ºæ•°æ®é›†
from torch.utils.data import DataLoader
dataset = HLBDDataset(training_texts, tokenizer, max_length=512)
dataloader = DataLoader(dataset, batch_size=8, shuffle=True)
```

## ğŸ“ è®­ç»ƒæ•ˆæœ

HLBDè®­ç»ƒèƒ½è®©æ¨¡å‹å­¦ä¼šï¼š

### âœ… è·¨å±‚çº§ç†è§£
- **ç¬¦å· â†’ è¯­è¨€**ï¼š`ğŸŒ§ï¸` â†’ `It's raining`
- **æ‹¼éŸ³ â†’ ä¸­æ–‡**ï¼š`xiÃ  yÇ” le` â†’ `å¤©ç©ºæ­£åœ¨ä¸‹é›¨`
- **ç»“æ„ â†’ è‡ªç„¶è¯­è¨€**ï¼š`weather(rain)` â†’ `å¤©æ°”æ­£åœ¨ä¸‹é›¨`

### âœ… å¤šè¯­è¨€ç¿»è¯‘
- **è‹±æ–‡ â†’ ä¸­æ–‡**ï¼š`I love you` â†’ `æˆ‘çˆ±ä½ `
- **ä¸­æ–‡ â†’ æ—¥æ–‡**ï¼š`æˆ‘çˆ±ä½ ` â†’ `æ„›ã—ã¦ã„ã¾ã™`
- **ä¸­æ–‡ â†’ éŸ©æ–‡**ï¼š`æˆ‘çˆ±ä½ ` â†’ `ì‚¬ë‘í•´ìš”`

### âœ… æ¦‚å¿µæ¨ç†
- **ä»æ¦‚å¿µç”Ÿæˆ**ï¼š`æ¦‚å¿µ: å¿«ä¹` â†’ `happiness, joy, cheerfulness`
- **æ¦‚å¿µå®Œæˆ**ï¼š`å®‰æŸæ˜¯` â†’ `å®‰æŸæ˜¯è’™å¾·åŸçš„ä¾¦å¯Ÿéª‘å£«ï¼Œæ“…é•¿å¼“ç®­å’Œä¾¦å¯Ÿ`

## ğŸ”¬ å¿«é€Ÿå®éªŒï¼šHLBDæµ‹è¯•è„šæœ¬

é¡¹ç›®æä¾›äº†ä¸€ä¸ªå¿«é€Ÿæµ‹è¯•è„šæœ¬ï¼Œå±•ç¤ºHLBDçš„å¼ºå¤§èƒ½åŠ›ï¼š

```bash
# è¿è¡ŒHLBDå¿«é€Ÿå­¦ä¹ æµ‹è¯•ï¼ˆ500 epochsï¼‰
python tests/test_hlbd_quick_learning.py
```

è¯¥æµ‹è¯•å±•ç¤ºï¼š
- âœ… ä½¿ç”¨20ä¸ªHLBDæ¦‚å¿µæ ·æœ¬
- âœ… åˆ›å»º80+ä¸ªè®­ç»ƒå¯¹ï¼ˆemojiâ†’ä¸­æ–‡ã€æ‹¼éŸ³â†’ä¸­æ–‡ã€è‹±æ–‡â†’ä¸­æ–‡ç­‰ï¼‰
- âœ… 500ä¸ªepochçš„å¼ºåŒ–è®­ç»ƒ
- âœ… å®æ—¶æ˜¾ç¤ºæ¨¡å‹çš„ç”Ÿæˆè¿›åº¦

**é¢„æœŸè¾“å‡ºç¤ºä¾‹**ï¼š
```
è¾“å…¥: ğŸŒ§ï¸
æœŸæœ›æ¦‚å¿µ: ä¸‹é›¨
ç”Ÿæˆ: å¤©ç©ºæ­£åœ¨ä¸‹é›¨ï¼Œåœ°é¢æ¹¿æ¶¦

è¾“å…¥: â¤ï¸
æœŸæœ›æ¦‚å¿µ: æˆ‘çˆ±ä½ 
ç”Ÿæˆ: æˆ‘çˆ±ä½ ï¼Œäº²çˆ±çš„

è¾“å…¥: I love you
æœŸæœ›æ¦‚å¿µ: æˆ‘çˆ±ä½ 
ç”Ÿæˆ: æˆ‘çˆ±ä½ ï¼Œæˆ‘éå¸¸çˆ±ä½ 
```

## ğŸŒ æ‰©å±•åˆ°æ›´å¤šè¯­è¨€

HLBDæ”¯æŒåŠ¨æ€æ·»åŠ æ–°è¯­è¨€å±‚çº§ï¼ˆlevel_9, level_10...ï¼‰ï¼š

```python
# æ·»åŠ æ³•è¯­å’Œå¾·è¯­å±‚çº§
extra_languages = {
    "level_9": "æ³•è¯­",
    "level_10": "å¾·è¯­"
}

processor = HLBDDataProcessor(
    data_path="åˆ†å±‚è¯­è¨€å¯è’™æ•°æ®é›†.txt",
    extra_languages=extra_languages
)

# æ‰©å±•æ•°æ®æ ¼å¼
sample = {
    "concept": "ä¸‹é›¨",
    "level_1": {"å­—å¡": "é›¨", "emoji": "ğŸŒ§ï¸"},
    "level_6": {"ä¸­æ–‡": "å¤©ç©ºæ­£åœ¨ä¸‹é›¨"},
    "level_9": {"æ³•è¯­": "Il pleut"},
    "level_10": {"å¾·è¯­": "Es regnet"}
}
```

## ğŸ“ˆ è¯„ä¼°æŒ‡æ ‡

HLBDè¯„ä¼°å™¨æä¾›å¤šç»´åº¦è¯„ä¼°ï¼š

```python
from apt_model.data.hlbd.hlbd_adapter import HLBDModelEvaluator

evaluator = HLBDModelEvaluator(
    model=trained_model,
    tokenizer=tokenizer,
    processor=processor
)

# 1. è¯„ä¼°æ‰€æœ‰è¯­è¨€å¯¹ç¿»è¯‘èƒ½åŠ›
results = evaluator.evaluate_all_language_pairs(num_samples=5)
print(f"æ€»ä½“å¹³å‡ç›¸ä¼¼åº¦: {results['overall_avg_similarity']:.4f}")

# 2. è¯„ä¼°ç‰¹å®šè¯­è¨€å¯¹
en_to_zh = evaluator.evaluate_language_generation(
    source_lang="è‹±æ–‡",
    target_lang="ä¸­æ–‡",
    num_samples=10
)

# 3. è¯„ä¼°æ¦‚å¿µå®Œæˆèƒ½åŠ›
concept_results = evaluator.evaluate_concept_completion(num_samples=5)
```

## ğŸ“ æ–‡ä»¶ä½ç½®

- **æ•°æ®é›†æ–‡ä»¶**: `apt_model/åˆ†å±‚è¯­è¨€å¯è’™æ•°æ®é›†.txt`
- **é€‚é…å™¨æ¨¡å—**: `apt_model/data/hlbd/hlbd_adapter.py`
- **è®­ç»ƒè„šæœ¬**: `apt_model/data/hlbd/hlbd.py`
- **å¿«é€Ÿæµ‹è¯•**: `tests/test_hlbd_quick_learning.py`

## âš™ï¸ å‘½ä»¤è¡Œå‚æ•°å®Œæ•´åˆ—è¡¨

### è®­ç»ƒå‚æ•°
```bash
--hlbd-path PATH          # HLBDæ•°æ®é›†æ–‡ä»¶è·¯å¾„ï¼ˆå¿…éœ€ï¼‰
--output-dir DIR          # æ¨¡å‹è¾“å‡ºç›®å½•ï¼ˆå¿…éœ€ï¼‰
--epochs N                # è®­ç»ƒè½®æ•°ï¼ˆé»˜è®¤ï¼š20ï¼‰
--batch-size N            # æ‰¹æ¬¡å¤§å°ï¼ˆé»˜è®¤ï¼š8ï¼‰
--lr FLOAT                # å­¦ä¹ ç‡ï¼ˆé»˜è®¤ï¼š3e-5ï¼‰
--max-length N            # æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆé»˜è®¤ï¼š512ï¼‰
--warmup-steps N          # é¢„çƒ­æ­¥æ•°ï¼ˆé»˜è®¤ï¼š1000ï¼‰
--gradient-clip FLOAT     # æ¢¯åº¦è£å‰ªé˜ˆå€¼ï¼ˆé»˜è®¤ï¼š1.0ï¼‰
```

### æ¨¡å‹å‚æ•°
```bash
--d-model N               # æ¨¡å‹ç»´åº¦ï¼ˆé»˜è®¤ï¼š768ï¼‰
--num-heads N             # æ³¨æ„åŠ›å¤´æ•°ï¼ˆé»˜è®¤ï¼š12ï¼‰
--num-layers N            # å±‚æ•°ï¼ˆé»˜è®¤ï¼š6ï¼‰
```

### æ•°æ®å‚æ•°
```bash
--include-multilingual    # åŒ…å«å¤šè¯­è¨€æ–‡æœ¬ï¼ˆé»˜è®¤ï¼šTrueï¼‰
--include-separate-levels # åŒ…å«å•ç‹¬å±‚çº§æ ·æœ¬ï¼ˆé»˜è®¤ï¼šTrueï¼‰
```

### å…¶ä»–å‚æ•°
```bash
--device {auto,cuda,cpu}  # è®¡ç®—è®¾å¤‡ï¼ˆé»˜è®¤ï¼šautoï¼‰
--evaluate-only           # ä»…è¯„ä¼°æ¨¡å¼
--resume PATH             # ä»æ£€æŸ¥ç‚¹æ¢å¤
--monitor-resources       # å¯ç”¨èµ„æºç›‘æ§
--monitor-interval N      # ç›‘æ§é—´éš”ï¼ˆç§’ï¼‰
--log-file PATH           # æ—¥å¿—æ–‡ä»¶è·¯å¾„
--seed N                  # éšæœºç§å­ï¼ˆé»˜è®¤ï¼š42ï¼‰
--verbose                 # è¯¦ç»†è¾“å‡ºæ¨¡å¼
```

## ğŸ¯ ä½¿ç”¨åœºæ™¯

### 1. å¤šè¯­è¨€æœºå™¨ç¿»è¯‘
HLBDæä¾›äº†ä¸°å¯Œçš„å¹³è¡Œè¯­æ–™ï¼Œé€‚åˆè®­ç»ƒå¤šè¯­è¨€ç¿»è¯‘æ¨¡å‹ã€‚

### 2. æ¦‚å¿µå­¦ä¹ ç ”ç©¶
é€šè¿‡åˆ†å±‚ç»“æ„ç ”ç©¶æ¨¡å‹å¦‚ä½•ç†è§£å’Œè¡¨ç¤ºæŠ½è±¡æ¦‚å¿µã€‚

### 3. è·¨æ¨¡æ€ç†è§£
ç ”ç©¶emojiã€ç¬¦å·ä¸è‡ªç„¶è¯­è¨€ä¹‹é—´çš„æ˜ å°„å…³ç³»ã€‚

### 4. è¯­è¨€å¯è’™æ•™å­¦
æ¨¡ä»¿äººç±»ä»ç®€å•ç¬¦å·åˆ°å¤æ‚è¯­è¨€çš„å­¦ä¹ è·¯å¾„ã€‚

### 5. ä½èµ„æºè¯­è¨€è®­ç»ƒ
é€šè¿‡è·¨è¯­è¨€å¯¹é½ï¼Œå¸®åŠ©æ¨¡å‹å­¦ä¹ èµ„æºè¾ƒå°‘çš„è¯­è¨€ã€‚

## ğŸ” æŠ€æœ¯ç‰¹ç‚¹

- âœ… **è‡ªåŠ¨åˆ†è¯å™¨é€‰æ‹©**ï¼šæ ¹æ®æ•°æ®è‡ªåŠ¨é€‰æ‹©æœ€ä½³å¤šè¯­è¨€åˆ†è¯å™¨
- âœ… **å†…å­˜ä¼˜åŒ–**ï¼šæ”¯æŒå¤§è§„æ¨¡æ•°æ®é›†çš„é«˜æ•ˆå¤„ç†
- âœ… **çµæ´»æ‰©å±•**ï¼šè½»æ¾æ·»åŠ æ–°è¯­è¨€å±‚çº§
- âœ… **å®Œæ•´è¯„ä¼°**ï¼šæä¾›å¤šç»´åº¦æ¨¡å‹æ€§èƒ½è¯„ä¼°
- âœ… **å…¼å®¹APTæ¶æ„**ï¼šæ— ç¼é›†æˆåˆ°APT-Transformeræ¡†æ¶

## ğŸš€ æ€§èƒ½å»ºè®®

### è®­ç»ƒå»ºè®®
- **å°æ•°æ®é›†**ï¼ˆ<50æ¦‚å¿µï¼‰ï¼š20-50 epochsï¼Œbatch_size=4
- **ä¸­ç­‰æ•°æ®é›†**ï¼ˆ50-200æ¦‚å¿µï¼‰ï¼š50-100 epochsï¼Œbatch_size=8
- **å¤§æ•°æ®é›†**ï¼ˆ>200æ¦‚å¿µï¼‰ï¼š100-500 epochsï¼Œbatch_size=16

### ç¡¬ä»¶å»ºè®®
- **CPUè®­ç»ƒ**ï¼šbatch_sizeâ‰¤4ï¼Œd_modelâ‰¤512
- **å•GPU**ï¼ˆ8GBï¼‰ï¼šbatch_sizeâ‰¤8ï¼Œd_modelâ‰¤768
- **å•GPU**ï¼ˆ16GB+ï¼‰ï¼šbatch_sizeâ‰¤16ï¼Œd_modelâ‰¤1024

## ğŸ†˜ å¸¸è§é—®é¢˜

### Q1: è®­ç»ƒæ—¶æ˜¾å­˜ä¸è¶³æ€ä¹ˆåŠï¼Ÿ
**A**: å‡å°`--batch-size`ã€`--max-length`æˆ–`--d-model`å‚æ•°ã€‚

### Q2: å¦‚ä½•æ·»åŠ è‡ªå®šä¹‰è¯­è¨€å±‚çº§ï¼Ÿ
**A**: ä½¿ç”¨`extra_languages`å‚æ•°ï¼š
```python
extra_languages = {"level_9": "æ³•è¯­", "level_10": "å¾·è¯­"}
processor = HLBDDataProcessor(data_path=path, extra_languages=extra_languages)
```

### Q3: æ¨¡å‹ç”Ÿæˆè´¨é‡ä¸ä½³ï¼Ÿ
**A**: å°è¯•å¢åŠ è®­ç»ƒè½®æ•°ï¼ˆepochsï¼‰ã€è°ƒæ•´å­¦ä¹ ç‡ï¼Œæˆ–ä½¿ç”¨æ›´å¤§çš„æ¨¡å‹é…ç½®ã€‚

### Q4: å¦‚ä½•åªè®­ç»ƒç‰¹å®šè¯­è¨€å¯¹ï¼Ÿ
**A**: åœ¨æ•°æ®å¤„ç†æ—¶è¿‡æ»¤ï¼š
```python
# åªä¿ç•™ä¸­è‹±æ–‡å¯¹
filtered_texts = [t for t in training_texts if "è‹±æ–‡:" in t or "ä¸­æ–‡:" in t]
```

## ğŸ“š å‚è€ƒèµ„æ–™

- **APTæ¨¡å‹è®ºæ–‡**: æŸ¥çœ‹é¡¹ç›®æ ¹ç›®å½•çš„ç ”ç©¶è®ºæ–‡
- **åˆ†è¯å™¨é›†æˆ**: `apt_model/modeling/chinese_tokenizer_integration.py`
- **è®­ç»ƒä¼˜åŒ–å™¨**: `apt_model/training/optimizer.py`
- **æ£€æŸ¥ç‚¹ç®¡ç†**: `apt_model/training/checkpoint.py`

---

**è´¡çŒ®è€…**: APT-Transformerå›¢é˜Ÿ
**æœ€åæ›´æ–°**: 2025-12-04
**è®¸å¯**: ä¸APT-Transformeré¡¹ç›®ç›¸åŒ
