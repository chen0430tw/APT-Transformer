# APTè®­ç»ƒåç«¯ä½¿ç”¨æŒ‡å—

æœ¬é¡¹ç›®æ”¯æŒ**5ç§è®­ç»ƒåç«¯**ï¼Œæ»¡è¶³ä»å•å¡æœ¬åœ°å®éªŒåˆ°å¤§è§„æ¨¡äº‘ç«¯åˆ†å¸ƒå¼è®­ç»ƒçš„æ‰€æœ‰éœ€æ±‚ã€‚

## ğŸ“‹ å¿«é€Ÿé€‰æ‹©

| ä½¿ç”¨åœºæ™¯ | æ¨èåç«¯ | ç‰¹ç‚¹ |
|---------|---------|------|
| ğŸ§ª **HLBDæ•°æ®é›†å®éªŒ** | Playground | Cosineé‡å¯å­¦ä¹ ç‡ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ |
| ğŸ’» **å•å¡æœ¬åœ°è®­ç»ƒ** | Playground / HuggingFace | RTX 3070ä¼˜åŒ–ï¼Œæ··åˆç²¾åº¦ |
| ğŸš€ **å¤šå¡åˆ†å¸ƒå¼è®­ç»ƒ** | DeepSpeed | ZeRO-2/3ä¼˜åŒ–ï¼Œæ”¯æŒè¶…å¤§æ¨¡å‹ |
| â˜ï¸ **äº‘ç«¯è®­ç»ƒ** | Azure ML | MLflowè·Ÿè¸ªï¼Œè‡ªåŠ¨è¶…å‚æ•°è°ƒä¼˜ |
| ğŸ¤— **ç”Ÿæ€ç³»ç»Ÿé›†æˆ** | HuggingFace Trainer | W&Bã€TensorBoardã€Hubé›†æˆ |

---

## ğŸ® Backend 1: Playgroundè®­ç»ƒ

**æ¨èç”¨äºHLBD Hardcoreæ•°æ®é›†è®­ç»ƒ**

### ç‰¹æ€§
- âœ… Playground Theoryï¼ˆCosineAnnealingWarmRestartsï¼‰
- âœ… åŠ¨æ€æ ‡ç­¾æ”¯æŒï¼ˆ[EMOJI], [EN], [PY], [JP], [KR]ï¼‰
- âœ… RTX 3070ä¼˜åŒ–ï¼ˆæ··åˆç²¾åº¦ + æ¢¯åº¦ç´¯ç§¯ï¼‰
- âœ… DBC-DACæ¢¯åº¦ç¨³å®š
- âœ… å®æ—¶å¯è§†åŒ–æ”¯æŒ

### ä½¿ç”¨æ–¹æ³•

```bash
# æ–¹å¼1: ç›´æ¥è¿è¡Œ
python training/train_hlbd_playground.py --dataset HLBD_Hardcore_Full.json --epochs 100

# æ–¹å¼2: ç»Ÿä¸€å¯åŠ¨å™¨
python training/train.py --backend playground --epochs 100

# è‡ªå®šä¹‰å‚æ•°
python training/train_hlbd_playground.py \
    --dataset HLBD_Hardcore_Full.json \
    --epochs 100 \
    --save-dir hlbd_playground \
    --save-interval 25
```

### é…ç½®å‚æ•°

```python
# æ¨¡å‹é…ç½®ï¼ˆPlaygroundConfigï¼‰
d_model = 256          # æ¨¡å‹ç»´åº¦
n_layers = 6           # å±‚æ•°
n_heads = 8            # æ³¨æ„åŠ›å¤´æ•°
batch_size = 16        # Batchå¤§å°
gradient_accumulation_steps = 2  # æ¢¯åº¦ç´¯ç§¯

# Playground Theory
base_lr = 3e-4         # åŸºç¡€å­¦ä¹ ç‡
min_lr = 1e-5          # æœ€å°å­¦ä¹ ç‡
T_0 = 10               # Cosineé‡å¯å‘¨æœŸ
T_mult = 2             # å‘¨æœŸå€å¢ç³»æ•°
```

### è¾“å‡ºæ–‡ä»¶

```
hlbd_playground/
â”œâ”€â”€ checkpoint_epoch_25.pt      # Checkpoint (æ¯25è½®)
â”œâ”€â”€ checkpoint_epoch_50.pt
â”œâ”€â”€ final_model.pt              # æœ€ç»ˆæ¨¡å‹
â””â”€â”€ experiment_report.json      # è®­ç»ƒæŠ¥å‘Šï¼ˆä¾›å¯è§†åŒ–ï¼‰
```

---

## ğŸš€ Backend 2: DeepSpeedåˆ†å¸ƒå¼è®­ç»ƒ

**æ¨èç”¨äºå¤šGPUè®­ç»ƒå’Œè¶…å¤§æ¨¡å‹**

### ç‰¹æ€§
- âœ… ZeRO-1/2/3ä¼˜åŒ–ï¼ˆå†…å­˜ä¼˜åŒ–10-15å€ï¼‰
- âœ… CPUå¸è½½ï¼ˆæ”¯æŒ100B+æ¨¡å‹ï¼‰
- âœ… æ··åˆç²¾åº¦ï¼ˆFP16/BF16ï¼‰
- âœ… æ¢¯åº¦ç´¯ç§¯
- âœ… åˆ†å¸ƒå¼æ•°æ®å¹¶è¡Œ

### å®‰è£…ä¾èµ–

```bash
pip install deepspeed
```

### ä½¿ç”¨æ–¹æ³•

```bash
# æ–¹å¼1: DeepSpeedå¯åŠ¨ï¼ˆæ¨èï¼‰
deepspeed --num_gpus 2 train_deepspeed.py \
    --dataset HLBD_Hardcore_Full.json \
    --epochs 100 \
    --zero-stage 2 \
    --fp16

# æ–¹å¼2: ç»Ÿä¸€å¯åŠ¨å™¨
python training/train.py --backend deepspeed \
    --num-gpus 2 \
    --zero-stage 2 \
    --fp16 \
    --epochs 100

# ZeRO-3 + CPUå¸è½½ï¼ˆè¶…å¤§æ¨¡å‹ï¼‰
deepspeed --num_gpus 4 train_deepspeed.py \
    --zero-stage 3 \
    --cpu-offload \
    --fp16 \
    --train-batch-size 256 \
    --gradient-accumulation 4
```

### ZeROé˜¶æ®µé€‰æ‹©

| ZeROé˜¶æ®µ | å†…å­˜èŠ‚çœ | é€‚ç”¨åœºæ™¯ |
|---------|---------|---------|
| **ZeRO-1** | 4x | å•å¡æ”¾ä¸ä¸‹ä¼˜åŒ–å™¨çŠ¶æ€ |
| **ZeRO-2** | 8x | å¤šå¡è®­ç»ƒï¼Œæ˜¾å­˜ä¸è¶³ |
| **ZeRO-3** | 10-15x | è¶…å¤§æ¨¡å‹ï¼ˆ100B+å‚æ•°ï¼‰ |

### DeepSpeedé…ç½®æ–‡ä»¶

```json
{
  "train_batch_size": 64,
  "gradient_accumulation_steps": 4,
  "zero_optimization": {
    "stage": 2,
    "offload_optimizer": {
      "device": "cpu",
      "pin_memory": true
    }
  },
  "fp16": {
    "enabled": true,
    "loss_scale": 0,
    "loss_scale_window": 1000
  }
}
```

### è¾“å‡ºæ–‡ä»¶

```
deepspeed_output/
â”œâ”€â”€ deepspeed_config.json       # DeepSpeedé…ç½®
â”œâ”€â”€ checkpoint_epoch_25/        # DeepSpeed checkpoint
â”‚   â”œâ”€â”€ mp_rank_00_model_states.pt
â”‚   â”œâ”€â”€ zero_pp_rank_0_mp_rank_00_optim_states.pt
â”‚   â””â”€â”€ tokenizer_state.json
â””â”€â”€ checkpoint_epoch_100/
```

---

## â˜ï¸ Backend 3: Azure MLäº‘ç«¯è®­ç»ƒ

**æ¨èç”¨äºäº‘ç«¯å¤§è§„æ¨¡è®­ç»ƒå’Œå®éªŒç®¡ç†**

### ç‰¹æ€§
- âœ… Azure MLè®¡ç®—é›†ç¾¤è‡ªåŠ¨ç®¡ç†
- âœ… MLflowå®éªŒè·Ÿè¸ªå’Œæ¨¡å‹æ³¨å†Œ
- âœ… è¶…å‚æ•°æ‰«æï¼ˆSweep jobsï¼‰
- âœ… TensorBoardé›†æˆ
- âœ… äº‘ç«¯checkpointç®¡ç†

### å®‰è£…ä¾èµ–

```bash
pip install azure-ai-ml mlflow azureml-mlflow
az login  # Azureç™»å½•
```

### ä½¿ç”¨æ–¹æ³•

```bash
# æ–¹å¼1: ç›´æ¥æäº¤
python training/train_azure_ml.py \
    --subscription-id <YOUR_SUBSCRIPTION_ID> \
    --resource-group <YOUR_RESOURCE_GROUP> \
    --workspace-name <YOUR_WORKSPACE> \
    --dataset HLBD_Hardcore_Full.json \
    --epochs 100 \
    --compute-name gpu-cluster \
    --vm-size Standard_NC6s_v3

# æ–¹å¼2: ç»Ÿä¸€å¯åŠ¨å™¨
python training/train.py --backend azure \
    --azure-subscription-id <ID> \
    --azure-resource-group <RG> \
    --azure-workspace-name <WS> \
    --epochs 100

# è¶…å‚æ•°æ‰«æ
python training/train_azure_ml.py \
    --subscription-id <ID> \
    --resource-group <RG> \
    --workspace-name <WS> \
    --sweep  # å¯ç”¨è¶…å‚æ•°æ‰«æ
```

### Azure ML VMè§„æ ¼æ¨è

| VMè§„æ ¼ | GPU | å†…å­˜ | é€‚ç”¨åœºæ™¯ |
|--------|-----|------|---------|
| **Standard_NC6s_v3** | 1x V100 16GB | 112GB | å•å¡è®­ç»ƒ |
| **Standard_NC12s_v3** | 2x V100 16GB | 224GB | å¤šå¡è®­ç»ƒ |
| **Standard_NC24s_v3** | 4x V100 16GB | 448GB | å¤§è§„æ¨¡è®­ç»ƒ |
| **Standard_ND40rs_v2** | 8x V100 32GB | 672GB | è¶…å¤§æ¨¡å‹ |

### è¶…å‚æ•°æ‰«æé…ç½®

```python
search_space = {
    "batch_size": Choice([8, 16, 32]),
    "d_model": Choice([128, 256, 512]),
    "n_layers": Choice([4, 6, 8]),
    "learning_rate": Uniform(1e-5, 1e-3),
    "weight_decay": Uniform(0.001, 0.1)
}
```

### æŸ¥çœ‹è®­ç»ƒè¿›åº¦

```bash
# æŸ¥çœ‹ä»»åŠ¡çŠ¶æ€
az ml job show --name <JOB_NAME>

# å®æ—¶æ—¥å¿—æµ
az ml job stream --name <JOB_NAME>
```

---

## ğŸ¤— Backend 4: HuggingFace Trainer

**æ¨èç”¨äºç”Ÿæ€ç³»ç»Ÿé›†æˆå’Œå¿«é€ŸåŸå‹**

### ç‰¹æ€§
- âœ… HuggingFace Trainer APIï¼ˆå¼€ç®±å³ç”¨æœ€ä½³å®è·µï¼‰
- âœ… Weights & Biasesé›†æˆ
- âœ… TensorBoardé›†æˆ
- âœ… æ—©åœï¼ˆEarly Stoppingï¼‰
- âœ… HuggingFace Hubæ¨¡å‹ä¸Šä¼ 
- âœ… æ”¯æŒDeepSpeedï¼ˆé€šè¿‡Trainerï¼‰

### å®‰è£…ä¾èµ–

```bash
pip install transformers datasets accelerate wandb
```

### ä½¿ç”¨æ–¹æ³•

```bash
# æ–¹å¼1: åŸºç¡€è®­ç»ƒ
python training/train_hf_trainer.py \
    --dataset HLBD_Hardcore_Full.json \
    --epochs 100 \
    --fp16

# æ–¹å¼2: ç»Ÿä¸€å¯åŠ¨å™¨
python training/train.py --backend huggingface --epochs 100

# å¯ç”¨Weights & Biases
python training/train_hf_trainer.py \
    --wandb \
    --wandb-project apt-hlbd-training \
    --epochs 100

# å¯ç”¨æ—©åœ
python training/train_hf_trainer.py \
    --early-stopping \
    --early-stopping-patience 5 \
    --epochs 100

# HuggingFace Trainer + DeepSpeed
python training/train_hf_trainer.py \
    --deepspeed ds_config.json \
    --fp16 \
    --epochs 100
```

### TrainingArgumentsé…ç½®

```python
TrainingArguments(
    output_dir="hf_output",
    num_train_epochs=100,
    per_device_train_batch_size=16,
    gradient_accumulation_steps=2,
    learning_rate=3e-4,
    weight_decay=0.01,
    warmup_steps=500,
    fp16=True,                    # æ··åˆç²¾åº¦
    logging_steps=20,
    save_steps=500,
    save_total_limit=3,
    report_to="wandb",            # W&Bè·Ÿè¸ª
    load_best_model_at_end=True,  # æ—©åœ
)
```

### ä¸Šä¼ åˆ°HuggingFace Hub

```bash
# è®¾ç½®HF token
export HF_HUB_TOKEN=<YOUR_TOKEN>

# è®­ç»ƒå¹¶ä¸Šä¼ 
python training/train_hf_trainer.py --epochs 100
# æ¨¡å‹ä¼šè‡ªåŠ¨ä¸Šä¼ åˆ° https://huggingface.co/apt-model-256d-6l
```

### è¾“å‡ºæ–‡ä»¶

```
hf_output/
â”œâ”€â”€ checkpoint-500/              # Checkpointï¼ˆæ¯500æ­¥ï¼‰
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â””â”€â”€ trainer_state.json
â”œâ”€â”€ final_model/                 # æœ€ç»ˆæ¨¡å‹
â”‚   â”œâ”€â”€ config.json
â”‚   â”œâ”€â”€ pytorch_model.bin
â”‚   â””â”€â”€ tokenizer_state.json
â””â”€â”€ logs/                        # TensorBoardæ—¥å¿—
    â””â”€â”€ events.out.tfevents...
```

---

## ğŸ¯ ç»Ÿä¸€å¯åŠ¨å™¨

æ‰€æœ‰åç«¯éƒ½å¯ä»¥é€šè¿‡ç»Ÿä¸€å¯åŠ¨å™¨ `train.py` ä½¿ç”¨ï¼š

```bash
# æŸ¥çœ‹æ‰€æœ‰å¯ç”¨åç«¯
python training/train.py --list-backends

# Playgroundè®­ç»ƒ
python training/train.py --backend playground --epochs 100

# DeepSpeedè®­ç»ƒ
python training/train.py --backend deepspeed --num-gpus 2 --zero-stage 2

# Azure MLè®­ç»ƒ
python training/train.py --backend azure \
    --azure-subscription-id <ID> \
    --azure-resource-group <RG> \
    --azure-workspace-name <WS>

# HuggingFaceè®­ç»ƒ
python training/train.py --backend huggingface --wandb --epochs 100
```

---

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

### å•å¡RTX 3070ï¼ˆ8GBæ˜¾å­˜ï¼‰

| åç«¯ | Batch Size | æ··åˆç²¾åº¦ | å†…å­˜ä½¿ç”¨ | é€Ÿåº¦ |
|------|-----------|---------|---------|------|
| **Playground** | 16 | FP16 | 6.2GB | â­â­â­â­ |
| **HuggingFace** | 16 | FP16 | 6.5GB | â­â­â­â­ |
| **DeepSpeed (ZeRO-2)** | 32 | FP16 | 7.8GB | â­â­â­â­â­ |

### å¤šå¡è®­ç»ƒï¼ˆ4x RTX 3090ï¼‰

| åç«¯ | ZeROé˜¶æ®µ | Batch Size | ååé‡ |
|------|---------|-----------|--------|
| **DeepSpeed** | ZeRO-2 | 128 | â­â­â­â­â­ |
| **HuggingFace + DS** | ZeRO-2 | 128 | â­â­â­â­â­ |

---

## ğŸ”§ å¸¸è§é—®é¢˜

### Q1: åº”è¯¥é€‰æ‹©å“ªä¸ªåç«¯ï¼Ÿ

- **å­¦ä¹ å’Œå®éªŒ**: Playgroundï¼ˆç®€å•ç›´è§‚ï¼‰
- **å¤šå¡è®­ç»ƒ**: DeepSpeedï¼ˆæ€§èƒ½æœ€ä¼˜ï¼‰
- **äº‘ç«¯è®­ç»ƒ**: Azure MLï¼ˆç®¡ç†æ–¹ä¾¿ï¼‰
- **é›†æˆå’Œåˆ†äº«**: HuggingFaceï¼ˆç”Ÿæ€ä¸°å¯Œï¼‰

### Q2: å¦‚ä½•æ¢å¤è®­ç»ƒï¼Ÿ

```bash
# Playground
python training/train_hlbd_playground.py --resume checkpoint_epoch_50.pt

# DeepSpeed
deepspeed train_deepspeed.py --load-checkpoint deepspeed_output/checkpoint_epoch_50

# HuggingFace
python training/train_hf_trainer.py --resume-from-checkpoint hf_output/checkpoint-500
```

### Q3: å¦‚ä½•éªŒè¯HLBDæ¨¡å‹ï¼Ÿ

```bash
python tools/verify_hlbd_model.py --model <æ¨¡å‹è·¯å¾„> --dataset HLBD_Hardcore_Full.json
```

### Q4: å¦‚ä½•å¯è§†åŒ–è®­ç»ƒï¼Ÿ

```bash
# å®æ—¶å¯è§†åŒ–
python tools/visualize_training.py --log-dir hlbd_playground --mode realtime

# ç¦»çº¿å¯è§†åŒ–
python tools/visualize_training.py --log-dir hlbd_playground --mode offline

# å¤šè®­ç»ƒç›‘æ§
python tools/monitor_all_trainings.py
```

### Q5: DeepSpeed OOMæ€ä¹ˆåŠï¼Ÿ

```bash
# 1. å¯ç”¨ZeRO-3
deepspeed train_deepspeed.py --zero-stage 3

# 2. å¯ç”¨CPUå¸è½½
deepspeed train_deepspeed.py --zero-stage 3 --cpu-offload

# 3. å¢åŠ æ¢¯åº¦ç´¯ç§¯
deepspeed train_deepspeed.py --gradient-accumulation 8

# 4. å‡å°batch size
deepspeed train_deepspeed.py --train-batch-size 32
```

---

## ğŸ“š ç›¸å…³æ–‡æ¡£

- [HLBDæ•°æ®é›†ç”Ÿæˆ](generate_hlbd_hardcore.py)
- [æ¨¡å‹éªŒè¯æŒ‡å—](verify_hlbd_model.py)
- [å¯è§†åŒ–ä½¿ç”¨æŒ‡å—](VISUALIZATION_GUIDE.md)
- [è®­ç»ƒæ¢å¤æŒ‡å—](training_resume_guide.py)
- [é—®é¢˜è¯Šæ–­å·¥å…·](diagnose_issues.py)

---

## ğŸ“ æ¨èè®­ç»ƒæµç¨‹

### æ–°æ‰‹æµç¨‹

```bash
# 1. ç”ŸæˆHLBDæ•°æ®é›†
python generate_hlbd_hardcore.py

# 2. Playgroundè®­ç»ƒ
python training/train.py --backend playground --epochs 100

# 3. éªŒè¯æ¨¡å‹
python tools/verify_hlbd_model.py --model hlbd_playground/final_model.pt

# 4. å¯è§†åŒ–ç»“æœ
python tools/visualize_training.py --log-dir hlbd_playground --mode offline
```

### è¿›é˜¶æµç¨‹

```bash
# 1. å¤šGPU DeepSpeedè®­ç»ƒ
python training/train.py --backend deepspeed --num-gpus 4 --zero-stage 2 --epochs 100

# 2. å®æ—¶ç›‘æ§
python tools/monitor_all_trainings.py &

# 3. éªŒè¯å’Œè¯Šæ–­
python tools/verify_hlbd_model.py --model deepspeed_output/checkpoint_epoch_100
python tools/diagnose_issues.py
```

### äº‘ç«¯æµç¨‹

```bash
# 1. æäº¤Azure MLä»»åŠ¡
python training/train.py --backend azure \
    --azure-subscription-id <ID> \
    --azure-resource-group <RG> \
    --azure-workspace-name <WS> \
    --epochs 100

# 2. æŸ¥çœ‹MLflowå®éªŒ
# åœ¨Azure ML Studioä¸­æŸ¥çœ‹

# 3. ä¸‹è½½æœ€ä½³æ¨¡å‹
az ml model download --name apt-model --version 1
```

---

**é€‰æ‹©åˆé€‚çš„åç«¯ï¼Œå¼€å§‹ä½ çš„APTè®­ç»ƒä¹‹æ—…ï¼** ğŸš€
