# APTæ¨¡å‹è®­ç»ƒCheckpointè¿ç§»æŒ‡å—

**æ—¥æœŸ**: 2025-10-27
**ç›®çš„**: è¯´æ˜å¦‚ä½•å¤‡ä»½å’Œè¿ç§»è®­ç»ƒcheckpointåˆ°å…¶ä»–ç”µè„‘ç»§ç»­è®­ç»ƒ

---

## ğŸ“ Checkpointå’Œç¼“å­˜ä¿å­˜ä½ç½®

### 1. è®­ç»ƒæ¨¡å‹ä¿å­˜ä½ç½®

#### é»˜è®¤ä¿å­˜è·¯å¾„ï¼š
```
./apt_model/          # å½“å‰å·¥ä½œç›®å½•ä¸‹
â”œâ”€â”€ model.pt          # æ¨¡å‹æƒé‡
â”œâ”€â”€ config.json       # æ¨¡å‹é…ç½®
â””â”€â”€ tokenizer/        # åˆ†è¯å™¨æ–‡ä»¶
    â”œâ”€â”€ vocab.json
    â”œâ”€â”€ merges.txt
    â””â”€â”€ tokenizer_config.json
```

**å‘½ä»¤è¡Œå‚æ•°**:
```bash
python -m apt_model train --save-path ./my_model
# å°†ä¿å­˜åˆ° ./my_model/ ç›®å½•
```

---

### 2. Checkpointä¿å­˜ä½ç½®ï¼ˆä½¿ç”¨CheckpointManageræ—¶ï¼‰

#### ç»“æ„ï¼š
```
<save_path>/
â”œâ”€â”€ checkpoints/                    # checkpointç›®å½•
â”‚   â”œâ”€â”€ apt_model_epoch1_step500.pt
â”‚   â”œâ”€â”€ apt_model_epoch2_step1000.pt
â”‚   â”œâ”€â”€ apt_model_epoch3_step1500_best.pt
â”‚   â””â”€â”€ ...
â”œâ”€â”€ metadata.json                   # è®­ç»ƒå…ƒæ•°æ®
â”œâ”€â”€ tokenizer/                      # åˆ†è¯å™¨
â”‚   â”œâ”€â”€ vocab.json
â”‚   â””â”€â”€ ...
â””â”€â”€ model.pt                        # æœ€ç»ˆæ¨¡å‹ï¼ˆå¯é€‰ï¼‰
```

#### Checkpointæ–‡ä»¶å†…å®¹ï¼š
æ¯ä¸ª`.pt`æ–‡ä»¶åŒ…å«ï¼š
```python
{
    'epoch': å½“å‰epoch,
    'global_step': å…¨å±€æ­¥æ•°,
    'model_state_dict': æ¨¡å‹å‚æ•°,
    'optimizer_state_dict': ä¼˜åŒ–å™¨çŠ¶æ€,
    'scheduler_state_dict': å­¦ä¹ ç‡è°ƒåº¦å™¨çŠ¶æ€,
    'loss_history': æŸå¤±å†å²,
    'metrics': è¯„ä¼°æŒ‡æ ‡,
    'config': æ¨¡å‹é…ç½®
}
```

---

### 3. ç³»ç»Ÿç¼“å­˜ä½ç½®

#### é»˜è®¤ç¼“å­˜ç›®å½•ï¼š
```
~/.apt_cache/                # ç”¨æˆ·ä¸»ç›®å½•ä¸‹
â”œâ”€â”€ models/                  # é¢„è®­ç»ƒæ¨¡å‹ç¼“å­˜
â”œâ”€â”€ datasets/                # æ•°æ®é›†ç¼“å­˜
â”œâ”€â”€ tokenizers/              # åˆ†è¯å™¨ç¼“å­˜
â”œâ”€â”€ checkpoints/             # è®­ç»ƒcheckpointï¼ˆå¦‚æœä½¿ç”¨CacheManagerï¼‰
â”œâ”€â”€ logs/                    # æ—¥å¿—æ–‡ä»¶
â””â”€â”€ temp/                    # ä¸´æ—¶æ–‡ä»¶
```

**Linux/Mac**: `~/.apt_cache/` â†’ `/home/username/.apt_cache/`
**Windows**: `~/.apt_cache/` â†’ `C:\Users\username\.apt_cache\`

---

## ğŸ“¦ éœ€è¦å¤‡ä»½çš„æ–‡ä»¶

### æ–¹æ¡ˆA: æœ€å°å¤‡ä»½ï¼ˆä»…ç»§ç»­è®­ç»ƒï¼‰

```bash
# ä»…éœ€è¦å¤‡ä»½checkpointæ–‡ä»¶
<save_path>/
â”œâ”€â”€ checkpoints/
â”‚   â””â”€â”€ apt_model_epoch3_step1500_best.pt  # æœ€æ–°æˆ–æœ€ä½³checkpoint
â””â”€â”€ metadata.json                           # å…ƒæ•°æ®ï¼ˆå¯é€‰ï¼‰
```

**å¤§å°**: å–å†³äºæ¨¡å‹å¤§å°ï¼Œé€šå¸¸100MB - 2GB

---

### æ–¹æ¡ˆB: å®Œæ•´å¤‡ä»½ï¼ˆæ¨èï¼‰

```bash
<save_path>/
â”œâ”€â”€ checkpoints/          # æ‰€æœ‰checkpoint
â”œâ”€â”€ metadata.json         # è®­ç»ƒå…ƒæ•°æ®
â”œâ”€â”€ tokenizer/            # åˆ†è¯å™¨
â”œâ”€â”€ model.pt              # æœ€ç»ˆæ¨¡å‹ï¼ˆå¦‚æœæœ‰ï¼‰
â””â”€â”€ config.json           # é…ç½®æ–‡ä»¶
```

**å¤§å°**: çº¦ä¸ºå•ä¸ªcheckpointçš„2-3å€

---

### æ–¹æ¡ˆC: å®Œæ•´å¤‡ä»½+ç¼“å­˜

```bash
# è®­ç»ƒç›®å½•
<save_path>/
â””â”€â”€ ... (åŒæ–¹æ¡ˆB)

# ç³»ç»Ÿç¼“å­˜
~/.apt_cache/
â”œâ”€â”€ datasets/            # å¦‚æœä½¿ç”¨äº†è‡ªå®šä¹‰æ•°æ®é›†
â””â”€â”€ tokenizers/          # å¦‚æœä½¿ç”¨äº†è‡ªå®šä¹‰åˆ†è¯å™¨
```

**å¤§å°**: å¯èƒ½è¾¾åˆ°æ•°GBï¼ˆå–å†³äºæ•°æ®é›†å¤§å°ï¼‰

---

## ğŸš€ è·¨ç”µè„‘è¿ç§»æ­¥éª¤

### æƒ…æ™¯1: ä»ç”µè„‘Aè¿ç§»åˆ°ç”µè„‘Bç»§ç»­è®­ç»ƒ

#### åœ¨ç”µè„‘Aï¼ˆæºç”µè„‘ï¼‰ï¼š

**æ­¥éª¤1: ç¡®è®¤checkpointä½ç½®**
```bash
# æŸ¥çœ‹è®­ç»ƒä¿å­˜è·¯å¾„ï¼ˆå‡è®¾æ˜¯ ./my_trainingï¼‰
ls -lh ./my_training/checkpoints/

# è¾“å‡ºç¤ºä¾‹ï¼š
# apt_model_epoch1_step500.pt
# apt_model_epoch2_step1000.pt
# apt_model_epoch3_step1500_best.pt
```

**æ­¥éª¤2: æ‰“åŒ…checkpoint**
```bash
# æ–¹æ³•1: æ‰“åŒ…æ•´ä¸ªè®­ç»ƒç›®å½•
tar -czf training_backup.tar.gz ./my_training/

# æ–¹æ³•2: åªæ‰“åŒ…checkpointå’Œå¿…è¦æ–‡ä»¶
tar -czf training_backup.tar.gz \
    ./my_training/checkpoints/ \
    ./my_training/metadata.json \
    ./my_training/tokenizer/ \
    ./my_training/config.json
```

**æ­¥éª¤3: ä¼ è¾“æ–‡ä»¶**
```bash
# æ–¹æ³•1: ä½¿ç”¨Uç›˜/ç§»åŠ¨ç¡¬ç›˜
cp training_backup.tar.gz /media/usb/

# æ–¹æ³•2: ä½¿ç”¨scp (å¦‚æœä¸¤å°ç”µè„‘åœ¨åŒä¸€ç½‘ç»œ)
scp training_backup.tar.gz user@computerB:/path/to/destination/

# æ–¹æ³•3: ä½¿ç”¨äº‘å­˜å‚¨
# ä¸Šä¼ åˆ°Google Drive/Dropbox/OneDriveç­‰
```

---

#### åœ¨ç”µè„‘Bï¼ˆç›®æ ‡ç”µè„‘ï¼‰ï¼š

**æ­¥éª¤1: å‡†å¤‡ç¯å¢ƒ**
```bash
# ç¡®ä¿å·²å®‰è£…APT Modelå’Œä¾èµ–
pip install -r requirements.txt

# å…‹éš†ä»£ç ä»“åº“ï¼ˆå¦‚æœè¿˜æ²¡æœ‰ï¼‰
git clone https://github.com/your-repo/APT-Transformer.git
cd APT-Transformer
```

**æ­¥éª¤2: è§£å‹checkpoint**
```bash
# è§£å‹åˆ°ç›¸åŒæˆ–æ–°çš„ç›®å½•
tar -xzf training_backup.tar.gz

# æˆ–è§£å‹åˆ°æŒ‡å®šç›®å½•
mkdir -p ./restored_training
tar -xzf training_backup.tar.gz -C ./restored_training/
```

**æ­¥éª¤3: éªŒè¯æ–‡ä»¶å®Œæ•´æ€§**
```bash
# æ£€æŸ¥checkpointæ–‡ä»¶
ls -lh ./my_training/checkpoints/

# æ£€æŸ¥metadata
cat ./my_training/metadata.json
```

**æ­¥éª¤4: æ¢å¤è®­ç»ƒ**

##### æ–¹æ³•A: ä½¿ç”¨CheckpointManagerï¼ˆæ¨èï¼‰
```python
from apt_model.training.checkpoint import CheckpointManager

# åˆå§‹åŒ–CheckpointManager
checkpoint_manager = CheckpointManager(
    save_dir="./my_training",
    model_name="apt_model",
    logger=logger
)

# åŠ è½½æœ€æ–°checkpoint
epoch, global_step, loss_history, metrics = checkpoint_manager.load_checkpoint(
    model=model,
    optimizer=optimizer,
    scheduler=scheduler,
    latest=True  # æˆ– best=True åŠ è½½æœ€ä½³checkpoint
)

print(f"ä» epoch {epoch}, step {global_step} æ¢å¤è®­ç»ƒ")
print(f"ä¹‹å‰çš„losså†å²: {loss_history[-5:]}")

# ç»§ç»­è®­ç»ƒï¼ˆä»epoch+1å¼€å§‹ï¼‰
for epoch in range(epoch + 1, total_epochs):
    # ... è®­ç»ƒå¾ªç¯
```

##### æ–¹æ³•B: æ‰‹åŠ¨åŠ è½½checkpoint
```python
import torch

# åŠ è½½checkpoint
checkpoint_path = "./my_training/checkpoints/apt_model_epoch3_step1500_best.pt"
checkpoint = torch.load(checkpoint_path, map_location=device)

# æ¢å¤æ¨¡å‹
model.load_state_dict(checkpoint['model_state_dict'])

# æ¢å¤ä¼˜åŒ–å™¨ï¼ˆé‡è¦ï¼ä¿æŒå­¦ä¹ ç‡ï¼‰
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

# æ¢å¤schedulerï¼ˆé‡è¦ï¼ä¿æŒwarmupç­‰ç­–ç•¥ï¼‰
if checkpoint['scheduler_state_dict']:
    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])

# è·å–è®­ç»ƒçŠ¶æ€
start_epoch = checkpoint['epoch'] + 1
global_step = checkpoint['global_step']
loss_history = checkpoint['loss_history']

print(f"ä» epoch {start_epoch} ç»§ç»­è®­ç»ƒ")
```

---

## âš ï¸ é‡è¦æ³¨æ„äº‹é¡¹

### 1. ç¡¬ä»¶å·®å¼‚
**é—®é¢˜**: ç”µè„‘Aç”¨GPUè®­ç»ƒï¼Œç”µè„‘Båªæœ‰CPU

**è§£å†³æ–¹æ¡ˆ**:
```python
# åŠ è½½checkpointæ—¶æŒ‡å®šmap_location
checkpoint = torch.load(
    checkpoint_path,
    map_location='cpu'  # å¼ºåˆ¶ä½¿ç”¨CPU
)

# æˆ–è‡ªåŠ¨æ£€æµ‹
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
checkpoint = torch.load(checkpoint_path, map_location=device)
```

### 2. PyTorchç‰ˆæœ¬å·®å¼‚
**é—®é¢˜**: ç”µè„‘Aç”¨PyTorch 2.0ï¼Œç”µè„‘Bç”¨PyTorch 1.13

**è§£å†³æ–¹æ¡ˆ**:
```bash
# åœ¨ç”µè„‘Bå®‰è£…ç›¸åŒç‰ˆæœ¬çš„PyTorch
pip install torch==2.0.0  # ä½¿ç”¨ä¸ç”µè„‘Aç›¸åŒçš„ç‰ˆæœ¬
```

### 3. è·¯å¾„å·®å¼‚
**é—®é¢˜**: ç”µè„‘Açš„è·¯å¾„æ˜¯`/home/userA/training`ï¼Œç”µè„‘Bè·¯å¾„ä¸åŒ

**è§£å†³æ–¹æ¡ˆ**:
- Checkpointä¸­ä¿å­˜çš„æ˜¯ç›¸å¯¹è·¯å¾„ï¼Œé€šå¸¸ä¸å½±å“
- å¦‚æœæœ‰ç»å¯¹è·¯å¾„é—®é¢˜ï¼Œä¿®æ”¹metadata.jsonä¸­çš„è·¯å¾„

### 4. æ•°æ®é›†ä½ç½®
**é—®é¢˜**: ç»§ç»­è®­ç»ƒéœ€è¦åŸå§‹æ•°æ®é›†

**è§£å†³æ–¹æ¡ˆ**:
```bash
# æ–¹æ³•1: ä¸€èµ·æ‰“åŒ…æ•°æ®é›†
tar -czf full_backup.tar.gz \
    ./my_training/ \
    ./datasets/

# æ–¹æ³•2: åœ¨ç”µè„‘Bé‡æ–°å‡†å¤‡ç›¸åŒæ•°æ®é›†
# ç¡®ä¿æ•°æ®é›†è·¯å¾„å’Œå†…å®¹ä¸ç”µè„‘Aä¸€è‡´
```

---

## ğŸ”„ è‡ªåŠ¨åŒ–è¿ç§»è„šæœ¬

### å¤‡ä»½è„šæœ¬ï¼ˆåœ¨ç”µè„‘Aæ‰§è¡Œï¼‰

åˆ›å»º `backup_training.sh`:
```bash
#!/bin/bash
# APTæ¨¡å‹è®­ç»ƒå¤‡ä»½è„šæœ¬

TRAINING_DIR="./my_training"
BACKUP_NAME="apt_training_backup_$(date +%Y%m%d_%H%M%S).tar.gz"

echo "å¼€å§‹å¤‡ä»½è®­ç»ƒæ•°æ®..."
echo "æºç›®å½•: $TRAINING_DIR"
echo "å¤‡ä»½æ–‡ä»¶: $BACKUP_NAME"

# æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
if [ ! -d "$TRAINING_DIR" ]; then
    echo "é”™è¯¯: è®­ç»ƒç›®å½•ä¸å­˜åœ¨: $TRAINING_DIR"
    exit 1
fi

# æ‰“åŒ…checkpointå’Œå¿…è¦æ–‡ä»¶
tar -czf "$BACKUP_NAME" \
    "$TRAINING_DIR/checkpoints/" \
    "$TRAINING_DIR/metadata.json" \
    "$TRAINING_DIR/tokenizer/" \
    "$TRAINING_DIR/config.json" \
    2>/dev/null

# æ£€æŸ¥æ˜¯å¦æˆåŠŸ
if [ $? -eq 0 ]; then
    SIZE=$(du -h "$BACKUP_NAME" | cut -f1)
    echo "âœ… å¤‡ä»½å®Œæˆï¼"
    echo "æ–‡ä»¶: $BACKUP_NAME"
    echo "å¤§å°: $SIZE"
else
    echo "âŒ å¤‡ä»½å¤±è´¥"
    exit 1
fi
```

ä½¿ç”¨ï¼š
```bash
chmod +x backup_training.sh
./backup_training.sh
```

---

### æ¢å¤è„šæœ¬ï¼ˆåœ¨ç”µè„‘Bæ‰§è¡Œï¼‰

åˆ›å»º `restore_training.sh`:
```bash
#!/bin/bash
# APTæ¨¡å‹è®­ç»ƒæ¢å¤è„šæœ¬

BACKUP_FILE="$1"
RESTORE_DIR="${2:-./restored_training}"

if [ -z "$BACKUP_FILE" ]; then
    echo "ç”¨æ³•: ./restore_training.sh <backup_file> [restore_dir]"
    exit 1
fi

if [ ! -f "$BACKUP_FILE" ]; then
    echo "é”™è¯¯: å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: $BACKUP_FILE"
    exit 1
fi

echo "å¼€å§‹æ¢å¤è®­ç»ƒæ•°æ®..."
echo "å¤‡ä»½æ–‡ä»¶: $BACKUP_FILE"
echo "æ¢å¤ç›®å½•: $RESTORE_DIR"

# åˆ›å»ºæ¢å¤ç›®å½•
mkdir -p "$RESTORE_DIR"

# è§£å‹
tar -xzf "$BACKUP_FILE" -C "$RESTORE_DIR" --strip-components=1

# æ£€æŸ¥æ˜¯å¦æˆåŠŸ
if [ $? -eq 0 ]; then
    echo "âœ… æ¢å¤å®Œæˆï¼"
    echo "è®­ç»ƒæ•°æ®å·²æ¢å¤åˆ°: $RESTORE_DIR"
    echo ""
    echo "å¯ç”¨çš„checkpoint:"
    ls -lh "$RESTORE_DIR/checkpoints/"
else
    echo "âŒ æ¢å¤å¤±è´¥"
    exit 1
fi
```

ä½¿ç”¨ï¼š
```bash
chmod +x restore_training.sh
./restore_training.sh apt_training_backup_20251027_120000.tar.gz ./my_training
```

---

## ğŸ“ Pythonæ¢å¤è®­ç»ƒç¤ºä¾‹

åˆ›å»º `resume_training.py`:
```python
#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""ä»checkpointæ¢å¤è®­ç»ƒçš„ç¤ºä¾‹è„šæœ¬"""

import os
import argparse
from apt_model.training.checkpoint import CheckpointManager
from apt_model.config.apt_config import APTConfig
from apt_model.modeling.apt_model import APTLargeModel
import torch

def resume_training(checkpoint_dir, device='auto'):
    """
    ä»checkpointç›®å½•æ¢å¤è®­ç»ƒ

    Args:
        checkpoint_dir: checkpointç›®å½•è·¯å¾„
        device: è®¡ç®—è®¾å¤‡ ('auto', 'cpu', 'cuda')
    """
    print(f"ä» {checkpoint_dir} æ¢å¤è®­ç»ƒ...")

    # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡
    if device == 'auto':
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    else:
        device = torch.device(device)

    print(f"ä½¿ç”¨è®¾å¤‡: {device}")

    # åˆå§‹åŒ–CheckpointManager
    checkpoint_manager = CheckpointManager(
        save_dir=checkpoint_dir,
        model_name="apt_model"
    )

    # æ£€æŸ¥æ˜¯å¦æœ‰checkpoint
    if not checkpoint_manager.metadata.get("checkpoints"):
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ°ä»»ä½•checkpoint")
        return None

    # æ˜¾ç¤ºå¯ç”¨checkpoint
    print("\nå¯ç”¨çš„checkpoint:")
    for i, ckpt in enumerate(checkpoint_manager.metadata["checkpoints"]):
        print(f"  {i+1}. Epoch {ckpt['epoch']}, Step {ckpt['global_step']}")
        if ckpt.get('is_best'):
            print(f"     â­ æœ€ä½³æ¨¡å‹")

    # åŠ è½½é…ç½®
    config_path = os.path.join(checkpoint_dir, "config.json")
    if os.path.exists(config_path):
        config = APTConfig.from_json(config_path)
    else:
        print("âš ï¸  è­¦å‘Š: æœªæ‰¾åˆ°config.jsonï¼Œä½¿ç”¨é»˜è®¤é…ç½®")
        config = APTConfig()

    # åˆ›å»ºæ¨¡å‹
    model = APTLargeModel(config).to(device)

    # åˆ›å»ºä¼˜åŒ–å™¨å’Œscheduler
    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)
    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(
        optimizer, T_max=100
    )

    # åŠ è½½æœ€æ–°checkpoint
    epoch, global_step, loss_history, metrics = checkpoint_manager.load_checkpoint(
        model=model,
        optimizer=optimizer,
        scheduler=scheduler,
        latest=True
    )

    print(f"\nâœ… æˆåŠŸåŠ è½½checkpoint:")
    print(f"   Epoch: {epoch}")
    print(f"   Global Step: {global_step}")
    print(f"   æœ€è¿‘5ä¸ªloss: {loss_history[-5:]}")
    if metrics:
        print(f"   Metrics: {metrics}")

    return {
        'model': model,
        'optimizer': optimizer,
        'scheduler': scheduler,
        'epoch': epoch,
        'global_step': global_step,
        'loss_history': loss_history,
        'config': config
    }

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="ä»checkpointæ¢å¤è®­ç»ƒ")
    parser.add_argument('--checkpoint-dir', type=str, required=True,
                      help='Checkpointç›®å½•è·¯å¾„')
    parser.add_argument('--device', type=str, default='auto',
                      choices=['auto', 'cpu', 'cuda'],
                      help='è®¡ç®—è®¾å¤‡')

    args = parser.parse_args()

    training_state = resume_training(args.checkpoint_dir, args.device)

    if training_state:
        print("\nå‡†å¤‡ç»§ç»­è®­ç»ƒ...")
        print(f"ä» epoch {training_state['epoch'] + 1} å¼€å§‹")
        # åœ¨è¿™é‡Œæ·»åŠ æ‚¨çš„è®­ç»ƒå¾ªç¯
```

ä½¿ç”¨ï¼š
```bash
python resume_training.py --checkpoint-dir ./my_training --device auto
```

---

## ğŸ¯ æœ€ä½³å®è·µ

### 1. å®šæœŸå¤‡ä»½
```bash
# ä½¿ç”¨cronå®šæœŸå¤‡ä»½ï¼ˆLinux/Macï¼‰
# æ¯å¤©å‡Œæ™¨2ç‚¹å¤‡ä»½
0 2 * * * /path/to/backup_training.sh

# æˆ–æ‰‹åŠ¨åœ¨è®­ç»ƒæ—¶å®šæœŸå¤‡ä»½
# æ¯è®­ç»ƒ5ä¸ªepochå¤‡ä»½ä¸€æ¬¡
```

### 2. å¤šç‰ˆæœ¬å¤‡ä»½
```bash
# ä¿ç•™å¤šä¸ªæ—¶é—´ç‚¹çš„å¤‡ä»½
apt_training_backup_20251027_120000.tar.gz
apt_training_backup_20251028_120000.tar.gz
apt_training_backup_20251029_120000.tar.gz
```

### 3. éªŒè¯å¤‡ä»½å®Œæ•´æ€§
```bash
# å¤‡ä»½åéªŒè¯
tar -tzf training_backup.tar.gz | head -20

# è®¡ç®—æ ¡éªŒå’Œ
md5sum training_backup.tar.gz > training_backup.md5
```

### 4. äº‘ç«¯åŒæ­¥ï¼ˆæ¨èï¼‰
```bash
# ä½¿ç”¨rcloneåŒæ­¥åˆ°äº‘ç«¯
rclone sync ./my_training/ gdrive:apt_training_backup/

# æˆ–ä½¿ç”¨rsyncåŒæ­¥åˆ°è¿œç¨‹æœåŠ¡å™¨
rsync -avz ./my_training/ user@backup-server:/backups/apt_training/
```

---

## ğŸ”§ å¸¸è§é—®é¢˜è§£å†³

### Q1: è¿ç§»åè®­ç»ƒlossçªç„¶å˜åŒ–
**åŸå› **: ä¼˜åŒ–å™¨çŠ¶æ€æœªæ­£ç¡®æ¢å¤

**è§£å†³**:
```python
# ç¡®ä¿åŠ è½½optimizerçŠ¶æ€
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])

# ç¡®ä¿åŠ è½½schedulerçŠ¶æ€
scheduler.load_state_dict(checkpoint['scheduler_state_dict'])
```

### Q2: "RuntimeError: CUDA out of memory"
**åŸå› **: ç›®æ ‡ç”µè„‘GPUå†…å­˜è¾ƒå°

**è§£å†³**:
```python
# å‡å°batch size
# æˆ–ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯
# æˆ–ä½¿ç”¨CPUè®­ç»ƒ
```

### Q3: æ‰¾ä¸åˆ°checkpointæ–‡ä»¶
**åŸå› **: è·¯å¾„é”™è¯¯æˆ–æ–‡ä»¶æœªå®Œæ•´ä¼ è¾“

**è§£å†³**:
```bash
# æ£€æŸ¥æ–‡ä»¶å®Œæ•´æ€§
ls -lh ./my_training/checkpoints/

# éªŒè¯taråŒ…å®Œæ•´æ€§
tar -tzf training_backup.tar.gz
```

---

**æ€»ç»“**: é€šè¿‡æ­£ç¡®å¤‡ä»½checkpointå’Œé…ç½®æ–‡ä»¶ï¼Œå¯ä»¥è½»æ¾åœ¨ä¸åŒç”µè„‘é—´è¿ç§»è®­ç»ƒè¿›åº¦ã€‚å…³é”®æ˜¯ç¡®ä¿ä¿å­˜å®Œæ•´çš„è®­ç»ƒçŠ¶æ€ï¼ˆæ¨¡å‹+ä¼˜åŒ–å™¨+schedulerï¼‰ã€‚
