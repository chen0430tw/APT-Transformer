#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""æµ‹è¯•æ¨¡å‹å‹ç¼©æ’ä»¶"""

import sys
import torch
import torch.nn as nn

# æ·»åŠ è·¯å¾„
sys.path.insert(0, '/home/user/APT-Transformer')

from legacy_plugins.batch1.model_pruning_plugin import ModelPruningPlugin
from legacy_plugins.batch1.model_distillation_plugin import ModelDistillationPlugin


class TestModel(nn.Module):
    """ç®€å•çš„æµ‹è¯•æ¨¡å‹"""
    def __init__(self, input_size=100, hidden_size=50, output_size=10):
        super().__init__()
        self.fc1 = nn.Linear(input_size, hidden_size)
        self.fc2 = nn.Linear(hidden_size, output_size)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.relu(self.fc1(x))
        return self.fc2(x)


def test_pruning_plugin():
    """æµ‹è¯•æ¨¡å‹å‰ªææ’ä»¶"""
    print("\n" + "="*60)
    print("ğŸ§ª æµ‹è¯•æ¨¡å‹å‰ªææ’ä»¶")
    print("="*60)

    # é…ç½®
    config = {
        'prune_ratio': 0.3,
        'prune_type': 'magnitude',
        'structured': False,
    }

    # åˆ›å»ºæ’ä»¶å’Œæ¨¡å‹
    plugin = ModelPruningPlugin(config)
    model = TestModel()

    # ç»Ÿè®¡åŸå§‹å‚æ•°
    original_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ“Š åŸå§‹æ¨¡å‹å‚æ•°é‡: {original_params:,}")

    # åº”ç”¨æƒé‡å¤§å°å‰ªæ
    print("\nâœ‚ï¸ åº”ç”¨æƒé‡å¤§å°å‰ªæ...")
    model = plugin.magnitude_pruning(model, prune_ratio=0.3, structured=False)

    # è·å–å‰ªæç»Ÿè®¡
    stats = plugin.get_pruning_statistics(model)
    print(f"\nğŸ“ˆ å‰ªæç»Ÿè®¡:")
    print(f"   æ€»å‚æ•°: {stats['total_params']:,}")
    print(f"   å‰ªæå‚æ•°: {stats['pruned_params']:,}")
    print(f"   å‰©ä½™å‚æ•°: {stats['remaining_params']:,}")
    print(f"   ç¨€ç–åº¦: {stats['sparsity']*100:.2f}%")

    # æµ‹è¯•æ¨¡å‹å‰å‘ä¼ æ’­
    print("\nğŸ” æµ‹è¯•å‰å‘ä¼ æ’­...")
    test_input = torch.randn(8, 100)
    with torch.no_grad():
        output = model(test_input)
    print(f"   è¾“å…¥å½¢çŠ¶: {test_input.shape}")
    print(f"   è¾“å‡ºå½¢çŠ¶: {output.shape}")
    print(f"   âœ… å‰å‘ä¼ æ’­æˆåŠŸ!")

    return True


def test_distillation_plugin():
    """æµ‹è¯•çŸ¥è¯†è’¸é¦æ’ä»¶"""
    print("\n" + "="*60)
    print("ğŸ§ª æµ‹è¯•çŸ¥è¯†è’¸é¦æ’ä»¶")
    print("="*60)

    # é…ç½®
    config = {
        'temperature': 4.0,
        'alpha': 0.7,
        'beta': 0.3,
        'distill_type': 'response',
    }

    # åˆ›å»ºæ’ä»¶
    plugin = ModelDistillationPlugin(config)

    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    batch_size, seq_len, vocab_size = 8, 128, 1000
    student_logits = torch.randn(batch_size, seq_len, vocab_size)
    teacher_logits = torch.randn(batch_size, seq_len, vocab_size)
    labels = torch.randint(0, vocab_size, (batch_size, seq_len))

    print(f"ğŸ“Š æµ‹è¯•æ•°æ®:")
    print(f"   Batch size: {batch_size}")
    print(f"   Sequence length: {seq_len}")
    print(f"   Vocab size: {vocab_size}")

    # æµ‹è¯•å“åº”è’¸é¦
    print("\nğŸ“ æµ‹è¯•å“åº”è’¸é¦...")
    response_loss = plugin.response_distillation_loss(
        student_logits, teacher_logits, labels
    )
    print(f"   å“åº”è’¸é¦æŸå¤±: {response_loss.item():.4f}")
    print(f"   âœ… å“åº”è’¸é¦æµ‹è¯•æˆåŠŸ!")

    # æµ‹è¯•ç‰¹å¾è’¸é¦
    print("\nğŸ“ æµ‹è¯•ç‰¹å¾è’¸é¦...")
    student_features = torch.randn(batch_size, seq_len, 512)
    teacher_features = torch.randn(batch_size, seq_len, 512)
    feature_loss = plugin.feature_distillation_loss(
        student_features, teacher_features
    )
    print(f"   ç‰¹å¾è’¸é¦æŸå¤±: {feature_loss.item():.4f}")
    print(f"   âœ… ç‰¹å¾è’¸é¦æµ‹è¯•æˆåŠŸ!")

    # æµ‹è¯•å…³ç³»è’¸é¦
    print("\nğŸ“ æµ‹è¯•å…³ç³»è’¸é¦...")
    student_outputs = torch.randn(batch_size, 512)
    teacher_outputs = torch.randn(batch_size, 512)
    relation_loss = plugin.relation_distillation_loss(
        student_outputs, teacher_outputs
    )
    print(f"   å…³ç³»è’¸é¦æŸå¤±: {relation_loss.item():.4f}")
    print(f"   âœ… å…³ç³»è’¸é¦æµ‹è¯•æˆåŠŸ!")

    return True


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\nğŸš€ å¼€å§‹æµ‹è¯•æ¨¡å‹å‹ç¼©æ’ä»¶...")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"è®¾å¤‡: {'CUDA' if torch.cuda.is_available() else 'CPU'}")

    try:
        # æµ‹è¯•å‰ªæ
        pruning_ok = test_pruning_plugin()

        # æµ‹è¯•è’¸é¦
        distillation_ok = test_distillation_plugin()

        # æ€»ç»“
        print("\n" + "="*60)
        print("ğŸ“ æµ‹è¯•æ€»ç»“")
        print("="*60)
        print(f"âœ… æ¨¡å‹å‰ªææ’ä»¶: {'é€šè¿‡' if pruning_ok else 'å¤±è´¥'}")
        print(f"âœ… çŸ¥è¯†è’¸é¦æ’ä»¶: {'é€šè¿‡' if distillation_ok else 'å¤±è´¥'}")

        if pruning_ok and distillation_ok:
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
            return 0
        else:
            print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
            return 1

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())
