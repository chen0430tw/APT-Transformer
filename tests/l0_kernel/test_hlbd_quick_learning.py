#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""å¿«é€Ÿæµ‹è¯•HLBDè®­ç»ƒ - çœ‹APTæ¨¡å‹èƒ½å¦å¿«é€Ÿå­¦ä¼šè¯´è¯"""

import sys
import os
import re
import time
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import json
# from transformers import BertTokenizer  # å·²æ›¿æ¢ä¸º SimpleCharTokenizer_BACKUPï¼ˆæ”¯æŒ emojiï¼‰

# æ·»åŠ è·¯å¾„ï¼ˆåŠ¨æ€è®¡ç®—é¡¹ç›®æ ¹ç›®å½•ï¼‰
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, project_root)

from apt.apt_model.modeling.apt_model import (
    APTModel,
    APTModelConfiguration,
    DBCDAC_Optimizer,
    create_gradient_stabilizer_hook
)
from apt.core.generation.generator import generate_natural_text
from apt.core.generation.evaluator import evaluate_text_quality


class SimpleCharTokenizer_BACKUP:
    """ç®€å•çš„å­—ç¬¦çº§åˆ†è¯å™¨"""
    def __init__(self):
        # åˆ›å»ºä¸€ä¸ªåŸºç¡€å­—ç¬¦è¡¨ï¼ˆåŒ…æ‹¬ä¸­æ–‡ã€è‹±æ–‡ã€emojiç­‰ï¼‰
        # æ·»åŠ è¯­è¨€æ ‡ç­¾ç”¨äºåŒºåˆ†ä¸åŒè¾“å…¥ç±»å‹
        self.vocab = {
            '[PAD]': 0, '[UNK]': 1, '[BOS]': 2, '[EOS]': 3,
            '[EMOJI]': 4, '[PHRASE]': 5, '[EN]': 6, '[PY]': 7, '[JP]': 8, '[KR]': 9,
        }
        self.pad_token_id = 0
        self.unk_token_id = 1
        self.bos_token_id = 2
        self.eos_token_id = 3
        self.vocab_size = 5000  # é¢„ç•™è¶³å¤Ÿçš„è¯æ±‡ç©ºé—´

        # æ·»åŠ å¸¸ç”¨å­—ç¬¦
        self.char_to_id = self.vocab.copy()
        self.id_to_char = {v: k for k, v in self.vocab.items()}
        self.next_id = 10  # ä»10å¼€å§‹ï¼Œå› ä¸º0-9å·²è¢«ç‰¹æ®Štokenå ç”¨

        # â­ æ–°å¢ï¼šé¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ï¼ŒåŒ¹é… [TAG]
        self.tag_pattern = re.compile(r'(\[EMOJI\]|\[PHRASE\]|\[EN\]|\[PY\]|\[JP\]|\[KR\])')
    
    def _tokenize_text(self, text):
        """â­ æ ¸å¿ƒä¿®å¤ï¼šå…ˆåˆ‡åˆ†æ ‡ç­¾ï¼Œå†åˆ‡åˆ†å­—ç¬¦"""
        tokens = []
        # æŒ‰æ ‡ç­¾åˆ‡åˆ†
        parts = self.tag_pattern.split(text)
        for part in parts:
            if part in self.vocab:
                # å¦‚æœæ˜¯æ ‡ç­¾ï¼Œç›´æ¥æ·»åŠ ID
                tokens.append(self.vocab[part])
            else:
                # å¦‚æœæ˜¯æ™®é€šæ–‡æœ¬ï¼Œé€å­—å¤„ç†
                for char in part:
                    # è·³è¿‡ç©ºç™½å­—ç¬¦ï¼ˆå¯é€‰ï¼Œçœ‹ä½ éœ€æ±‚ï¼‰
                    if char.strip():
                        tokens.append(self._get_or_add_char(char))
                    elif char == ' ': # ä¿ç•™ç©ºæ ¼
                        tokens.append(self._get_or_add_char(char))
        return tokens

    def _get_or_add_char(self, char):
        """è·å–å­—ç¬¦IDï¼Œå¦‚æœä¸å­˜åœ¨åˆ™æ·»åŠ """
        if char not in self.char_to_id:
            if self.next_id < self.vocab_size:
                self.char_to_id[char] = self.next_id
                self.id_to_char[self.next_id] = char
                self.next_id += 1
            else:
                return self.unk_token_id
        return self.char_to_id[char]

    def encode(self, text, return_tensors=None):
        """ç¼–ç æ–‡æœ¬ä¸ºIDåºåˆ—"""
        ids = [self.bos_token_id]
        ids.extend(self._tokenize_text(text))
        ids.append(self.eos_token_id)

        if return_tensors == 'pt':
            return torch.tensor([ids])
        return ids

    def __call__(self, text, max_length=64, padding='max_length',
                 truncation=True, return_tensors='pt'):
        
        """åˆ†è¯æ¥å£ï¼ˆå…¼å®¹transformersï¼‰"""
        # 1. åˆå§‹åŒ– ids
        ids = [self.bos_token_id]

        # 2. â­ ä½¿ç”¨æ–°çš„åˆ‡åˆ†é€»è¾‘ (æ”¯æŒ [EMOJI])
        token_ids = self._tokenize_text(text)
        ids.extend(token_ids)
        
        # 3. åŠ  EOS
        ids.append(self.eos_token_id)

        # æˆªæ–­
        if truncation and len(ids) > max_length:
            ids = ids[:max_length]

        # å¡«å……
        if padding == 'max_length':
            while len(ids) < max_length:
                ids.append(self.pad_token_id)

        if return_tensors == 'pt':
            return {'input_ids': torch.tensor([ids])}
        return {'input_ids': ids}

    def decode(self, ids, skip_special_tokens=True):
        """è§£ç IDåºåˆ—ä¸ºæ–‡æœ¬"""
        chars = []
        for id in ids:
            if isinstance(id, torch.Tensor):
                id = id.item()

            if skip_special_tokens and id in [self.pad_token_id, self.bos_token_id,
                                               self.eos_token_id, self.unk_token_id]:
                continue

            char = self.id_to_char.get(id, '[UNK]')
            chars.append(char)

        return ''.join(chars)


def register_dbc_hooks(model):
    """ä¸ºæ¨¡å‹æ³¨å†ŒDBC-DAC hooks"""
    opt = DBCDAC_Optimizer()
    hooks = []
    for _, p in model.named_parameters():
        if p.requires_grad:
            hooks.append(p.register_hook(create_gradient_stabilizer_hook(opt)))
    return hooks


def load_hlbd_samples(data_path, max_samples=20):
    """åŠ è½½HLBDæ•°æ®æ ·æœ¬"""
    print(f"ğŸ“‚ åŠ è½½HLBDæ•°æ®: {data_path}")

    with open(data_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # æå–sampleséƒ¨åˆ†
    start_idx = content.find('samples = [')
    if start_idx == -1:
        raise ValueError("æ‰¾ä¸åˆ°samplesæ•°æ®")

    # æå–JSONæ•°ç»„
    json_start = content.find('[', start_idx)
    # æ‰¾åˆ°åŒ¹é…çš„å³æ‹¬å·ï¼ˆç®€å•å¤„ç†ï¼‰
    bracket_count = 0
    json_end = json_start
    for i, char in enumerate(content[json_start:]):
        if char == '[':
            bracket_count += 1
        elif char == ']':
            bracket_count -= 1
            if bracket_count == 0:
                json_end = json_start + i + 1
                break

    json_str = content[json_start:json_end]
    samples = json.loads(json_str)

    if max_samples:
        samples = samples[:max_samples]

    print(f"   åŠ è½½äº† {len(samples)} ä¸ªæ ·æœ¬")
    return samples


def create_training_pairs(samples):
    """ä»HLBDæ ·æœ¬åˆ›å»ºè®­ç»ƒå¯¹"""
    pairs = []

    for sample in samples:
        concept = sample['concept']

        # åˆ›å»ºå¤šç§è®­ç»ƒå¯¹
        # 1. emoji -> ä¸­æ–‡
        if 'level_1' in sample and 'level_6' in sample:
            emoji = sample['level_1'].get('emoji', '')
            chinese = sample['level_6'].get('ä¸­æ–‡', '')
            if emoji and chinese:
                pairs.append((f"[EMOJI] {emoji}", chinese))

        # 2. çŸ­è¯­ -> ä¸­æ–‡
        if 'level_2' in sample and 'level_6' in sample:
            phrase = sample['level_2'].get('çŸ­è¯­', '')
            chinese = sample['level_6'].get('ä¸­æ–‡', '')
            if phrase and chinese:
                pairs.append((f"[PHRASE] {phrase}", chinese))

        # 3. è‹±æ–‡ -> ä¸­æ–‡
        if 'level_5' in sample and 'level_6' in sample:
            english = sample['level_5'].get('è‹±æ–‡', '')
            chinese = sample['level_6'].get('ä¸­æ–‡', '')
            if english and chinese:
                pairs.append((f"[EN] {english}", chinese))

        # 4. æ‹¼éŸ³ -> ä¸­æ–‡
        if 'level_4' in sample and 'level_6' in sample:
            pinyin = sample['level_4'].get('æ‹¼éŸ³', '')
            chinese = sample['level_6'].get('ä¸­æ–‡', '')
            if pinyin and chinese:
                pairs.append((f"[PY] {pinyin}", chinese))

        # 5. æ—¥æ–‡ -> ä¸­æ–‡
        if 'level_7' in sample and 'level_6' in sample:
            japanese = sample['level_7'].get('æ—¥æ–‡', '')
            chinese = sample['level_6'].get('ä¸­æ–‡', '')
            if japanese and chinese:
                pairs.append((f"[JP] {japanese}", chinese))

        # 6. éŸ©æ–‡ -> ä¸­æ–‡
        if 'level_8' in sample and 'level_6' in sample:
            korean = sample['level_8'].get('éŸ©ë¬¸', sample['level_8'].get('éŸ©æ–‡', ''))  # å…¼å®¹ä¸¤ç§é”®å
            chinese = sample['level_6'].get('ä¸­æ–‡', '')
            if korean and chinese:
                pairs.append((f"[KR] {korean}", chinese))

    print(f"   åˆ›å»ºäº† {len(pairs)} ä¸ªè®­ç»ƒå¯¹ï¼ˆå¸¦è¯­è¨€æ ‡ç­¾ï¼‰")
    return pairs


class SimpleDialogueDataset(Dataset):
    """ç®€å•å¯¹è¯æ•°æ®é›†"""
    def __init__(self, pairs, tokenizer, max_length=64):
        self.pairs = pairs
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        src_text, tgt_text = self.pairs[idx]

        # ç¼–ç æºæ–‡æœ¬
        src_encoding = self.tokenizer(
            src_text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        # ç¼–ç ç›®æ ‡æ–‡æœ¬
        tgt_encoding = self.tokenizer(
            tgt_text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        return (
            src_encoding['input_ids'].squeeze(0),
            tgt_encoding['input_ids'].squeeze(0)
        )


def create_small_hlbd_config(vocab_size):
    """åˆ›å»ºå°å‹HLBD APTé…ç½®"""
    config = APTModelConfiguration(
        vocab_size=vocab_size,
        d_model=256,              # ä¸­ç­‰ç»´åº¦
        max_seq_len=64,           # çŸ­åºåˆ—
        num_encoder_layers=3,     # 3å±‚ç¼–ç å™¨
        num_decoder_layers=3,     # 3å±‚è§£ç å™¨
        num_heads=8,              # 8ä¸ªæ³¨æ„åŠ›å¤´
        d_ff=1024,                # å‰é¦ˆç½‘ç»œ
        dropout=0.1,             # é€‚åº¦dropout
        use_autopoietic=True,     # å¯ç”¨è‡ªç”Ÿæˆæœºåˆ¶
        use_dbc_dac=True,         # å¯ç”¨DBC-DAC
    )
    return config


from tqdm import tqdm # ç¡®ä¿æ–‡ä»¶å¼€å¤´å¯¼å…¥äº† tqdm


def generate_with_vocab_mask(model, input_ids, valid_token_ids, max_length,
                             repetition_penalty, pad_token_id, device,
                             temperature=1.0, top_p=0.9):
    """
    ä½¿ç”¨ vocab mask é™åˆ¶ç”ŸæˆèŒƒå›´çš„è‡ªå®šä¹‰ç”Ÿæˆå‡½æ•°

    Args:
        model: APT æ¨¡å‹
        input_ids: è¾“å…¥ token IDs
        valid_token_ids: å…è®¸çš„ token ID é›†åˆ
        max_length: æœ€å¤§é•¿åº¦
        repetition_penalty: é‡å¤æƒ©ç½š
        pad_token_id: padding ID
        device: è®¾å¤‡
        temperature: é‡‡æ ·æ¸©åº¦
        top_p: nucleus é‡‡æ ·å‚æ•°
    """
    model.eval()
    generated = input_ids.clone()

    # åˆ›å»º vocab maskï¼ˆåªå…è®¸ç”Ÿæˆå·²çŸ¥çš„ tokenï¼‰
    vocab_size = model.config.vocab_size
    vocab_mask = torch.zeros(vocab_size, dtype=torch.bool, device=device)
    for valid_id in valid_token_ids:
        if 0 <= valid_id < vocab_size:
            vocab_mask[valid_id] = True

    with torch.no_grad():
        for _ in range(max_length - input_ids.size(1)):
            # å‰å‘ä¼ æ’­
            outputs = model(generated, generated)
            logits = outputs[:, -1, :]  # [batch_size, vocab_size]

            # ğŸ”§ åº”ç”¨ vocab mask - åªå…è®¸ç”Ÿæˆå·²çŸ¥çš„ token
            logits[:, ~vocab_mask] = -float('inf')

            # é‡å¤æƒ©ç½š
            if repetition_penalty != 1.0:
                for token_id in set(generated[0].tolist()):
                    if token_id in valid_token_ids:
                        logits[0, token_id] /= repetition_penalty

            # æ¸©åº¦è°ƒæ•´
            logits = logits / max(temperature, 1e-5)

            # Top-p é‡‡æ ·
            probs = torch.nn.functional.softmax(logits, dim=-1)
            sorted_probs, sorted_indices = torch.sort(probs, descending=True)
            cumsum_probs = torch.cumsum(sorted_probs, dim=-1)

            # æ‰¾åˆ°ç´¯ç§¯æ¦‚ç‡è¶…è¿‡ top_p çš„ä½ç½®
            remove_mask = cumsum_probs > top_p
            remove_mask[:, 1:] = remove_mask[:, :-1].clone()
            remove_mask[:, 0] = False

            # ç§»é™¤ä½æ¦‚ç‡çš„ token
            sorted_probs[remove_mask] = 0.0
            probs_sum = sorted_probs.sum(dim=-1, keepdim=True)
            if probs_sum > 0:
                sorted_probs = sorted_probs / probs_sum

            # é‡‡æ ·
            try:
                next_token_idx = torch.multinomial(sorted_probs, num_samples=1)
                next_token = sorted_indices.gather(-1, next_token_idx)
            except:
                # å¦‚æœé‡‡æ ·å¤±è´¥ï¼Œä½¿ç”¨è´ªå¿ƒè§£ç 
                next_token = torch.argmax(logits, dim=-1, keepdim=True)

            generated = torch.cat([generated, next_token], dim=1)

    return generated


def train_epoch(model, dataloader, optimizer, criterion, device, use_dbc=False, accumulation_steps=4):
    """è®­ç»ƒä¸€ä¸ªepochï¼ˆå¸¦æ€§èƒ½è¯Šæ–­ç‰ˆï¼‰"""
    model.train()
    total_loss = 0
    total_steps = 0

    ACCUMULATION_STEPS = accumulation_steps

    progress_bar = tqdm(
        dataloader,
        desc="Training",
        leave=False,
        mininterval=0.1,
        ascii=True
    )

    # â±ï¸ [è¯Šæ–­åˆå§‹åŒ–]
    t_start = time.time()
    times = {"data": 0, "forward": 0, "backward": 0, "step": 0}

    # æ³¨æ„ï¼šå¦‚æœä½ çš„ dataloader è¿”å›çš„æ˜¯å­—å…¸ï¼ˆå¦‚ HLBD è„šæœ¬ï¼‰ï¼Œè¿™é‡Œå¯èƒ½éœ€è¦æ”¹æˆ for i, batch in enumerate...
    # ä¸‹é¢ä¿ç•™ä½ æä¾›çš„è§£åŒ…æ ¼å¼ï¼Œè¯·æ ¹æ®å®é™…æƒ…å†µè°ƒæ•´
    for i, batch_data in enumerate(progress_bar):
        
        # â±ï¸ [1. è®°å½•æ•°æ®åŠ è½½è€—æ—¶] (ä»ä¸Šè½®å¾ªç¯ç»“æŸåˆ°è¿™é‡Œçš„æ—¶é—´)
        t_data_end = time.time()
        times["data"] = t_data_end - t_start

        # å…¼å®¹å¤„ç†ï¼šæ£€æŸ¥æ˜¯å…ƒç»„è¿˜æ˜¯å­—å…¸
        if isinstance(batch_data, dict):
            src_ids = batch_data['input_ids'].to(device)
            # å‡è®¾ HLBD ä»»åŠ¡ä¸­ target å°±æ˜¯ input
            tgt_ids = src_ids.clone() 
        else:
            src_ids, tgt_ids = batch_data
            src_ids = src_ids.to(device)
            tgt_ids = tgt_ids.to(device)

        # â±ï¸ [2. è®°å½•å‰å‘ä¼ æ’­è€—æ—¶]
        t_fw_start = time.time()
        
        # å‰å‘ä¼ æ’­
        if hasattr(model.config, 'pad_token_id'): # ç®€å•çš„ input/label å¤„ç†
             # è‡ªå›å½’ä»»åŠ¡é€šå¸¸è¾“å…¥æ˜¯ srcï¼Œç›®æ ‡ä¹Ÿæ˜¯ srcï¼ˆé”™ä½ï¼‰
             output = model(src_ids, tgt_ids[:, :-1])
        else:
             output = model(src_ids, tgt_ids[:, :-1])

        # è®¡ç®—æŸå¤±
        loss = criterion(
            output.reshape(-1, output.size(-1)),
            tgt_ids[:, 1:].reshape(-1)
        )
        
        # æŸå¤±å½’ä¸€åŒ–
        loss = loss / ACCUMULATION_STEPS
        
        torch.cuda.synchronize() if device.type == 'cuda' else None
        t_fw_end = time.time()
        times["forward"] = t_fw_end - t_fw_start

        # â±ï¸ [3. è®°å½•åå‘ä¼ æ’­è€—æ—¶] (è¿™é‡Œæ˜¯DBCé’©å­èµ·ä½œç”¨çš„åœ°æ–¹!)
        t_bw_start = time.time()
        
        loss.backward()
        
        torch.cuda.synchronize() if device.type == 'cuda' else None
        t_bw_end = time.time()
        times["backward"] = t_bw_end - t_bw_start

        # æ¡ä»¶ä¼˜åŒ–å’Œæ¸…é›¶ï¼ˆæ¯Næ­¥æ‰§è¡Œä¸€æ¬¡ï¼‰
        step_time = 0
        if (i + 1) % ACCUMULATION_STEPS == 0:
            # â±ï¸ [4. è®°å½•å‚æ•°æ›´æ–°è€—æ—¶]
            t_step_start = time.time()
            
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            optimizer.zero_grad()
            
            torch.cuda.synchronize() if device.type == 'cuda' else None
            t_step_end = time.time()
            step_time = t_step_end - t_step_start
            times["step"] = step_time

        total_loss += loss.item() * ACCUMULATION_STEPS
        total_steps += 1

        # å®æ—¶æ›´æ–°è¿›åº¦æ¡ä¸Šçš„Losså’Œå…³é”®è€—æ—¶
        progress_bar.set_postfix({
            'loss': f'{loss.item() * ACCUMULATION_STEPS:.4f}',
            'fw': f"{times['forward']*1000:.1f}ms",
            'bw': f"{times['backward']*1000:.1f}ms"
        })

        # ğŸ›‘ [è¯Šæ–­æ‰“å°] æ¯10ä¸ªBatchæ‰“å°ä¸€æ¬¡è¯¦ç»†æŠ¥å‘Š
        # if i % 10 == 0:
            # print(f"\n[è¯Šæ–­ Batch {i}] "
                  # f"æ•°æ®åŠ è½½: {times['data']*1000:.1f}ms | "
                  # f"å‰å‘(APT): {times['forward']*1000:.1f}ms | "
                  # f"åå‘(DBC): {times['backward']*1000:.1f}ms | "
                  # f"æ›´æ–°: {times['step']*1000:.1f}ms")

        # é‡ç½®ä¸‹ä¸€è½®è®¡æ—¶èµ·ç‚¹
        t_start = time.time()

    # ã€æœ€åä¸€æ­¥æ¸…ç†ã€‘å¤„ç†å‰©ä½™çš„ç´¯ç§¯æ¢¯åº¦
    try:
        if (i + 1) % ACCUMULATION_STEPS != 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            optimizer.zero_grad()
    except NameError:
         pass

    return (total_loss / total_steps) if total_steps > 0 else 0

def generate_text(model, tokenizer, input_text, device, max_length=50, repetition_penalty=1.5):
    """
    ç”Ÿæˆæ–‡æœ¬ï¼ˆä¿®æ”¹ç‰ˆï¼šç›´æ¥è°ƒç”¨æ¨¡å‹å†…éƒ¨ä¿®å¥½çš„ generate æ–¹æ³•ï¼‰
    """
    # 1. ç¼–ç  Encoder è¾“å…¥ (Prompt)
    # è¿™ä¸€æ­¥å¿…é¡»åšï¼ŒæŠŠå­—ç¬¦ä¸²å˜æˆ Tensor
    input_encoded = tokenizer(input_text, max_length=64, padding=False, return_tensors='pt')
    input_ids = input_encoded['input_ids'].to(device)

    # 2. ã€å…³é”®ä¿®æ”¹ã€‘ç›´æ¥è°ƒç”¨æ¨¡å‹å†…éƒ¨æ–¹æ³• (æ­£è§„å†›)
    # æˆ‘ä»¬åˆšåˆšåœ¨ apt_model.py é‡Œä¿®å¥½äº† generateï¼Œç°åœ¨è¿™é‡Œç›´æ¥ç”¨å®ƒï¼
    # å®ƒå†…éƒ¨å·²ç»åŒ…å«äº†ï¼šEncoder-Decoderé€»è¾‘ã€EOSåˆ‡é™¤ã€å¼ºåŠ›å»é‡
    output_ids = model.generate(
        input_ids=input_ids,
        max_length=max_length,
        repetition_penalty=repetition_penalty,  # ä¼ é€’æƒ©ç½šç³»æ•°ï¼Œæ²»æ„ˆå¤è¯»æœº
        temperature=0.1,      # å¯ä»¥å¾®è°ƒï¼Œ1.0 æ¯”è¾ƒæ ‡å‡†
        top_p=0.5,              # â• æ–°å¢è¿™è¡Œï¼åªçœ‹å‰ 50% å¯ä¿¡çš„è¯ï¼Œè¿‡æ»¤æ‰èƒ¡è¨€ä¹±è¯­
        do_sample=True        # å»ºè®® Trueï¼Œè®©å›ç­”ç¨å¾®çµæ´»ç‚¹ï¼›å¦‚æœè¦æ­»æ¿å‡†ç¡®å°± False
    )

    # 3. è§£ç 
    # model.generate è¿”å›çš„æ˜¯ [batch, seq_len]ï¼Œæˆ‘ä»¬å–ç¬¬ä¸€ä¸ªæ ·æœ¬ [0]
    return tokenizer.decode(output_ids[0], skip_special_tokens=True)

def test_generation(model, tokenizer, test_cases, device):
    """æµ‹è¯•ç”Ÿæˆèƒ½åŠ›"""
    print("\n" + "="*60)
    print("ğŸ—£ï¸ æµ‹è¯•å¯¹è¯ç”Ÿæˆèƒ½åŠ›")
    print("="*60)

    # è®¾å®šå¼ºåŠ›é‡å¤æƒ©ç½šå› å­
    REPETITION_FACTOR = 1.5

    for input_text, expected_concept in test_cases:
        generated = generate_text(model, tokenizer, input_text, device, repetition_penalty=REPETITION_FACTOR)
        input_ids = tokenizer.encode(input_text)

        ids_display = str(input_ids)
        if len(input_ids) > 8:
            ids_display = f"[{input_ids[0]}, {input_ids[1]}, ..., {input_ids[-1]}]"
        
        print(f"ğŸ•µï¸ Debug: Len={len(input_ids)} | IDs={ids_display}")
        print(f"\nè¾“å…¥: {input_text}")
        print(f"æœŸæœ›æ¦‚å¿µ: {expected_concept}")
        print(f"ç”Ÿæˆ: {generated}")


def save_model_and_tokenizer(model, tokenizer, config, save_dir, num_epochs, final_loss):
    """
    ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹å’Œ tokenizer

    Args:
        model: è®­ç»ƒå¥½çš„ APT æ¨¡å‹
        tokenizer: SimpleCharTokenizer_BACKUP å®ä¾‹
        config: APTModelConfiguration å®ä¾‹
        save_dir: ä¿å­˜ç›®å½•
        num_epochs: è®­ç»ƒçš„æ€» epoch æ•°
        final_loss: æœ€ç»ˆçš„æŸå¤±å€¼
    """
    import datetime

    os.makedirs(save_dir, exist_ok=True)

    # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    model_filename = f'hlbd_model_{timestamp}.pt'
    model_path = os.path.join(save_dir, model_filename)

    # ä¿å­˜æ¨¡å‹ã€tokenizer å’Œé…ç½®
    checkpoint = {
        'model_state_dict': model.state_dict(),
        'tokenizer_char_to_id': tokenizer.char_to_id,
        'tokenizer_id_to_char': tokenizer.id_to_char,
        'tokenizer_next_id': tokenizer.next_id,
        'tokenizer_vocab_size': tokenizer.vocab_size,
        'config': {
            'vocab_size': config.vocab_size,
            'd_model': config.d_model,
            'max_seq_len': config.max_seq_len,
            'num_encoder_layers': config.num_encoder_layers,
            'num_decoder_layers': config.num_decoder_layers,
            'num_heads': config.num_heads,
            'd_ff': config.d_ff,
            'dropout': config.dropout,
            'use_autopoietic': config.use_autopoietic,
            'use_dbc_dac': config.use_dbc_dac,
        },
        'training_info': {
            'num_epochs': num_epochs,
            'final_loss': final_loss,
            'timestamp': timestamp,
        }
    }

    torch.save(checkpoint, model_path)

    print(f"\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜:")
    print(f"   è·¯å¾„: {os.path.abspath(model_path)}")
    print(f"   å¤§å°: {os.path.getsize(model_path) / 1024 / 1024:.2f} MB")

    return model_path


def load_model_and_tokenizer(model_path, device):
    """
    åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹å’Œ tokenizer

    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        device: è®¾å¤‡ï¼ˆcuda æˆ– cpuï¼‰

    Returns:
        model: åŠ è½½çš„ APT æ¨¡å‹
        tokenizer: åŠ è½½çš„ SimpleCharTokenizer_BACKUP
        training_info: è®­ç»ƒä¿¡æ¯å­—å…¸
    """
    print(f"\nğŸ“‚ åŠ è½½æ¨¡å‹: {model_path}")

    # åŠ è½½ checkpoint
    checkpoint = torch.load(model_path, map_location=device)

    # é‡å»ºé…ç½®
    config_dict = checkpoint['config']
    config = APTModelConfiguration(
        vocab_size=config_dict['vocab_size'],
        d_model=config_dict['d_model'],
        max_seq_len=config_dict['max_seq_len'],
        num_encoder_layers=config_dict['num_encoder_layers'],
        num_decoder_layers=config_dict['num_decoder_layers'],
        num_heads=config_dict['num_heads'],
        d_ff=config_dict['d_ff'],
        dropout=config_dict['dropout'],
        use_autopoietic=config_dict['use_autopoietic'],
        use_dbc_dac=config_dict['use_dbc_dac'],
    )

    # é‡å»ºæ¨¡å‹
    model = APTModel(config).to(device)
    model.load_state_dict(checkpoint['model_state_dict'])

    # é‡å»º tokenizer
    tokenizer = SimpleCharTokenizer_BACKUP()
    tokenizer.char_to_id = checkpoint['tokenizer_char_to_id']
    tokenizer.id_to_char = checkpoint['tokenizer_id_to_char']
    tokenizer.next_id = checkpoint['tokenizer_next_id']
    tokenizer.vocab_size = checkpoint['tokenizer_vocab_size']

    # è·å–è®­ç»ƒä¿¡æ¯
    training_info = checkpoint.get('training_info', {})

    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
    print(f"   è®­ç»ƒ epoch: {training_info.get('num_epochs', 'N/A')}")
    print(f"   æœ€ç»ˆæŸå¤±: {training_info.get('final_loss', 'N/A'):.4f}")
    print(f"   ä¿å­˜æ—¶é—´: {training_info.get('timestamp', 'N/A')}")
    print(f"   è¯æ±‡è¡¨å¤§å°: {len(tokenizer.char_to_id)}")

    return model, tokenizer, training_info


def evaluate_hlbd_model(untrained_model, trained_model, tokenizer, device):
    """è¯„ä¼°HLBDè®­ç»ƒå‰åçš„æ¨¡å‹è´¨é‡"""
    # æµ‹è¯•æç¤ºï¼ˆå¸¦è¯­è¨€æ ‡ç­¾ï¼‰
    test_prompts = [
        "[EMOJI] ğŸŒ§ï¸",  # emojiæµ‹è¯•
        "[EMOJI] â¤ï¸",  # emojiæµ‹è¯•
        "[EN] It's raining",  # è‹±æ–‡æµ‹è¯•
        "[PY] wÇ’ Ã i nÇ",  # æ‹¼éŸ³æµ‹è¯•
        "[JP] æ„›ã—ã¦ã‚‹",  # æ—¥æ–‡æµ‹è¯•
        "[KR] ì‚¬ë‘í•´",  # éŸ©æ–‡æµ‹è¯•
    ]

    untrained_model.eval()
    trained_model.eval()
    untrained_scores = []
    trained_scores = []

    print(f"\n" + "="*60)
    print("å®‰æŸã®è©•ä¾¡ | Amber's Evaluation")
    print("="*60)

    for prompt in test_prompts:
        with torch.no_grad():
            # æœªè®­ç»ƒæ¨¡å‹
            untrained_text, _, _, _ = generate_natural_text(untrained_model, tokenizer, prompt, max_steps=15)
            untrained_score, untrained_feedback = evaluate_text_quality(untrained_text)
            untrained_scores.append(untrained_score)

            # è®­ç»ƒåæ¨¡å‹
            trained_text, _, _, _ = generate_natural_text(trained_model, tokenizer, prompt, max_steps=15)
            trained_score, trained_feedback = evaluate_text_quality(trained_text)
            trained_scores.append(trained_score)

    avg_untrained = sum(untrained_scores) / len(untrained_scores) if untrained_scores else 0
    avg_trained = sum(trained_scores) / len(trained_scores) if trained_scores else 0
    improvement = avg_trained - avg_untrained

    # æœ€ç»ˆè¯„ä¼°
    print(f"\næ•´ä½“è¯„ä¼°:")
    print(f"æœªè®­ç»ƒæ¨¡å‹å¹³å‡è´¨é‡: {avg_untrained:.2f}/100")
    print(f"è®­ç»ƒåæ¨¡å‹å¹³å‡è´¨é‡: {avg_trained:.2f}/100")
    print(f"è´¨é‡æå‡: {improvement:.2f} åˆ†")

    # å®‰æŸçš„æœ€ç»ˆè¯„ä»·
    if improvement < -5:
        print("\nå®‰æŸï¼šå¥‡æ€ªâ€¦â€¦æ€ä¹ˆæ„Ÿè§‰å®ƒå˜ç¬¨äº†ï¼Ÿï¼ˆè´¨é‡ä¸‹é™ï¼Œå»ºè®®æ£€æŸ¥è¶…å‚æ•°ï¼‰")
    elif improvement < 0:
        print("\nå®‰æŸï¼šçœ‹èµ·æ¥æ•ˆæœå·®ä¸å¤šï¼Œä¹Ÿè®¸è¿˜éœ€è¦æ›´å¤šè®­ç»ƒæ•°æ®ï¼Ÿ")
    elif avg_trained < 50:
        print("\nå®‰æŸï¼šè™½ç„¶æœ‰è¿›æ­¥ï¼Œä½†è¿˜è¿œè¿œä¸å¤Ÿå“¦ï¼ç»§ç»­åŠ æ²¹ï¼")
    else:
        print("\nå®‰æŸï¼šè®­ç»ƒå®Œæˆå¾—ä¸é”™ï¼ä¾¦å¯Ÿéª‘å£«ä¸ºä½ ç‚¹èµï¼")

    print("="*60)


def main():
    """ä¸»å‡½æ•°"""
    print("\nğŸš€ HLBDå¿«é€Ÿå­¦ä¹ æµ‹è¯• - APTæ¨¡å‹èƒ½å¦å¿«é€Ÿå­¦ä¼šè¯´è¯?")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")

    ACCUMULATION_STEPS = 2  # ä¿æŒåŸå§‹é…ç½®ï¼šbatch_size=4, 4 * 8 = 32 çš„æœ‰æ•ˆæ‰¹æ¬¡å¤§å°

    # è‡ªåŠ¨æ£€æµ‹ï¼šæœ‰æ˜¾å¡å°±ç”¨æ˜¾å¡ï¼Œæ²¡æœ‰æ‰ç”¨ CPU
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"è®¾å¤‡: {device}")

    # 1. åŠ è½½HLBDæ•°æ®
    current_dir = os.path.dirname(os.path.abspath(__file__)) # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½• (tests)
    project_root = os.path.dirname(current_dir)              # è·å–é¡¹ç›®æ ¹ç›®å½• (APT-Transformer)
    data_path = os.path.join(project_root, 'apt_model', 'åˆ†å±‚è¯­è¨€å¯è’™æ•°æ®é›†.txt')
    samples = load_hlbd_samples(data_path, max_samples=None)

    # æ˜¾ç¤ºå‡ ä¸ªæ ·æœ¬
    print(f"\nğŸ“ æ ·æœ¬ç¤ºä¾‹:")
    for i, sample in enumerate(samples[:3]):
        print(f"\n   æ ·æœ¬ {i+1}: {sample['concept']}")
        print(f"      Emoji: {sample['level_1'].get('emoji', 'N/A')}")
        print(f"      ä¸­æ–‡: {sample['level_6'].get('ä¸­æ–‡', 'N/A')[:30]}...")

    # 2. åˆ›å»ºè®­ç»ƒå¯¹
    training_pairs = create_training_pairs(samples)

    # 3. å‡†å¤‡è¯»æ¡£æˆ–æ–°å»ºæ¨¡å‹
    # ğŸš¨ã€å…³é”®è®¾ç½®ã€‘åœ¨è¿™é‡Œå¡«å…¥æ‚¨çš„å­˜æ¡£æ–‡ä»¶åï¼
    # å¦‚æœå¡«ç©ºå­—ç¬¦ä¸² ""ï¼Œåˆ™ä»£è¡¨ã€ä»å¤´å¼€å§‹æ–°è®­ç»ƒã€‘
    resume_checkpoint = "hlbd_model_20251222_074140.pt"   # <--- è¯·ä¿®æ”¹è¿™é‡Œçš„æ–‡ä»¶åï¼
    resume_path = os.path.join(project_root, 'tests', 'saved_models', resume_checkpoint)

    model = None
    tokenizer = None
    config = None

    # [é€»è¾‘åˆ†æ”¯] å†³å®šæ˜¯â€œè¯»æ¡£â€è¿˜æ˜¯â€œæ–°å»ºâ€
    if resume_checkpoint and os.path.exists(resume_path):
        print(f"\nğŸ”„ å‘ç°å­˜æ¡£ï¼Œæ­£åœ¨æ¢å¤è®­ç»ƒ: {resume_path}")
        # A. è¯»æ¡£æ¨¡å¼ï¼šåŠ è½½æ—§çš„åˆ†è¯å™¨å’Œæ¨¡å‹ (æ¢å¤è®°å¿†)
        model, tokenizer, info = load_model_and_tokenizer(resume_path, device)
        config = model.config
        print(f"   å·²ç»§æ‰¿ä¹‹å‰çš„è¯æ±‡è¡¨ (Size: {len(tokenizer.char_to_id)})")

        # ğŸ‘‡ğŸ‘‡ğŸ‘‡ ã€è¿™é‡Œæ˜¯æ’å…¥ç‚¹ï¼MATH ç–«è‹—è¡¥ä¸ã€‘ ğŸ‘‡ğŸ‘‡ğŸ‘‡
        # å³ä½¿ä½ ç°åœ¨åªç”¨æ—§æ•°æ®ï¼Œæ‰“ä¸Šè¿™ä¸ªè¡¥ä¸ä¹Ÿæ²¡æœ‰åå¤„ï¼Œé˜²æ­¢æœªæ¥æŠ¥é”™
        if '[MATH]' not in tokenizer.char_to_id:
            print(f"\nğŸ’‰ æ£€æµ‹åˆ°æ—§æ¨¡å‹ç¼ºå°‘ [MATH] æ ‡ç­¾ï¼Œæ­£åœ¨åŠ¨æ€è¡¥ä¸...")
            
            # 1. åˆ†é…æ–°ID
            new_id = tokenizer.next_id
            tokenizer.char_to_id['[MATH]'] = new_id
            tokenizer.id_to_char[new_id] = '[MATH]'
            tokenizer.vocab['[MATH]'] = new_id  # ä¿æŒå­—å…¸åŒæ­¥
            tokenizer.next_id += 1
            tokenizer.vocab_size += 1 # è¯è¡¨å¤§å°+1
            
            # 2. ğŸš¨ å…³é”®ï¼šæ›´æ–°æ­£åˆ™ï¼å¦åˆ™åˆ†è¯å™¨çœ‹ä¸è§è¿™ä¸ªæ ‡ç­¾
            # å¿…é¡»æŠŠ [MATH] åŠ å…¥åˆ°è¯†åˆ«è§„åˆ™é‡Œ
            tokenizer.tag_pattern = re.compile(r'(\[EMOJI\]|\[PHRASE\]|\[EN\]|\[PY\]|\[JP\]|\[KR\]|\[MATH\])')
            
            print(f"   âœ… [MATH] è¡¥ä¸åº”ç”¨æˆåŠŸ (ID: {new_id})")
        else:
            print(f"   âœ… æ¨¡å‹å·²åŒ…å« [MATH] æ ‡ç­¾ (ID: {tokenizer.char_to_id['[MATH]']})")
        # ğŸ‘†ğŸ‘†ğŸ‘† ã€è¡¥ä¸ç»“æŸã€‘ ğŸ‘†ğŸ‘†ğŸ‘†

        # ğŸ‘‡ğŸ‘‡ğŸ‘‡ ã€è¿™é‡Œæ’å…¥ï¼šå¼ºåˆ¶è„‘ç§‘æ‰‹æœ¯ã€‘ ğŸ‘‡ğŸ‘‡ğŸ‘‡
        # å¿…é¡»åœ¨è¿™é‡Œæ‰‹åŠ¨æŠŠ Dropout æ‹‰é«˜ï¼Œå› ä¸º torch.load ä¼šæ¢å¤æˆæ—§çš„ 0.0 æˆ– 0.1
        print(f"\nğŸ’‰ [æ‰‹æœ¯ä¸­] æ­£åœ¨å¼ºåˆ¶æå‡ Dropout (é˜²æ­¢æ­»è®°ç¡¬èƒŒ)...")
        print(f"   åŸ Dropout é…ç½®: {model.config.dropout}")
        
        # 1. ä¿®æ”¹é…ç½®å‚æ•° (ä¸ºäº†ä¿å­˜æ—¶æ­£ç¡®)
        model.config.dropout = 0.1
        
        # 2. ğŸš¨ å…³é”®ï¼šé€’å½’ä¿®æ”¹æ‰€æœ‰æ­£åœ¨è¿è¡Œçš„å±‚
        # å…‰æ”¹ config æ²¡ç”¨ï¼Œå¿…é¡»æ·±å…¥åˆ°æ¯ä¸€å±‚ç¥ç»ç½‘ç»œé‡Œå»æ”¹ p å€¼
        modified_count = 0
        for name, module in model.named_modules():
            if isinstance(module, torch.nn.Dropout):
                module.p = 0.1
                modified_count += 1
                
        print(f"   âœ… æ‰‹æœ¯å®Œæˆï¼å·²å°† {modified_count} ä¸ª Dropout å±‚çš„æ¦‚ç‡å¼ºåˆ¶è®¾ä¸º {model.config.dropout}")
        print(f"   å½“å‰ Dropout é…ç½®: {model.config.dropout}")
        # ğŸ‘†ğŸ‘†ğŸ‘† ã€æ‰‹æœ¯ç»“æŸã€‘ ğŸ‘†ğŸ‘†ğŸ‘†

    else:
        print(f"\nğŸ†• æœªæ‰¾åˆ°å­˜æ¡£æˆ–æœªæŒ‡å®šï¼Œå¼€å§‹ã€ä»å¤´è®­ç»ƒã€‘...")
        # B. æ–°å»ºæ¨¡å¼ï¼šåˆ›å»ºæ–°çš„ç©ºç™½åˆ†è¯å™¨
        print(f"ğŸ”§ å‡†å¤‡åˆ†è¯å™¨...")
        tokenizer = SimpleCharTokenizer_BACKUP()

    # 4. åˆ›å»ºæ•°æ®é›† 
    # (æ— è®ºè¯»æ¡£è¿˜æ˜¯æ–°å»ºï¼Œéƒ½è¦è·‘è¿™ä¸€æ­¥ã€‚å¦‚æœæ˜¯è¯»æ¡£ï¼Œtokenizer ä¼šè‡ªåŠ¨æ²¿ç”¨æ—§IDï¼Œä¸ä¼šä¹±ç )
    print(f"\nğŸ“Š åˆ›å»ºæ•°æ®é›†...")
    dataset = SimpleDialogueDataset(training_pairs, tokenizer)

    # é¢„å¡«å……/æ›´æ–°è¯æ±‡è¡¨
    print(f"\nğŸ“ æ£€æŸ¥è¯æ±‡è¡¨è¦†ç›–ç‡...")
    for src, tgt in training_pairs:
        tokenizer.encode(src)
        tokenizer.encode(tgt)
    print(f"   å½“å‰è¯æ±‡è¡¨å¤§å°: {len(tokenizer.char_to_id)} (é¢„ç•™ç©ºé—´: {tokenizer.vocab_size})")

    dataloader = DataLoader(
        dataset,
        batch_size=16, 
        shuffle=True,
        num_workers=0,
        pin_memory=True,
        persistent_workers=False
    )
    print(f"   è®­ç»ƒæ‰¹æ¬¡æ•°: {len(dataloader)}")

    # 5. ç¡®ä¿æ¨¡å‹å·²åˆ›å»º 
    # (å¦‚æœæ˜¯æ–°å»ºæ¨¡å¼ï¼Œç°åœ¨æ‰åˆ›å»ºæ¨¡å‹ï¼›å¦‚æœæ˜¯è¯»æ¡£ï¼Œä¸Šé¢å·²ç»æœ‰äº†)
    if model is None:
        print(f"\nğŸ—ï¸ åˆ›å»ºAPTæ¨¡å‹ (Fresh Start)...")
        config = create_small_hlbd_config(tokenizer.vocab_size)
        model = APTModel(config).to(device)

    total_params = sum(p.numel() for p in model.parameters())
    print(f"   æ¨¡å‹å‚æ•°: {total_params:,}")

    # åˆ›å»ºä¸€ä¸ªå‰¯æœ¬ç”¨äºå¯¹æ¯” (é˜²æ­¢æŠ¥é”™ï¼Œéšä¾¿å»ºä¸€ä¸ª)
    untrained_model = APTModel(config).to(device)
    if not resume_checkpoint:
        untrained_model.load_state_dict(model.state_dict())
    untrained_model.eval()

    # ============================================================
    # ğŸ›ï¸ æˆ˜æœ¯æŒ‡æŒ¥ä¸­å¿ƒ (åªéœ€è¦æ”¹è¿™é‡Œï¼)
    # ============================================================
    # æ¨¡å¼é€‰æ‹©:
    # "BREAKOUT" (æš´åŠ›ç ´å±€) -> LR=1e-4, DBC=å…³ (ç”¨äºæŠŠ Loss ç‚¸é«˜ï¼Œè·³å‡ºå±€éƒ¨æœ€ä¼˜)
    # "LANDING"  (å¹³ç¨³é™è½) -> LR=1e-5, DBC=å¼€ (ç”¨äºæŠŠé«˜ Loss æ”¶æ•›ï¼Œç²¾ç»†å­¦ä¹ )
    
    TACTICAL_MODE = "LANDING"  # ğŸ‘ˆ å½“å‰ä»»åŠ¡ï¼šé™è½ï¼(è¦æŠŠ Loss 1.8 é™ä¸‹æ¥)

    if TACTICAL_MODE == "BREAKOUT":
        current_lr = 8e-5
        use_dbc = False
        mode_msg = "ğŸ”¥ [æš´åŠ›ç ´å±€æ¨¡å¼] å…¨åŠ›è¾“å‡ºï¼Œå…è®¸æ¢¯åº¦çˆ†å‘"
    else: # LANDING
        current_lr = 1e-5
        use_dbc = True
        mode_msg = "â„ï¸ [å¹³ç¨³é™è½æ¨¡å¼] å¼€å¯è¾…åŠ©ï¼Œç²¾ç»†æ”¶æ•›"

    # ============================================================

    # 6. åˆ›å»ºä¼˜åŒ–å™¨
    # ------------------------------------------------------------
    optimizer = optim.Adam(model.parameters(), lr=current_lr, weight_decay=1e-2)
    print(f"ğŸ”§ ä¼˜åŒ–å™¨é…ç½®å®Œæˆ | æ¨¡å¼: {TACTICAL_MODE} | LR: {current_lr} | Weight Decay: 1e-2")
    
    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)

    # 7. æ³¨å†ŒDBC hooks (æ™ºèƒ½æ§åˆ¶ç‰ˆ)
    # ------------------------------------------------------------
    print(f"\nâš¡ [æ­¥éª¤ 7] DBC-DAC çŠ¶æ€: {mode_msg}")
    
    # å…ˆæ¸…ç†æ—§é’©å­ (é˜²æ­¢æ®‹ç•™)
    if hasattr(model, 'gradient_hooks'):
        model.gradient_hooks = []
    
    if use_dbc:
        # âœ… å¼€å¯æ¨¡å¼ï¼šæ³¨å†Œé’©å­
        hooks = register_dbc_hooks(model)
        print(f"   âœ… å·²æ¿€æ´» {len(hooks)} ä¸ªæ¢¯åº¦ç¨³å®šé’©å­ (é™è½ä¼å·²æ‰“å¼€)")
    else:
        # ğŸ›‘ ç¦ç”¨æ¨¡å¼ï¼šç¡®ä¿è£¸å¥”
        print(f"   ğŸš« æ¢¯åº¦é’©å­å·²ç§»é™¤ (é™åˆ¶å·²è§£é™¤)")

    # 8. è®­ç»ƒæ¨¡å‹
    print(f"\n" + "="*60)
    print("ğŸƒ å¼€å§‹å¿«é€Ÿè®­ç»ƒ (çœ‹èƒ½å¦å¿«é€Ÿå­¦ä¼šè¯´è¯)")
    print("="*60)

    num_epochs = 30  # 600ä¸ªè®­ç»ƒå¯¹ï¼ˆ100æ¦‚å¿µÃ—6å±‚çº§ï¼šemoji/çŸ­è¯­/è‹±æ–‡/æ‹¼éŸ³/æ—¥æ–‡/éŸ©æ–‡â†’ä¸­æ–‡ï¼‰

    for epoch in range(num_epochs):
        loss = train_epoch(model, dataloader, optimizer, criterion, device, use_dbc=True, accumulation_steps=ACCUMULATION_STEPS)
        print(f"Epoch {epoch+1}/{num_epochs} - Loss: {loss:.4f}")

        # æ¯5ä¸ªepochæµ‹è¯•ä¸€æ¬¡
        if (epoch + 1) % 5 == 0 or epoch == num_epochs - 1:
            test_cases = [
                ("[EMOJI] ğŸŒ§ï¸", "ä¸‹é›¨"),
                ("[EMOJI] â¤ï¸", "æˆ‘çˆ±ä½ "),
                ("[EN] I love you", "æˆ‘çˆ±ä½ "),
                ("[JP] æ„›ã—ã¦ã‚‹", "æˆ‘çˆ±ä½ "),  # æ—¥æ–‡æµ‹è¯•
                ("[KR] ì‚¬ë‘í•´", "æˆ‘çˆ±ä½ "),  # éŸ©æ–‡æµ‹è¯•
            ]
            test_generation(model, tokenizer, test_cases, device)

            # ğŸ‘‡ğŸ‘‡ğŸ‘‡ ã€è¿™é‡Œæ’å…¥è‡ªåŠ¨å­˜æ¡£ä»£ç ã€‘ ğŸ‘‡ğŸ‘‡ğŸ‘‡
            print(f"ğŸ’¾ æ­£åœ¨è‡ªåŠ¨å­˜æ¡£ (Epoch {epoch+1})...")
            save_dir = os.path.join(project_root, 'tests', 'saved_models')
            
            # è¿™é‡Œè°ƒç”¨ä¿å­˜å‡½æ•°
            # æ³¨æ„ï¼šnum_epochs å‚æ•°ä¼ å…¥å½“å‰çš„ epoch+1ï¼Œè¿™æ ·ä½ çŸ¥é“è¿™ä¸ªæ¡£æ˜¯è·‘äº†å¤šå°‘è½®çš„
            save_model_and_tokenizer(
                model=model,
                tokenizer=tokenizer,
                config=model.config,  # ç¡®ä¿ä¼ å…¥é…ç½®
                save_dir=save_dir,
                num_epochs=epoch+1,   # è®°å½•å½“å‰è¿›åº¦
                final_loss=loss
            )
            print("------------------------------------------------")

    # 9. æœ€ç»ˆæµ‹è¯•
    print(f"\n" + "="*60)
    print("ğŸ¯ æœ€ç»ˆæµ‹è¯• - APTå­¦ä¼šè¯´è¯äº†å—?")
    print("="*60)

    final_test_cases = [
        ("[EMOJI] ğŸŒ§ï¸", "ä¸‹é›¨"),
        ("[EMOJI] â¤ï¸", "æˆ‘çˆ±ä½ "),
        ("[EMOJI] ğŸ½ï¸", "åƒé¥­"),
        ("[EMOJI] ğŸ“–", "çœ‹ä¹¦"),
        ("[EN] I love you", "æˆ‘çˆ±ä½ "),
        ("[EN] It's raining", "ä¸‹é›¨"),
        ("[PY] wÇ’ Ã i nÇ", "æˆ‘çˆ±ä½ "),
        ("[JP] æ„›ã—ã¦ã‚‹", "æˆ‘çˆ±ä½ "),  # æ—¥æ–‡
        ("[JP] é›¨ãŒé™ã£ã¦ã„ã¾ã™", "ä¸‹é›¨"),  # æ—¥æ–‡
        ("[KR] ì‚¬ë‘í•´", "æˆ‘çˆ±ä½ "),  # éŸ©æ–‡
        ("[KR] ë¹„ê°€ ì˜¤ê³  ìˆì–´ìš”", "ä¸‹é›¨"),  # éŸ©æ–‡
    ]

    test_generation(model, tokenizer, final_test_cases, device)

    # 9.5 å®‰æŸè¯„ä¼°
    try:
        evaluate_hlbd_model(untrained_model, model, tokenizer, device)
    except Exception as e:
        print(f"\nâš ï¸ å®‰æŸè¯„ä¼°å‡ºé”™: {e}")

    # 10. æ€»ç»“
    print(f"\n" + "="*60)
    print("ğŸ“ æµ‹è¯•æ€»ç»“")
    print("="*60)
    print(f"âœ… è®­ç»ƒå®Œæˆ: {num_epochs} epochs")
    print(f"âœ… è®­ç»ƒæ ·æœ¬: {len(samples)} æ¦‚å¿µ, {len(training_pairs)} å¯¹")
    print(f"âœ… DBCåŠ é€Ÿ: {len(hooks)} ä¸ªé’©å­æ¿€æ´»")
    print(f"âœ… æ¨¡å‹å‚æ•°: {total_params:,}")
    print(f"\nğŸ’¡ è§‚å¯Ÿ:")
    print(f"   - APTæ¨¡å‹ä½¿ç”¨è‡ªç”Ÿæˆæ³¨æ„åŠ›æœºåˆ¶")
    print(f"   - DBC-DACç¨³å®šäº†æ¢¯åº¦è®­ç»ƒ")
    print(f"   - åˆ†å±‚è¯­è¨€å­¦ä¹ å¸®åŠ©å¿«é€ŸæŒæ¡æ¦‚å¿µ")
    print(f"   - ä»emoji/æ‹¼éŸ³/è‹±æ–‡åˆ°ä¸­æ–‡çš„å¤šå±‚æ˜ å°„")

    # 11. ä¿å­˜æ¨¡å‹
    save_dir = os.path.join(project_root, 'tests', 'saved_models')
    model_path = save_model_and_tokenizer(
        model=model,
        tokenizer=tokenizer,
        config=config,
        save_dir=save_dir,
        num_epochs=num_epochs,
        final_loss=loss
    )

    return model, tokenizer, model_path


if __name__ == "__main__":
    model, tokenizer, model_path = main()

    # å¯é€‰ï¼šæµ‹è¯•åŠ è½½åŠŸèƒ½
    print("\n" + "="*60)
    print("ğŸ”„ æµ‹è¯•æ¨¡å‹åŠ è½½åŠŸèƒ½")
    print("="*60)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    loaded_model, loaded_tokenizer, training_info = load_model_and_tokenizer(model_path, device)

    # éªŒè¯åŠ è½½çš„æ¨¡å‹
    test_cases = [
        ("[EMOJI] ğŸŒ§ï¸", "ä¸‹é›¨"),
        ("[EMOJI] â¤ï¸", "æˆ‘çˆ±ä½ "),
    ]

    print("\nä½¿ç”¨åŠ è½½çš„æ¨¡å‹ç”Ÿæˆ:")
    test_generation(loaded_model, loaded_tokenizer, test_cases, device)

    print("\nâœ… æ¨¡å‹ä¿å­˜å’ŒåŠ è½½åŠŸèƒ½æ­£å¸¸ï¼")
