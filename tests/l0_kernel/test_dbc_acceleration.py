#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""æµ‹è¯•DBC-DACåŠ é€Ÿè®­ç»ƒåŠŸèƒ½"""

import sys
import torch
import torch.nn as nn
import torch.optim as optim

# æ·»åŠ è·¯å¾„
sys.path.insert(0, '/home/user/APT-Transformer')

from apt.apt_model.modeling.apt_model import DBCDAC_Optimizer, create_gradient_stabilizer_hook

# ç›´æ¥å®ç°æ³¨å†Œå‡½æ•°ï¼Œé¿å…å¯¼å…¥å¤æ‚ä¾èµ–
def register_dbc_dac_hooks(model):
    """ä¸ºæ¨¡å‹æ³¨å†ŒDBC-DAC hooks"""
    opt = DBCDAC_Optimizer()
    hooks = []
    for _, p in model.named_parameters():
        if p.requires_grad:
            hooks.append(p.register_hook(create_gradient_stabilizer_hook(opt)))
    return hooks


class SimpleTestModel(nn.Module):
    """ç®€å•çš„æµ‹è¯•æ¨¡å‹"""
    def __init__(self):
        super().__init__()
        self.linear1 = nn.Linear(64, 128)
        self.linear2 = nn.Linear(128, 64)
        self.linear3 = nn.Linear(64, 10)

    def forward(self, x):
        x = torch.relu(self.linear1(x))
        x = torch.relu(self.linear2(x))
        return self.linear3(x)


def test_dbc_optimizer():
    """æµ‹è¯•DBCDACä¼˜åŒ–å™¨"""
    print("\n" + "="*60)
    print("ğŸ§ª æµ‹è¯•DBCDACä¼˜åŒ–å™¨")
    print("="*60)

    # åˆ›å»ºä¼˜åŒ–å™¨
    dbc_optimizer = DBCDAC_Optimizer(
        rank_ratio_proj=0.1,
        rank_ratio_res=0.05,
        apply_to_gradients=True
    )

    print(f"ğŸ“Š ä¼˜åŒ–å™¨é…ç½®:")
    print(f"   rank_ratio_proj: {dbc_optimizer.rank_ratio_proj}")
    print(f"   rank_ratio_res: {dbc_optimizer.rank_ratio_res}")
    print(f"   threshold: {dbc_optimizer.threshold}")
    print(f"   apply_to_gradients: {dbc_optimizer.apply_to_gradients}")

    # æµ‹è¯•çŸ©é˜µç¨³å®š
    print("\nğŸ”§ æµ‹è¯•çŸ©é˜µç¨³å®šåŠŸèƒ½...")
    test_matrix = torch.randn(64, 128)
    print(f"   åŸå§‹çŸ©é˜µå½¢çŠ¶: {test_matrix.shape}")
    print(f"   åŸå§‹çŸ©é˜µèŒƒæ•°: {torch.norm(test_matrix).item():.4f}")

    stabilized = dbc_optimizer.stabilize_matrix(test_matrix)
    print(f"   ç¨³å®šåçŸ©é˜µå½¢çŠ¶: {stabilized.shape}")
    print(f"   ç¨³å®šåçŸ©é˜µèŒƒæ•°: {torch.norm(stabilized).item():.4f}")
    print(f"   âœ… çŸ©é˜µç¨³å®šæµ‹è¯•æˆåŠŸ!")

    # æµ‹è¯•æ¢¯åº¦ç¨³å®š
    print("\nğŸ”§ æµ‹è¯•æ¢¯åº¦ç¨³å®šåŠŸèƒ½...")
    test_grad = torch.randn(128, 64)
    print(f"   åŸå§‹æ¢¯åº¦å½¢çŠ¶: {test_grad.shape}")
    print(f"   åŸå§‹æ¢¯åº¦èŒƒæ•°: {torch.norm(test_grad).item():.4f}")

    stabilized_grad = dbc_optimizer.stabilize_gradients(test_grad)
    print(f"   ç¨³å®šåæ¢¯åº¦å½¢çŠ¶: {stabilized_grad.shape}")
    print(f"   ç¨³å®šåæ¢¯åº¦èŒƒæ•°: {torch.norm(stabilized_grad).item():.4f}")
    print(f"   âœ… æ¢¯åº¦ç¨³å®šæµ‹è¯•æˆåŠŸ!")

    # æµ‹è¯•å¤„ç†NaNæ¢¯åº¦
    print("\nğŸ”§ æµ‹è¯•NaNæ¢¯åº¦å¤„ç†...")
    nan_grad = torch.randn(64, 128)
    nan_grad[0, 0] = float('nan')
    print(f"   åŒ…å«NaN: {torch.isnan(nan_grad).any().item()}")

    hook = create_gradient_stabilizer_hook(dbc_optimizer)
    cleaned_grad = hook(nan_grad)
    print(f"   å¤„ç†ååŒ…å«NaN: {torch.isnan(cleaned_grad).any().item()}")
    print(f"   âœ… NaNæ¢¯åº¦å¤„ç†æˆåŠŸ!")

    return True


def test_dbc_hooks():
    """æµ‹è¯•DBC-DAC hooksæ³¨å†Œ"""
    print("\n" + "="*60)
    print("ğŸ§ª æµ‹è¯•DBC-DAC Hooksæ³¨å†Œ")
    print("="*60)

    # åˆ›å»ºæ¨¡å‹
    model = SimpleTestModel()
    print(f"ğŸ“Š æ¨¡å‹å‚æ•°æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")

    # æ³¨å†Œhooks
    print("\nğŸ”§ æ³¨å†ŒDBC-DAC hooks...")
    hooks = register_dbc_dac_hooks(model)
    print(f"   æ³¨å†Œçš„hookæ•°é‡: {len(hooks)}")
    print(f"   âœ… Hooksæ³¨å†ŒæˆåŠŸ!")

    return True


def test_training_with_dbc():
    """æµ‹è¯•å¸¦DBCçš„è®­ç»ƒæµç¨‹"""
    print("\n" + "="*60)
    print("ğŸ§ª æµ‹è¯•å¸¦DBCçš„è®­ç»ƒæµç¨‹")
    print("="*60)

    # åˆ›å»ºæ¨¡å‹å’Œä¼˜åŒ–å™¨
    model = SimpleTestModel()
    optimizer = optim.Adam(model.parameters(), lr=0.001)

    # æ³¨å†ŒDBC hooks
    print("ğŸ”§ æ³¨å†ŒDBC-DAC hooks...")
    hooks = register_dbc_dac_hooks(model)
    print(f"   æ³¨å†Œäº† {len(hooks)} ä¸ªhooks")

    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ®
    batch_size = 16
    x = torch.randn(batch_size, 64)
    y = torch.randint(0, 10, (batch_size,))

    print(f"\nğŸ“Š è®­ç»ƒæ•°æ®:")
    print(f"   è¾“å…¥å½¢çŠ¶: {x.shape}")
    print(f"   æ ‡ç­¾å½¢çŠ¶: {y.shape}")

    # è®­ç»ƒå‡ æ­¥
    print("\nğŸƒ è¿è¡Œè®­ç»ƒæ­¥éª¤...")
    model.train()
    num_steps = 5

    for step in range(num_steps):
        optimizer.zero_grad()

        # å‰å‘ä¼ æ’­
        output = model(x)
        loss = nn.functional.cross_entropy(output, y)

        # åå‘ä¼ æ’­ (DBC hooksä¼šè‡ªåŠ¨åº”ç”¨)
        loss.backward()

        # æ£€æŸ¥æ¢¯åº¦
        grad_norm = sum(p.grad.norm().item() for p in model.parameters() if p.grad is not None)

        # ä¼˜åŒ–å™¨æ­¥éª¤
        optimizer.step()

        print(f"   Step {step+1}/{num_steps} | Loss: {loss.item():.4f} | Grad Norm: {grad_norm:.4f}")

    print(f"   âœ… è®­ç»ƒæµç¨‹æµ‹è¯•æˆåŠŸ!")

    return True


def test_gradient_stability():
    """æµ‹è¯•æ¢¯åº¦ç¨³å®šæ€§"""
    print("\n" + "="*60)
    print("ğŸ§ª æµ‹è¯•æ¢¯åº¦ç¨³å®šæ€§å¯¹æ¯”")
    print("="*60)

    # åˆ›å»ºä¸¤ä¸ªç›¸åŒçš„æ¨¡å‹
    model_without_dbc = SimpleTestModel()
    model_with_dbc = SimpleTestModel()

    # ç¡®ä¿å‚æ•°ç›¸åŒ
    model_with_dbc.load_state_dict(model_without_dbc.state_dict())

    # åªç»™ä¸€ä¸ªæ¨¡å‹æ³¨å†ŒDBC hooks
    print("ğŸ”§ ä¸ºç¬¬äºŒä¸ªæ¨¡å‹æ³¨å†ŒDBC hooks...")
    hooks = register_dbc_dac_hooks(model_with_dbc)

    # åˆ›å»ºæ¨¡æ‹Ÿæ•°æ® (æ•…æ„åˆ¶é€ ä¸€äº›æç«¯å€¼)
    x = torch.randn(8, 64) * 10  # æ”¾å¤§è¾“å…¥
    y = torch.randint(0, 10, (8,))

    print(f"\nğŸ“Š æµ‹è¯•æ•°æ® (åŒ…å«æç«¯å€¼):")
    print(f"   è¾“å…¥èŒƒå›´: [{x.min().item():.2f}, {x.max().item():.2f}]")

    # æµ‹è¯•æ— DBC
    print("\nğŸ” æ— DBCæ¨¡å‹:")
    model_without_dbc.train()
    output1 = model_without_dbc(x)
    loss1 = nn.functional.cross_entropy(output1, y)
    loss1.backward()
    grad_norm1 = sum(p.grad.norm().item() for p in model_without_dbc.parameters() if p.grad is not None)
    print(f"   Loss: {loss1.item():.4f}")
    print(f"   æ¢¯åº¦èŒƒæ•°: {grad_norm1:.4f}")

    # æµ‹è¯•æœ‰DBC
    print("\nğŸ” æœ‰DBCæ¨¡å‹:")
    model_with_dbc.train()
    output2 = model_with_dbc(x)
    loss2 = nn.functional.cross_entropy(output2, y)
    loss2.backward()
    grad_norm2 = sum(p.grad.norm().item() for p in model_with_dbc.parameters() if p.grad is not None)
    print(f"   Loss: {loss2.item():.4f}")
    print(f"   æ¢¯åº¦èŒƒæ•°: {grad_norm2:.4f}")

    print(f"\nğŸ“ˆ å¯¹æ¯”:")
    print(f"   æ¢¯åº¦èŒƒæ•°å·®å¼‚: {abs(grad_norm1 - grad_norm2):.4f}")
    print(f"   DBCä½¿æ¢¯åº¦æ›´{'ç¨³å®š' if grad_norm2 < grad_norm1 else 'æ¿€è¿›'}")
    print(f"   âœ… ç¨³å®šæ€§æµ‹è¯•å®Œæˆ!")

    return True


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\nğŸš€ å¼€å§‹æµ‹è¯•DBC-DACåŠ é€Ÿè®­ç»ƒåŠŸèƒ½...")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"è®¾å¤‡: {'CUDA' if torch.cuda.is_available() else 'CPU'}")

    try:
        # æµ‹è¯•DBCä¼˜åŒ–å™¨
        test1 = test_dbc_optimizer()

        # æµ‹è¯•hooksæ³¨å†Œ
        test2 = test_dbc_hooks()

        # æµ‹è¯•è®­ç»ƒæµç¨‹
        test3 = test_training_with_dbc()

        # æµ‹è¯•æ¢¯åº¦ç¨³å®šæ€§
        test4 = test_gradient_stability()

        # æ€»ç»“
        print("\n" + "="*60)
        print("ğŸ“ æµ‹è¯•æ€»ç»“")
        print("="*60)
        print(f"âœ… DBCDACä¼˜åŒ–å™¨: {'é€šè¿‡' if test1 else 'å¤±è´¥'}")
        print(f"âœ… Hooksæ³¨å†Œ: {'é€šè¿‡' if test2 else 'å¤±è´¥'}")
        print(f"âœ… è®­ç»ƒæµç¨‹: {'é€šè¿‡' if test3 else 'å¤±è´¥'}")
        print(f"âœ… æ¢¯åº¦ç¨³å®šæ€§: {'é€šè¿‡' if test4 else 'å¤±è´¥'}")

        if all([test1, test2, test3, test4]):
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
            print("\nğŸ’¡ DBC-DACåŠŸèƒ½è¯´æ˜:")
            print("   - ç»´åº¦å¹³è¡¡å‹ç¼©æ³•(DBC)å‡å°‘æ¢¯åº¦å™ªå£°")
            print("   - ç»´åº¦ä¼´éšè¡¥å¿æ³•(DAC)ä¿æŒæ¢¯åº¦ä¿¡æ¯")
            print("   - è‡ªåŠ¨å¤„ç†NaN/Infæ¢¯åº¦")
            print("   - æé«˜è®­ç»ƒç¨³å®šæ€§å’Œæ”¶æ•›é€Ÿåº¦")
            return 0
        else:
            print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
            return 1

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())
