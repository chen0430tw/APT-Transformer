#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
å®Œæ•´çš„è®­ç»ƒå™¨æµ‹è¯•å¥—ä»¶

åŒ…å«ï¼š
1. åŸºç¡€è®­ç»ƒæµ‹è¯•
2. Checkpointä¿å­˜/åŠ è½½æµ‹è¯•
3. æ¢å¤è®­ç»ƒæµ‹è¯•
4. æ—©åœæœºåˆ¶æµ‹è¯•
5. æ··åˆç²¾åº¦æµ‹è¯•
6. æ¢¯åº¦ç´¯ç§¯æµ‹è¯•
7. Temp checkpointæµ‹è¯•
8. ğŸ”® APIæ¥å£ä¼ç¬”
9. ğŸ”® åˆ†å¸ƒå¼è®­ç»ƒä¼ç¬”
"""

import pytest
import torch
import torch.nn as nn
import tempfile
import shutil
from pathlib import Path
import json

# å¯¼å…¥è®­ç»ƒç›¸å…³æ¨¡å—
import sys
sys.path.insert(0, str(Path(__file__).parent.parent))

from apt.core.training.trainer import train_model
from apt.core.training.checkpoint import CheckpointManager, save_model, load_model
from apt.core.config.apt_config import APTConfig


# ============================================================================
# æµ‹è¯•fixtures
# ============================================================================

@pytest.fixture
def temp_dir():
    """åˆ›å»ºä¸´æ—¶ç›®å½•"""
    tmp_dir = tempfile.mkdtemp()
    yield Path(tmp_dir)
    # æ¸…ç†
    shutil.rmtree(tmp_dir, ignore_errors=True)


@pytest.fixture
def sample_texts():
    """ç¤ºä¾‹è®­ç»ƒæ–‡æœ¬"""
    return [
        "This is a test sentence for training.",
        "Machine learning is amazing.",
        "Deep learning models are powerful.",
        "Natural language processing is fascinating.",
        "AI will change the world.",
        "Testing is important for code quality.",
        "Python is a great programming language.",
        "Neural networks learn from data."
    ]


@pytest.fixture
def mini_config():
    """æœ€å°åŒ–é…ç½®ï¼ˆç”¨äºå¿«é€Ÿæµ‹è¯•ï¼‰"""
    config = APTConfig()
    config.d_model = 128
    config.nhead = 4
    config.num_encoder_layers = 2
    config.num_decoder_layers = 2
    config.dim_feedforward = 256
    config.vocab_size = 1000
    return config


# ============================================================================
# åŸºç¡€è®­ç»ƒæµ‹è¯•
# ============================================================================

class TestBasicTraining:
    """åŸºç¡€è®­ç»ƒåŠŸèƒ½æµ‹è¯•"""

    def test_train_basic_flow(self, temp_dir, sample_texts, mini_config):
        """æµ‹è¯•åŸºç¡€è®­ç»ƒæµç¨‹"""
        model, tokenizer, config = train_model(
            epochs=2,
            batch_size=2,
            learning_rate=1e-4,
            checkpoint_dir=temp_dir / "outputs",
            texts=sample_texts,
            tokenizer=None  # è‡ªåŠ¨åˆ›å»º
        )

        # éªŒè¯æ¨¡å‹å·²è®­ç»ƒ
        assert model is not None
        assert tokenizer is not None
        assert config is not None

        # éªŒè¯checkpointç›®å½•åˆ›å»º
        checkpoint_dir = temp_dir / "outputs" / "checkpoints"
        assert checkpoint_dir.exists()

        # éªŒè¯è‡³å°‘æœ‰checkpointæ–‡ä»¶
        checkpoint_files = list(checkpoint_dir.glob("*.pt"))
        assert len(checkpoint_files) >= 1

    def test_train_returns_valid_model(self, temp_dir, sample_texts):
        """æµ‹è¯•è®­ç»ƒè¿”å›å¯ç”¨æ¨¡å‹"""
        model, tokenizer, config = train_model(
            epochs=1,
            batch_size=2,
            checkpoint_dir=temp_dir / "outputs",
            texts=sample_texts
        )

        # æµ‹è¯•æ¨¡å‹å¯ä»¥æ¨ç†
        test_input = tokenizer.encode("test input", return_tensors='pt')
        with torch.no_grad():
            output = model(
                src_tokens=test_input,
                tgt_tokens=test_input
            )

        assert output is not None
        assert output.shape[0] == 1  # batch size


# ============================================================================
# Checkpointæµ‹è¯•
# ============================================================================

class TestCheckpointSystem:
    """Checkpointç³»ç»Ÿæµ‹è¯•"""

    def test_checkpoint_save_complete_state(self, temp_dir):
        """æµ‹è¯•checkpointä¿å­˜å®Œæ•´çŠ¶æ€"""
        mgr = CheckpointManager(save_dir=temp_dir, model_name="test_model")

        # åˆ›å»ºè™šæ‹Ÿæ¨¡å‹å’Œä¼˜åŒ–å™¨
        model = nn.Linear(10, 10)
        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)

        # ä¿å­˜checkpoint
        checkpoint_path = mgr.save_checkpoint(
            model=model,
            optimizer=optimizer,
            scheduler=scheduler,
            epoch=5,
            global_step=1000,
            loss_history=[2.5, 2.3, 2.1, 1.9, 1.8],
            metrics={'avg_loss': 1.8, 'best_loss': 1.8},
            is_best=True
        )

        # éªŒè¯æ–‡ä»¶å­˜åœ¨
        assert Path(checkpoint_path).exists()

        # åŠ è½½å¹¶éªŒè¯å†…å®¹
        checkpoint = torch.load(checkpoint_path)

        assert 'model_state_dict' in checkpoint
        assert 'optimizer_state_dict' in checkpoint
        assert 'scheduler_state_dict' in checkpoint
        assert 'epoch' in checkpoint
        assert 'global_step' in checkpoint
        assert 'loss_history' in checkpoint
        assert 'metrics' in checkpoint

        assert checkpoint['epoch'] == 5
        assert checkpoint['global_step'] == 1000
        assert len(checkpoint['loss_history']) == 5

    def test_checkpoint_load_restores_state(self, temp_dir):
        """æµ‹è¯•checkpointåŠ è½½æ¢å¤çŠ¶æ€"""
        mgr = CheckpointManager(save_dir=temp_dir)

        # åˆ›å»ºå¹¶ä¿å­˜
        model = nn.Linear(10, 10)
        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)
        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)

        original_weights = model.weight.data.clone()

        checkpoint_path = mgr.save_checkpoint(
            model=model,
            optimizer=optimizer,
            scheduler=scheduler,
            epoch=3,
            global_step=500,
            loss_history=[2.0, 1.5, 1.2],
            metrics={'avg_loss': 1.2}
        )

        # ä¿®æ”¹æ¨¡å‹ï¼ˆæ¨¡æ‹Ÿç»§ç»­è®­ç»ƒï¼‰
        model.weight.data += 1.0

        # åŠ è½½checkpoint
        epoch, step, loss_history, metrics = mgr.load_checkpoint(
            model=model,
            optimizer=optimizer,
            scheduler=scheduler,
            checkpoint_path=checkpoint_path
        )

        # éªŒè¯çŠ¶æ€æ¢å¤
        assert epoch == 3
        assert step == 500
        assert len(loss_history) == 3
        assert metrics['avg_loss'] == 1.2

        # éªŒè¯æƒé‡æ¢å¤
        assert torch.allclose(model.weight.data, original_weights)

    def test_metadata_tracking(self, temp_dir):
        """æµ‹è¯•å…ƒæ•°æ®è¿½è¸ª"""
        mgr = CheckpointManager(save_dir=temp_dir)

        model = nn.Linear(10, 10)
        optimizer = torch.optim.Adam(model.parameters())
        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=1)

        # ä¿å­˜å¤šä¸ªcheckpoint
        for epoch in range(3):
            mgr.save_checkpoint(
                model=model,
                optimizer=optimizer,
                scheduler=scheduler,
                epoch=epoch,
                global_step=epoch * 100,
                loss_history=[],
                metrics={'avg_loss': 2.0 - epoch * 0.5},
                is_best=(epoch == 2)
            )

        # éªŒè¯metadata.json
        metadata_path = temp_dir / "metadata.json"
        assert metadata_path.exists()

        with open(metadata_path) as f:
            metadata = json.load(f)

        assert len(metadata['checkpoints']) == 3
        assert metadata['checkpoints'][-1]['is_best'] == True


# ============================================================================
# æ¢å¤è®­ç»ƒæµ‹è¯•
# ============================================================================

class TestResumeTraining:
    """æ¢å¤è®­ç»ƒæµ‹è¯•"""

    def test_resume_from_checkpoint(self, temp_dir, sample_texts):
        """æµ‹è¯•ä»checkpointæ¢å¤è®­ç»ƒ"""
        # ç¬¬ä¸€æ¬¡è®­ç»ƒåˆ°epoch 2
        model1, tokenizer1, config1 = train_model(
            epochs=2,
            batch_size=2,
            checkpoint_dir=temp_dir / "outputs",
            texts=sample_texts
        )

        # æ‰¾åˆ°æœ€åçš„checkpoint
        checkpoint_files = sorted((temp_dir / "outputs" / "checkpoints").glob("*.pt"))
        last_checkpoint = checkpoint_files[-1]

        # æ¢å¤è®­ç»ƒåˆ°epoch 4
        model2, tokenizer2, config2 = train_model(
            epochs=4,
            batch_size=2,
            checkpoint_dir=temp_dir / "outputs",
            resume_from=str(last_checkpoint),
            texts=sample_texts
        )

        # éªŒè¯ç»§ç»­è®­ç»ƒ
        checkpoint_files_after = list((temp_dir / "outputs" / "checkpoints").glob("*.pt"))
        # åº”è¯¥æœ‰4ä¸ªcheckpointï¼ˆæˆ–æ›´å¤šï¼Œå¦‚æœæœ‰bestæ ‡è®°ï¼‰
        assert len(checkpoint_files_after) >= 4

    @pytest.mark.skipif(not torch.cuda.is_available(), reason="éœ€è¦GPU")
    def test_resume_preserves_optimizer_state(self, temp_dir, sample_texts):
        """æµ‹è¯•æ¢å¤è®­ç»ƒä¿æŒoptimizerçŠ¶æ€"""
        # è¿™ä¸ªæµ‹è¯•éªŒè¯optimizeråŠ¨é‡ç­‰çŠ¶æ€æ˜¯å¦æ­£ç¡®æ¢å¤
        # å®é™…å®ç°éœ€è¦æ›´è¯¦ç»†çš„éªŒè¯é€»è¾‘
        pass


# ============================================================================
# æ—©åœæœºåˆ¶æµ‹è¯•
# ============================================================================

class TestEarlyStopping:
    """æ—©åœæœºåˆ¶æµ‹è¯•"""

    def test_early_stopping_triggers(self, temp_dir, sample_texts):
        """æµ‹è¯•æ—©åœæœºåˆ¶è§¦å‘"""
        # æ³¨æ„ï¼šè¿™ä¸ªæµ‹è¯•å¯èƒ½ä¸ç¨³å®šï¼Œå› ä¸ºè®­ç»ƒå¯èƒ½çœŸçš„æ”¶æ•›
        # å®é™…é¡¹ç›®ä¸­å¯èƒ½éœ€è¦mockæˆ–ä½¿ç”¨ç‰¹æ®Šçš„æµ‹è¯•æ•°æ®
        model, tokenizer, config = train_model(
            epochs=100,  # è®¾ç½®å¾ˆå¤§çš„epoch
            batch_size=2,
            checkpoint_dir=temp_dir / "outputs",
            texts=sample_texts
        )

        # éªŒè¯æ—©åœç”Ÿæ•ˆï¼ˆåº”è¯¥è¿œå°‘äº100ä¸ªcheckpointï¼‰
        checkpoint_files = list((temp_dir / "outputs" / "checkpoints").glob("*.pt"))
        assert len(checkpoint_files) < 100, "æ—©åœåº”è¯¥ç”Ÿæ•ˆ"


# ============================================================================
# æ··åˆç²¾åº¦å’Œæ¢¯åº¦ç´¯ç§¯æµ‹è¯•
# ============================================================================

class TestAdvancedTraining:
    """é«˜çº§è®­ç»ƒåŠŸèƒ½æµ‹è¯•"""

    @pytest.mark.skipif(not torch.cuda.is_available(), reason="éœ€è¦GPU")
    def test_mixed_precision_training(self, temp_dir, sample_texts):
        """æµ‹è¯•æ··åˆç²¾åº¦è®­ç»ƒ"""
        # trainer.pyå·²ç»ä½¿ç”¨äº†torch.amp.autocast
        model, tokenizer, config = train_model(
            epochs=1,
            batch_size=2,
            checkpoint_dir=temp_dir / "outputs",
            texts=sample_texts
        )

        # å¦‚æœèƒ½æˆåŠŸè®­ç»ƒï¼Œè¯´æ˜æ··åˆç²¾åº¦å·¥ä½œæ­£å¸¸
        assert model is not None

    def test_gradient_accumulation(self, temp_dir, sample_texts):
        """æµ‹è¯•æ¢¯åº¦ç´¯ç§¯"""
        # trainer.pyä¸­accumulation_steps=4
        # è¿™ä¸ªæµ‹è¯•éªŒè¯æ¢¯åº¦ç´¯ç§¯æ˜¯å¦æ­£å¸¸å·¥ä½œ
        model, tokenizer, config = train_model(
            epochs=1,
            batch_size=2,
            checkpoint_dir=temp_dir / "outputs",
            texts=sample_texts
        )

        assert model is not None


# ============================================================================
# Temp Checkpointæµ‹è¯•
# ============================================================================

class TestTempCheckpoint:
    """ä¸´æ—¶checkpointæµ‹è¯•"""

    def test_temp_checkpoint_creation(self, temp_dir, sample_texts):
        """æµ‹è¯•ä¸´æ—¶checkpointåˆ›å»º"""
        model, tokenizer, config = train_model(
            epochs=1,
            batch_size=2,
            checkpoint_dir=temp_dir / "outputs",
            temp_checkpoint_freq=5,  # æ¯5æ­¥ä¿å­˜
            texts=sample_texts
        )

        # æ³¨æ„ï¼šè®­ç»ƒç»“æŸåtempæ–‡ä»¶åº”è¯¥è¢«æ¸…ç†
        temp_dir_path = Path(".cache/temp")
        if temp_dir_path.exists():
            temp_files = list(temp_dir_path.glob("temp_*.pt"))
            # åº”è¯¥è¢«æ¸…ç†
            assert len(temp_files) == 0

    def test_temp_checkpoint_cleanup(self, temp_dir, sample_texts):
        """æµ‹è¯•ä¸´æ—¶checkpointæ¸…ç†"""
        # è¿™ä¸ªæµ‹è¯•éªŒè¯epochç»“æŸåtempæ–‡ä»¶è¢«æ¸…ç†
        # å·²åœ¨test_temp_checkpoint_creationä¸­éªŒè¯
        pass


# ============================================================================
# ğŸ”® APIæ¥å£ä¼ç¬”æµ‹è¯•
# ============================================================================

class TestAPIReadiness:
    """APIå°±ç»ªæ€§æµ‹è¯•ï¼ˆä¸ºæœªæ¥çš„APIæœåŠ¡åŸ‹ä¼ç¬”ï¼‰"""

    def test_model_serialization_for_api(self, temp_dir, sample_texts):
        """æµ‹è¯•æ¨¡å‹åºåˆ—åŒ–ï¼ˆAPIéƒ¨ç½²éœ€è¦ï¼‰"""
        model, tokenizer, config = train_model(
            epochs=1,
            batch_size=2,
            checkpoint_dir=temp_dir / "outputs",
            texts=sample_texts
        )

        # ğŸ”® APIä¼ç¬”ï¼šéªŒè¯æ¨¡å‹å¯ä»¥è¢«åºåˆ—åŒ–
        checkpoint_path = temp_dir / "api_model.pt"
        save_model(model, tokenizer, path=temp_dir / "api_model", config=config)

        # éªŒè¯å¯ä»¥åŠ è½½ï¼ˆAPIæœåŠ¡éœ€è¦ï¼‰
        loaded_model, loaded_tokenizer, loaded_config = load_model(
            temp_dir / "api_model"
        )

        assert loaded_model is not None
        assert loaded_tokenizer is not None

    def test_inference_interface(self, temp_dir, sample_texts):
        """æµ‹è¯•æ¨ç†æ¥å£ï¼ˆAPI endpointéœ€è¦ï¼‰"""
        model, tokenizer, config = train_model(
            epochs=1,
            batch_size=2,
            checkpoint_dir=temp_dir / "outputs",
            texts=sample_texts
        )

        # ğŸ”® APIä¼ç¬”ï¼šæ¨¡æ‹ŸAPIè¯·æ±‚çš„æ¨ç†
        def api_inference(model, tokenizer, text, max_length=50):
            """
            è¿™æ˜¯æœªæ¥APIæœåŠ¡çš„æ¨ç†æ¥å£åŸå‹

            POST /api/generate
            {
                "text": "input text",
                "max_length": 50
            }
            """
            input_ids = tokenizer.encode(text, return_tensors='pt')

            model.eval()
            with torch.no_grad():
                output = model(
                    src_tokens=input_ids,
                    tgt_tokens=input_ids
                )

            return {
                'success': True,
                'output_shape': list(output.shape),
                'input_length': input_ids.shape[1]
            }

        # æµ‹è¯•æ¨ç†æ¥å£
        result = api_inference(model, tokenizer, "test input for API")

        assert result['success'] == True
        assert 'output_shape' in result

    def test_batch_inference_for_api(self, temp_dir, sample_texts):
        """æµ‹è¯•æ‰¹é‡æ¨ç†ï¼ˆAPIæ‰¹å¤„ç†éœ€è¦ï¼‰"""
        model, tokenizer, config = train_model(
            epochs=1,
            batch_size=2,
            checkpoint_dir=temp_dir / "outputs",
            texts=sample_texts
        )

        # ğŸ”® APIä¼ç¬”ï¼šæ‰¹é‡æ¨ç†æ¥å£
        def api_batch_inference(model, tokenizer, texts, max_length=50):
            """
            æœªæ¥APIæœåŠ¡çš„æ‰¹é‡æ¨ç†æ¥å£

            POST /api/batch_generate
            {
                "texts": ["text1", "text2", ...],
                "max_length": 50
            }
            """
            # æ‰¹é‡ç¼–ç 
            encoded = [tokenizer.encode(text, return_tensors='pt') for text in texts]

            # æ‰¹é‡æ¨ç†
            model.eval()
            results = []
            with torch.no_grad():
                for input_ids in encoded:
                    output = model(src_tokens=input_ids, tgt_tokens=input_ids)
                    results.append(output)

            return {
                'success': True,
                'batch_size': len(texts),
                'results_count': len(results)
            }

        # æµ‹è¯•æ‰¹é‡æ¨ç†
        result = api_batch_inference(
            model, tokenizer,
            ["test 1", "test 2", "test 3"]
        )

        assert result['batch_size'] == 3
        assert result['results_count'] == 3


# ============================================================================
# ğŸ”® åˆ†å¸ƒå¼è®­ç»ƒä¼ç¬”æµ‹è¯•
# ============================================================================

class TestDistributedReadiness:
    """åˆ†å¸ƒå¼è®­ç»ƒå°±ç»ªæ€§æµ‹è¯•ï¼ˆä¸ºæœªæ¥çš„DDPåŸ‹ä¼ç¬”ï¼‰"""

    def test_model_supports_ddp_wrapping(self, temp_dir, sample_texts):
        """æµ‹è¯•æ¨¡å‹æ”¯æŒDDPåŒ…è£…"""
        model, tokenizer, config = train_model(
            epochs=1,
            batch_size=2,
            checkpoint_dir=temp_dir / "outputs",
            texts=sample_texts
        )

        # ğŸ”® åˆ†å¸ƒå¼ä¼ç¬”ï¼šéªŒè¯æ¨¡å‹å¯ä»¥è¢«DDPåŒ…è£…
        if torch.cuda.is_available() and torch.cuda.device_count() > 1:
            # å¦‚æœæœ‰å¤šGPUï¼Œæµ‹è¯•DDPåŒ…è£…
            from torch.nn.parallel import DistributedDataParallel as DDP

            model = model.cuda()
            # ddp_model = DDP(model, device_ids=[0])  # å®é™…éœ€è¦å…ˆinit_process_group

            # ç›®å‰åªéªŒè¯æ¨¡å‹ç»“æ„å…¼å®¹DDP
            assert hasattr(model, 'parameters')
            assert hasattr(model, 'state_dict')

    def test_checkpoint_supports_distributed_loading(self, temp_dir, sample_texts):
        """æµ‹è¯•checkpointæ”¯æŒåˆ†å¸ƒå¼åŠ è½½"""
        model, tokenizer, config = train_model(
            epochs=1,
            batch_size=2,
            checkpoint_dir=temp_dir / "outputs",
            texts=sample_texts
        )

        # ğŸ”® åˆ†å¸ƒå¼ä¼ç¬”ï¼šéªŒè¯checkpointå¯ä»¥åœ¨ä¸åŒrankåŠ è½½
        checkpoint_files = list((temp_dir / "outputs" / "checkpoints").glob("*.pt"))
        checkpoint_path = checkpoint_files[0]

        # æ¨¡æ‹Ÿä¸åŒrankåŠ è½½åŒä¸€ä¸ªcheckpoint
        def load_on_rank(rank, world_size):
            """
            æœªæ¥åˆ†å¸ƒå¼è®­ç»ƒä¸­ï¼Œæ¯ä¸ªrankéƒ½ä¼šåŠ è½½checkpoint
            """
            mgr = CheckpointManager(save_dir=temp_dir / "outputs")

            # åˆ›å»ºæ–°æ¨¡å‹å®ä¾‹
            from apt.core.modeling.apt_model import APTLargeModel
            new_model = APTLargeModel(config)

            # åŠ è½½checkpoint
            epoch, step, loss_history, metrics = mgr.load_checkpoint(
                model=new_model,
                checkpoint_path=checkpoint_path
            )

            return {
                'rank': rank,
                'epoch': epoch,
                'step': step,
                'model_loaded': True
            }

        # æ¨¡æ‹Ÿrank 0å’Œrank 1åŠ è½½
        result_rank0 = load_on_rank(0, 2)
        result_rank1 = load_on_rank(1, 2)

        # éªŒè¯ä¸¤ä¸ªrankåŠ è½½çš„epochä¸€è‡´
        assert result_rank0['epoch'] == result_rank1['epoch']
        assert result_rank0['model_loaded'] == True

    def test_training_state_for_distributed_sync(self, temp_dir, sample_texts):
        """æµ‹è¯•è®­ç»ƒçŠ¶æ€æ”¯æŒåˆ†å¸ƒå¼åŒæ­¥"""
        # ğŸ”® åˆ†å¸ƒå¼ä¼ç¬”ï¼šéªŒè¯è®­ç»ƒçŠ¶æ€å¯ä»¥è·¨è¿›ç¨‹åŒæ­¥

        # æœªæ¥DDPè®­ç»ƒéœ€è¦åŒæ­¥ï¼š
        # 1. global_stepï¼ˆæ‰€æœ‰rankä¸€è‡´ï¼‰
        # 2. epochï¼ˆæ‰€æœ‰rankä¸€è‡´ï¼‰
        # 3. lossï¼ˆéœ€è¦all_reduceï¼‰
        # 4. æœ€ä½³æ¨¡å‹åˆ¤æ–­ï¼ˆéœ€è¦all_reduceæ¯”è¾ƒï¼‰

        model, tokenizer, config = train_model(
            epochs=1,
            batch_size=2,
            checkpoint_dir=temp_dir / "outputs",
            texts=sample_texts
        )

        # éªŒè¯checkpointåŒ…å«å¯åŒæ­¥çš„çŠ¶æ€
        checkpoint_files = list((temp_dir / "outputs" / "checkpoints").glob("*.pt"))
        checkpoint = torch.load(checkpoint_files[0])

        # è¿™äº›å­—æ®µåœ¨åˆ†å¸ƒå¼è®­ç»ƒä¸­éœ€è¦åŒæ­¥
        assert 'epoch' in checkpoint
        assert 'global_step' in checkpoint
        assert 'metrics' in checkpoint


# ============================================================================
# ğŸ”® WebUIæ•°æ®æ¥å£ä¼ç¬”æµ‹è¯•
# ============================================================================

class TestWebUIDataInterface:
    """WebUIæ•°æ®æ¥å£æµ‹è¯•ï¼ˆä¸ºæœªæ¥çš„Webç•Œé¢åŸ‹ä¼ç¬”ï¼‰"""

    def test_training_metrics_export(self, temp_dir, sample_texts):
        """æµ‹è¯•è®­ç»ƒæŒ‡æ ‡å¯¼å‡ºï¼ˆWebUIéœ€è¦ï¼‰"""
        model, tokenizer, config = train_model(
            epochs=3,
            batch_size=2,
            checkpoint_dir=temp_dir / "outputs",
            texts=sample_texts
        )

        # ğŸ”® WebUIä¼ç¬”ï¼šå¯¼å‡ºè®­ç»ƒæŒ‡æ ‡ä¸ºJSONï¼ˆä¾›å‰ç«¯å±•ç¤ºï¼‰
        def export_metrics_for_webui(checkpoint_dir):
            """
            å¯¼å‡ºè®­ç»ƒæŒ‡æ ‡ä¸ºJSON

            WebUIä¼šé€šè¿‡APIè·å–è¿™äº›æ•°æ®ï¼š
            GET /api/training/metrics
            """
            metadata_path = checkpoint_dir / "metadata.json"

            if not metadata_path.exists():
                return None

            with open(metadata_path) as f:
                metadata = json.load(f)

            # æå–è®­ç»ƒæ›²çº¿æ•°æ®
            metrics_timeline = []
            for checkpoint_info in metadata['checkpoints']:
                metrics_timeline.append({
                    'epoch': checkpoint_info['epoch'],
                    'step': checkpoint_info['global_step'],
                    'metrics': checkpoint_info.get('metrics', {}),
                    'is_best': checkpoint_info.get('is_best', False),
                    'timestamp': checkpoint_info.get('created_at', '')
                })

            return {
                'model_name': metadata['model_name'],
                'total_checkpoints': len(metadata['checkpoints']),
                'metrics_timeline': metrics_timeline,
                'last_updated': metadata['last_updated']
            }

        # æµ‹è¯•å¯¼å‡º
        webui_data = export_metrics_for_webui(temp_dir / "outputs")

        assert webui_data is not None
        assert 'metrics_timeline' in webui_data
        assert len(webui_data['metrics_timeline']) == 3  # 3ä¸ªepoch

    def test_checkpoint_list_for_webui(self, temp_dir, sample_texts):
        """æµ‹è¯•checkpointåˆ—è¡¨æ¥å£ï¼ˆWebUIæ¨¡å‹ç®¡ç†éœ€è¦ï¼‰"""
        model, tokenizer, config = train_model(
            epochs=2,
            batch_size=2,
            checkpoint_dir=temp_dir / "outputs",
            texts=sample_texts
        )

        # ğŸ”® WebUIä¼ç¬”ï¼šcheckpointåˆ—è¡¨API
        def get_checkpoint_list_for_webui(checkpoint_dir):
            """
            è·å–checkpointåˆ—è¡¨

            GET /api/checkpoints
            """
            checkpoint_path = checkpoint_dir / "checkpoints"

            checkpoints = []
            for ckpt_file in sorted(checkpoint_path.glob("*.pt")):
                # åŠ è½½checkpointå…ƒä¿¡æ¯
                ckpt = torch.load(ckpt_file, map_location='cpu')

                checkpoints.append({
                    'filename': ckpt_file.name,
                    'path': str(ckpt_file),
                    'epoch': ckpt.get('epoch', 0),
                    'global_step': ckpt.get('global_step', 0),
                    'metrics': ckpt.get('metrics', {}),
                    'size_mb': ckpt_file.stat().st_size / 1024 / 1024
                })

            return {
                'total': len(checkpoints),
                'checkpoints': checkpoints
            }

        # æµ‹è¯•æ¥å£
        ckpt_list = get_checkpoint_list_for_webui(temp_dir / "outputs")

        assert ckpt_list['total'] >= 2
        assert all('filename' in c for c in ckpt_list['checkpoints'])


# ============================================================================
# è¿è¡Œæµ‹è¯•
# ============================================================================

if __name__ == "__main__":
    pytest.main([__file__, "-v", "-s"])
