# HLBD æ¨¡å‹ä¿å­˜å’ŒåŠ è½½æŒ‡å—

## æ¦‚è¿°

æœ¬æŒ‡å—è¯´æ˜å¦‚ä½•ä½¿ç”¨ HLBDï¼ˆåˆ†å±‚è¯­è¨€å¯è’™æ•°æ®é›†ï¼‰è®­ç»ƒçš„æ¨¡å‹ä¿å­˜å’ŒåŠ è½½åŠŸèƒ½ã€‚

---

## æ–‡ä»¶ç»“æ„

```
tests/
â”œâ”€â”€ test_hlbd_quick_learning.py   # è®­ç»ƒè„šæœ¬ï¼ˆåŒ…å«ä¿å­˜åŠŸèƒ½ï¼‰
â”œâ”€â”€ load_hlbd_model.py             # åŠ è½½è„šæœ¬ï¼ˆæ¨ç†å’Œäº¤äº’ï¼‰
â”œâ”€â”€ saved_models/                  # ä¿å­˜çš„æ¨¡å‹ç›®å½•
â”‚   â”œâ”€â”€ hlbd_model_20250116_143022.pt
â”‚   â”œâ”€â”€ hlbd_model_20250116_150135.pt
â”‚   â””â”€â”€ ...
â””â”€â”€ README_HLBD_MODELS.md          # æœ¬æ–‡æ¡£
```

---

## è®­ç»ƒå¹¶ä¿å­˜æ¨¡å‹

### è¿è¡Œè®­ç»ƒè„šæœ¬

```bash
python tests/test_hlbd_quick_learning.py
```

### è®­ç»ƒè¿‡ç¨‹

1. åŠ è½½ HLBD æ•°æ®é›†ï¼ˆ100ä¸ªæ¦‚å¿µï¼Œ400ä¸ªè®­ç»ƒå¯¹ï¼‰
2. ä½¿ç”¨ SimpleCharTokenizerï¼ˆæ”¯æŒ emojiï¼‰
3. è®­ç»ƒ 500 epochs
4. æ¯ 3 epochs æµ‹è¯•ä¸€æ¬¡ç”Ÿæˆèƒ½åŠ›
5. **è‡ªåŠ¨ä¿å­˜æ¨¡å‹åˆ° `tests/saved_models/`**
6. éªŒè¯ä¿å­˜å’ŒåŠ è½½åŠŸèƒ½

### ä¿å­˜çš„å†…å®¹

æ¯ä¸ªä¿å­˜çš„æ¨¡å‹æ–‡ä»¶ï¼ˆ`.pt`ï¼‰åŒ…å«ï¼š

```python
{
    'model_state_dict': dict,          # æ¨¡å‹æƒé‡
    'tokenizer_char_to_id': dict,      # å­—ç¬¦ â†’ ID æ˜ å°„
    'tokenizer_id_to_char': dict,      # ID â†’ å­—ç¬¦æ˜ å°„
    'tokenizer_next_id': int,          # ä¸‹ä¸€ä¸ªå¯ç”¨ ID
    'tokenizer_vocab_size': int,       # è¯æ±‡è¡¨å¤§å°
    'config': {                        # æ¨¡å‹é…ç½®
        'vocab_size': int,
        'd_model': int,
        'num_encoder_layers': int,
        'num_decoder_layers': int,
        ...
    },
    'training_info': {                 # è®­ç»ƒä¿¡æ¯
        'num_epochs': int,
        'final_loss': float,
        'timestamp': str,
    }
}
```

### æ¨¡å‹å‘½å

æ¨¡å‹æ–‡ä»¶åæ ¼å¼ï¼š`hlbd_model_YYYYMMDD_HHMMSS.pt`

ç¤ºä¾‹ï¼š`hlbd_model_20250116_143022.pt`

---

## åŠ è½½å’Œæ¨ç†

### æ–¹æ³• 1ï¼šä½¿ç”¨åŠ è½½è„šæœ¬ï¼ˆæ¨èï¼‰

```bash
# è‡ªåŠ¨åŠ è½½æœ€æ–°æ¨¡å‹
python tests/load_hlbd_model.py

# æŒ‡å®šæ¨¡å‹è·¯å¾„
python tests/load_hlbd_model.py tests/saved_models/hlbd_model_20250116_143022.pt
```

**åŠŸèƒ½ï¼š**
- âœ… è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°æ¨¡å‹
- âœ… è¿è¡Œé¢„å®šä¹‰æµ‹è¯•ç”¨ä¾‹
- âœ… äº¤äº’å¼æ¨ç†æ¨¡å¼

**äº¤äº’å¼ç¤ºä¾‹ï¼š**
```
ğŸ’¬ äº¤äº’å¼æ¨ç†æ¨¡å¼ (è¾“å…¥ 'quit' é€€å‡º)
======================================================================

è¯·è¾“å…¥æ–‡æœ¬: ğŸŒ§ï¸
ç”Ÿæˆ: ä»Šå¤©å¤©æ°”é˜´æ²‰ï¼Œä¸‹é›¨äº†ã€‚

è¯·è¾“å…¥æ–‡æœ¬: I love you
ç”Ÿæˆ: è¡¨è¾¾çœŸæŒšæƒ…æ„Ÿï¼Œæˆ‘çˆ±ä½ ã€‚

è¯·è¾“å…¥æ–‡æœ¬: quit
ğŸ‘‹ å†è§ï¼
```

### æ–¹æ³• 2ï¼šåœ¨ Python ä»£ç ä¸­åŠ è½½

```python
import torch
from tests.test_hlbd_quick_learning import load_model_and_tokenizer, generate_text

# 1. åŠ è½½æ¨¡å‹
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model, tokenizer, info = load_model_and_tokenizer('tests/saved_models/hlbd_model_xxx.pt', device)

# 2. æ¨ç†
input_text = "ğŸŒ§ï¸"
output = generate_text(model, tokenizer, input_text, device)
print(f"è¾“å…¥: {input_text}")
print(f"è¾“å‡º: {output}")
```

---

## ç»§ç»­è®­ç»ƒ

å¦‚æœæƒ³åœ¨å·²ä¿å­˜çš„æ¨¡å‹åŸºç¡€ä¸Šç»§ç»­è®­ç»ƒï¼š

```python
import torch
from tests.test_hlbd_quick_learning import load_model_and_tokenizer

# 1. åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model, tokenizer, info = load_model_and_tokenizer('tests/saved_models/hlbd_model_xxx.pt', device)

# 2. å‡†å¤‡è®­ç»ƒæ•°æ®å’Œä¼˜åŒ–å™¨
from torch import nn, optim
from torch.utils.data import DataLoader

# ... åˆ›å»ºæ•°æ®é›† ...
optimizer = optim.Adam(model.parameters(), lr=5e-5)
criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)

# 3. ç»§ç»­è®­ç»ƒ
model.train()
for epoch in range(100):  # å†è®­ç»ƒ 100 epochs
    # ... è®­ç»ƒé€»è¾‘ ...
    pass

# 4. ä¿å­˜æ–°æ¨¡å‹
from tests.test_hlbd_quick_learning import save_model_and_tokenizer
save_model_and_tokenizer(model, tokenizer, config, 'tests/saved_models', ...)
```

---

## æ¨¡å‹ä¿¡æ¯

### æŸ¥çœ‹æ¨¡å‹ä¿¡æ¯

```python
import torch

checkpoint = torch.load('tests/saved_models/hlbd_model_xxx.pt', map_location='cpu')

print("è®­ç»ƒä¿¡æ¯:")
print(f"  Epoch: {checkpoint['training_info']['num_epochs']}")
print(f"  Loss: {checkpoint['training_info']['final_loss']:.4f}")
print(f"  æ—¶é—´: {checkpoint['training_info']['timestamp']}")

print("\næ¨¡å‹é…ç½®:")
for key, value in checkpoint['config'].items():
    print(f"  {key}: {value}")

print(f"\nè¯æ±‡è¡¨å¤§å°: {len(checkpoint['tokenizer_char_to_id'])}")
```

### å…¸å‹çš„è®­ç»ƒå¥½çš„æ¨¡å‹

- **å‚æ•°é‡**: ~10M (d_model=256, 3 encoder + 3 decoder layers)
- **æ–‡ä»¶å¤§å°**: ~40-50 MB
- **è¯æ±‡è¡¨**: ~300-500 å­—ç¬¦ï¼ˆåŠ¨æ€å¢é•¿ï¼‰
- **è®­ç»ƒæ—¶é—´**: 500 epochs ~30-60åˆ†é’Ÿï¼ˆCPUï¼‰

---

## æ³¨æ„äº‹é¡¹

### âœ… ä¼˜ç‚¹

1. **å®Œæ•´ä¿å­˜**ï¼šæ¨¡å‹æƒé‡ + tokenizer + é…ç½®ï¼Œä¸€æ¬¡æ€§åŠ è½½å³å¯ä½¿ç”¨
2. **è·¨å¹³å°**ï¼šå¯ä»¥åœ¨ CPU è®­ç»ƒï¼ŒGPU åŠ è½½ï¼Œåä¹‹äº¦ç„¶
3. **å¯è¿½æº¯**ï¼šåŒ…å«è®­ç»ƒä¿¡æ¯ï¼ˆepochã€lossã€æ—¶é—´æˆ³ï¼‰
4. **æ”¯æŒ emoji**ï¼šSimpleCharTokenizer åŠ¨æ€æ·»åŠ å­—ç¬¦ï¼Œæ— æŸä¿å­˜

### âš ï¸ æ³¨æ„

1. **è¯æ±‡è¡¨å…¼å®¹æ€§**ï¼š
   - åŠ è½½çš„ tokenizer è¯æ±‡è¡¨å¿…é¡»ä¸è®­ç»ƒæ—¶ä¸€è‡´
   - ä¸è¦æ‰‹åŠ¨ä¿®æ”¹ `char_to_id` æˆ– `id_to_char`

2. **è®¾å¤‡å…¼å®¹æ€§**ï¼š
   - ä½¿ç”¨ `map_location` å‚æ•°ç¡®ä¿è·¨è®¾å¤‡åŠ è½½
   - CPU è®­ç»ƒçš„æ¨¡å‹å¯ä»¥åœ¨ GPU ä¸Šæ¨ç†

3. **ç‰ˆæœ¬å…¼å®¹æ€§**ï¼š
   - ç¡®ä¿ PyTorch ç‰ˆæœ¬å…¼å®¹ï¼ˆå»ºè®® >= 1.10ï¼‰
   - APTModel æ¶æ„ä¸èƒ½æ”¹å˜

4. **æ–‡ä»¶ç®¡ç†**ï¼š
   - å®šæœŸæ¸…ç†æ—§æ¨¡å‹ï¼ˆåªä¿ç•™æœ€ä¼˜æ¨¡å‹ï¼‰
   - å»ºè®®ä½¿ç”¨æ—¶é—´æˆ³å‘½åï¼Œä¾¿äºè¿½æº¯

---

## æ•…éšœæ’æŸ¥

### é—®é¢˜ 1ï¼šåŠ è½½å¤±è´¥

```
RuntimeError: Error(s) in loading state_dict
```

**åŸå› **ï¼šæ¨¡å‹æ¶æ„æ”¹å˜

**è§£å†³**ï¼šç¡®ä¿ APTModelConfiguration å‚æ•°ä¸ä¿å­˜æ—¶ä¸€è‡´

### é—®é¢˜ 2ï¼šEmoji æ— æ³•è¯†åˆ«

```
è¾“å…¥: ğŸŒ§ï¸
è¾“å‡º: [ç©º]
```

**åŸå› **ï¼šä½¿ç”¨äº†é”™è¯¯çš„ tokenizerï¼ˆBertTokenizerï¼‰

**è§£å†³**ï¼šç¡®ä¿åŠ è½½çš„æ˜¯ SimpleCharTokenizer

### é—®é¢˜ 3ï¼šæ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶

```
âŒ æœªæ‰¾åˆ°å·²ä¿å­˜çš„æ¨¡å‹ï¼
```

**åŸå› **ï¼šæœªè¿è¡Œè®­ç»ƒè„šæœ¬æˆ–ä¿å­˜ç›®å½•ä¸å­˜åœ¨

**è§£å†³**ï¼šå…ˆè¿è¡Œ `python tests/test_hlbd_quick_learning.py`

---

## é«˜çº§ç”¨æ³•

### æ¨¡å‹èåˆ

```python
# åŠ è½½ä¸¤ä¸ªæ¨¡å‹ï¼Œèåˆæƒé‡
model1, _, _ = load_model_and_tokenizer('model1.pt', device)
model2, _, _ = load_model_and_tokenizer('model2.pt', device)

# å¹³å‡æƒé‡
for p1, p2 in zip(model1.parameters(), model2.parameters()):
    p1.data = (p1.data + p2.data) / 2
```

### å¯¼å‡ºä¸º ONNX

```python
import torch.onnx

# å‡†å¤‡ç¤ºä¾‹è¾“å…¥
dummy_input = torch.randint(0, tokenizer.vocab_size, (1, 10)).to(device)

# å¯¼å‡º
torch.onnx.export(
    model,
    dummy_input,
    'hlbd_model.onnx',
    input_names=['input_ids'],
    output_names=['output'],
    dynamic_axes={'input_ids': {0: 'batch', 1: 'sequence'}}
)
```

---

## å‚è€ƒ

- [test_hlbd_quick_learning.py](test_hlbd_quick_learning.py) - è®­ç»ƒè„šæœ¬
- [load_hlbd_model.py](load_hlbd_model.py) - åŠ è½½è„šæœ¬
- [emoji_handling_analysis.md](../emoji_handling_analysis.md) - Emoji å¤„ç†åˆ†æ

**æœ€åæ›´æ–°**: 2025-01-16
