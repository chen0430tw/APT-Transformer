#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""æµ‹è¯•å°å‹APTæ¨¡å‹ï¼ˆåŒ…æ‹¬å‹ç¼©å’ŒDBCåŠ é€Ÿï¼‰"""

import sys
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import TensorDataset, DataLoader

# æ·»åŠ è·¯å¾„
sys.path.insert(0, '/home/user/APT-Transformer')

from apt_model.modeling.apt_model import (
    APTModel,
    APTModelConfiguration,
    DBCDAC_Optimizer,
    create_gradient_stabilizer_hook
)
from legacy_plugins.batch1.model_pruning_plugin import ModelPruningPlugin


def create_small_apt_config():
    """åˆ›å»ºå°å‹APTæ¨¡å‹é…ç½®"""
    config = APTModelConfiguration(
        vocab_size=1000,          # å°è¯æ±‡è¡¨
        d_model=128,              # å°ç»´åº¦
        max_seq_len=64,           # çŸ­åºåˆ—
        num_encoder_layers=2,     # 2å±‚ç¼–ç å™¨
        num_decoder_layers=2,     # 2å±‚è§£ç å™¨
        num_heads=4,              # 4ä¸ªæ³¨æ„åŠ›å¤´
        d_ff=512,                 # å°å‰é¦ˆç½‘ç»œ
        dropout=0.1,
        use_autopoietic=True,     # å¯ç”¨è‡ªç”Ÿæˆæœºåˆ¶
        use_dbc_dac=True,         # å¯ç”¨DBC-DAC
    )
    return config


def register_dbc_hooks(model):
    """ä¸ºæ¨¡å‹æ³¨å†ŒDBC-DAC hooks"""
    opt = DBCDAC_Optimizer()
    hooks = []
    for _, p in model.named_parameters():
        if p.requires_grad:
            hooks.append(p.register_hook(create_gradient_stabilizer_hook(opt)))
    return hooks


def create_dummy_data(batch_size=8, seq_len=32, vocab_size=1000):
    """åˆ›å»ºæ¨¡æ‹Ÿè®­ç»ƒæ•°æ®"""
    # è¾“å…¥åºåˆ—
    src = torch.randint(1, vocab_size, (batch_size, seq_len))
    # ç›®æ ‡åºåˆ—ï¼ˆç®€å•èµ·è§ï¼Œä½¿ç”¨è¾“å…¥çš„åç§»ç‰ˆæœ¬ï¼‰
    tgt = torch.randint(1, vocab_size, (batch_size, seq_len))

    return src, tgt


def test_apt_forward():
    """æµ‹è¯•APTæ¨¡å‹å‰å‘ä¼ æ’­"""
    print("\n" + "="*60)
    print("ğŸ§ª æµ‹è¯•APTæ¨¡å‹å‰å‘ä¼ æ’­")
    print("="*60)

    # åˆ›å»ºå°å‹APTæ¨¡å‹
    config = create_small_apt_config()
    model = APTModel(config)

    # ç»Ÿè®¡å‚æ•°
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)

    print(f"ğŸ“Š æ¨¡å‹é…ç½®:")
    print(f"   è¯æ±‡è¡¨å¤§å°: {config.vocab_size}")
    print(f"   æ¨¡å‹ç»´åº¦: {config.d_model}")
    print(f"   ç¼–ç å™¨å±‚æ•°: {config.num_encoder_layers}")
    print(f"   è§£ç å™¨å±‚æ•°: {config.num_decoder_layers}")
    print(f"   æ³¨æ„åŠ›å¤´æ•°: {config.num_heads}")
    print(f"   æ€»å‚æ•°é‡: {total_params:,}")
    print(f"   å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")

    # åˆ›å»ºè¾“å…¥
    batch_size, seq_len = 4, 32
    src, tgt = create_dummy_data(batch_size, seq_len, config.vocab_size)

    print(f"\nğŸ“ è¾“å…¥æ•°æ®:")
    print(f"   æºåºåˆ—å½¢çŠ¶: {src.shape}")
    print(f"   ç›®æ ‡åºåˆ—å½¢çŠ¶: {tgt.shape}")

    # å‰å‘ä¼ æ’­
    print("\nğŸ” æ‰§è¡Œå‰å‘ä¼ æ’­...")
    model.eval()
    with torch.no_grad():
        try:
            output = model(src, tgt)
            print(f"   è¾“å‡ºå½¢çŠ¶: {output.shape}")
            print(f"   è¾“å‡ºèŒƒå›´: [{output.min().item():.4f}, {output.max().item():.4f}]")
            print(f"   âœ… å‰å‘ä¼ æ’­æˆåŠŸ!")
            return True
        except Exception as e:
            print(f"   âŒ å‰å‘ä¼ æ’­å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return False


def test_apt_training():
    """æµ‹è¯•APTæ¨¡å‹è®­ç»ƒ"""
    print("\n" + "="*60)
    print("ğŸ§ª æµ‹è¯•APTæ¨¡å‹è®­ç»ƒ")
    print("="*60)

    # åˆ›å»ºæ¨¡å‹
    config = create_small_apt_config()
    model = APTModel(config)

    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = optim.Adam(model.parameters(), lr=1e-4)
    criterion = nn.CrossEntropyLoss(ignore_index=config.pad_token_id)

    # åˆ›å»ºæ•°æ®
    batch_size, seq_len = 8, 32
    src, tgt = create_dummy_data(batch_size, seq_len, config.vocab_size)

    print(f"ğŸ“Š è®­ç»ƒè®¾ç½®:")
    print(f"   ä¼˜åŒ–å™¨: Adam (lr=1e-4)")
    print(f"   æŸå¤±å‡½æ•°: CrossEntropyLoss")
    print(f"   Batch size: {batch_size}")

    # è®­ç»ƒå‡ æ­¥
    print("\nğŸƒ è¿è¡Œè®­ç»ƒæ­¥éª¤...")
    model.train()
    num_steps = 5

    for step in range(num_steps):
        optimizer.zero_grad()

        # å‰å‘ä¼ æ’­
        output = model(src, tgt[:, :-1])  # è§£ç å™¨è¾“å…¥å»æ‰æœ€åä¸€ä¸ªtoken

        # è®¡ç®—æŸå¤±
        loss = criterion(
            output.reshape(-1, config.vocab_size),
            tgt[:, 1:].reshape(-1)  # ç›®æ ‡å»æ‰ç¬¬ä¸€ä¸ªtoken
        )

        # åå‘ä¼ æ’­
        loss.backward()

        # è®¡ç®—æ¢¯åº¦èŒƒæ•°
        grad_norm = sum(p.grad.norm().item() for p in model.parameters() if p.grad is not None)

        # ä¼˜åŒ–
        optimizer.step()

        print(f"   Step {step+1}/{num_steps} | Loss: {loss.item():.4f} | Grad Norm: {grad_norm:.4f}")

    print(f"   âœ… è®­ç»ƒæµ‹è¯•æˆåŠŸ!")
    return True


def test_apt_with_dbc():
    """æµ‹è¯•APTæ¨¡å‹ + DBCåŠ é€Ÿ"""
    print("\n" + "="*60)
    print("ğŸ§ª æµ‹è¯•APTæ¨¡å‹ + DBC-DACåŠ é€Ÿ")
    print("="*60)

    # åˆ›å»ºä¸¤ä¸ªç›¸åŒçš„æ¨¡å‹
    config = create_small_apt_config()
    model_without_dbc = APTModel(config)
    model_with_dbc = APTModel(config)

    # ç¡®ä¿å‚æ•°ç›¸åŒ
    model_with_dbc.load_state_dict(model_without_dbc.state_dict())

    # åªç»™ä¸€ä¸ªæ¨¡å‹æ³¨å†ŒDBC hooks
    print("ğŸ”§ ä¸ºç¬¬äºŒä¸ªæ¨¡å‹æ³¨å†ŒDBC hooks...")
    hooks = register_dbc_hooks(model_with_dbc)
    print(f"   æ³¨å†Œäº† {len(hooks)} ä¸ªhooks")

    # åˆ›å»ºä¼˜åŒ–å™¨
    optimizer1 = optim.Adam(model_without_dbc.parameters(), lr=1e-4)
    optimizer2 = optim.Adam(model_with_dbc.parameters(), lr=1e-4)
    criterion = nn.CrossEntropyLoss(ignore_index=config.pad_token_id)

    # åˆ›å»ºæ•°æ®ï¼ˆä½¿ç”¨è¾ƒå¤§çš„å€¼æ¥æµ‹è¯•ç¨³å®šæ€§ï¼‰
    batch_size, seq_len = 8, 32
    src, tgt = create_dummy_data(batch_size, seq_len, config.vocab_size)

    print(f"\nğŸ“Š å¯¹æ¯”æµ‹è¯• (5æ­¥è®­ç»ƒ):")
    print(f"   æ¨¡å‹1: æ— DBCåŠ é€Ÿ")
    print(f"   æ¨¡å‹2: æœ‰DBCåŠ é€Ÿ")

    # è®­ç»ƒå¯¹æ¯”
    print("\nğŸƒ å¼€å§‹è®­ç»ƒ...")
    num_steps = 5

    for step in range(num_steps):
        # æ— DBCæ¨¡å‹
        optimizer1.zero_grad()
        model_without_dbc.train()
        output1 = model_without_dbc(src, tgt[:, :-1])
        loss1 = criterion(output1.reshape(-1, config.vocab_size), tgt[:, 1:].reshape(-1))
        loss1.backward()
        grad_norm1 = sum(p.grad.norm().item() for p in model_without_dbc.parameters() if p.grad is not None)
        optimizer1.step()

        # æœ‰DBCæ¨¡å‹
        optimizer2.zero_grad()
        model_with_dbc.train()
        output2 = model_with_dbc(src, tgt[:, :-1])
        loss2 = criterion(output2.reshape(-1, config.vocab_size), tgt[:, 1:].reshape(-1))
        loss2.backward()
        grad_norm2 = sum(p.grad.norm().item() for p in model_with_dbc.parameters() if p.grad is not None)
        optimizer2.step()

        print(f"\n   Step {step+1}/{num_steps}:")
        print(f"     æ— DBC: Loss={loss1.item():.4f}, Grad={grad_norm1:.4f}")
        print(f"     æœ‰DBC: Loss={loss2.item():.4f}, Grad={grad_norm2:.4f}")
        print(f"     æ¢¯åº¦å·®å¼‚: {abs(grad_norm1 - grad_norm2):.4f}")

    print(f"\n   âœ… DBCåŠ é€Ÿæµ‹è¯•å®Œæˆ!")
    return True


def test_apt_with_pruning():
    """æµ‹è¯•APTæ¨¡å‹ + å‰ªæå‹ç¼©"""
    print("\n" + "="*60)
    print("ğŸ§ª æµ‹è¯•APTæ¨¡å‹ + å‰ªæå‹ç¼©")
    print("="*60)

    # åˆ›å»ºæ¨¡å‹
    config = create_small_apt_config()
    model = APTModel(config)

    # ç»Ÿè®¡åŸå§‹å‚æ•°
    original_params = sum(p.numel() for p in model.parameters())
    print(f"ğŸ“Š åŸå§‹æ¨¡å‹å‚æ•°: {original_params:,}")

    # åˆ›å»ºå‰ªææ’ä»¶
    prune_config = {
        'prune_ratio': 0.2,  # å‰ªæ20%
        'prune_type': 'magnitude',
        'structured': False,
    }
    plugin = ModelPruningPlugin(prune_config)

    # åº”ç”¨å‰ªæ
    print(f"\nâœ‚ï¸ åº”ç”¨æƒé‡å¤§å°å‰ªæ (20%)...")
    model = plugin.magnitude_pruning(model, prune_ratio=0.2, structured=False)

    # è·å–å‰ªæç»Ÿè®¡
    stats = plugin.get_pruning_statistics(model)
    print(f"\nğŸ“ˆ å‰ªæåç»Ÿè®¡:")
    print(f"   æ€»å‚æ•°: {stats['total_params']:,}")
    print(f"   å‰ªæå‚æ•°: {stats['pruned_params']:,}")
    print(f"   å‰©ä½™å‚æ•°: {stats['remaining_params']:,}")
    print(f"   ç¨€ç–åº¦: {stats['sparsity']*100:.2f}%")

    # æµ‹è¯•å‰ªæåçš„æ¨¡å‹æ˜¯å¦ä»èƒ½å·¥ä½œ
    print(f"\nğŸ” æµ‹è¯•å‰ªæåæ¨¡å‹...")
    batch_size, seq_len = 4, 32
    src, tgt = create_dummy_data(batch_size, seq_len, config.vocab_size)

    model.eval()
    with torch.no_grad():
        try:
            output = model(src, tgt)
            print(f"   è¾“å‡ºå½¢çŠ¶: {output.shape}")
            print(f"   âœ… å‰ªæåæ¨¡å‹ä»å¯æ­£å¸¸å·¥ä½œ!")
        except Exception as e:
            print(f"   âŒ å‰ªæåæ¨¡å‹å¤±è´¥: {e}")
            return False

    return True


def test_full_pipeline():
    """æµ‹è¯•å®Œæ•´æµç¨‹ï¼šè®­ç»ƒ -> å‰ªæ -> DBCåŠ é€Ÿ"""
    print("\n" + "="*60)
    print("ğŸ§ª æµ‹è¯•å®Œæ•´æµç¨‹ (è®­ç»ƒ -> å‰ªæ -> DBC)")
    print("="*60)

    # 1. åˆ›å»ºå¹¶è®­ç»ƒæ¨¡å‹
    print("\nğŸ“ é˜¶æ®µ1: åˆå§‹è®­ç»ƒ")
    config = create_small_apt_config()
    model = APTModel(config)
    optimizer = optim.Adam(model.parameters(), lr=1e-4)
    criterion = nn.CrossEntropyLoss(ignore_index=config.pad_token_id)

    batch_size, seq_len = 8, 32
    src, tgt = create_dummy_data(batch_size, seq_len, config.vocab_size)

    # è®­ç»ƒ3æ­¥
    model.train()
    for step in range(3):
        optimizer.zero_grad()
        output = model(src, tgt[:, :-1])
        loss = criterion(output.reshape(-1, config.vocab_size), tgt[:, 1:].reshape(-1))
        loss.backward()
        optimizer.step()
        print(f"   è®­ç»ƒæ­¥éª¤ {step+1}/3: Loss={loss.item():.4f}")

    # 2. åº”ç”¨å‰ªæ
    print("\nğŸ“ é˜¶æ®µ2: æ¨¡å‹å‰ªæ")
    prune_config = {'prune_ratio': 0.3, 'prune_type': 'magnitude', 'structured': False}
    plugin = ModelPruningPlugin(prune_config)
    model = plugin.magnitude_pruning(model, prune_ratio=0.3, structured=False)

    stats = plugin.get_pruning_statistics(model)
    print(f"   å‰ªæåç¨€ç–åº¦: {stats['sparsity']*100:.2f}%")

    # 3. ä½¿ç”¨DBCç»§ç»­è®­ç»ƒ
    print("\nğŸ“ é˜¶æ®µ3: DBCåŠ é€Ÿå¾®è°ƒ")
    hooks = register_dbc_hooks(model)
    print(f"   æ³¨å†Œäº† {len(hooks)} ä¸ªDBC hooks")

    optimizer = optim.Adam(model.parameters(), lr=5e-5)  # é™ä½å­¦ä¹ ç‡è¿›è¡Œå¾®è°ƒ

    model.train()
    for step in range(3):
        optimizer.zero_grad()
        output = model(src, tgt[:, :-1])
        loss = criterion(output.reshape(-1, config.vocab_size), tgt[:, 1:].reshape(-1))
        loss.backward()
        grad_norm = sum(p.grad.norm().item() for p in model.parameters() if p.grad is not None)
        optimizer.step()
        print(f"   å¾®è°ƒæ­¥éª¤ {step+1}/3: Loss={loss.item():.4f}, Grad={grad_norm:.4f}")

    print(f"\n   âœ… å®Œæ•´æµç¨‹æµ‹è¯•æˆåŠŸ!")
    return True


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("\nğŸš€ å¼€å§‹æµ‹è¯•å°å‹APTæ¨¡å‹...")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")
    print(f"è®¾å¤‡: {'CUDA' if torch.cuda.is_available() else 'CPU'}")

    try:
        # æµ‹è¯•å‰å‘ä¼ æ’­
        test1 = test_apt_forward()

        # æµ‹è¯•è®­ç»ƒ
        test2 = test_apt_training()

        # æµ‹è¯•DBCåŠ é€Ÿ
        test3 = test_apt_with_dbc()

        # æµ‹è¯•å‰ªæ
        test4 = test_apt_with_pruning()

        # æµ‹è¯•å®Œæ•´æµç¨‹
        test5 = test_full_pipeline()

        # æ€»ç»“
        print("\n" + "="*60)
        print("ğŸ“ æµ‹è¯•æ€»ç»“")
        print("="*60)
        print(f"âœ… APTå‰å‘ä¼ æ’­: {'é€šè¿‡' if test1 else 'å¤±è´¥'}")
        print(f"âœ… APTæ¨¡å‹è®­ç»ƒ: {'é€šè¿‡' if test2 else 'å¤±è´¥'}")
        print(f"âœ… DBCåŠ é€Ÿè®­ç»ƒ: {'é€šè¿‡' if test3 else 'å¤±è´¥'}")
        print(f"âœ… æ¨¡å‹å‰ªæå‹ç¼©: {'é€šè¿‡' if test4 else 'å¤±è´¥'}")
        print(f"âœ… å®Œæ•´æµç¨‹é›†æˆ: {'é€šè¿‡' if test5 else 'å¤±è´¥'}")

        if all([test1, test2, test3, test4, test5]):
            print("\nğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡!")
            print("\nğŸ’¡ APTæ¨¡å‹ç‰¹æ€§:")
            print("   âœ… è‡ªç”Ÿæˆæ³¨æ„åŠ›æœºåˆ¶ (Autopoietic Attention)")
            print("   âœ… DBC-DACæ¢¯åº¦ç¨³å®š")
            print("   âœ… æ”¯æŒæ¨¡å‹å‰ªæå‹ç¼©")
            print("   âœ… ç¼–ç å™¨-è§£ç å™¨æ¶æ„")
            print("   âœ… å¯æ‰©å±•çš„é…ç½®ç³»ç»Ÿ")
            return 0
        else:
            print("\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥")
            return 1

    except Exception as e:
        print(f"\nâŒ æµ‹è¯•å‡ºé”™: {e}")
        import traceback
        traceback.print_exc()
        return 1


if __name__ == "__main__":
    exit(main())
