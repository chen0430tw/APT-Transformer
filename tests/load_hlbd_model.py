#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
åŠ è½½å·²ä¿å­˜çš„ HLBD æ¨¡å‹å¹¶è¿›è¡Œæ¨ç†

ä½¿ç”¨æ–¹æ³•:
    python tests/load_hlbd_model.py [æ¨¡å‹è·¯å¾„]

å¦‚æœä¸æä¾›æ¨¡å‹è·¯å¾„ï¼Œä¼šè‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°ä¿å­˜çš„æ¨¡å‹ã€‚
"""

import sys
import os
import torch
import glob

# æ·»åŠ è·¯å¾„
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, project_root)

from test_hlbd_quick_learning import (
    load_model_and_tokenizer,
    generate_text,
    test_generation,
)


def find_latest_model(save_dir):
    """æŸ¥æ‰¾æœ€æ–°ä¿å­˜çš„æ¨¡å‹"""
    pattern = os.path.join(save_dir, 'hlbd_model_*.pt')
    model_files = glob.glob(pattern)

    if not model_files:
        return None

    # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œè¿”å›æœ€æ–°çš„
    latest = max(model_files, key=os.path.getmtime)
    return latest


def main():
    """ä¸»å‡½æ•°"""
    print("\nğŸ” HLBD æ¨¡å‹åŠ è½½å™¨")
    print("="*70)

    # è‡ªåŠ¨æ£€æµ‹è®¾å¤‡
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"è®¾å¤‡: {device}")

    # ç¡®å®šæ¨¡å‹è·¯å¾„
    if len(sys.argv) > 1:
        model_path = sys.argv[1]
        print(f"\nä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹: {model_path}")
    else:
        # è‡ªåŠ¨æŸ¥æ‰¾æœ€æ–°æ¨¡å‹
        save_dir = os.path.join(project_root, 'tests', 'saved_models')
        model_path = find_latest_model(save_dir)

        if model_path is None:
            print(f"\nâŒ æœªæ‰¾åˆ°å·²ä¿å­˜çš„æ¨¡å‹ï¼")
            print(f"   ä¿å­˜ç›®å½•: {save_dir}")
            print(f"\nè¯·å…ˆè¿è¡Œè®­ç»ƒè„šæœ¬:")
            print(f"   python tests/test_hlbd_quick_learning.py")
            return 1

        print(f"\nä½¿ç”¨æœ€æ–°çš„æ¨¡å‹: {os.path.basename(model_path)}")

    # åŠ è½½æ¨¡å‹
    try:
        model, tokenizer, training_info = load_model_and_tokenizer(model_path, device)
    except Exception as e:
        print(f"\nâŒ åŠ è½½æ¨¡å‹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 1

    # æµ‹è¯•ç”¨ä¾‹
    print("\n" + "="*70)
    print("ğŸ§ª å¼€å§‹æ¨ç†æµ‹è¯•")
    print("="*70)

    test_cases = [
        # Emoji æµ‹è¯•
        ("ğŸŒ§ï¸", "ä¸‹é›¨"),
        ("â¤ï¸", "æˆ‘çˆ±ä½ "),
        ("ğŸ½ï¸", "åƒé¥­"),
        ("ğŸ“–", "çœ‹ä¹¦"),
        ("ğŸ˜´", "ç¡è§‰"),

        # è‹±æ–‡æµ‹è¯•
        ("I love you", "æˆ‘çˆ±ä½ "),
        ("It's raining", "ä¸‹é›¨"),
        ("Read a book", "çœ‹ä¹¦"),

        # æ‹¼éŸ³æµ‹è¯•
        ("wÇ’ Ã i nÇ", "æˆ‘çˆ±ä½ "),
        ("xiÃ  yÇ”", "ä¸‹é›¨"),
        ("chÄ« fÃ n", "åƒé¥­"),

        # ä¸­æ–‡æµ‹è¯•
        ("ä¸‹é›¨", "å¤©æ°”"),
        ("æˆ‘çˆ±ä½ ", "æƒ…æ„Ÿ"),
        ("åƒé¥­", "ç”Ÿæ´»"),
    ]

    test_generation(model, tokenizer, test_cases, device)

    # äº¤äº’å¼æ¨ç†
    print("\n" + "="*70)
    print("ğŸ’¬ äº¤äº’å¼æ¨ç†æ¨¡å¼ (è¾“å…¥ 'quit' é€€å‡º)")
    print("="*70)

    while True:
        try:
            user_input = input("\nè¯·è¾“å…¥æ–‡æœ¬: ").strip()

            if user_input.lower() in ['quit', 'exit', 'q']:
                print("\nğŸ‘‹ å†è§ï¼")
                break

            if not user_input:
                continue

            # ç”Ÿæˆ
            generated = generate_text(model, tokenizer, user_input, device)
            print(f"ç”Ÿæˆ: {generated}")

        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ å†è§ï¼")
            break
        except Exception as e:
            print(f"âŒ é”™è¯¯: {e}")

    return 0


if __name__ == "__main__":
    exit(main())
