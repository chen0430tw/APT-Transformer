#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""å¿«é€Ÿæµ‹è¯•HLBDè®­ç»ƒ - çœ‹APTæ¨¡å‹èƒ½å¦å¿«é€Ÿå­¦ä¼šè¯´è¯"""

import sys
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import json
from transformers import BertTokenizer

# æ·»åŠ è·¯å¾„ï¼ˆåŠ¨æ€è®¡ç®—é¡¹ç›®æ ¹ç›®å½•ï¼‰
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, project_root)

from apt_model.modeling.apt_model import (
    APTModel,
    APTModelConfiguration,
    DBCDAC_Optimizer,
    create_gradient_stabilizer_hook
)


class SimpleCharTokenizer_BACKUP:
    """ç®€å•çš„å­—ç¬¦çº§åˆ†è¯å™¨"""
    def __init__(self):
        # åˆ›å»ºä¸€ä¸ªåŸºç¡€å­—ç¬¦è¡¨ï¼ˆåŒ…æ‹¬ä¸­æ–‡ã€è‹±æ–‡ã€emojiç­‰ï¼‰
        self.vocab = {
            '[PAD]': 0, '[UNK]': 1, '[BOS]': 2, '[EOS]': 3,
        }
        self.pad_token_id = 0
        self.unk_token_id = 1
        self.bos_token_id = 2
        self.eos_token_id = 3
        self.vocab_size = 5000  # é¢„ç•™è¶³å¤Ÿçš„è¯æ±‡ç©ºé—´

        # æ·»åŠ å¸¸ç”¨å­—ç¬¦
        self.char_to_id = self.vocab.copy()
        self.id_to_char = {v: k for k, v in self.vocab.items()}
        self.next_id = 4

    def _get_or_add_char(self, char):
        """è·å–å­—ç¬¦IDï¼Œå¦‚æœä¸å­˜åœ¨åˆ™æ·»åŠ """
        if char not in self.char_to_id:
            if self.next_id < self.vocab_size:
                self.char_to_id[char] = self.next_id
                self.id_to_char[self.next_id] = char
                self.next_id += 1
            else:
                return self.unk_token_id
        return self.char_to_id[char]

    def encode(self, text, return_tensors=None):
        """ç¼–ç æ–‡æœ¬ä¸ºIDåºåˆ—"""
        ids = [self.bos_token_id]
        for char in text:
            ids.append(self._get_or_add_char(char))
        ids.append(self.eos_token_id)

        if return_tensors == 'pt':
            return torch.tensor([ids])
        return ids

    def __call__(self, text, max_length=64, padding='max_length',
                 truncation=True, return_tensors='pt'):
        """åˆ†è¯æ¥å£ï¼ˆå…¼å®¹transformersï¼‰"""
        ids = []
        for char in text:
            ids.append(self._get_or_add_char(char))

        # æˆªæ–­
        if truncation and len(ids) > max_length:
            ids = ids[:max_length]

        # å¡«å……
        if padding == 'max_length':
            while len(ids) < max_length:
                ids.append(self.pad_token_id)

        if return_tensors == 'pt':
            return {'input_ids': torch.tensor([ids])}
        return {'input_ids': ids}

    def decode(self, ids, skip_special_tokens=True):
        """è§£ç IDåºåˆ—ä¸ºæ–‡æœ¬"""
        chars = []
        for id in ids:
            if isinstance(id, torch.Tensor):
                id = id.item()

            if skip_special_tokens and id in [self.pad_token_id, self.bos_token_id,
                                               self.eos_token_id, self.unk_token_id]:
                continue

            char = self.id_to_char.get(id, '[UNK]')
            chars.append(char)

        return ''.join(chars)


def register_dbc_hooks(model):
    """ä¸ºæ¨¡å‹æ³¨å†ŒDBC-DAC hooks"""
    opt = DBCDAC_Optimizer()
    hooks = []
    for _, p in model.named_parameters():
        if p.requires_grad:
            hooks.append(p.register_hook(create_gradient_stabilizer_hook(opt)))
    return hooks


def load_hlbd_samples(data_path, max_samples=20):
    """åŠ è½½HLBDæ•°æ®æ ·æœ¬"""
    print(f"ğŸ“‚ åŠ è½½HLBDæ•°æ®: {data_path}")

    with open(data_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # æå–sampleséƒ¨åˆ†
    start_idx = content.find('samples = [')
    if start_idx == -1:
        raise ValueError("æ‰¾ä¸åˆ°samplesæ•°æ®")

    # æå–JSONæ•°ç»„
    json_start = content.find('[', start_idx)
    # æ‰¾åˆ°åŒ¹é…çš„å³æ‹¬å·ï¼ˆç®€å•å¤„ç†ï¼‰
    bracket_count = 0
    json_end = json_start
    for i, char in enumerate(content[json_start:]):
        if char == '[':
            bracket_count += 1
        elif char == ']':
            bracket_count -= 1
            if bracket_count == 0:
                json_end = json_start + i + 1
                break

    json_str = content[json_start:json_end]
    samples = json.loads(json_str)

    if max_samples:
        samples = samples[:max_samples]

    print(f"   åŠ è½½äº† {len(samples)} ä¸ªæ ·æœ¬")
    return samples


def create_training_pairs(samples):
    """ä»HLBDæ ·æœ¬åˆ›å»ºè®­ç»ƒå¯¹"""
    pairs = []

    for sample in samples:
        concept = sample['concept']

        # åˆ›å»ºå¤šç§è®­ç»ƒå¯¹
        # 1. emoji -> ä¸­æ–‡
        if 'level_1' in sample and 'level_6' in sample:
            emoji = sample['level_1'].get('emoji', '')
            chinese = sample['level_6'].get('ä¸­æ–‡', '')
            if emoji and chinese:
                pairs.append((emoji, chinese))

        # 2. çŸ­è¯­ -> ä¸­æ–‡
        if 'level_2' in sample and 'level_6' in sample:
            phrase = sample['level_2'].get('çŸ­è¯­', '')
            chinese = sample['level_6'].get('ä¸­æ–‡', '')
            if phrase and chinese:
                pairs.append((phrase, chinese))

        # 3. è‹±æ–‡ -> ä¸­æ–‡
        if 'level_5' in sample and 'level_6' in sample:
            english = sample['level_5'].get('è‹±æ–‡', '')
            chinese = sample['level_6'].get('ä¸­æ–‡', '')
            if english and chinese:
                pairs.append((english, chinese))

        # 4. æ‹¼éŸ³ -> ä¸­æ–‡
        if 'level_4' in sample and 'level_6' in sample:
            pinyin = sample['level_4'].get('æ‹¼éŸ³', '')
            chinese = sample['level_6'].get('ä¸­æ–‡', '')
            if pinyin and chinese:
                pairs.append((pinyin, chinese))

    print(f"   åˆ›å»ºäº† {len(pairs)} ä¸ªè®­ç»ƒå¯¹")
    return pairs


class SimpleDialogueDataset(Dataset):
    """ç®€å•å¯¹è¯æ•°æ®é›†"""
    def __init__(self, pairs, tokenizer, max_length=64):
        self.pairs = pairs
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        src_text, tgt_text = self.pairs[idx]

        # ç¼–ç æºæ–‡æœ¬
        src_encoding = self.tokenizer(
            src_text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        # ç¼–ç ç›®æ ‡æ–‡æœ¬
        tgt_encoding = self.tokenizer(
            tgt_text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        return (
            src_encoding['input_ids'].squeeze(0),
            tgt_encoding['input_ids'].squeeze(0)
        )


def create_small_hlbd_config(vocab_size):
    """åˆ›å»ºå°å‹HLBD APTé…ç½®"""
    config = APTModelConfiguration(
        vocab_size=vocab_size,
        d_model=256,              # ä¸­ç­‰ç»´åº¦
        max_seq_len=64,           # çŸ­åºåˆ—
        num_encoder_layers=3,     # 3å±‚ç¼–ç å™¨
        num_decoder_layers=3,     # 3å±‚è§£ç å™¨
        num_heads=8,              # 8ä¸ªæ³¨æ„åŠ›å¤´
        d_ff=1024,                # å‰é¦ˆç½‘ç»œ
        dropout=0.1,
        use_autopoietic=True,     # å¯ç”¨è‡ªç”Ÿæˆæœºåˆ¶
        use_dbc_dac=True,         # å¯ç”¨DBC-DAC
    )
    return config


from tqdm import tqdm # ç¡®ä¿æ–‡ä»¶å¼€å¤´å¯¼å…¥äº† tqdm

def train_epoch(model, dataloader, optimizer, criterion, device, use_dbc=False, accumulation_steps=4): # <--- ã€å…³é”®ä¿®æ”¹ 1ã€‘æ¥æ”¶ accumulation_steps
    """è®­ç»ƒä¸€ä¸ªepoch (ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯)"""
    model.train()
    total_loss = 0
    total_steps = 0
    
    ACCUMULATION_STEPS = accumulation_steps # ã€å…³é”®ä¿®æ­£ã€‘åœ¨å‡½æ•°ä½“å†…å®šä¹‰ ACCUMULATION_STEPS
    
    progress_bar = tqdm(
        dataloader, 
        desc="Training", 
        leave=False, 
        mininterval=0.1, 
        ascii=True
    )

    # ã€å…³é”®ä¿®æ”¹ 2ã€‘ä½¿ç”¨ enumerate æ¥è·å–æ‰¹æ¬¡ç´¢å¼• i
    for i, (src_ids, tgt_ids) in enumerate(progress_bar): 
        
        src_ids = src_ids.to(device)
        tgt_ids = tgt_ids.to(device)

        # å‰å‘ä¼ æ’­
        output = model(src_ids, tgt_ids[:, :-1])

        # è®¡ç®—æŸå¤±
        loss = criterion(
            output.reshape(-1, output.size(-1)),
            tgt_ids[:, 1:].reshape(-1)
        )

        # æŸå¤±å½’ä¸€åŒ– (Loss Scaling)
        loss = loss / ACCUMULATION_STEPS 

        # åå‘ä¼ æ’­ (ä¸æ¸…é™¤æ¢¯åº¦)
        loss.backward()

        # æ¡ä»¶ä¼˜åŒ–å’Œæ¸…é›¶ (æ¯ N æ­¥æ‰§è¡Œä¸€æ¬¡)
        if (i + 1) % ACCUMULATION_STEPS == 0:
            # æƒé‡æ›´æ–° (å³ä½¿ DBC æ¿€æ´»ï¼Œä¿ç•™è£å‰ªä¹Ÿæ— å®³)
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            optimizer.zero_grad() # æ¸…é™¤ç´¯ç§¯çš„æ¢¯åº¦
            
        total_loss += loss.item() * ACCUMULATION_STEPS # æ¢å¤å®é™… Loss
        total_steps += 1
        
        # å®æ—¶æ›´æ–°è¿›åº¦æ¡ä¸Šçš„ Loss
        progress_bar.set_postfix({'loss': f'{loss.item() * ACCUMULATION_STEPS:.4f}'})
        
    # ã€æœ€åä¸€æ­¥æ¸…ç†ã€‘å¤„ç†å‰©ä½™çš„ç´¯ç§¯æ¢¯åº¦ (i éœ€è¦åœ¨å¾ªç¯å¤–å¯ç”¨)
    # Note: ç¡®ä¿ i åœ¨å¾ªç¯å¤–èƒ½è®¿é—®åˆ°ï¼Œå°½ç®¡è¿™ä¸æ˜¯æ ‡å‡† Python åšæ³•
    try:
        if (i + 1) % ACCUMULATION_STEPS != 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            optimizer.zero_grad()
    except NameError:
         # å¦‚æœ i æ²¡å®šä¹‰ (æ¯”å¦‚ dataloader æ˜¯ç©ºçš„)ï¼Œåˆ™å¿½ç•¥æ¸…ç†
         pass 

    return (total_loss / total_steps) if total_steps > 0 else 0


def generate_text(model, tokenizer, input_text, device, max_length=50, repetition_penalty=1.5):
    """
    ç”Ÿæˆæ–‡æœ¬ (å·²ä¿®å¤ä¸ºè°ƒç”¨æ¨¡å‹å†…ç½®çš„ generate æ–¹æ³•ï¼Œæ¿€æ´»æ‰€æœ‰é«˜çº§å‚æ•°)
    """
    model.eval()

    # 1. è·å– BOS/PAD ID
    bos_id = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.cls_token_id
    pad_id = tokenizer.pad_token_id
    
    # 2. ç¼–ç è¾“å…¥æ–‡æœ¬ (ä½œä¸º Prompt)
    # ç¼–ç æ—¶ï¼Œä¸åŠ ç‰¹æ®Š tokensï¼Œåªç¼–ç åŸå§‹æ–‡æœ¬
    input_encoding = tokenizer.encode(input_text, return_tensors='pt', add_special_tokens=False).to(device)
    
    # 3. å‡†å¤‡æ¨¡å‹è¾“å…¥ input_ids = [BOS] + Prompt Tokens
    bos_tensor = torch.tensor([[bos_id]], device=device)
    # å°† BOS æ ‡è®°å’Œ Prompt æ‹¼æ¥æˆå®Œæ•´çš„è¾“å…¥åºåˆ—
    initial_ids = torch.cat([bos_tensor, input_encoding], dim=1)
    
    # 4. è°ƒç”¨ APTModel è‡ªèº«çš„ generate æ–¹æ³• (æ ¸å¿ƒæ›¿æ¢)
    generated_ids = model.generate(
        input_ids=initial_ids,
        # max_length å¿…é¡»åŠ ä¸Šåˆå§‹ sequence çš„é•¿åº¦
        max_length=max_length + initial_ids.size(1), 
        repetition_penalty=repetition_penalty,
        do_sample=True, # å¯ç”¨é‡‡æ · (Top-K/Top-P)
        num_beams=1, # ä¿æŒ beam search æ•°é‡ (æ³¨æ„ï¼Œæ¨¡å‹å†…éƒ¨ä¼šå¿½ç•¥ num_beams > 1)
        pad_token_id=pad_id 
        # å®Œæ•´çš„ Top-K/Top-P/Temperature å‚æ•°åº”è¯¥åœ¨ model.generate å†…éƒ¨é…ç½®
    )
    
    # 5. è§£ç 
    # generated_ids[0] æ˜¯ç¬¬ä¸€ä¸ª batch çš„è¾“å‡ºåºåˆ—
    generated_text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)
    
    return generated_text


def test_generation(model, tokenizer, test_cases, device):
    """æµ‹è¯•ç”Ÿæˆèƒ½åŠ›"""
    print("\n" + "="*60)
    print("ğŸ—£ï¸ æµ‹è¯•å¯¹è¯ç”Ÿæˆèƒ½åŠ›")
    print("="*60)

    # è®¾å®šå¼ºåŠ›é‡å¤æƒ©ç½šå› å­
    REPETITION_FACTOR = 1.5

    for input_text, expected_concept in test_cases:
        generated = generate_text(model, tokenizer, input_text, device, repetition_penalty=REPETITION_FACTOR)
        print(f"\nè¾“å…¥: {input_text}")
        print(f"æœŸæœ›æ¦‚å¿µ: {expected_concept}")
        print(f"ç”Ÿæˆ: {generated}")


def main():
    """ä¸»å‡½æ•°"""
    print("\nğŸš€ HLBDå¿«é€Ÿå­¦ä¹ æµ‹è¯• - APTæ¨¡å‹èƒ½å¦å¿«é€Ÿå­¦ä¼šè¯´è¯?")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")

    ACCUMULATION_STEPS = 8  # æ¨¡æ‹Ÿ 4 * 8 = 32 çš„æœ‰æ•ˆæ‰¹æ¬¡å¤§å°

    # è‡ªåŠ¨æ£€æµ‹ï¼šæœ‰æ˜¾å¡å°±ç”¨æ˜¾å¡ï¼Œæ²¡æœ‰æ‰ç”¨ CPU
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"è®¾å¤‡: {device}")

    # 1. åŠ è½½HLBDæ•°æ®
    current_dir = os.path.dirname(os.path.abspath(__file__)) # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½• (tests)
    project_root = os.path.dirname(current_dir)              # è·å–é¡¹ç›®æ ¹ç›®å½• (APT-Transformer)
    data_path = os.path.join(project_root, 'apt_model', 'åˆ†å±‚è¯­è¨€å¯è’™æ•°æ®é›†.txt')
    samples = load_hlbd_samples(data_path, max_samples=None)

    # é¡ºä¾¿æŠŠä¸‹é¢é‚£ä¸ª BERT çš„è·¯å¾„ä¹Ÿä¸€èµ·ä¿®äº†ï¼Œä¸ç„¶ç­‰ä¼šè¿˜ä¼šæŠ¥é”™
    bert_path = os.path.join(project_root, 'bert', 'bert-base-chinese')

    # æ˜¾ç¤ºå‡ ä¸ªæ ·æœ¬
    print(f"\nğŸ“ æ ·æœ¬ç¤ºä¾‹:")
    for i, sample in enumerate(samples[:3]):
        print(f"\n   æ ·æœ¬ {i+1}: {sample['concept']}")
        print(f"      Emoji: {sample['level_1'].get('emoji', 'N/A')}")
        print(f"      ä¸­æ–‡: {sample['level_6'].get('ä¸­æ–‡', 'N/A')[:30]}...")

    # 2. åˆ›å»ºè®­ç»ƒå¯¹
    training_pairs = create_training_pairs(samples)

    # 3. å‡†å¤‡åˆ†è¯å™¨
    print(f"\nğŸ”§ å‡†å¤‡åˆ†è¯å™¨...")
    # ä½¿ç”¨æœ¬åœ°çš„bert-base-chinese tokenizer
    tokenizer = BertTokenizer.from_pretrained(
        bert_path,
        local_files_only=True,  # <-- å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ–‡ä»¶ï¼Œç¦æ­¢è”ç½‘
        vocab_file=os.path.join(bert_path, 'vocab.txt') # <-- æ˜¾å¼æŒ‡å®šè¯è¡¨ä½ç½®
    ) # ä½¿ç”¨ä¸Šé¢è®¡ç®—å¥½çš„è·¯å¾„
    print(f"   ä½¿ç”¨çš„åˆ†è¯å™¨: {type(tokenizer).__name__}")
    print(f"   è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")

    # 4. åˆ›å»ºæ•°æ®é›†
    print(f"\nğŸ“Š åˆ›å»ºæ•°æ®é›†...")
    dataset = SimpleDialogueDataset(training_pairs, tokenizer)
    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)
    print(f"   è®­ç»ƒæ‰¹æ¬¡æ•°: {len(dataloader)}")

    # ã€æ–°å¢éªŒè¯ä»£ç ï¼šæ£€æŸ¥å®é™…æ ·æœ¬æ•°ã€‘
    actual_pairs = len(dataset)
    print(f"--- é•¿åº¦éªŒè¯ ---")
    print(f"æ¨¡å‹å®é™…çœ‹åˆ°çš„è®­ç»ƒå¯¹æ•°é‡: {actual_pairs} (åº”ä¸º 80 æˆ–æ›´å¤š)")
    print(f"----------------")

    # 5. åˆ›å»ºæ¨¡å‹
    print(f"\nğŸ—ï¸ åˆ›å»ºAPTæ¨¡å‹...")
    config = create_small_hlbd_config(tokenizer.vocab_size)
    model = APTModel(config).to(device)

    total_params = sum(p.numel() for p in model.parameters())
    print(f"   æ¨¡å‹å‚æ•°: {total_params:,}")
    print(f"   é…ç½®: d_model={config.d_model}, layers={config.num_encoder_layers}")

    # 6. åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = optim.Adam(model.parameters(), lr=5e-5)
    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)

    # 7. æ³¨å†ŒDBC hooks
    print(f"\nâš¡ æ³¨å†ŒDBC-DACåŠ é€Ÿ...")
    #hooks = register_dbc_hooks(model)
    hooks = [] # ä¿æŒ hooks å˜é‡å­˜åœ¨ï¼Œé˜²æ­¢åé¢æŠ¥é”™
    #print(f"   æ³¨å†Œäº† {len(hooks)} ä¸ªæ¢¯åº¦ç¨³å®šé’©å­")

    # 8. è®­ç»ƒæ¨¡å‹
    print(f"\n" + "="*60)
    print("ğŸƒ å¼€å§‹å¿«é€Ÿè®­ç»ƒ (çœ‹èƒ½å¦å¿«é€Ÿå­¦ä¼šè¯´è¯)")
    print("="*60)

    num_epochs = 500  # åªè®­ç»ƒ10ä¸ªepoch

    for epoch in range(num_epochs):
        loss = train_epoch(model, dataloader, optimizer, criterion, device, use_dbc=True, accumulation_steps=ACCUMULATION_STEPS)
        print(f"Epoch {epoch+1}/{num_epochs} - Loss: {loss:.4f}")

        # æ¯3ä¸ªepochæµ‹è¯•ä¸€æ¬¡
        if (epoch + 1) % 3 == 0 or epoch == num_epochs - 1:
            test_cases = [
                ("ğŸŒ§ï¸", "ä¸‹é›¨"),
                ("â¤ï¸", "æˆ‘çˆ±ä½ "),
                ("I love you", "æˆ‘çˆ±ä½ "),
                ("ä¸‹é›¨", "å¤©æ°”"),
            ]
            test_generation(model, tokenizer, test_cases, device)

    # 9. æœ€ç»ˆæµ‹è¯•
    print(f"\n" + "="*60)
    print("ğŸ¯ æœ€ç»ˆæµ‹è¯• - APTå­¦ä¼šè¯´è¯äº†å—?")
    print("="*60)

    final_test_cases = [
        ("ğŸŒ§ï¸", "ä¸‹é›¨"),
        ("â¤ï¸", "æˆ‘çˆ±ä½ "),
        ("ğŸ½ï¸", "åƒé¥­"),
        ("ğŸ“–", "çœ‹ä¹¦"),
        ("I love you", "æˆ‘çˆ±ä½ "),
        ("It's raining", "ä¸‹é›¨"),
        ("wÇ’ Ã i nÇ", "æˆ‘çˆ±ä½ "),
    ]

    test_generation(model, tokenizer, final_test_cases, device)

    # 10. æ€»ç»“
    print(f"\n" + "="*60)
    print("ğŸ“ æµ‹è¯•æ€»ç»“")
    print("="*60)
    print(f"âœ… è®­ç»ƒå®Œæˆ: {num_epochs} epochs")
    print(f"âœ… è®­ç»ƒæ ·æœ¬: {len(samples)} æ¦‚å¿µ, {len(training_pairs)} å¯¹")
    print(f"âœ… DBCåŠ é€Ÿ: {len(hooks)} ä¸ªé’©å­æ¿€æ´»")
    print(f"âœ… æ¨¡å‹å‚æ•°: {total_params:,}")
    print(f"\nğŸ’¡ è§‚å¯Ÿ:")
    print(f"   - APTæ¨¡å‹ä½¿ç”¨è‡ªç”Ÿæˆæ³¨æ„åŠ›æœºåˆ¶")
    print(f"   - DBC-DACç¨³å®šäº†æ¢¯åº¦è®­ç»ƒ")
    print(f"   - åˆ†å±‚è¯­è¨€å­¦ä¹ å¸®åŠ©å¿«é€ŸæŒæ¡æ¦‚å¿µ")
    print(f"   - ä»emoji/æ‹¼éŸ³/è‹±æ–‡åˆ°ä¸­æ–‡çš„å¤šå±‚æ˜ å°„")

    return model, tokenizer


if __name__ == "__main__":
    model, tokenizer = main()
