#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""å¿«é€Ÿæµ‹è¯•HLBDè®­ç»ƒ - çœ‹APTæ¨¡å‹èƒ½å¦å¿«é€Ÿå­¦ä¼šè¯´è¯"""

import sys
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import json
# from transformers import BertTokenizer  # å·²æ›¿æ¢ä¸º SimpleCharTokenizer_BACKUPï¼ˆæ”¯æŒ emojiï¼‰

# æ·»åŠ è·¯å¾„ï¼ˆåŠ¨æ€è®¡ç®—é¡¹ç›®æ ¹ç›®å½•ï¼‰
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, project_root)

from apt_model.modeling.apt_model import (
    APTModel,
    APTModelConfiguration,
    DBCDAC_Optimizer,
    create_gradient_stabilizer_hook
)


class SimpleCharTokenizer_BACKUP:
    """ç®€å•çš„å­—ç¬¦çº§åˆ†è¯å™¨"""
    def __init__(self):
        # åˆ›å»ºä¸€ä¸ªåŸºç¡€å­—ç¬¦è¡¨ï¼ˆåŒ…æ‹¬ä¸­æ–‡ã€è‹±æ–‡ã€emojiç­‰ï¼‰
        self.vocab = {
            '[PAD]': 0, '[UNK]': 1, '[BOS]': 2, '[EOS]': 3,
        }
        self.pad_token_id = 0
        self.unk_token_id = 1
        self.bos_token_id = 2
        self.eos_token_id = 3
        self.vocab_size = 5000  # é¢„ç•™è¶³å¤Ÿçš„è¯æ±‡ç©ºé—´

        # æ·»åŠ å¸¸ç”¨å­—ç¬¦
        self.char_to_id = self.vocab.copy()
        self.id_to_char = {v: k for k, v in self.vocab.items()}
        self.next_id = 4

    def _get_or_add_char(self, char):
        """è·å–å­—ç¬¦IDï¼Œå¦‚æœä¸å­˜åœ¨åˆ™æ·»åŠ """
        if char not in self.char_to_id:
            if self.next_id < self.vocab_size:
                self.char_to_id[char] = self.next_id
                self.id_to_char[self.next_id] = char
                self.next_id += 1
            else:
                return self.unk_token_id
        return self.char_to_id[char]

    def encode(self, text, return_tensors=None):
        """ç¼–ç æ–‡æœ¬ä¸ºIDåºåˆ—"""
        ids = [self.bos_token_id]
        for char in text:
            ids.append(self._get_or_add_char(char))
        ids.append(self.eos_token_id)

        if return_tensors == 'pt':
            return torch.tensor([ids])
        return ids

    def __call__(self, text, max_length=64, padding='max_length',
                 truncation=True, return_tensors='pt'):
        """åˆ†è¯æ¥å£ï¼ˆå…¼å®¹transformersï¼‰"""
        ids = []
        for char in text:
            ids.append(self._get_or_add_char(char))

        # æˆªæ–­
        if truncation and len(ids) > max_length:
            ids = ids[:max_length]

        # å¡«å……
        if padding == 'max_length':
            while len(ids) < max_length:
                ids.append(self.pad_token_id)

        if return_tensors == 'pt':
            return {'input_ids': torch.tensor([ids])}
        return {'input_ids': ids}

    def decode(self, ids, skip_special_tokens=True):
        """è§£ç IDåºåˆ—ä¸ºæ–‡æœ¬"""
        chars = []
        for id in ids:
            if isinstance(id, torch.Tensor):
                id = id.item()

            if skip_special_tokens and id in [self.pad_token_id, self.bos_token_id,
                                               self.eos_token_id, self.unk_token_id]:
                continue

            char = self.id_to_char.get(id, '[UNK]')
            chars.append(char)

        return ''.join(chars)


def register_dbc_hooks(model):
    """ä¸ºæ¨¡å‹æ³¨å†ŒDBC-DAC hooks"""
    opt = DBCDAC_Optimizer()
    hooks = []
    for _, p in model.named_parameters():
        if p.requires_grad:
            hooks.append(p.register_hook(create_gradient_stabilizer_hook(opt)))
    return hooks


def load_hlbd_samples(data_path, max_samples=20):
    """åŠ è½½HLBDæ•°æ®æ ·æœ¬"""
    print(f"ğŸ“‚ åŠ è½½HLBDæ•°æ®: {data_path}")

    with open(data_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # æå–sampleséƒ¨åˆ†
    start_idx = content.find('samples = [')
    if start_idx == -1:
        raise ValueError("æ‰¾ä¸åˆ°samplesæ•°æ®")

    # æå–JSONæ•°ç»„
    json_start = content.find('[', start_idx)
    # æ‰¾åˆ°åŒ¹é…çš„å³æ‹¬å·ï¼ˆç®€å•å¤„ç†ï¼‰
    bracket_count = 0
    json_end = json_start
    for i, char in enumerate(content[json_start:]):
        if char == '[':
            bracket_count += 1
        elif char == ']':
            bracket_count -= 1
            if bracket_count == 0:
                json_end = json_start + i + 1
                break

    json_str = content[json_start:json_end]
    samples = json.loads(json_str)

    if max_samples:
        samples = samples[:max_samples]

    print(f"   åŠ è½½äº† {len(samples)} ä¸ªæ ·æœ¬")
    return samples


def create_training_pairs(samples):
    """ä»HLBDæ ·æœ¬åˆ›å»ºè®­ç»ƒå¯¹"""
    pairs = []

    for sample in samples:
        concept = sample['concept']

        # åˆ›å»ºå¤šç§è®­ç»ƒå¯¹
        # 1. emoji -> ä¸­æ–‡
        if 'level_1' in sample and 'level_6' in sample:
            emoji = sample['level_1'].get('emoji', '')
            chinese = sample['level_6'].get('ä¸­æ–‡', '')
            if emoji and chinese:
                pairs.append((emoji, chinese))

        # 2. çŸ­è¯­ -> ä¸­æ–‡
        if 'level_2' in sample and 'level_6' in sample:
            phrase = sample['level_2'].get('çŸ­è¯­', '')
            chinese = sample['level_6'].get('ä¸­æ–‡', '')
            if phrase and chinese:
                pairs.append((phrase, chinese))

        # 3. è‹±æ–‡ -> ä¸­æ–‡
        if 'level_5' in sample and 'level_6' in sample:
            english = sample['level_5'].get('è‹±æ–‡', '')
            chinese = sample['level_6'].get('ä¸­æ–‡', '')
            if english and chinese:
                pairs.append((english, chinese))

        # 4. æ‹¼éŸ³ -> ä¸­æ–‡
        if 'level_4' in sample and 'level_6' in sample:
            pinyin = sample['level_4'].get('æ‹¼éŸ³', '')
            chinese = sample['level_6'].get('ä¸­æ–‡', '')
            if pinyin and chinese:
                pairs.append((pinyin, chinese))

    print(f"   åˆ›å»ºäº† {len(pairs)} ä¸ªè®­ç»ƒå¯¹")
    return pairs


class SimpleDialogueDataset(Dataset):
    """ç®€å•å¯¹è¯æ•°æ®é›†"""
    def __init__(self, pairs, tokenizer, max_length=64):
        self.pairs = pairs
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        src_text, tgt_text = self.pairs[idx]

        # ç¼–ç æºæ–‡æœ¬
        src_encoding = self.tokenizer(
            src_text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        # ç¼–ç ç›®æ ‡æ–‡æœ¬
        tgt_encoding = self.tokenizer(
            tgt_text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        return (
            src_encoding['input_ids'].squeeze(0),
            tgt_encoding['input_ids'].squeeze(0)
        )


def create_small_hlbd_config(vocab_size):
    """åˆ›å»ºå°å‹HLBD APTé…ç½®"""
    config = APTModelConfiguration(
        vocab_size=vocab_size,
        d_model=256,              # ä¸­ç­‰ç»´åº¦
        max_seq_len=64,           # çŸ­åºåˆ—
        num_encoder_layers=3,     # 3å±‚ç¼–ç å™¨
        num_decoder_layers=3,     # 3å±‚è§£ç å™¨
        num_heads=8,              # 8ä¸ªæ³¨æ„åŠ›å¤´
        d_ff=1024,                # å‰é¦ˆç½‘ç»œ
        dropout=0.1,
        use_autopoietic=True,     # å¯ç”¨è‡ªç”Ÿæˆæœºåˆ¶
        use_dbc_dac=True,         # å¯ç”¨DBC-DAC
    )
    return config


from tqdm import tqdm # ç¡®ä¿æ–‡ä»¶å¼€å¤´å¯¼å…¥äº† tqdm


def generate_with_vocab_mask(model, input_ids, valid_token_ids, max_length,
                             repetition_penalty, pad_token_id, device,
                             temperature=1.0, top_p=0.9):
    """
    ä½¿ç”¨ vocab mask é™åˆ¶ç”ŸæˆèŒƒå›´çš„è‡ªå®šä¹‰ç”Ÿæˆå‡½æ•°

    Args:
        model: APT æ¨¡å‹
        input_ids: è¾“å…¥ token IDs
        valid_token_ids: å…è®¸çš„ token ID é›†åˆ
        max_length: æœ€å¤§é•¿åº¦
        repetition_penalty: é‡å¤æƒ©ç½š
        pad_token_id: padding ID
        device: è®¾å¤‡
        temperature: é‡‡æ ·æ¸©åº¦
        top_p: nucleus é‡‡æ ·å‚æ•°
    """
    model.eval()
    generated = input_ids.clone()

    # åˆ›å»º vocab maskï¼ˆåªå…è®¸ç”Ÿæˆå·²çŸ¥çš„ tokenï¼‰
    vocab_size = model.config.vocab_size
    vocab_mask = torch.zeros(vocab_size, dtype=torch.bool, device=device)
    for valid_id in valid_token_ids:
        if 0 <= valid_id < vocab_size:
            vocab_mask[valid_id] = True

    with torch.no_grad():
        for _ in range(max_length - input_ids.size(1)):
            # å‰å‘ä¼ æ’­
            outputs = model(generated, generated)
            logits = outputs[:, -1, :]  # [batch_size, vocab_size]

            # ğŸ”§ åº”ç”¨ vocab mask - åªå…è®¸ç”Ÿæˆå·²çŸ¥çš„ token
            logits[:, ~vocab_mask] = -float('inf')

            # é‡å¤æƒ©ç½š
            if repetition_penalty != 1.0:
                for token_id in set(generated[0].tolist()):
                    if token_id in valid_token_ids:
                        logits[0, token_id] /= repetition_penalty

            # æ¸©åº¦è°ƒæ•´
            logits = logits / max(temperature, 1e-5)

            # Top-p é‡‡æ ·
            probs = torch.nn.functional.softmax(logits, dim=-1)
            sorted_probs, sorted_indices = torch.sort(probs, descending=True)
            cumsum_probs = torch.cumsum(sorted_probs, dim=-1)

            # æ‰¾åˆ°ç´¯ç§¯æ¦‚ç‡è¶…è¿‡ top_p çš„ä½ç½®
            remove_mask = cumsum_probs > top_p
            remove_mask[:, 1:] = remove_mask[:, :-1].clone()
            remove_mask[:, 0] = False

            # ç§»é™¤ä½æ¦‚ç‡çš„ token
            sorted_probs[remove_mask] = 0.0
            probs_sum = sorted_probs.sum(dim=-1, keepdim=True)
            if probs_sum > 0:
                sorted_probs = sorted_probs / probs_sum

            # é‡‡æ ·
            try:
                next_token_idx = torch.multinomial(sorted_probs, num_samples=1)
                next_token = sorted_indices.gather(-1, next_token_idx)
            except:
                # å¦‚æœé‡‡æ ·å¤±è´¥ï¼Œä½¿ç”¨è´ªå¿ƒè§£ç 
                next_token = torch.argmax(logits, dim=-1, keepdim=True)

            generated = torch.cat([generated, next_token], dim=1)

    return generated


def train_epoch(model, dataloader, optimizer, criterion, device, use_dbc=False, accumulation_steps=4): # <--- ã€å…³é”®ä¿®æ”¹ 1ã€‘æ¥æ”¶ accumulation_steps
    """è®­ç»ƒä¸€ä¸ªepoch (ä½¿ç”¨æ¢¯åº¦ç´¯ç§¯)"""
    model.train()
    total_loss = 0
    total_steps = 0
    
    ACCUMULATION_STEPS = accumulation_steps # ã€å…³é”®ä¿®æ­£ã€‘åœ¨å‡½æ•°ä½“å†…å®šä¹‰ ACCUMULATION_STEPS
    
    progress_bar = tqdm(
        dataloader, 
        desc="Training", 
        leave=False, 
        mininterval=0.1, 
        ascii=True
    )

    # ã€å…³é”®ä¿®æ”¹ 2ã€‘ä½¿ç”¨ enumerate æ¥è·å–æ‰¹æ¬¡ç´¢å¼• i
    for i, (src_ids, tgt_ids) in enumerate(progress_bar): 
        
        src_ids = src_ids.to(device)
        tgt_ids = tgt_ids.to(device)

        # å‰å‘ä¼ æ’­
        output = model(src_ids, tgt_ids[:, :-1])

        # è®¡ç®—æŸå¤±
        loss = criterion(
            output.reshape(-1, output.size(-1)),
            tgt_ids[:, 1:].reshape(-1)
        )

        # æŸå¤±å½’ä¸€åŒ– (Loss Scaling)
        loss = loss / ACCUMULATION_STEPS 

        # åå‘ä¼ æ’­ (ä¸æ¸…é™¤æ¢¯åº¦)
        loss.backward()

        # æ¡ä»¶ä¼˜åŒ–å’Œæ¸…é›¶ (æ¯ N æ­¥æ‰§è¡Œä¸€æ¬¡)
        if (i + 1) % ACCUMULATION_STEPS == 0:
            # æƒé‡æ›´æ–° (å³ä½¿ DBC æ¿€æ´»ï¼Œä¿ç•™è£å‰ªä¹Ÿæ— å®³)
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            optimizer.zero_grad() # æ¸…é™¤ç´¯ç§¯çš„æ¢¯åº¦
            
        total_loss += loss.item() * ACCUMULATION_STEPS # æ¢å¤å®é™… Loss
        total_steps += 1
        
        # å®æ—¶æ›´æ–°è¿›åº¦æ¡ä¸Šçš„ Loss
        progress_bar.set_postfix({'loss': f'{loss.item() * ACCUMULATION_STEPS:.4f}'})
        
    # ã€æœ€åä¸€æ­¥æ¸…ç†ã€‘å¤„ç†å‰©ä½™çš„ç´¯ç§¯æ¢¯åº¦ (i éœ€è¦åœ¨å¾ªç¯å¤–å¯ç”¨)
    # Note: ç¡®ä¿ i åœ¨å¾ªç¯å¤–èƒ½è®¿é—®åˆ°ï¼Œå°½ç®¡è¿™ä¸æ˜¯æ ‡å‡† Python åšæ³•
    try:
        if (i + 1) % ACCUMULATION_STEPS != 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            optimizer.zero_grad()
    except NameError:
         # å¦‚æœ i æ²¡å®šä¹‰ (æ¯”å¦‚ dataloader æ˜¯ç©ºçš„)ï¼Œåˆ™å¿½ç•¥æ¸…ç†
         pass 

    return (total_loss / total_steps) if total_steps > 0 else 0


def generate_text(model, tokenizer, input_text, device, max_length=50, repetition_penalty=1.5):
    """
    ç”Ÿæˆæ–‡æœ¬ï¼ˆä¿®å¤ï¼šæ”¯æŒ emojiï¼Œå»é™¤è¾“å…¥å¤è¯»ï¼Œé™åˆ¶ vocab èŒƒå›´ï¼‰
    """
    model.eval()

    # 1. è·å– BOS/PAD ID
    bos_id = tokenizer.bos_token_id
    pad_id = tokenizer.pad_token_id

    # 2. ç¼–ç è¾“å…¥æ–‡æœ¬ï¼ˆä½¿ç”¨ __call__ æ–¹æ³•ï¼Œä¸æ·»åŠ ç‰¹æ®Š tokenï¼‰
    # SimpleCharTokenizer_BACKUP çš„ __call__ ä¸æ·»åŠ  BOS/EOS
    input_result = tokenizer(input_text, max_length=64, padding=False,
                            truncation=True, return_tensors='pt')
    input_ids = input_result['input_ids'].to(device)  # shape: [1, seq_len]

    # å»é™¤ paddingï¼ˆå¦‚æœæœ‰ï¼‰
    # æ‰¾åˆ°ç¬¬ä¸€ä¸ªé pad token çš„ä½ç½®
    input_ids = input_ids[input_ids != pad_id].unsqueeze(0) if pad_id in input_ids else input_ids

    # 3. å‡†å¤‡æ¨¡å‹è¾“å…¥ input_ids = [BOS] + Prompt Tokens
    bos_tensor = torch.tensor([[bos_id]], device=device)
    initial_ids = torch.cat([bos_tensor, input_ids], dim=1)

    # 4. ğŸ”§ ã€ä¿®å¤ã€‘ä½¿ç”¨è‡ªå®šä¹‰ç”Ÿæˆï¼Œé™åˆ¶ vocab èŒƒå›´
    # åªå…è®¸ç”Ÿæˆ tokenizer å·²çŸ¥çš„ token IDs
    valid_ids = set(tokenizer.id_to_char.keys())
    max_valid_id = max(valid_ids)

    generated_ids = generate_with_vocab_mask(
        model=model,
        input_ids=initial_ids,
        valid_token_ids=valid_ids,
        max_length=max_length + initial_ids.size(1),
        repetition_penalty=repetition_penalty,
        pad_token_id=pad_id,
        device=device
    )

    # 5. ã€ä¿®å¤ã€‘åªè§£ç æ–°ç”Ÿæˆçš„éƒ¨åˆ†ï¼Œå»æ‰è¾“å…¥
    input_length = initial_ids.size(1)
    generated_only = generated_ids[0][input_length:]  # å»æ‰è¾“å…¥éƒ¨åˆ†
    generated_text = tokenizer.decode(generated_only, skip_special_tokens=True)

    return generated_text


def test_generation(model, tokenizer, test_cases, device):
    """æµ‹è¯•ç”Ÿæˆèƒ½åŠ›"""
    print("\n" + "="*60)
    print("ğŸ—£ï¸ æµ‹è¯•å¯¹è¯ç”Ÿæˆèƒ½åŠ›")
    print("="*60)

    # è®¾å®šå¼ºåŠ›é‡å¤æƒ©ç½šå› å­
    REPETITION_FACTOR = 1.5

    for input_text, expected_concept in test_cases:
        generated = generate_text(model, tokenizer, input_text, device, repetition_penalty=REPETITION_FACTOR)
        print(f"\nè¾“å…¥: {input_text}")
        print(f"æœŸæœ›æ¦‚å¿µ: {expected_concept}")
        print(f"ç”Ÿæˆ: {generated}")


def save_model_and_tokenizer(model, tokenizer, config, save_dir, num_epochs, final_loss):
    """
    ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹å’Œ tokenizer

    Args:
        model: è®­ç»ƒå¥½çš„ APT æ¨¡å‹
        tokenizer: SimpleCharTokenizer_BACKUP å®ä¾‹
        config: APTModelConfiguration å®ä¾‹
        save_dir: ä¿å­˜ç›®å½•
        num_epochs: è®­ç»ƒçš„æ€» epoch æ•°
        final_loss: æœ€ç»ˆçš„æŸå¤±å€¼
    """
    import datetime

    os.makedirs(save_dir, exist_ok=True)

    # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    model_filename = f'hlbd_model_{timestamp}.pt'
    model_path = os.path.join(save_dir, model_filename)

    # ä¿å­˜æ¨¡å‹ã€tokenizer å’Œé…ç½®
    checkpoint = {
        'model_state_dict': model.state_dict(),
        'tokenizer_char_to_id': tokenizer.char_to_id,
        'tokenizer_id_to_char': tokenizer.id_to_char,
        'tokenizer_next_id': tokenizer.next_id,
        'tokenizer_vocab_size': tokenizer.vocab_size,
        'config': {
            'vocab_size': config.vocab_size,
            'd_model': config.d_model,
            'max_seq_len': config.max_seq_len,
            'num_encoder_layers': config.num_encoder_layers,
            'num_decoder_layers': config.num_decoder_layers,
            'num_heads': config.num_heads,
            'd_ff': config.d_ff,
            'dropout': config.dropout,
            'use_autopoietic': config.use_autopoietic,
            'use_dbc_dac': config.use_dbc_dac,
        },
        'training_info': {
            'num_epochs': num_epochs,
            'final_loss': final_loss,
            'timestamp': timestamp,
        }
    }

    torch.save(checkpoint, model_path)

    print(f"\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜:")
    print(f"   è·¯å¾„: {os.path.abspath(model_path)}")
    print(f"   å¤§å°: {os.path.getsize(model_path) / 1024 / 1024:.2f} MB")

    return model_path


def load_model_and_tokenizer(model_path, device):
    """
    åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹å’Œ tokenizer

    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        device: è®¾å¤‡ï¼ˆcuda æˆ– cpuï¼‰

    Returns:
        model: åŠ è½½çš„ APT æ¨¡å‹
        tokenizer: åŠ è½½çš„ SimpleCharTokenizer_BACKUP
        training_info: è®­ç»ƒä¿¡æ¯å­—å…¸
    """
    print(f"\nğŸ“‚ åŠ è½½æ¨¡å‹: {model_path}")

    # åŠ è½½ checkpoint
    checkpoint = torch.load(model_path, map_location=device)

    # é‡å»ºé…ç½®
    config_dict = checkpoint['config']
    config = APTModelConfiguration(
        vocab_size=config_dict['vocab_size'],
        d_model=config_dict['d_model'],
        max_seq_len=config_dict['max_seq_len'],
        num_encoder_layers=config_dict['num_encoder_layers'],
        num_decoder_layers=config_dict['num_decoder_layers'],
        num_heads=config_dict['num_heads'],
        d_ff=config_dict['d_ff'],
        dropout=config_dict['dropout'],
        use_autopoietic=config_dict['use_autopoietic'],
        use_dbc_dac=config_dict['use_dbc_dac'],
    )

    # é‡å»ºæ¨¡å‹
    model = APTModel(config).to(device)
    model.load_state_dict(checkpoint['model_state_dict'])

    # é‡å»º tokenizer
    tokenizer = SimpleCharTokenizer_BACKUP()
    tokenizer.char_to_id = checkpoint['tokenizer_char_to_id']
    tokenizer.id_to_char = checkpoint['tokenizer_id_to_char']
    tokenizer.next_id = checkpoint['tokenizer_next_id']
    tokenizer.vocab_size = checkpoint['tokenizer_vocab_size']

    # è·å–è®­ç»ƒä¿¡æ¯
    training_info = checkpoint.get('training_info', {})

    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
    print(f"   è®­ç»ƒ epoch: {training_info.get('num_epochs', 'N/A')}")
    print(f"   æœ€ç»ˆæŸå¤±: {training_info.get('final_loss', 'N/A'):.4f}")
    print(f"   ä¿å­˜æ—¶é—´: {training_info.get('timestamp', 'N/A')}")
    print(f"   è¯æ±‡è¡¨å¤§å°: {len(tokenizer.char_to_id)}")

    return model, tokenizer, training_info


def main():
    """ä¸»å‡½æ•°"""
    print("\nğŸš€ HLBDå¿«é€Ÿå­¦ä¹ æµ‹è¯• - APTæ¨¡å‹èƒ½å¦å¿«é€Ÿå­¦ä¼šè¯´è¯?")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")

    ACCUMULATION_STEPS = 8  # æ¨¡æ‹Ÿ 4 * 8 = 32 çš„æœ‰æ•ˆæ‰¹æ¬¡å¤§å°

    # è‡ªåŠ¨æ£€æµ‹ï¼šæœ‰æ˜¾å¡å°±ç”¨æ˜¾å¡ï¼Œæ²¡æœ‰æ‰ç”¨ CPU
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"è®¾å¤‡: {device}")

    # 1. åŠ è½½HLBDæ•°æ®
    current_dir = os.path.dirname(os.path.abspath(__file__)) # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½• (tests)
    project_root = os.path.dirname(current_dir)              # è·å–é¡¹ç›®æ ¹ç›®å½• (APT-Transformer)
    data_path = os.path.join(project_root, 'apt_model', 'åˆ†å±‚è¯­è¨€å¯è’™æ•°æ®é›†.txt')
    samples = load_hlbd_samples(data_path, max_samples=None)

    # æ˜¾ç¤ºå‡ ä¸ªæ ·æœ¬
    print(f"\nğŸ“ æ ·æœ¬ç¤ºä¾‹:")
    for i, sample in enumerate(samples[:3]):
        print(f"\n   æ ·æœ¬ {i+1}: {sample['concept']}")
        print(f"      Emoji: {sample['level_1'].get('emoji', 'N/A')}")
        print(f"      ä¸­æ–‡: {sample['level_6'].get('ä¸­æ–‡', 'N/A')[:30]}...")

    # 2. åˆ›å»ºè®­ç»ƒå¯¹
    training_pairs = create_training_pairs(samples)

    # 3. å‡†å¤‡åˆ†è¯å™¨
    print(f"\nğŸ”§ å‡†å¤‡åˆ†è¯å™¨...")
    # ä½¿ç”¨ SimpleCharTokenizer_BACKUPï¼ˆæ”¯æŒ emoji åŠ¨æ€æ·»åŠ ï¼‰
    tokenizer = SimpleCharTokenizer_BACKUP()
    print(f"   ä½¿ç”¨çš„åˆ†è¯å™¨: {type(tokenizer).__name__}")
    print(f"   è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")

    # 4. åˆ›å»ºæ•°æ®é›†
    print(f"\nğŸ“Š åˆ›å»ºæ•°æ®é›†...")
    dataset = SimpleDialogueDataset(training_pairs, tokenizer)
    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)
    print(f"   è®­ç»ƒæ‰¹æ¬¡æ•°: {len(dataloader)}")

    # ã€æ–°å¢éªŒè¯ä»£ç ï¼šæ£€æŸ¥å®é™…æ ·æœ¬æ•°ã€‘
    actual_pairs = len(dataset)
    print(f"--- é•¿åº¦éªŒè¯ ---")
    print(f"æ¨¡å‹å®é™…çœ‹åˆ°çš„è®­ç»ƒå¯¹æ•°é‡: {actual_pairs} (åº”ä¸º 80 æˆ–æ›´å¤š)")
    print(f"----------------")

    # 5. åˆ›å»ºæ¨¡å‹
    print(f"\nğŸ—ï¸ åˆ›å»ºAPTæ¨¡å‹...")
    config = create_small_hlbd_config(tokenizer.vocab_size)
    model = APTModel(config).to(device)

    total_params = sum(p.numel() for p in model.parameters())
    print(f"   æ¨¡å‹å‚æ•°: {total_params:,}")
    print(f"   é…ç½®: d_model={config.d_model}, layers={config.num_encoder_layers}")

    # 6. åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = optim.Adam(model.parameters(), lr=5e-5)
    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)

    # 7. æ³¨å†ŒDBC hooks
    print(f"\nâš¡ æ³¨å†ŒDBC-DACåŠ é€Ÿ...")
    #hooks = register_dbc_hooks(model)
    hooks = [] # ä¿æŒ hooks å˜é‡å­˜åœ¨ï¼Œé˜²æ­¢åé¢æŠ¥é”™
    #print(f"   æ³¨å†Œäº† {len(hooks)} ä¸ªæ¢¯åº¦ç¨³å®šé’©å­")

    # 8. è®­ç»ƒæ¨¡å‹
    print(f"\n" + "="*60)
    print("ğŸƒ å¼€å§‹å¿«é€Ÿè®­ç»ƒ (çœ‹èƒ½å¦å¿«é€Ÿå­¦ä¼šè¯´è¯)")
    print("="*60)

    num_epochs = 30  # å¿«é€Ÿè®­ç»ƒæµ‹è¯•ï¼ˆæ•°æ®é›†å°ï¼Œ30è½®è¶³å¤Ÿï¼‰

    for epoch in range(num_epochs):
        loss = train_epoch(model, dataloader, optimizer, criterion, device, use_dbc=True, accumulation_steps=ACCUMULATION_STEPS)
        print(f"Epoch {epoch+1}/{num_epochs} - Loss: {loss:.4f}")

        # æ¯3ä¸ªepochæµ‹è¯•ä¸€æ¬¡
        if (epoch + 1) % 3 == 0 or epoch == num_epochs - 1:
            test_cases = [
                ("ğŸŒ§ï¸", "ä¸‹é›¨"),
                ("â¤ï¸", "æˆ‘çˆ±ä½ "),
                ("I love you", "æˆ‘çˆ±ä½ "),
                ("ä¸‹é›¨", "å¤©æ°”"),
            ]
            test_generation(model, tokenizer, test_cases, device)

    # 9. æœ€ç»ˆæµ‹è¯•
    print(f"\n" + "="*60)
    print("ğŸ¯ æœ€ç»ˆæµ‹è¯• - APTå­¦ä¼šè¯´è¯äº†å—?")
    print("="*60)

    final_test_cases = [
        ("ğŸŒ§ï¸", "ä¸‹é›¨"),
        ("â¤ï¸", "æˆ‘çˆ±ä½ "),
        ("ğŸ½ï¸", "åƒé¥­"),
        ("ğŸ“–", "çœ‹ä¹¦"),
        ("I love you", "æˆ‘çˆ±ä½ "),
        ("It's raining", "ä¸‹é›¨"),
        ("wÇ’ Ã i nÇ", "æˆ‘çˆ±ä½ "),
    ]

    test_generation(model, tokenizer, final_test_cases, device)

    # 10. æ€»ç»“
    print(f"\n" + "="*60)
    print("ğŸ“ æµ‹è¯•æ€»ç»“")
    print("="*60)
    print(f"âœ… è®­ç»ƒå®Œæˆ: {num_epochs} epochs")
    print(f"âœ… è®­ç»ƒæ ·æœ¬: {len(samples)} æ¦‚å¿µ, {len(training_pairs)} å¯¹")
    print(f"âœ… DBCåŠ é€Ÿ: {len(hooks)} ä¸ªé’©å­æ¿€æ´»")
    print(f"âœ… æ¨¡å‹å‚æ•°: {total_params:,}")
    print(f"\nğŸ’¡ è§‚å¯Ÿ:")
    print(f"   - APTæ¨¡å‹ä½¿ç”¨è‡ªç”Ÿæˆæ³¨æ„åŠ›æœºåˆ¶")
    print(f"   - DBC-DACç¨³å®šäº†æ¢¯åº¦è®­ç»ƒ")
    print(f"   - åˆ†å±‚è¯­è¨€å­¦ä¹ å¸®åŠ©å¿«é€ŸæŒæ¡æ¦‚å¿µ")
    print(f"   - ä»emoji/æ‹¼éŸ³/è‹±æ–‡åˆ°ä¸­æ–‡çš„å¤šå±‚æ˜ å°„")

    # 11. ä¿å­˜æ¨¡å‹
    save_dir = os.path.join(project_root, 'tests', 'saved_models')
    model_path = save_model_and_tokenizer(
        model=model,
        tokenizer=tokenizer,
        config=config,
        save_dir=save_dir,
        num_epochs=num_epochs,
        final_loss=loss
    )

    return model, tokenizer, model_path


if __name__ == "__main__":
    model, tokenizer, model_path = main()

    # å¯é€‰ï¼šæµ‹è¯•åŠ è½½åŠŸèƒ½
    print("\n" + "="*60)
    print("ğŸ”„ æµ‹è¯•æ¨¡å‹åŠ è½½åŠŸèƒ½")
    print("="*60)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    loaded_model, loaded_tokenizer, training_info = load_model_and_tokenizer(model_path, device)

    # éªŒè¯åŠ è½½çš„æ¨¡å‹
    test_cases = [
        ("ğŸŒ§ï¸", "ä¸‹é›¨"),
        ("â¤ï¸", "æˆ‘çˆ±ä½ "),
    ]

    print("\nä½¿ç”¨åŠ è½½çš„æ¨¡å‹ç”Ÿæˆ:")
    test_generation(loaded_model, loaded_tokenizer, test_cases, device)

    print("\nâœ… æ¨¡å‹ä¿å­˜å’ŒåŠ è½½åŠŸèƒ½æ­£å¸¸ï¼")
