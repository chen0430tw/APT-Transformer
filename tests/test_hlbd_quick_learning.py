#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""å¿«é€Ÿæµ‹è¯•HLBDè®­ç»ƒ - çœ‹APTæ¨¡å‹èƒ½å¦å¿«é€Ÿå­¦ä¼šè¯´è¯"""

import sys
import os
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import json
from transformers import BertTokenizer

# æ·»åŠ è·¯å¾„
sys.path.insert(0, 'APT-Transformer')

from apt_model.modeling.apt_model import (
    APTModel,
    APTModelConfiguration,
    DBCDAC_Optimizer,
    create_gradient_stabilizer_hook
)


class SimpleCharTokenizer_BACKUP:
    """ç®€å•çš„å­—ç¬¦çº§åˆ†è¯å™¨"""
    def __init__(self):
        # åˆ›å»ºä¸€ä¸ªåŸºç¡€å­—ç¬¦è¡¨ï¼ˆåŒ…æ‹¬ä¸­æ–‡ã€è‹±æ–‡ã€emojiç­‰ï¼‰
        self.vocab = {
            '[PAD]': 0, '[UNK]': 1, '[BOS]': 2, '[EOS]': 3,
        }
        self.pad_token_id = 0
        self.unk_token_id = 1
        self.bos_token_id = 2
        self.eos_token_id = 3
        self.vocab_size = 5000  # é¢„ç•™è¶³å¤Ÿçš„è¯æ±‡ç©ºé—´

        # æ·»åŠ å¸¸ç”¨å­—ç¬¦
        self.char_to_id = self.vocab.copy()
        self.id_to_char = {v: k for k, v in self.vocab.items()}
        self.next_id = 4

    def _get_or_add_char(self, char):
        """è·å–å­—ç¬¦IDï¼Œå¦‚æœä¸å­˜åœ¨åˆ™æ·»åŠ """
        if char not in self.char_to_id:
            if self.next_id < self.vocab_size:
                self.char_to_id[char] = self.next_id
                self.id_to_char[self.next_id] = char
                self.next_id += 1
            else:
                return self.unk_token_id
        return self.char_to_id[char]

    def encode(self, text, return_tensors=None):
        """ç¼–ç æ–‡æœ¬ä¸ºIDåºåˆ—"""
        ids = [self.bos_token_id]
        for char in text:
            ids.append(self._get_or_add_char(char))
        ids.append(self.eos_token_id)

        if return_tensors == 'pt':
            return torch.tensor([ids])
        return ids

    def __call__(self, text, max_length=64, padding='max_length',
                 truncation=True, return_tensors='pt'):
        """åˆ†è¯æ¥å£ï¼ˆå…¼å®¹transformersï¼‰"""
        ids = []
        for char in text:
            ids.append(self._get_or_add_char(char))

        # æˆªæ–­
        if truncation and len(ids) > max_length:
            ids = ids[:max_length]

        # å¡«å……
        if padding == 'max_length':
            while len(ids) < max_length:
                ids.append(self.pad_token_id)

        if return_tensors == 'pt':
            return {'input_ids': torch.tensor([ids])}
        return {'input_ids': ids}

    def decode(self, ids, skip_special_tokens=True):
        """è§£ç IDåºåˆ—ä¸ºæ–‡æœ¬"""
        chars = []
        for id in ids:
            if isinstance(id, torch.Tensor):
                id = id.item()

            if skip_special_tokens and id in [self.pad_token_id, self.bos_token_id,
                                               self.eos_token_id, self.unk_token_id]:
                continue

            char = self.id_to_char.get(id, '[UNK]')
            chars.append(char)

        return ''.join(chars)


def register_dbc_hooks(model):
    """ä¸ºæ¨¡å‹æ³¨å†ŒDBC-DAC hooks"""
    opt = DBCDAC_Optimizer()
    hooks = []
    for _, p in model.named_parameters():
        if p.requires_grad:
            hooks.append(p.register_hook(create_gradient_stabilizer_hook(opt)))
    return hooks


def load_hlbd_samples(data_path, max_samples=20):
    """åŠ è½½HLBDæ•°æ®æ ·æœ¬"""
    print(f"ğŸ“‚ åŠ è½½HLBDæ•°æ®: {data_path}")

    with open(data_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # æå–sampleséƒ¨åˆ†
    start_idx = content.find('samples = [')
    if start_idx == -1:
        raise ValueError("æ‰¾ä¸åˆ°samplesæ•°æ®")

    # æå–JSONæ•°ç»„
    json_start = content.find('[', start_idx)
    # æ‰¾åˆ°åŒ¹é…çš„å³æ‹¬å·ï¼ˆç®€å•å¤„ç†ï¼‰
    bracket_count = 0
    json_end = json_start
    for i, char in enumerate(content[json_start:]):
        if char == '[':
            bracket_count += 1
        elif char == ']':
            bracket_count -= 1
            if bracket_count == 0:
                json_end = json_start + i + 1
                break

    json_str = content[json_start:json_end]
    samples = json.loads(json_str)

    if max_samples:
        samples = samples[:max_samples]

    print(f"   åŠ è½½äº† {len(samples)} ä¸ªæ ·æœ¬")
    return samples


def create_training_pairs(samples):
    """ä»HLBDæ ·æœ¬åˆ›å»ºè®­ç»ƒå¯¹"""
    pairs = []

    for sample in samples:
        concept = sample['concept']

        # åˆ›å»ºå¤šç§è®­ç»ƒå¯¹
        # 1. emoji -> ä¸­æ–‡
        if 'level_1' in sample and 'level_6' in sample:
            emoji = sample['level_1'].get('emoji', '')
            chinese = sample['level_6'].get('ä¸­æ–‡', '')
            if emoji and chinese:
                pairs.append((emoji, chinese))

        # 2. çŸ­è¯­ -> ä¸­æ–‡
        if 'level_2' in sample and 'level_6' in sample:
            phrase = sample['level_2'].get('çŸ­è¯­', '')
            chinese = sample['level_6'].get('ä¸­æ–‡', '')
            if phrase and chinese:
                pairs.append((phrase, chinese))

        # 3. è‹±æ–‡ -> ä¸­æ–‡
        if 'level_5' in sample and 'level_6' in sample:
            english = sample['level_5'].get('è‹±æ–‡', '')
            chinese = sample['level_6'].get('ä¸­æ–‡', '')
            if english and chinese:
                pairs.append((english, chinese))

        # 4. æ‹¼éŸ³ -> ä¸­æ–‡
        if 'level_4' in sample and 'level_6' in sample:
            pinyin = sample['level_4'].get('æ‹¼éŸ³', '')
            chinese = sample['level_6'].get('ä¸­æ–‡', '')
            if pinyin and chinese:
                pairs.append((pinyin, chinese))

    print(f"   åˆ›å»ºäº† {len(pairs)} ä¸ªè®­ç»ƒå¯¹")
    return pairs


class SimpleDialogueDataset(Dataset):
    """ç®€å•å¯¹è¯æ•°æ®é›†"""
    def __init__(self, pairs, tokenizer, max_length=64):
        self.pairs = pairs
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        src_text, tgt_text = self.pairs[idx]

        # ç¼–ç æºæ–‡æœ¬
        src_encoding = self.tokenizer(
            src_text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        # ç¼–ç ç›®æ ‡æ–‡æœ¬
        tgt_encoding = self.tokenizer(
            tgt_text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        return (
            src_encoding['input_ids'].squeeze(0),
            tgt_encoding['input_ids'].squeeze(0)
        )


def create_small_hlbd_config(vocab_size):
    """åˆ›å»ºå°å‹HLBD APTé…ç½®"""
    config = APTModelConfiguration(
        vocab_size=vocab_size,
        d_model=256,              # ä¸­ç­‰ç»´åº¦
        max_seq_len=64,           # çŸ­åºåˆ—
        num_encoder_layers=3,     # 3å±‚ç¼–ç å™¨
        num_decoder_layers=3,     # 3å±‚è§£ç å™¨
        num_heads=8,              # 8ä¸ªæ³¨æ„åŠ›å¤´
        d_ff=1024,                # å‰é¦ˆç½‘ç»œ
        dropout=0.1,
        use_autopoietic=True,     # å¯ç”¨è‡ªç”Ÿæˆæœºåˆ¶
        use_dbc_dac=True,         # å¯ç”¨DBC-DAC
    )
    return config


from tqdm import tqdm # ç¡®ä¿æ–‡ä»¶å¼€å¤´å¯¼å…¥äº† tqdm

def train_epoch(model, dataloader, optimizer, criterion, device, use_dbc=False):
    """è®­ç»ƒä¸€ä¸ªepoch (ä¼˜åŒ–åŠ¨ç”»æ˜¾ç¤º)"""
    model.train()
    total_loss = 0
    total_steps = 0

    # ã€è§†è§‰ä¿®å¤ã€‘å¼ºåˆ¶ä½¿ç”¨æ›´å¿«çš„åˆ·æ–°é¢‘ç‡ (mininterval=0.1) å’Œ ASCII å­—ç¬¦ (ascii=True)
    progress_bar = tqdm(
        dataloader, 
        desc="Training", 
        leave=False, 
        mininterval=0.1, 
        ascii=True
    )

    for src_ids, tgt_ids in progress_bar:
        src_ids = src_ids.to(device)
        tgt_ids = tgt_ids.to(device)

        optimizer.zero_grad()

        # å‰å‘ä¼ æ’­
        output = model(src_ids, tgt_ids[:, :-1])

        # è®¡ç®—æŸå¤±
        loss = criterion(
            output.reshape(-1, output.size(-1)),
            tgt_ids[:, 1:].reshape(-1)
        )

        # åå‘ä¼ æ’­
        loss.backward()

        # æ¢¯åº¦è£å‰ª
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)

        # ä¼˜åŒ–
        optimizer.step()

        total_loss += loss.item()
        total_steps += 1
        
        # å®æ—¶æ›´æ–°è¿›åº¦æ¡ä¸Šçš„ Loss
        progress_bar.set_postfix({'loss': f'{loss.item():.4f}'})

    return total_loss / total_steps if total_steps > 0 else 0


def generate_text(model, tokenizer, input_text, device, max_length=50):
    """ç”Ÿæˆæ–‡æœ¬"""
    model.eval()

    # ç¼–ç è¾“å…¥
    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(device)

    # ã€å…³é”®ä¿®å¤ã€‘è·å–æ­£ç¡®çš„å¼€å§‹(BOS)å’Œç»“æŸ(EOS)æ ‡è®°
    # BERT åˆ†è¯å™¨ bos_token_id é»˜è®¤ä¸º Noneï¼Œéœ€è¦å›é€€åˆ° cls_token_id
    bos_id = tokenizer.bos_token_id if tokenizer.bos_token_id is not None else tokenizer.cls_token_id
    # BERT åˆ†è¯å™¨ eos_token_id é»˜è®¤ä¸º Noneï¼Œéœ€è¦å›é€€åˆ° sep_token_id
    eos_id = tokenizer.eos_token_id if tokenizer.eos_token_id is not None else tokenizer.sep_token_id

    # å†æ¬¡æ£€æŸ¥ï¼Œé˜²æ­¢ bos_id ä¾ç„¶æ˜¯ None (æå°‘æ•°æƒ…å†µ)
    if bos_id is None: bos_id = 101  # BERT é»˜è®¤ CLS ID
    if eos_id is None: eos_id = 102  # BERT é»˜è®¤ SEP ID

    # ä»BOS tokenå¼€å§‹
    generated = torch.tensor([[bos_id]], device=device)

    with torch.no_grad():
        for _ in range(max_length):
            # å‰å‘ä¼ æ’­
            output = model(input_ids, generated)

            # è·å–ä¸‹ä¸€ä¸ªtoken
            next_token_logits = output[:, -1, :]
            next_token = torch.argmax(next_token_logits, dim=-1, keepdim=True)

            # æ·»åŠ åˆ°ç”Ÿæˆåºåˆ—
            generated = torch.cat([generated, next_token], dim=1)

            # å¦‚æœç”Ÿæˆäº†EOSï¼Œåœæ­¢
            if next_token.item() == eos_id:
                # break
                pass

    # è§£ç 
    generated_text = tokenizer.decode(generated[0], skip_special_tokens=True)
    return generated_text


def test_generation(model, tokenizer, test_cases, device):
    """æµ‹è¯•ç”Ÿæˆèƒ½åŠ›"""
    print("\n" + "="*60)
    print("ğŸ—£ï¸ æµ‹è¯•å¯¹è¯ç”Ÿæˆèƒ½åŠ›")
    print("="*60)

    for input_text, expected_concept in test_cases:
        generated = generate_text(model, tokenizer, input_text, device)
        print(f"\nè¾“å…¥: {input_text}")
        print(f"æœŸæœ›æ¦‚å¿µ: {expected_concept}")
        print(f"ç”Ÿæˆ: {generated}")


def main():
    """ä¸»å‡½æ•°"""
    print("\nğŸš€ HLBDå¿«é€Ÿå­¦ä¹ æµ‹è¯• - APTæ¨¡å‹èƒ½å¦å¿«é€Ÿå­¦ä¼šè¯´è¯?")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")

    # è‡ªåŠ¨æ£€æµ‹ï¼šæœ‰æ˜¾å¡å°±ç”¨æ˜¾å¡ï¼Œæ²¡æœ‰æ‰ç”¨ CPU
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"è®¾å¤‡: {device}")

    # 1. åŠ è½½HLBDæ•°æ®
    current_dir = os.path.dirname(os.path.abspath(__file__)) # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½• (tests)
    project_root = os.path.dirname(current_dir)              # è·å–é¡¹ç›®æ ¹ç›®å½• (APT-Transformer)
    data_path = os.path.join(project_root, 'apt_model', 'åˆ†å±‚è¯­è¨€å¯è’™æ•°æ®é›†.txt')
    samples = load_hlbd_samples(data_path, max_samples=20)

    # é¡ºä¾¿æŠŠä¸‹é¢é‚£ä¸ª BERT çš„è·¯å¾„ä¹Ÿä¸€èµ·ä¿®äº†ï¼Œä¸ç„¶ç­‰ä¼šè¿˜ä¼šæŠ¥é”™
    bert_path = os.path.join(project_root, 'bert', 'bert-base-chinese')

    # æ˜¾ç¤ºå‡ ä¸ªæ ·æœ¬
    print(f"\nğŸ“ æ ·æœ¬ç¤ºä¾‹:")
    for i, sample in enumerate(samples[:3]):
        print(f"\n   æ ·æœ¬ {i+1}: {sample['concept']}")
        print(f"      Emoji: {sample['level_1'].get('emoji', 'N/A')}")
        print(f"      ä¸­æ–‡: {sample['level_6'].get('ä¸­æ–‡', 'N/A')[:30]}...")

    # 2. åˆ›å»ºè®­ç»ƒå¯¹
    training_pairs = create_training_pairs(samples)

    # 3. å‡†å¤‡åˆ†è¯å™¨
    print(f"\nğŸ”§ å‡†å¤‡åˆ†è¯å™¨...")
    # ä½¿ç”¨æœ¬åœ°çš„bert-base-chinese tokenizer
    tokenizer = BertTokenizer.from_pretrained(
        bert_path,
        local_files_only=True,  # <-- å¼ºåˆ¶ä½¿ç”¨æœ¬åœ°æ–‡ä»¶ï¼Œç¦æ­¢è”ç½‘
        vocab_file=os.path.join(bert_path, 'vocab.txt') # <-- æ˜¾å¼æŒ‡å®šè¯è¡¨ä½ç½®
    ) # ä½¿ç”¨ä¸Šé¢è®¡ç®—å¥½çš„è·¯å¾„
    print(f"   ä½¿ç”¨çš„åˆ†è¯å™¨: {type(tokenizer).__name__}")
    print(f"   è¯æ±‡è¡¨å¤§å°: {tokenizer.vocab_size}")

    # 4. åˆ›å»ºæ•°æ®é›†
    print(f"\nğŸ“Š åˆ›å»ºæ•°æ®é›†...")
    dataset = SimpleDialogueDataset(training_pairs, tokenizer)
    dataloader = DataLoader(dataset, batch_size=4, shuffle=True)
    print(f"   è®­ç»ƒæ‰¹æ¬¡æ•°: {len(dataloader)}")

    # 5. åˆ›å»ºæ¨¡å‹
    print(f"\nğŸ—ï¸ åˆ›å»ºAPTæ¨¡å‹...")
    config = create_small_hlbd_config(tokenizer.vocab_size)
    model = APTModel(config).to(device)

    total_params = sum(p.numel() for p in model.parameters())
    print(f"   æ¨¡å‹å‚æ•°: {total_params:,}")
    print(f"   é…ç½®: d_model={config.d_model}, layers={config.num_encoder_layers}")

    # 6. åˆ›å»ºä¼˜åŒ–å™¨
    optimizer = optim.Adam(model.parameters(), lr=5e-4)
    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)

    # 7. æ³¨å†ŒDBC hooks
    print(f"\nâš¡ æ³¨å†ŒDBC-DACåŠ é€Ÿ...")
    #hooks = register_dbc_hooks(model)
    hooks = [] # ä¿æŒ hooks å˜é‡å­˜åœ¨ï¼Œé˜²æ­¢åé¢æŠ¥é”™
    #print(f"   æ³¨å†Œäº† {len(hooks)} ä¸ªæ¢¯åº¦ç¨³å®šé’©å­")

    # 8. è®­ç»ƒæ¨¡å‹
    print(f"\n" + "="*60)
    print("ğŸƒ å¼€å§‹å¿«é€Ÿè®­ç»ƒ (çœ‹èƒ½å¦å¿«é€Ÿå­¦ä¼šè¯´è¯)")
    print("="*60)

    num_epochs = 10  # åªè®­ç»ƒ10ä¸ªepoch

    for epoch in range(num_epochs):
        loss = train_epoch(model, dataloader, optimizer, criterion, device, use_dbc=True)
        print(f"Epoch {epoch+1}/{num_epochs} - Loss: {loss:.4f}")

        # æ¯3ä¸ªepochæµ‹è¯•ä¸€æ¬¡
        if (epoch + 1) % 3 == 0 or epoch == num_epochs - 1:
            test_cases = [
                ("ğŸŒ§ï¸", "ä¸‹é›¨"),
                ("â¤ï¸", "æˆ‘çˆ±ä½ "),
                ("I love you", "æˆ‘çˆ±ä½ "),
                ("ä¸‹é›¨", "å¤©æ°”"),
            ]
            test_generation(model, tokenizer, test_cases, device)

    # 9. æœ€ç»ˆæµ‹è¯•
    print(f"\n" + "="*60)
    print("ğŸ¯ æœ€ç»ˆæµ‹è¯• - APTå­¦ä¼šè¯´è¯äº†å—?")
    print("="*60)

    final_test_cases = [
        ("ğŸŒ§ï¸", "ä¸‹é›¨"),
        ("â¤ï¸", "æˆ‘çˆ±ä½ "),
        ("ğŸ½ï¸", "åƒé¥­"),
        ("ğŸ“–", "çœ‹ä¹¦"),
        ("I love you", "æˆ‘çˆ±ä½ "),
        ("It's raining", "ä¸‹é›¨"),
        ("wÇ’ Ã i nÇ", "æˆ‘çˆ±ä½ "),
    ]

    test_generation(model, tokenizer, final_test_cases, device)

    # 10. æ€»ç»“
    print(f"\n" + "="*60)
    print("ğŸ“ æµ‹è¯•æ€»ç»“")
    print("="*60)
    print(f"âœ… è®­ç»ƒå®Œæˆ: {num_epochs} epochs")
    print(f"âœ… è®­ç»ƒæ ·æœ¬: {len(samples)} æ¦‚å¿µ, {len(training_pairs)} å¯¹")
    print(f"âœ… DBCåŠ é€Ÿ: {len(hooks)} ä¸ªé’©å­æ¿€æ´»")
    print(f"âœ… æ¨¡å‹å‚æ•°: {total_params:,}")
    print(f"\nğŸ’¡ è§‚å¯Ÿ:")
    print(f"   - APTæ¨¡å‹ä½¿ç”¨è‡ªç”Ÿæˆæ³¨æ„åŠ›æœºåˆ¶")
    print(f"   - DBC-DACç¨³å®šäº†æ¢¯åº¦è®­ç»ƒ")
    print(f"   - åˆ†å±‚è¯­è¨€å­¦ä¹ å¸®åŠ©å¿«é€ŸæŒæ¡æ¦‚å¿µ")
    print(f"   - ä»emoji/æ‹¼éŸ³/è‹±æ–‡åˆ°ä¸­æ–‡çš„å¤šå±‚æ˜ å°„")

    return model, tokenizer


if __name__ == "__main__":
    model, tokenizer = main()
