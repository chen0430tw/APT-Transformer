#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""å¿«é€Ÿæµ‹è¯•HLBDè®­ç»ƒ - çœ‹APTæ¨¡å‹èƒ½å¦å¿«é€Ÿå­¦ä¼šè¯´è¯"""

import sys
import os
import re
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
from tqdm import tqdm
import json
# from transformers import BertTokenizer  # å·²æ›¿æ¢ä¸º SimpleCharTokenizer_BACKUPï¼ˆæ”¯æŒ emojiï¼‰

# æ·»åŠ è·¯å¾„ï¼ˆåŠ¨æ€è®¡ç®—é¡¹ç›®æ ¹ç›®å½•ï¼‰
current_dir = os.path.dirname(os.path.abspath(__file__))
project_root = os.path.dirname(current_dir)
sys.path.insert(0, project_root)

from apt_model.modeling.apt_model import (
    APTModel,
    APTModelConfiguration,
    DBCDAC_Optimizer,
    create_gradient_stabilizer_hook
)
from apt_model.generation.generator import generate_natural_text
from apt_model.generation.evaluator import evaluate_text_quality


class SimpleCharTokenizer_BACKUP:
    """ç®€å•çš„å­—ç¬¦çº§åˆ†è¯å™¨"""
    def __init__(self):
        # åˆ›å»ºä¸€ä¸ªåŸºç¡€å­—ç¬¦è¡¨ï¼ˆåŒ…æ‹¬ä¸­æ–‡ã€è‹±æ–‡ã€emojiç­‰ï¼‰
        # æ·»åŠ è¯­è¨€æ ‡ç­¾ç”¨äºåŒºåˆ†ä¸åŒè¾“å…¥ç±»å‹
        self.vocab = {
            '[PAD]': 0, '[UNK]': 1, '[BOS]': 2, '[EOS]': 3,
            '[EMOJI]': 4, '[PHRASE]': 5, '[EN]': 6, '[PY]': 7, '[JP]': 8, '[KR]': 9,
        }
        self.pad_token_id = 0
        self.unk_token_id = 1
        self.bos_token_id = 2
        self.eos_token_id = 3
        self.vocab_size = 5000  # é¢„ç•™è¶³å¤Ÿçš„è¯æ±‡ç©ºé—´

        # æ·»åŠ å¸¸ç”¨å­—ç¬¦
        self.char_to_id = self.vocab.copy()
        self.id_to_char = {v: k for k, v in self.vocab.items()}
        self.next_id = 10  # ä»10å¼€å§‹ï¼Œå› ä¸º0-9å·²è¢«ç‰¹æ®Štokenå ç”¨

        # â­ æ–°å¢ï¼šé¢„ç¼–è¯‘æ­£åˆ™è¡¨è¾¾å¼ï¼ŒåŒ¹é… [TAG]
        self.tag_pattern = re.compile(r'(\[EMOJI\]|\[PHRASE\]|\[EN\]|\[PY\]|\[JP\]|\[KR\])')
    
    def _tokenize_text(self, text):
        """â­ æ ¸å¿ƒä¿®å¤ï¼šå…ˆåˆ‡åˆ†æ ‡ç­¾ï¼Œå†åˆ‡åˆ†å­—ç¬¦"""
        tokens = []
        # æŒ‰æ ‡ç­¾åˆ‡åˆ†
        parts = self.tag_pattern.split(text)
        for part in parts:
            if part in self.vocab:
                # å¦‚æœæ˜¯æ ‡ç­¾ï¼Œç›´æ¥æ·»åŠ ID
                tokens.append(self.vocab[part])
            else:
                # å¦‚æœæ˜¯æ™®é€šæ–‡æœ¬ï¼Œé€å­—å¤„ç†
                for char in part:
                    # è·³è¿‡ç©ºç™½å­—ç¬¦ï¼ˆå¯é€‰ï¼Œçœ‹ä½ éœ€æ±‚ï¼‰
                    if char.strip():
                        tokens.append(self._get_or_add_char(char))
                    elif char == ' ': # ä¿ç•™ç©ºæ ¼
                        tokens.append(self._get_or_add_char(char))
        return tokens

    def _get_or_add_char(self, char):
        """è·å–å­—ç¬¦IDï¼Œå¦‚æœä¸å­˜åœ¨åˆ™æ·»åŠ """
        if char not in self.char_to_id:
            if self.next_id < self.vocab_size:
                self.char_to_id[char] = self.next_id
                self.id_to_char[self.next_id] = char
                self.next_id += 1
            else:
                return self.unk_token_id
        return self.char_to_id[char]

    def encode(self, text, return_tensors=None):
        """ç¼–ç æ–‡æœ¬ä¸ºIDåºåˆ—"""
        ids = [self.bos_token_id]
        ids.extend(self._tokenize_text(text))
        ids.append(self.eos_token_id)

        if return_tensors == 'pt':
            return torch.tensor([ids])
        return ids

    def __call__(self, text, max_length=64, padding='max_length',
                 truncation=True, return_tensors='pt'):
        
        """åˆ†è¯æ¥å£ï¼ˆå…¼å®¹transformersï¼‰"""
        # 1. åˆå§‹åŒ– ids
        ids = [self.bos_token_id]

        # 2. â­ ä½¿ç”¨æ–°çš„åˆ‡åˆ†é€»è¾‘ (æ”¯æŒ [EMOJI])
        token_ids = self._tokenize_text(text)
        ids.extend(token_ids)
        
        # 3. åŠ  EOS
        ids.append(self.eos_token_id)

        # æˆªæ–­
        if truncation and len(ids) > max_length:
            ids = ids[:max_length]

        # å¡«å……
        if padding == 'max_length':
            while len(ids) < max_length:
                ids.append(self.pad_token_id)

        if return_tensors == 'pt':
            return {'input_ids': torch.tensor([ids])}
        return {'input_ids': ids}

    def decode(self, ids, skip_special_tokens=True):
        """è§£ç IDåºåˆ—ä¸ºæ–‡æœ¬"""
        chars = []
        for id in ids:
            if isinstance(id, torch.Tensor):
                id = id.item()

            if skip_special_tokens and id in [self.pad_token_id, self.bos_token_id,
                                               self.eos_token_id, self.unk_token_id]:
                continue

            char = self.id_to_char.get(id, '[UNK]')
            chars.append(char)

        return ''.join(chars)


def register_dbc_hooks(model):
    """ä¸ºæ¨¡å‹æ³¨å†ŒDBC-DAC hooks"""
    opt = DBCDAC_Optimizer()
    hooks = []
    for _, p in model.named_parameters():
        if p.requires_grad:
            hooks.append(p.register_hook(create_gradient_stabilizer_hook(opt)))
    return hooks


def load_hlbd_samples(data_path, max_samples=20):
    """åŠ è½½HLBDæ•°æ®æ ·æœ¬"""
    print(f"ğŸ“‚ åŠ è½½HLBDæ•°æ®: {data_path}")

    with open(data_path, 'r', encoding='utf-8') as f:
        content = f.read()

    # æå–sampleséƒ¨åˆ†
    start_idx = content.find('samples = [')
    if start_idx == -1:
        raise ValueError("æ‰¾ä¸åˆ°samplesæ•°æ®")

    # æå–JSONæ•°ç»„
    json_start = content.find('[', start_idx)
    # æ‰¾åˆ°åŒ¹é…çš„å³æ‹¬å·ï¼ˆç®€å•å¤„ç†ï¼‰
    bracket_count = 0
    json_end = json_start
    for i, char in enumerate(content[json_start:]):
        if char == '[':
            bracket_count += 1
        elif char == ']':
            bracket_count -= 1
            if bracket_count == 0:
                json_end = json_start + i + 1
                break

    json_str = content[json_start:json_end]
    samples = json.loads(json_str)

    if max_samples:
        samples = samples[:max_samples]

    print(f"   åŠ è½½äº† {len(samples)} ä¸ªæ ·æœ¬")
    return samples


def create_training_pairs(samples):
    """ä»HLBDæ ·æœ¬åˆ›å»ºè®­ç»ƒå¯¹"""
    pairs = []

    for sample in samples:
        concept = sample['concept']

        # åˆ›å»ºå¤šç§è®­ç»ƒå¯¹
        # 1. emoji -> ä¸­æ–‡
        if 'level_1' in sample and 'level_6' in sample:
            emoji = sample['level_1'].get('emoji', '')
            chinese = sample['level_6'].get('ä¸­æ–‡', '')
            if emoji and chinese:
                pairs.append((f"[EMOJI] {emoji}", chinese))

        # 2. çŸ­è¯­ -> ä¸­æ–‡
        if 'level_2' in sample and 'level_6' in sample:
            phrase = sample['level_2'].get('çŸ­è¯­', '')
            chinese = sample['level_6'].get('ä¸­æ–‡', '')
            if phrase and chinese:
                pairs.append((f"[PHRASE] {phrase}", chinese))

        # 3. è‹±æ–‡ -> ä¸­æ–‡
        if 'level_5' in sample and 'level_6' in sample:
            english = sample['level_5'].get('è‹±æ–‡', '')
            chinese = sample['level_6'].get('ä¸­æ–‡', '')
            if english and chinese:
                pairs.append((f"[EN] {english}", chinese))

        # 4. æ‹¼éŸ³ -> ä¸­æ–‡
        if 'level_4' in sample and 'level_6' in sample:
            pinyin = sample['level_4'].get('æ‹¼éŸ³', '')
            chinese = sample['level_6'].get('ä¸­æ–‡', '')
            if pinyin and chinese:
                pairs.append((f"[PY] {pinyin}", chinese))

        # 5. æ—¥æ–‡ -> ä¸­æ–‡
        if 'level_7' in sample and 'level_6' in sample:
            japanese = sample['level_7'].get('æ—¥æ–‡', '')
            chinese = sample['level_6'].get('ä¸­æ–‡', '')
            if japanese and chinese:
                pairs.append((f"[JP] {japanese}", chinese))

        # 6. éŸ©æ–‡ -> ä¸­æ–‡
        if 'level_8' in sample and 'level_6' in sample:
            korean = sample['level_8'].get('éŸ©ë¬¸', sample['level_8'].get('éŸ©æ–‡', ''))  # å…¼å®¹ä¸¤ç§é”®å
            chinese = sample['level_6'].get('ä¸­æ–‡', '')
            if korean and chinese:
                pairs.append((f"[KR] {korean}", chinese))

    print(f"   åˆ›å»ºäº† {len(pairs)} ä¸ªè®­ç»ƒå¯¹ï¼ˆå¸¦è¯­è¨€æ ‡ç­¾ï¼‰")
    return pairs


class SimpleDialogueDataset(Dataset):
    """ç®€å•å¯¹è¯æ•°æ®é›†"""
    def __init__(self, pairs, tokenizer, max_length=64):
        self.pairs = pairs
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.pairs)

    def __getitem__(self, idx):
        src_text, tgt_text = self.pairs[idx]

        # ç¼–ç æºæ–‡æœ¬
        src_encoding = self.tokenizer(
            src_text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        # ç¼–ç ç›®æ ‡æ–‡æœ¬
        tgt_encoding = self.tokenizer(
            tgt_text,
            max_length=self.max_length,
            padding='max_length',
            truncation=True,
            return_tensors='pt'
        )

        return (
            src_encoding['input_ids'].squeeze(0),
            tgt_encoding['input_ids'].squeeze(0)
        )


def create_small_hlbd_config(vocab_size):
    """åˆ›å»ºå°å‹HLBD APTé…ç½®"""
    config = APTModelConfiguration(
        vocab_size=vocab_size,
        d_model=256,              # ä¸­ç­‰ç»´åº¦
        max_seq_len=64,           # çŸ­åºåˆ—
        num_encoder_layers=3,     # 3å±‚ç¼–ç å™¨
        num_decoder_layers=3,     # 3å±‚è§£ç å™¨
        num_heads=8,              # 8ä¸ªæ³¨æ„åŠ›å¤´
        d_ff=1024,                # å‰é¦ˆç½‘ç»œ
        dropout=0.1,
        use_autopoietic=True,     # å¯ç”¨è‡ªç”Ÿæˆæœºåˆ¶
        use_dbc_dac=True,         # å¯ç”¨DBC-DAC
    )
    return config


from tqdm import tqdm # ç¡®ä¿æ–‡ä»¶å¼€å¤´å¯¼å…¥äº† tqdm


def generate_with_vocab_mask(model, input_ids, valid_token_ids, max_length,
                             repetition_penalty, pad_token_id, device,
                             temperature=1.0, top_p=0.9):
    """
    ä½¿ç”¨ vocab mask é™åˆ¶ç”ŸæˆèŒƒå›´çš„è‡ªå®šä¹‰ç”Ÿæˆå‡½æ•°

    Args:
        model: APT æ¨¡å‹
        input_ids: è¾“å…¥ token IDs
        valid_token_ids: å…è®¸çš„ token ID é›†åˆ
        max_length: æœ€å¤§é•¿åº¦
        repetition_penalty: é‡å¤æƒ©ç½š
        pad_token_id: padding ID
        device: è®¾å¤‡
        temperature: é‡‡æ ·æ¸©åº¦
        top_p: nucleus é‡‡æ ·å‚æ•°
    """
    model.eval()
    generated = input_ids.clone()

    # åˆ›å»º vocab maskï¼ˆåªå…è®¸ç”Ÿæˆå·²çŸ¥çš„ tokenï¼‰
    vocab_size = model.config.vocab_size
    vocab_mask = torch.zeros(vocab_size, dtype=torch.bool, device=device)
    for valid_id in valid_token_ids:
        if 0 <= valid_id < vocab_size:
            vocab_mask[valid_id] = True

    with torch.no_grad():
        for _ in range(max_length - input_ids.size(1)):
            # å‰å‘ä¼ æ’­
            outputs = model(generated, generated)
            logits = outputs[:, -1, :]  # [batch_size, vocab_size]

            # ğŸ”§ åº”ç”¨ vocab mask - åªå…è®¸ç”Ÿæˆå·²çŸ¥çš„ token
            logits[:, ~vocab_mask] = -float('inf')

            # é‡å¤æƒ©ç½š
            if repetition_penalty != 1.0:
                for token_id in set(generated[0].tolist()):
                    if token_id in valid_token_ids:
                        logits[0, token_id] /= repetition_penalty

            # æ¸©åº¦è°ƒæ•´
            logits = logits / max(temperature, 1e-5)

            # Top-p é‡‡æ ·
            probs = torch.nn.functional.softmax(logits, dim=-1)
            sorted_probs, sorted_indices = torch.sort(probs, descending=True)
            cumsum_probs = torch.cumsum(sorted_probs, dim=-1)

            # æ‰¾åˆ°ç´¯ç§¯æ¦‚ç‡è¶…è¿‡ top_p çš„ä½ç½®
            remove_mask = cumsum_probs > top_p
            remove_mask[:, 1:] = remove_mask[:, :-1].clone()
            remove_mask[:, 0] = False

            # ç§»é™¤ä½æ¦‚ç‡çš„ token
            sorted_probs[remove_mask] = 0.0
            probs_sum = sorted_probs.sum(dim=-1, keepdim=True)
            if probs_sum > 0:
                sorted_probs = sorted_probs / probs_sum

            # é‡‡æ ·
            try:
                next_token_idx = torch.multinomial(sorted_probs, num_samples=1)
                next_token = sorted_indices.gather(-1, next_token_idx)
            except:
                # å¦‚æœé‡‡æ ·å¤±è´¥ï¼Œä½¿ç”¨è´ªå¿ƒè§£ç 
                next_token = torch.argmax(logits, dim=-1, keepdim=True)

            generated = torch.cat([generated, next_token], dim=1)

    return generated


def train_epoch(model, dataloader, optimizer, criterion, device, use_dbc=False, accumulation_steps=4):
    """è®­ç»ƒä¸€ä¸ªepochï¼ˆä½¿ç”¨æ¢¯åº¦ç´¯ç§¯ï¼‰"""
    model.train()
    total_loss = 0
    total_steps = 0

    ACCUMULATION_STEPS = accumulation_steps

    progress_bar = tqdm(
        dataloader,
        desc="Training",
        leave=False,
        mininterval=0.1,
        ascii=True
    )

    for i, (src_ids, tgt_ids) in enumerate(progress_bar):

        # æ•°æ®ä¼ è¾“åˆ°è®¾å¤‡
        src_ids = src_ids.to(device)
        tgt_ids = tgt_ids.to(device)

        # å‰å‘ä¼ æ’­
        output = model(src_ids, tgt_ids[:, :-1])

        # è®¡ç®—æŸå¤±
        loss = criterion(
            output.reshape(-1, output.size(-1)),
            tgt_ids[:, 1:].reshape(-1)
        )

        # æŸå¤±å½’ä¸€åŒ–
        loss = loss / ACCUMULATION_STEPS

        # åå‘ä¼ æ’­
        loss.backward()

        # æ¡ä»¶ä¼˜åŒ–å’Œæ¸…é›¶ï¼ˆæ¯Næ­¥æ‰§è¡Œä¸€æ¬¡ï¼‰
        if (i + 1) % ACCUMULATION_STEPS == 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            optimizer.zero_grad()

        total_loss += loss.item() * ACCUMULATION_STEPS
        total_steps += 1

        # å®æ—¶æ›´æ–°è¿›åº¦æ¡ä¸Šçš„Loss
        progress_bar.set_postfix({'loss': f'{loss.item() * ACCUMULATION_STEPS:.4f}'})

    # ã€æœ€åä¸€æ­¥æ¸…ç†ã€‘å¤„ç†å‰©ä½™çš„ç´¯ç§¯æ¢¯åº¦
    try:
        if (i + 1) % ACCUMULATION_STEPS != 0:
            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)
            optimizer.step()
            optimizer.zero_grad()
    except NameError:
         # å¦‚æœiæ²¡å®šä¹‰ï¼ˆæ¯”å¦‚dataloaderæ˜¯ç©ºçš„ï¼‰ï¼Œåˆ™å¿½ç•¥æ¸…ç†
         pass

    return (total_loss / total_steps) if total_steps > 0 else 0


def generate_text(model, tokenizer, input_text, device, max_length=50, repetition_penalty=1.5):
    """
    ç”Ÿæˆæ–‡æœ¬ï¼ˆä¿®å¤ï¼šæ”¯æŒ emojiï¼Œå»é™¤è¾“å…¥å¤è¯»ï¼Œé™åˆ¶ vocab èŒƒå›´ï¼‰
    """
    model.eval()

    # 1. å‡†å¤‡ Encoder è¾“å…¥ (Prompt)
    # æ³¨æ„ï¼šè¿™é‡Œçš„ input_text åº”è¯¥åŒ…å«æ ‡ç­¾ï¼Œå¦‚ "[EMOJI] ğŸŒ§ï¸"
    input_encoded = tokenizer(input_text, max_length=64, padding=False, return_tensors='pt')
    src_ids = input_encoded['input_ids'].to(device) # [1, src_len]

    # 2. å‡†å¤‡ Decoder è¾“å…¥ (Start Token)
    # Decoder ä» [BOS] å¼€å§‹ï¼Œè€Œä¸æ˜¯ä» Prompt å¼€å§‹ï¼
    decoder_input = torch.tensor([[tokenizer.bos_token_id]], device=device) # [1, 1]
    
    generated_ids = []

    with torch.no_grad():
        for _ in range(max_length):
            # 3. å‡†å¤‡æ¨¡å‹è¾“å…¥ input_ids = [BOS] + Prompt Tokens
            # æ³¨æ„ï¼šAPTModel çš„ forward æ¥å— src_tokens å’Œ tgt_tokens
            outputs = model(src_tokens=src_ids, tgt_tokens=decoder_input)

            # å–æœ€åä¸€ä¸ª token çš„ logits
            logits = outputs[:, -1, :] # [batch, vocab]

            # --- å¼ºåˆ¶ç¦æ­¢å¤è¯»çš„å…³é”®ä»£ç  ---
            if repetition_penalty != 1.0:
                # éå†å·²ç»ç”Ÿæˆè¿‡çš„æ‰€æœ‰ token
                for token_id in set(generated_ids):
                    # å¦‚æœ logit æ˜¯æ­£æ•°ï¼Œé™¤ä»¥æƒ©ç½šç³»æ•°ï¼ˆå˜å°ï¼‰
                    if logits[0, token_id] > 0:
                        logits[0, token_id] /= repetition_penalty
                    # å¦‚æœ logit æ˜¯è´Ÿæ•°ï¼Œä¹˜ä»¥æƒ©ç½šç³»æ•°ï¼ˆå˜å¾—æ›´è´Ÿï¼Œæ›´ä¸å¯èƒ½è¢«é€‰ä¸­ï¼‰
                    else:
                        logits[0, token_id] *= repetition_penalty

            # 4. é™åˆ¶ç”ŸæˆèŒƒå›´ (Vocab Mask) & é‡å¤æƒ©ç½š
            # ... (ç®€åŒ–çš„é‡‡æ ·é€»è¾‘) ...
            logits[:, tokenizer.pad_token_id] = -float('inf') # åˆ«ç”Ÿæˆ PAD
            logits[:, tokenizer.unk_token_id] = -float('inf') # åˆ«ç”Ÿæˆ UNK

            # ç®€å•è´ªå©ªè§£ç  (ä¸ºäº†æµ‹è¯•ç¨³å®šæ€§ï¼Œå…ˆç”¨è´ªå©ª)
            next_token = torch.argmax(logits, dim=-1, keepdim=True)

            # 5. åœæ­¢æ¡ä»¶
            if next_token.item() == tokenizer.eos_token_id:
                break

            # 6. æŠŠæ–°å­—åŠ åˆ° Decoder è¾“å…¥é‡Œï¼Œå‡†å¤‡ä¸‹ä¸€è½®
            generated_ids.append(next_token.item())
            decoder_input = torch.cat([decoder_input, next_token], dim=1)

    return tokenizer.decode(generated_ids)


def test_generation(model, tokenizer, test_cases, device):
    """æµ‹è¯•ç”Ÿæˆèƒ½åŠ›"""
    print("\n" + "="*60)
    print("ğŸ—£ï¸ æµ‹è¯•å¯¹è¯ç”Ÿæˆèƒ½åŠ›")
    print("="*60)

    # è®¾å®šå¼ºåŠ›é‡å¤æƒ©ç½šå› å­
    REPETITION_FACTOR = 1.5

    for input_text, expected_concept in test_cases:
        generated = generate_text(model, tokenizer, input_text, device, repetition_penalty=REPETITION_FACTOR)
        input_ids = tokenizer.encode(input_text)

        ids_display = str(input_ids)
        if len(input_ids) > 8:
            ids_display = f"[{input_ids[0]}, {input_ids[1]}, ..., {input_ids[-1]}]"
        
        print(f"ğŸ•µï¸ Debug: Len={len(input_ids)} | IDs={ids_display}")
        print(f"\nè¾“å…¥: {input_text}")
        print(f"æœŸæœ›æ¦‚å¿µ: {expected_concept}")
        print(f"ç”Ÿæˆ: {generated}")


def save_model_and_tokenizer(model, tokenizer, config, save_dir, num_epochs, final_loss):
    """
    ä¿å­˜è®­ç»ƒå¥½çš„æ¨¡å‹å’Œ tokenizer

    Args:
        model: è®­ç»ƒå¥½çš„ APT æ¨¡å‹
        tokenizer: SimpleCharTokenizer_BACKUP å®ä¾‹
        config: APTModelConfiguration å®ä¾‹
        save_dir: ä¿å­˜ç›®å½•
        num_epochs: è®­ç»ƒçš„æ€» epoch æ•°
        final_loss: æœ€ç»ˆçš„æŸå¤±å€¼
    """
    import datetime

    os.makedirs(save_dir, exist_ok=True)

    # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    model_filename = f'hlbd_model_{timestamp}.pt'
    model_path = os.path.join(save_dir, model_filename)

    # ä¿å­˜æ¨¡å‹ã€tokenizer å’Œé…ç½®
    checkpoint = {
        'model_state_dict': model.state_dict(),
        'tokenizer_char_to_id': tokenizer.char_to_id,
        'tokenizer_id_to_char': tokenizer.id_to_char,
        'tokenizer_next_id': tokenizer.next_id,
        'tokenizer_vocab_size': tokenizer.vocab_size,
        'config': {
            'vocab_size': config.vocab_size,
            'd_model': config.d_model,
            'max_seq_len': config.max_seq_len,
            'num_encoder_layers': config.num_encoder_layers,
            'num_decoder_layers': config.num_decoder_layers,
            'num_heads': config.num_heads,
            'd_ff': config.d_ff,
            'dropout': config.dropout,
            'use_autopoietic': config.use_autopoietic,
            'use_dbc_dac': config.use_dbc_dac,
        },
        'training_info': {
            'num_epochs': num_epochs,
            'final_loss': final_loss,
            'timestamp': timestamp,
        }
    }

    torch.save(checkpoint, model_path)

    print(f"\nğŸ’¾ æ¨¡å‹å·²ä¿å­˜:")
    print(f"   è·¯å¾„: {os.path.abspath(model_path)}")
    print(f"   å¤§å°: {os.path.getsize(model_path) / 1024 / 1024:.2f} MB")

    return model_path


def load_model_and_tokenizer(model_path, device):
    """
    åŠ è½½å·²ä¿å­˜çš„æ¨¡å‹å’Œ tokenizer

    Args:
        model_path: æ¨¡å‹æ–‡ä»¶è·¯å¾„
        device: è®¾å¤‡ï¼ˆcuda æˆ– cpuï¼‰

    Returns:
        model: åŠ è½½çš„ APT æ¨¡å‹
        tokenizer: åŠ è½½çš„ SimpleCharTokenizer_BACKUP
        training_info: è®­ç»ƒä¿¡æ¯å­—å…¸
    """
    print(f"\nğŸ“‚ åŠ è½½æ¨¡å‹: {model_path}")

    # åŠ è½½ checkpoint
    checkpoint = torch.load(model_path, map_location=device)

    # é‡å»ºé…ç½®
    config_dict = checkpoint['config']
    config = APTModelConfiguration(
        vocab_size=config_dict['vocab_size'],
        d_model=config_dict['d_model'],
        max_seq_len=config_dict['max_seq_len'],
        num_encoder_layers=config_dict['num_encoder_layers'],
        num_decoder_layers=config_dict['num_decoder_layers'],
        num_heads=config_dict['num_heads'],
        d_ff=config_dict['d_ff'],
        dropout=config_dict['dropout'],
        use_autopoietic=config_dict['use_autopoietic'],
        use_dbc_dac=config_dict['use_dbc_dac'],
    )

    # é‡å»ºæ¨¡å‹
    model = APTModel(config).to(device)
    model.load_state_dict(checkpoint['model_state_dict'])

    # é‡å»º tokenizer
    tokenizer = SimpleCharTokenizer_BACKUP()
    tokenizer.char_to_id = checkpoint['tokenizer_char_to_id']
    tokenizer.id_to_char = checkpoint['tokenizer_id_to_char']
    tokenizer.next_id = checkpoint['tokenizer_next_id']
    tokenizer.vocab_size = checkpoint['tokenizer_vocab_size']

    # è·å–è®­ç»ƒä¿¡æ¯
    training_info = checkpoint.get('training_info', {})

    print(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸ!")
    print(f"   è®­ç»ƒ epoch: {training_info.get('num_epochs', 'N/A')}")
    print(f"   æœ€ç»ˆæŸå¤±: {training_info.get('final_loss', 'N/A'):.4f}")
    print(f"   ä¿å­˜æ—¶é—´: {training_info.get('timestamp', 'N/A')}")
    print(f"   è¯æ±‡è¡¨å¤§å°: {len(tokenizer.char_to_id)}")

    return model, tokenizer, training_info


def evaluate_hlbd_model(untrained_model, trained_model, tokenizer, device):
    """è¯„ä¼°HLBDè®­ç»ƒå‰åçš„æ¨¡å‹è´¨é‡"""
    # æµ‹è¯•æç¤ºï¼ˆå¸¦è¯­è¨€æ ‡ç­¾ï¼‰
    test_prompts = [
        "[EMOJI] ğŸŒ§ï¸",  # emojiæµ‹è¯•
        "[EMOJI] â¤ï¸",  # emojiæµ‹è¯•
        "[EN] It's raining",  # è‹±æ–‡æµ‹è¯•
        "[PY] wÇ’ Ã i nÇ",  # æ‹¼éŸ³æµ‹è¯•
        "[JP] æ„›ã—ã¦ã‚‹",  # æ—¥æ–‡æµ‹è¯•
        "[KR] ì‚¬ë‘í•´",  # éŸ©æ–‡æµ‹è¯•
    ]

    untrained_model.eval()
    trained_model.eval()
    untrained_scores = []
    trained_scores = []

    print(f"\n" + "="*60)
    print("å®‰æŸã®è©•ä¾¡ | Amber's Evaluation")
    print("="*60)

    for prompt in test_prompts:
        with torch.no_grad():
            # æœªè®­ç»ƒæ¨¡å‹
            untrained_text, _, _, _ = generate_natural_text(untrained_model, tokenizer, prompt, max_steps=15)
            untrained_score, untrained_feedback = evaluate_text_quality(untrained_text)
            untrained_scores.append(untrained_score)

            # è®­ç»ƒåæ¨¡å‹
            trained_text, _, _, _ = generate_natural_text(trained_model, tokenizer, prompt, max_steps=15)
            trained_score, trained_feedback = evaluate_text_quality(trained_text)
            trained_scores.append(trained_score)

    avg_untrained = sum(untrained_scores) / len(untrained_scores) if untrained_scores else 0
    avg_trained = sum(trained_scores) / len(trained_scores) if trained_scores else 0
    improvement = avg_trained - avg_untrained

    # æœ€ç»ˆè¯„ä¼°
    print(f"\næ•´ä½“è¯„ä¼°:")
    print(f"æœªè®­ç»ƒæ¨¡å‹å¹³å‡è´¨é‡: {avg_untrained:.2f}/100")
    print(f"è®­ç»ƒåæ¨¡å‹å¹³å‡è´¨é‡: {avg_trained:.2f}/100")
    print(f"è´¨é‡æå‡: {improvement:.2f} åˆ†")

    # å®‰æŸçš„æœ€ç»ˆè¯„ä»·
    if improvement < -5:
        print("\nå®‰æŸï¼šå¥‡æ€ªâ€¦â€¦æ€ä¹ˆæ„Ÿè§‰å®ƒå˜ç¬¨äº†ï¼Ÿï¼ˆè´¨é‡ä¸‹é™ï¼Œå»ºè®®æ£€æŸ¥è¶…å‚æ•°ï¼‰")
    elif improvement < 0:
        print("\nå®‰æŸï¼šçœ‹èµ·æ¥æ•ˆæœå·®ä¸å¤šï¼Œä¹Ÿè®¸è¿˜éœ€è¦æ›´å¤šè®­ç»ƒæ•°æ®ï¼Ÿ")
    elif avg_trained < 50:
        print("\nå®‰æŸï¼šè™½ç„¶æœ‰è¿›æ­¥ï¼Œä½†è¿˜è¿œè¿œä¸å¤Ÿå“¦ï¼ç»§ç»­åŠ æ²¹ï¼")
    else:
        print("\nå®‰æŸï¼šè®­ç»ƒå®Œæˆå¾—ä¸é”™ï¼ä¾¦å¯Ÿéª‘å£«ä¸ºä½ ç‚¹èµï¼")

    print("="*60)


def main():
    """ä¸»å‡½æ•°"""
    print("\nğŸš€ HLBDå¿«é€Ÿå­¦ä¹ æµ‹è¯• - APTæ¨¡å‹èƒ½å¦å¿«é€Ÿå­¦ä¼šè¯´è¯?")
    print(f"PyTorchç‰ˆæœ¬: {torch.__version__}")

    ACCUMULATION_STEPS = 8  # ä¿æŒåŸå§‹é…ç½®ï¼šbatch_size=4, 4 * 8 = 32 çš„æœ‰æ•ˆæ‰¹æ¬¡å¤§å°

    # è‡ªåŠ¨æ£€æµ‹ï¼šæœ‰æ˜¾å¡å°±ç”¨æ˜¾å¡ï¼Œæ²¡æœ‰æ‰ç”¨ CPU
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"è®¾å¤‡: {device}")

    # 1. åŠ è½½HLBDæ•°æ®
    current_dir = os.path.dirname(os.path.abspath(__file__)) # è·å–å½“å‰è„šæœ¬æ‰€åœ¨ç›®å½• (tests)
    project_root = os.path.dirname(current_dir)              # è·å–é¡¹ç›®æ ¹ç›®å½• (APT-Transformer)
    data_path = os.path.join(project_root, 'apt_model', 'åˆ†å±‚è¯­è¨€å¯è’™æ•°æ®é›†.txt')
    samples = load_hlbd_samples(data_path, max_samples=None)

    # æ˜¾ç¤ºå‡ ä¸ªæ ·æœ¬
    print(f"\nğŸ“ æ ·æœ¬ç¤ºä¾‹:")
    for i, sample in enumerate(samples[:3]):
        print(f"\n   æ ·æœ¬ {i+1}: {sample['concept']}")
        print(f"      Emoji: {sample['level_1'].get('emoji', 'N/A')}")
        print(f"      ä¸­æ–‡: {sample['level_6'].get('ä¸­æ–‡', 'N/A')[:30]}...")

    # 2. åˆ›å»ºè®­ç»ƒå¯¹
    training_pairs = create_training_pairs(samples)

    # 3. å‡†å¤‡åˆ†è¯å™¨
    print(f"\nğŸ”§ å‡†å¤‡åˆ†è¯å™¨...")
    # ä½¿ç”¨ SimpleCharTokenizer_BACKUPï¼ˆæ”¯æŒ emoji åŠ¨æ€æ·»åŠ ï¼‰
    tokenizer = SimpleCharTokenizer_BACKUP()
    print(f"   ä½¿ç”¨çš„åˆ†è¯å™¨: {type(tokenizer).__name__}")
    print(f"   åˆå§‹è¯æ±‡è¡¨: {len(tokenizer.char_to_id)} ä¸ªtoken (é¢„ç•™ç©ºé—´: {tokenizer.vocab_size})")
    print(f"   åˆå§‹token: {list(tokenizer.char_to_id.keys())}")

    # 4. åˆ›å»ºæ•°æ®é›†
    print(f"\nğŸ“Š åˆ›å»ºæ•°æ®é›†...")
    dataset = SimpleDialogueDataset(training_pairs, tokenizer)

    # ã€å…³é”®ä¿®å¤ã€‘é¢„å¡«å……è¯æ±‡è¡¨ï¼Œé¿å…å¤šè¿›ç¨‹é™·é˜±
    # åœ¨å¤šè¿›ç¨‹ DataLoader å¯åŠ¨å‰ï¼Œè®©ä¸»è¿›ç¨‹çš„ tokenizer å­¦ä¹ æ‰€æœ‰å­—ç¬¦
    print(f"\nğŸ“ é¢„å¡«å……è¯æ±‡è¡¨ï¼ˆé¿å…å¤šè¿›ç¨‹é™·é˜±ï¼‰...")
    for src, tgt in training_pairs:
        _ = tokenizer.encode(src)
        _ = tokenizer.encode(tgt)
    print(f"   è¯æ±‡è¡¨é¢„å¡«å……å®Œæˆ: {len(tokenizer.char_to_id)} ä¸ªtoken")

    # ä¼˜åŒ–ï¼šä¿æŒåŸå§‹batch_sizeï¼Œåªæ·»åŠ å¤šçº¿ç¨‹åŠ è½½
    dataloader = DataLoader(
        dataset,
        batch_size=4,  # ä¿æŒåŸå§‹batch_size=4ï¼ˆç¨³å®šæ€§ä¼˜å…ˆï¼‰
        shuffle=True,
        num_workers=4,  # ä½¿ç”¨4ä¸ªå·¥ä½œè¿›ç¨‹å¹¶è¡ŒåŠ è½½ï¼ˆç°åœ¨å®‰å…¨äº†ï¼‰
        pin_memory=True,  # å›ºå®šå†…å­˜ï¼ŒåŠ é€ŸCPUâ†’GPUä¼ è¾“
        persistent_workers=True  # ä¿æŒworkerå­˜æ´»ï¼Œé¿å…é‡å¤åˆ›å»º
    )
    print(f"   è®­ç»ƒæ‰¹æ¬¡æ•°: {len(dataloader)}")

    # ã€æ–°å¢éªŒè¯ä»£ç ï¼šæ£€æŸ¥å®é™…æ ·æœ¬æ•°ã€‘
    actual_pairs = len(dataset)
    print(f"--- é•¿åº¦éªŒè¯ ---")
    print(f"æ¨¡å‹å®é™…çœ‹åˆ°çš„è®­ç»ƒå¯¹æ•°é‡: {actual_pairs} (æ¯ä¸ªæ¦‚å¿µ6ä¸ªå±‚çº§æ˜ å°„)")
    print(f"   emoji/çŸ­è¯­/è‹±æ–‡/æ‹¼éŸ³/æ—¥æ–‡/éŸ©æ–‡ â†’ ä¸­æ–‡")
    print(f"----------------")

    # ã€è¯æ±‡è¡¨å¢é•¿éªŒè¯ã€‘
    print(f"\nğŸ“Š è¯æ±‡è¡¨åŠ¨æ€å¢é•¿æƒ…å†µ:")
    print(f"   å¤„ç†æ•°æ®åçš„è¯æ±‡è¡¨å¤§å°: {len(tokenizer.char_to_id)} ä¸ªtoken")
    print(f"   æ–°å¢tokenæ•°é‡: {len(tokenizer.char_to_id) - 10}")
    print(f"   ä¸‹ä¸€ä¸ªID: {tokenizer.next_id}")
    print(f"   é¢„ç•™ç©ºé—´åˆ©ç”¨ç‡: {len(tokenizer.char_to_id)}/{tokenizer.vocab_size} ({100*len(tokenizer.char_to_id)/tokenizer.vocab_size:.1f}%)")

    # æ˜¾ç¤ºå‰20ä¸ªåŠ¨æ€æ·»åŠ çš„å­—ç¬¦ï¼ˆè·³è¿‡ç‰¹æ®Štokenï¼‰
    dynamic_chars = [char for char, idx in sorted(tokenizer.char_to_id.items(), key=lambda x: x[1]) if idx >= 10][:20]
    print(f"   å‰20ä¸ªåŠ¨æ€æ·»åŠ çš„å­—ç¬¦: {dynamic_chars}")

    # 5. åˆ›å»ºæ¨¡å‹
    print(f"\nğŸ—ï¸ åˆ›å»ºAPTæ¨¡å‹...")
    config = create_small_hlbd_config(tokenizer.vocab_size)
    model = APTModel(config).to(device)

    total_params = sum(p.numel() for p in model.parameters())
    print(f"   æ¨¡å‹å‚æ•°: {total_params:,}")
    print(f"   é…ç½®: d_model={config.d_model}, layers={config.num_encoder_layers}")

    # ä¿å­˜æœªè®­ç»ƒæ¨¡å‹çš„å‰¯æœ¬ç”¨äºè¯„ä¼°å¯¹æ¯”
    untrained_model = APTModel(config).to(device)
    untrained_model.load_state_dict(model.state_dict())
    untrained_model.eval()

    # 6. åˆ›å»ºä¼˜åŒ–å™¨
    # ä½¿ç”¨åŸå§‹å­¦ä¹ ç‡ï¼ˆç¨³å®šæ€§ä¼˜å…ˆï¼‰
    optimizer = optim.Adam(model.parameters(), lr=5e-5)
    criterion = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)

    # 7. æ³¨å†ŒDBC hooks
    print(f"\nâš¡ æ³¨å†ŒDBC-DACåŠ é€Ÿ...")
    #hooks = register_dbc_hooks(model)
    hooks = [] # ä¿æŒ hooks å˜é‡å­˜åœ¨ï¼Œé˜²æ­¢åé¢æŠ¥é”™
    #print(f"   æ³¨å†Œäº† {len(hooks)} ä¸ªæ¢¯åº¦ç¨³å®šé’©å­")

    # 8. è®­ç»ƒæ¨¡å‹
    print(f"\n" + "="*60)
    print("ğŸƒ å¼€å§‹å¿«é€Ÿè®­ç»ƒ (çœ‹èƒ½å¦å¿«é€Ÿå­¦ä¼šè¯´è¯)")
    print("="*60)

    num_epochs = 150  # 600ä¸ªè®­ç»ƒå¯¹ï¼ˆ100æ¦‚å¿µÃ—6å±‚çº§ï¼šemoji/çŸ­è¯­/è‹±æ–‡/æ‹¼éŸ³/æ—¥æ–‡/éŸ©æ–‡â†’ä¸­æ–‡ï¼‰

    for epoch in range(num_epochs):
        loss = train_epoch(model, dataloader, optimizer, criterion, device, use_dbc=True, accumulation_steps=ACCUMULATION_STEPS)
        print(f"Epoch {epoch+1}/{num_epochs} - Loss: {loss:.4f}")

        # æ¯5ä¸ªepochæµ‹è¯•ä¸€æ¬¡
        if (epoch + 1) % 5 == 0 or epoch == num_epochs - 1:
            test_cases = [
                ("[EMOJI] ğŸŒ§ï¸", "ä¸‹é›¨"),
                ("[EMOJI] â¤ï¸", "æˆ‘çˆ±ä½ "),
                ("[EN] I love you", "æˆ‘çˆ±ä½ "),
                ("[JP] æ„›ã—ã¦ã‚‹", "æˆ‘çˆ±ä½ "),  # æ—¥æ–‡æµ‹è¯•
                ("[KR] ì‚¬ë‘í•´", "æˆ‘çˆ±ä½ "),  # éŸ©æ–‡æµ‹è¯•
            ]
            test_generation(model, tokenizer, test_cases, device)

    # 9. æœ€ç»ˆæµ‹è¯•
    print(f"\n" + "="*60)
    print("ğŸ¯ æœ€ç»ˆæµ‹è¯• - APTå­¦ä¼šè¯´è¯äº†å—?")
    print("="*60)

    final_test_cases = [
        ("[EMOJI] ğŸŒ§ï¸", "ä¸‹é›¨"),
        ("[EMOJI] â¤ï¸", "æˆ‘çˆ±ä½ "),
        ("[EMOJI] ğŸ½ï¸", "åƒé¥­"),
        ("[EMOJI] ğŸ“–", "çœ‹ä¹¦"),
        ("[EN] I love you", "æˆ‘çˆ±ä½ "),
        ("[EN] It's raining", "ä¸‹é›¨"),
        ("[PY] wÇ’ Ã i nÇ", "æˆ‘çˆ±ä½ "),
        ("[JP] æ„›ã—ã¦ã‚‹", "æˆ‘çˆ±ä½ "),  # æ—¥æ–‡
        ("[JP] é›¨ãŒé™ã£ã¦ã„ã¾ã™", "ä¸‹é›¨"),  # æ—¥æ–‡
        ("[KR] ì‚¬ë‘í•´", "æˆ‘çˆ±ä½ "),  # éŸ©æ–‡
        ("[KR] ë¹„ê°€ ì˜¤ê³  ìˆì–´ìš”", "ä¸‹é›¨"),  # éŸ©æ–‡
    ]

    test_generation(model, tokenizer, final_test_cases, device)

    # 9.5 å®‰æŸè¯„ä¼°
    try:
        evaluate_hlbd_model(untrained_model, model, tokenizer, device)
    except Exception as e:
        print(f"\nâš ï¸ å®‰æŸè¯„ä¼°å‡ºé”™: {e}")

    # 10. æ€»ç»“
    print(f"\n" + "="*60)
    print("ğŸ“ æµ‹è¯•æ€»ç»“")
    print("="*60)
    print(f"âœ… è®­ç»ƒå®Œæˆ: {num_epochs} epochs")
    print(f"âœ… è®­ç»ƒæ ·æœ¬: {len(samples)} æ¦‚å¿µ, {len(training_pairs)} å¯¹")
    print(f"âœ… DBCåŠ é€Ÿ: {len(hooks)} ä¸ªé’©å­æ¿€æ´»")
    print(f"âœ… æ¨¡å‹å‚æ•°: {total_params:,}")
    print(f"\nğŸ’¡ è§‚å¯Ÿ:")
    print(f"   - APTæ¨¡å‹ä½¿ç”¨è‡ªç”Ÿæˆæ³¨æ„åŠ›æœºåˆ¶")
    print(f"   - DBC-DACç¨³å®šäº†æ¢¯åº¦è®­ç»ƒ")
    print(f"   - åˆ†å±‚è¯­è¨€å­¦ä¹ å¸®åŠ©å¿«é€ŸæŒæ¡æ¦‚å¿µ")
    print(f"   - ä»emoji/æ‹¼éŸ³/è‹±æ–‡åˆ°ä¸­æ–‡çš„å¤šå±‚æ˜ å°„")

    # 11. ä¿å­˜æ¨¡å‹
    save_dir = os.path.join(project_root, 'tests', 'saved_models')
    model_path = save_model_and_tokenizer(
        model=model,
        tokenizer=tokenizer,
        config=config,
        save_dir=save_dir,
        num_epochs=num_epochs,
        final_loss=loss
    )

    return model, tokenizer, model_path


if __name__ == "__main__":
    model, tokenizer, model_path = main()

    # å¯é€‰ï¼šæµ‹è¯•åŠ è½½åŠŸèƒ½
    print("\n" + "="*60)
    print("ğŸ”„ æµ‹è¯•æ¨¡å‹åŠ è½½åŠŸèƒ½")
    print("="*60)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    loaded_model, loaded_tokenizer, training_info = load_model_and_tokenizer(model_path, device)

    # éªŒè¯åŠ è½½çš„æ¨¡å‹
    test_cases = [
        ("[EMOJI] ğŸŒ§ï¸", "ä¸‹é›¨"),
        ("[EMOJI] â¤ï¸", "æˆ‘çˆ±ä½ "),
    ]

    print("\nä½¿ç”¨åŠ è½½çš„æ¨¡å‹ç”Ÿæˆ:")
    test_generation(loaded_model, loaded_tokenizer, test_cases, device)

    print("\nâœ… æ¨¡å‹ä¿å­˜å’ŒåŠ è½½åŠŸèƒ½æ­£å¸¸ï¼")
