# APT-Transformer æœªå®Œæˆå·¥ä½œæ¸…å•

**ç”Ÿæˆæ—¥æœŸ**: 2025-11-29
**åˆ†æ”¯**: claude/review-memo-updates-01VZwZoRpMTGwNff9jviR9k7
**æ•´ä½“å®Œæˆåº¦**: 68%

---

## åˆ†ç±»è¯´æ˜

- ğŸ”´ **Critical**: é˜»å¡ç”Ÿäº§ä½¿ç”¨ï¼Œå¿…é¡»ç«‹å³ä¿®å¤
- ğŸŸ¡ **High**: ä¸¥é‡å½±å“å¯ç”¨æ€§ï¼Œè¿‘æœŸä¿®å¤
- ğŸŸ¢ **Medium**: å½±å“ç”¨æˆ·ä½“éªŒï¼Œè®¡åˆ’ä¿®å¤
- ğŸ”µ **Low**: é”¦ä¸Šæ·»èŠ±ï¼Œæœ‰æ—¶é—´å†åš

**å·¥ä½œé‡ä¼°è®¡**:
- Small: < 4å°æ—¶
- Medium: 4-16å°æ—¶ï¼ˆ0.5-2å¤©ï¼‰
- Large: 16-40å°æ—¶ï¼ˆ2-5å¤©ï¼‰
- XLarge: > 40å°æ—¶ï¼ˆ> 1å‘¨ï¼‰

---

## ğŸ”´ Critical - å¿…é¡»ç«‹å³ä¿®å¤ (3é¡¹)

### C1: é›†æˆCheckpointManageråˆ°è®­ç»ƒå™¨ ğŸ”´
**ä¼˜å…ˆçº§**: P0 - Critical
**å·¥ä½œé‡**: Medium (8-12å°æ—¶)
**å½±å“**: è®­ç»ƒä¸­æ–­åæ— æ³•æ¢å¤ï¼Œæµªè´¹è®¡ç®—èµ„æº

**é—®é¢˜æè¿°**:
- `apt_model/training/checkpoint.py` ä¸­çš„ `CheckpointManager` ç±»å®Œæ•´å®ç°äº†checkpointä¿å­˜/åŠ è½½
- ä½† `apt_model/training/trainer.py:780` åªè°ƒç”¨ `save_model()` ä¿å­˜æ¨¡å‹æƒé‡
- **ç¼ºå¤±**: optimizerçŠ¶æ€ã€schedulerçŠ¶æ€ã€epochã€stepã€losså†å²

**å½“å‰ä»£ç **:
```python
# trainer.py:780 (é”™è¯¯ç¤ºä¾‹)
save_model(model, tokenizer, path=save_path, config=config)
# âŒ åªä¿å­˜æ¨¡å‹æƒé‡ï¼Œæ— æ³•æ¢å¤è®­ç»ƒ
```

**æœŸæœ›ä»£ç **:
```python
# trainer.py (æ­£ç¡®ç¤ºä¾‹)
checkpoint_mgr = CheckpointManager(
    save_dir="./outputs",
    model_name="apt_model",
    save_freq=1
)

# æ¢å¤è®­ç»ƒ
start_epoch = 0
if resume_from:
    start_epoch, global_step, loss_history, metrics = checkpoint_mgr.load_checkpoint(
        model=model,
        optimizer=optimizer,
        scheduler=scheduler,
        checkpoint_path=resume_from
    )

# è®­ç»ƒå¾ªç¯
for epoch in range(start_epoch, epochs):
    # ... è®­ç»ƒä»£ç  ...

    # ä¿å­˜å®Œæ•´checkpoint
    checkpoint_mgr.save_checkpoint(
        model=model,
        optimizer=optimizer,
        scheduler=scheduler,
        epoch=epoch,
        global_step=global_step,
        loss_history=train_losses,
        metrics={"avg_loss": avg_loss},
        tokenizer=tokenizer,
        config=config,
        is_best=(avg_loss < best_loss)
    )
```

**æ–‡ä»¶ä¿®æ”¹**:
1. `apt_model/training/trainer.py`:
   - æ·»åŠ  `checkpoint_dir` å‚æ•° (default: `"./outputs"`)
   - æ·»åŠ  `resume_from` å‚æ•°
   - åˆå§‹åŒ– `CheckpointManager`
   - åœ¨ `on_epoch_end` è°ƒç”¨ `save_checkpoint()`
   - æ·»åŠ æ¢å¤è®­ç»ƒé€»è¾‘

**éªŒè¯**:
```bash
# æµ‹è¯•ä¸­æ–­æ¢å¤
python -m apt_model.training.trainer --epochs 10
# Ctrl+C åœ¨epoch 5ä¸­æ–­

python -m apt_model.training.trainer \
    --resume-from ./outputs/checkpoints/apt_model_epoch5_step2500.pt \
    --epochs 10
# âœ… åº”è¯¥ä»epoch 6ç»§ç»­
```

**ç›¸å…³æ–‡ä»¶**:
- `apt_model/training/trainer.py` (ä¿®æ”¹)
- `apt_model/training/checkpoint.py` (å·²å®Œæˆï¼Œæ— éœ€ä¿®æ”¹)

**å‚è€ƒæ–‡æ¡£**:
- `TRAINING_CHECKPOINT_MIGRATION_GUIDE.md` (lines 109-157)

---

### C2: ä¿®å¤è®­ç»ƒè¿ç§»é—®é¢˜ï¼ˆä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼‰ ğŸ”´
**ä¼˜å…ˆçº§**: P0 - Critical
**å·¥ä½œé‡**: Small (4-6å°æ—¶)
**å½±å“**: æ— æ³•å°†è®­ç»ƒè¿ç§»åˆ°å…¶ä»–ç”µè„‘/æœåŠ¡å™¨

**é—®é¢˜æè¿°**:
- `apt_model/utils/cache_manager.py` ä½¿ç”¨ç»å¯¹è·¯å¾„ `~/.apt_cache`
- å¯¼è‡´checkpointè·¯å¾„ç»‘å®šåˆ°ç‰¹å®šç”¨æˆ·homeç›®å½•
- **æ— æ³•æ‰“åŒ…è¿ç§»**åˆ°å…¶ä»–ç”µè„‘

**å½“å‰é—®é¢˜**:
```python
# cache_manager.py:42
self.cache_dir = os.path.expanduser("~/.apt_cache")
# â†’ /home/userA/.apt_cache (ç»å¯¹è·¯å¾„)

# è¿ç§»åˆ°ç”µè„‘Båï¼š
# /home/userB/.apt_cache âŒ æ‰¾ä¸åˆ°åŸcheckpoint
```

**è§£å†³æ–¹æ¡ˆ1**: ä½¿ç”¨é¡¹ç›®å†…ç›¸å¯¹è·¯å¾„ï¼ˆæ¨èï¼‰
```python
# ä¿®æ”¹trainer.py
def train(..., checkpoint_dir="./outputs"):
    """
    å‚æ•°:
        checkpoint_dir: checkpointä¿å­˜ç›®å½•ï¼ˆç›¸å¯¹è·¯å¾„ï¼‰
    """
    checkpoint_mgr = CheckpointManager(save_dir=checkpoint_dir)
    # ä¿å­˜åˆ°: APT-Transformer/outputs/checkpoints/
```

**è§£å†³æ–¹æ¡ˆ2**: æ”¹è¿›CacheManageræ”¯æŒå¯è¿ç§»è·¯å¾„
```python
# cache_manager.py
class CacheManager:
    def __init__(self, cache_dir: Optional[str] = None,
                 use_project_dir: bool = True):
        if cache_dir is None:
            if use_project_dir:
                # é¡¹ç›®å†…ç¼“å­˜ï¼ˆå¯è¿ç§»ï¼‰
                project_root = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))
                self.cache_dir = os.path.join(project_root, ".cache")
            else:
                # ç”¨æˆ·homeç¼“å­˜ï¼ˆä¼ ç»Ÿæ–¹å¼ï¼‰
                self.cache_dir = os.path.expanduser("~/.apt_cache")
```

**æ–‡ä»¶ç»“æ„**ï¼ˆä¿®æ”¹åï¼‰:
```
APT-Transformer/
â”œâ”€â”€ outputs/                    # âœ… ç›¸å¯¹è·¯å¾„ï¼Œå¯æ‰“åŒ…è¿ç§»
â”‚   â”œâ”€â”€ checkpoints/
â”‚   â”‚   â”œâ”€â”€ apt_model_epoch5_step2500_best.pt
â”‚   â”‚   â””â”€â”€ apt_model_epoch10_step5000.pt
â”‚   â”œâ”€â”€ metadata.json
â”‚   â””â”€â”€ tokenizer/
â””â”€â”€ .cache/                     # âœ… é¡¹ç›®å†…ç¼“å­˜ï¼ˆå¯é€‰ï¼‰
    â”œâ”€â”€ temp/
    â””â”€â”€ logs/
```

**è¿ç§»æµ‹è¯•**:
```bash
# ç”µè„‘A
cd /path/to/APT-Transformer
tar -czf training_backup.tar.gz outputs/ .cache/

# ç”µè„‘B
cd /new/path/to/APT-Transformer
tar -xzf training_backup.tar.gz
python -m apt_model.training.trainer \
    --resume-from outputs/checkpoints/apt_model_epoch5_step2500_best.pt
# âœ… åº”è¯¥èƒ½æˆåŠŸæ¢å¤
```

**æ–‡ä»¶ä¿®æ”¹**:
1. `apt_model/training/trainer.py`:
   - ä½¿ç”¨ `checkpoint_dir="./outputs"` (ç›¸å¯¹è·¯å¾„)
2. `apt_model/utils/cache_manager.py`:
   - æ·»åŠ  `use_project_dir` å‚æ•°
   - ä¿®æ”¹é»˜è®¤è¡Œä¸ºä¸ºé¡¹ç›®å†…è·¯å¾„

**ç›¸å…³æ–‡ä»¶**:
- `apt_model/training/trainer.py`
- `apt_model/utils/cache_manager.py`

**å‚è€ƒæ–‡æ¡£**:
- `TRAINING_CHECKPOINT_MIGRATION_GUIDE.md` (lines 176-220, 248-281)

---

### C3: å®ç°tempæ–‡ä»¶å¤¹åŠŸèƒ½ ğŸ”´
**ä¼˜å…ˆçº§**: P0 - Critical
**å·¥ä½œé‡**: Small (3-4å°æ—¶)
**å½±å“**: è®­ç»ƒä¸­é—´çŠ¶æ€æ— ç®¡ç†ï¼Œå´©æºƒåæ— æ³•æ¢å¤

**é—®é¢˜æè¿°**:
- `cache_manager.py` å®šä¹‰äº† `temp` å­ç›®å½•ä½†ä»æœªä½¿ç”¨
- è®­ç»ƒè¿‡ç¨‹ä¸­æ²¡æœ‰ä¿å­˜ä¸­é—´checkpoint
- å¦‚æœè®­ç»ƒåœ¨epochä¸­é—´å´©æºƒï¼Œä»epochå¼€å§‹é‡æ¥æµªè´¹æ—¶é—´

**å½“å‰çŠ¶æ€**:
```python
# cache_manager.py:58
"temp": os.path.join(self.cache_dir, "temp")  # âŒ å®šä¹‰ä½†æœªä½¿ç”¨
```

**æœŸæœ›åŠŸèƒ½**:
```python
# trainer.py (è®­ç»ƒå¾ªç¯ä¸­)
def train(...):
    temp_dir = ".cache/temp"
    os.makedirs(temp_dir, exist_ok=True)

    for epoch in range(epochs):
        for batch_idx, batch in enumerate(dataloader):
            # ... è®­ç»ƒ ...
            global_step += 1

            # æ¯100æ­¥ä¿å­˜ä¸´æ—¶checkpoint
            if global_step % 100 == 0:
                temp_checkpoint = os.path.join(
                    temp_dir,
                    f"temp_epoch{epoch}_step{global_step}.pt"
                )
                torch.save({
                    'epoch': epoch,
                    'global_step': global_step,
                    'model_state': model.state_dict(),
                    'optimizer_state': optimizer.state_dict(),
                    'batch_idx': batch_idx
                }, temp_checkpoint)

        # epochç»“æŸåæ¸…ç†tempæ–‡ä»¶
        for temp_file in glob.glob(os.path.join(temp_dir, "temp_*.pt")):
            os.remove(temp_file)
```

**ä½¿ç”¨åœºæ™¯**:
```bash
# åœºæ™¯1: è®­ç»ƒåœ¨epochä¸­é—´å´©æºƒ
Epoch 5, batch 750/1000 â†’ å´©æºƒ

# æ¢å¤:
æ‰¾åˆ°: .cache/temp/temp_epoch5_step3750.pt
ä»batch 750ç»§ç»­ï¼Œè€Œä¸æ˜¯ä»epoch 5å¼€å§‹é‡æ¥
èŠ‚çœ: 750 batches Ã— 2ç§’ = 25åˆ†é’Ÿ
```

**æ–‡ä»¶ä¿®æ”¹**:
1. `apt_model/training/trainer.py`:
   - æ·»åŠ temp checkpointä¿å­˜é€»è¾‘ï¼ˆæ¯Næ­¥ï¼‰
   - æ·»åŠ tempæ¸…ç†é€»è¾‘ï¼ˆepochç»“æŸï¼‰
   - æ·»åŠ ä»tempæ¢å¤åŠŸèƒ½

2. `apt_model/utils/cache_manager.py`:
   - æ·»åŠ  `clean_temp()` æ–¹æ³•

**é…ç½®å‚æ•°**:
```python
# config.yaml
training:
  temp_checkpoint_freq: 100  # æ¯100æ­¥ä¿å­˜ä¸€æ¬¡
  keep_temp_files: false     # epochç»“æŸæ˜¯å¦ä¿ç•™temp
```

**ç›¸å…³æ–‡ä»¶**:
- `apt_model/training/trainer.py`
- `apt_model/utils/cache_manager.py`

**å‚è€ƒæ–‡æ¡£**:
- `TRAINING_CHECKPOINT_MIGRATION_GUIDE.md` (lines 222-246)

---

## ğŸŸ¡ High - è¿‘æœŸä¿®å¤ (6é¡¹)

### H1: è¡¥å……è®­ç»ƒå™¨å•å…ƒæµ‹è¯• ğŸŸ¡
**ä¼˜å…ˆçº§**: P1 - High
**å·¥ä½œé‡**: Large (20-24å°æ—¶)
**å½±å“**: ä»£ç è´¨é‡æ— ä¿è¯ï¼Œæ˜“å¼•å…¥bug

**ç¼ºå¤±æµ‹è¯•**:
1. **è®­ç»ƒå¾ªç¯æµ‹è¯•**:
   ```python
   # tests/test_trainer.py
   def test_training_loop():
       trainer = Trainer(...)
       metrics = trainer.train(epochs=2, batch_size=4)
       assert 'loss' in metrics
       assert metrics['loss'][-1] < metrics['loss'][0]  # æŸå¤±ä¸‹é™
   ```

2. **Checkpointä¿å­˜/åŠ è½½æµ‹è¯•**:
   ```python
   def test_checkpoint_save_load():
       trainer.train(epochs=5)
       checkpoint = torch.load("outputs/checkpoints/apt_model_epoch5.pt")
       assert 'optimizer_state_dict' in checkpoint
       assert 'scheduler_state_dict' in checkpoint
       assert checkpoint['epoch'] == 5
   ```

3. **è®­ç»ƒæ¢å¤æµ‹è¯•**:
   ```python
   def test_resume_training():
       trainer1 = Trainer(...)
       trainer1.train(epochs=5)

       trainer2 = Trainer(...)
       trainer2.train(epochs=10, resume_from="outputs/.../epoch5.pt")
       # åº”è¯¥ä»epoch 6å¼€å§‹
       assert trainer2.start_epoch == 5
   ```

4. **Early stoppingæµ‹è¯•**:
   ```python
   def test_early_stopping():
       trainer = Trainer(callbacks=[EarlyStoppingCallback(patience=3)])
       metrics = trainer.train(epochs=100)
       # åº”è¯¥åœ¨<100 epochsåœæ­¢
       assert len(metrics['loss']) < 100
   ```

5. **å¤šGPUæµ‹è¯•**ï¼ˆå¯é€‰ï¼‰:
   ```python
   @pytest.mark.skipif(not torch.cuda.is_available(), reason="éœ€è¦GPU")
   def test_distributed_training():
       trainer = Trainer(use_ddp=True)
       # ...
   ```

**æµ‹è¯•è¦†ç›–ç›®æ ‡**:
- è®­ç»ƒå¾ªç¯: 90%+
- Checkpointç³»ç»Ÿ: 95%+
- Callbackç³»ç»Ÿ: 80%+

**æ–‡ä»¶åˆ›å»º**:
- `tests/test_trainer.py` (æ–°å»º)
- `tests/test_checkpoint.py` (æ–°å»º)
- `tests/test_callbacks.py` (æ–°å»º)

**å·¥å…·**:
- pytest
- pytest-cov (è¦†ç›–ç‡)
- pytest-mock (mockå¤–éƒ¨ä¾èµ–)

---

### H2: è¡¥å……æ¨¡å‹æ¶æ„å•å…ƒæµ‹è¯• ğŸŸ¡
**ä¼˜å…ˆçº§**: P1 - High
**å·¥ä½œé‡**: Medium (12-16å°æ—¶)
**å½±å“**: æ¨¡å‹æ­£ç¡®æ€§æ— éªŒè¯

**ç¼ºå¤±æµ‹è¯•**:
1. **Transformerå‰å‘ä¼ æ’­**:
   ```python
   # tests/test_transformer.py
   def test_transformer_forward():
       model = TransformerModel(vocab_size=1000, d_model=512)
       input_ids = torch.randint(0, 1000, (2, 10))
       output = model(input_ids)
       assert output.shape == (2, 10, 512)
   ```

2. **Attentionæœºåˆ¶**:
   ```python
   def test_multi_head_attention():
       attn = MultiHeadAttention(d_model=512, num_heads=8)
       q = k = v = torch.randn(2, 10, 512)
       output, weights = attn(q, k, v)
       assert output.shape == (2, 10, 512)
       assert weights.shape == (2, 8, 10, 10)  # (batch, heads, seq, seq)
   ```

3. **ä½ç½®ç¼–ç **:
   ```python
   def test_positional_encoding():
       pe = PositionalEncoding(d_model=512, max_len=100)
       x = torch.randn(2, 50, 512)
       output = pe(x)
       assert output.shape == x.shape
   ```

4. **æ¢¯åº¦æ£€æŸ¥**:
   ```python
   def test_model_gradients():
       model = TransformerModel(...)
       optimizer = torch.optim.Adam(model.parameters())
       input_ids = torch.randint(0, 1000, (2, 10))
       output = model(input_ids)
       loss = output.sum()
       loss.backward()
       # æ£€æŸ¥æ¢¯åº¦éé›¶
       assert any(p.grad is not None for p in model.parameters())
   ```

**æ–‡ä»¶åˆ›å»º**:
- `tests/test_transformer.py` (æ–°å»º)
- `tests/test_apt_model.py` (æ–°å»º)

---

### H3: ç¼–å†™å¿«é€Ÿå¼€å§‹æ–‡æ¡£ ğŸŸ¡
**ä¼˜å…ˆçº§**: P1 - High
**å·¥ä½œé‡**: Medium (8-10å°æ—¶)
**å½±å“**: æ–°ç”¨æˆ·æ— æ³•ä¸Šæ‰‹

**ç¼ºå¤±å†…å®¹**:
1. **å®‰è£…æŒ‡å—**:
   ```markdown
   # å¿«é€Ÿå¼€å§‹

   ## å®‰è£…

   ### ç¯å¢ƒè¦æ±‚
   - Python 3.8+
   - PyTorch 2.0+
   - CUDA 11.8+ (å¯é€‰ï¼Œç”¨äºGPUåŠ é€Ÿ)

   ### å®‰è£…æ­¥éª¤
   ```bash
   git clone https://github.com/your-org/APT-Transformer.git
   cd APT-Transformer
   pip install -r requirements.txt
   ```
   ```

2. **åŸºç¡€ä½¿ç”¨ç¤ºä¾‹**:
   ```markdown
   ## è®­ç»ƒæ¨¡å‹

   ### å‡†å¤‡æ•°æ®
   ```python
   from apt_model.data import create_dataloader
   train_loader = create_dataloader(
       data_path="data/train.json",
       batch_size=32
   )
   ```

   ### å¼€å§‹è®­ç»ƒ
   ```python
   from apt_model.training import train
   from apt_model.config import APTConfig

   config = APTConfig.from_yaml("config/default.yaml")
   metrics = train(
       config=config,
       train_dataloader=train_loader,
       epochs=10,
       checkpoint_dir="./outputs"
   )
   ```

   ### æ¢å¤è®­ç»ƒ
   ```python
   metrics = train(
       config=config,
       train_dataloader=train_loader,
       epochs=20,
       resume_from="./outputs/checkpoints/apt_model_epoch10.pt"
   )
   ```
   ```

3. **æ¨¡å‹æ¨ç†ç¤ºä¾‹**:
   ```markdown
   ## ä½¿ç”¨æ¨¡å‹

   ### åŠ è½½æ¨¡å‹
   ```python
   from apt_model.training.checkpoint import load_model
   model, tokenizer, config = load_model(
       "outputs/checkpoints/apt_model_best.pt"
   )
   ```

   ### æ¨ç†
   ```python
   input_text = "Hello, APT!"
   input_ids = tokenizer.encode(input_text)
   output = model(input_ids)
   ```
   ```

4. **EQIå†³ç­–æµæ°´çº¿ç¤ºä¾‹**:
   ```markdown
   ## ä½¿ç”¨EQIå†³ç­–ç³»ç»Ÿ

   ```python
   from apt_eqi_manager import DecisionPipeline, SAFModule, COCScenario

   # å®šä¹‰æ¨¡å—
   modules = [
       SAFModule(name="legacy_db", S=0.9, D=0.5, R=1.0),
       # ...
   ]

   # è¿è¡Œå†³ç­–æµæ°´çº¿
   pipeline = DecisionPipeline()
   report = pipeline.run_full_pipeline(
       modules=modules,
       scenarios={...},
       budget=100,
       max_parallel=2
   )
   print(report)
   ```
   ```

**æ–‡ä»¶åˆ›å»º**:
- `QUICK_START.md` (æ–°å»º)
- `docs/installation.md` (æ–°å»º)
- `docs/training_guide.md` (æ–°å»º)
- `docs/inference_guide.md` (æ–°å»º)

---

### H4: è¡¥å……APIæ–‡æ¡£ï¼ˆdocstringï¼‰ ğŸŸ¡
**ä¼˜å…ˆçº§**: P1 - High
**å·¥ä½œé‡**: Large (16-20å°æ—¶)
**å½±å“**: ä»£ç å¯è¯»æ€§å·®ï¼Œéš¾ä»¥ç»´æŠ¤

**é—®é¢˜æè¿°**:
- å¾ˆå¤šå‡½æ•°ç¼ºå°‘docstring
- ç°æœ‰docstringæ ¼å¼ä¸ç»Ÿä¸€
- ç¼ºå°‘å‚æ•°ç±»å‹æ ‡æ³¨

**å½“å‰ç¤ºä¾‹**ï¼ˆä¸å®Œæ•´ï¼‰:
```python
# trainer.py
def train(config, train_dataloader, epochs):
    # æ²¡æœ‰docstring âŒ
    pass
```

**æœŸæœ›æ ¼å¼**ï¼ˆGoogleé£æ ¼ï¼‰:
```python
def train(
    config: APTConfig,
    train_dataloader: DataLoader,
    epochs: int,
    checkpoint_dir: str = "./outputs",
    resume_from: Optional[str] = None,
    callbacks: Optional[List[TrainingCallback]] = None
) -> Dict[str, Any]:
    """è®­ç»ƒAPTæ¨¡å‹

    Args:
        config: æ¨¡å‹é…ç½®å¯¹è±¡
        train_dataloader: è®­ç»ƒæ•°æ®åŠ è½½å™¨
        epochs: è®­ç»ƒè½®æ•°
        checkpoint_dir: checkpointä¿å­˜ç›®å½•ï¼Œé»˜è®¤"./outputs"
        resume_from: æ¢å¤è®­ç»ƒçš„checkpointè·¯å¾„ï¼Œå¯é€‰
        callbacks: è®­ç»ƒå›è°ƒåˆ—è¡¨ï¼Œå¯é€‰

    Returns:
        dict: è®­ç»ƒæŒ‡æ ‡å­—å…¸ï¼ŒåŒ…å«:
            - loss (List[float]): æ¯ä¸ªepochçš„å¹³å‡æŸå¤±
            - accuracy (List[float]): æ¯ä¸ªepochçš„å‡†ç¡®ç‡
            - learning_rate (List[float]): æ¯ä¸ªepochçš„å­¦ä¹ ç‡

    Raises:
        FileNotFoundError: å½“resume_fromæŒ‡å®šçš„æ–‡ä»¶ä¸å­˜åœ¨æ—¶
        ValueError: å½“configéªŒè¯å¤±è´¥æ—¶

    Example:
        >>> config = APTConfig.from_yaml("config.yaml")
        >>> train_loader = create_dataloader("data/train.json")
        >>> metrics = train(config, train_loader, epochs=10)
        >>> print(f"Final loss: {metrics['loss'][-1]}")
    """
    pass
```

**å¾…è¡¥å……æ–‡ä»¶**ï¼ˆä¼˜å…ˆçº§æ’åºï¼‰:
1. `apt_model/training/trainer.py` - æ ¸å¿ƒè®­ç»ƒé€»è¾‘
2. `apt_model/training/checkpoint.py` - Checkpointç®¡ç†
3. `apt_model/models/apt_transformer.py` - æ¨¡å‹å®šä¹‰
4. `apt_eqi_manager.py` - å†³ç­–ç³»ç»Ÿ
5. `apt_model/data/dataloader.py` - æ•°æ®åŠ è½½
6. `apt_model/infrastructure/errors.py` - é”™è¯¯å¤„ç†

**å·¥å…·**:
- sphinx (ç”ŸæˆHTMLæ–‡æ¡£)
- sphinx-autodoc (è‡ªåŠ¨ä»docstringç”Ÿæˆæ–‡æ¡£)

**ç”Ÿæˆæ–‡æ¡£**:
```bash
cd docs/
sphinx-apidoc -o source/ ../apt_model/
make html
```

---

### H5: åˆ›å»ºDockeré•œåƒ ğŸŸ¡
**ä¼˜å…ˆçº§**: P1 - High
**å·¥ä½œé‡**: Medium (8-12å°æ—¶)
**å½±å“**: æ— æ³•å¿«é€Ÿéƒ¨ç½²ï¼Œç¯å¢ƒä¸€è‡´æ€§å·®

**ç¼ºå¤±å†…å®¹**:
1. **Dockerfile**:
   ```dockerfile
   # Dockerfile
   FROM pytorch/pytorch:2.0.1-cuda11.8-cudnn8-runtime

   WORKDIR /app

   # å®‰è£…ç³»ç»Ÿä¾èµ–
   RUN apt-get update && apt-get install -y \
       git \
       && rm -rf /var/lib/apt/lists/*

   # å¤åˆ¶é¡¹ç›®æ–‡ä»¶
   COPY requirements.txt .
   RUN pip install --no-cache-dir -r requirements.txt

   COPY . .

   # åˆ›å»ºè¾“å‡ºç›®å½•
   RUN mkdir -p /app/outputs /app/.cache

   # é»˜è®¤å‘½ä»¤
   CMD ["python", "-m", "apt_model.training.trainer", "--config", "config/default.yaml"]
   ```

2. **docker-compose.yml**:
   ```yaml
   # docker-compose.yml
   version: '3.8'
   services:
     apt-trainer:
       build: .
       image: apt-transformer:latest
       volumes:
         - ./data:/app/data
         - ./outputs:/app/outputs
         - ./config:/app/config
       environment:
         - CUDA_VISIBLE_DEVICES=0
       deploy:
         resources:
           reservations:
             devices:
               - driver: nvidia
                 count: 1
                 capabilities: [gpu]
   ```

3. **.dockerignore**:
   ```
   # .dockerignore
   .git
   .gitignore
   *.pyc
   __pycache__
   .cache
   outputs
   *.pt
   *.pth
   .pytest_cache
   ```

4. **æ„å»ºå’Œè¿è¡Œè„šæœ¬**:
   ```bash
   # scripts/docker_build.sh
   #!/bin/bash
   docker build -t apt-transformer:latest .

   # scripts/docker_train.sh
   #!/bin/bash
   docker run --gpus all \
       -v $(pwd)/data:/app/data \
       -v $(pwd)/outputs:/app/outputs \
       apt-transformer:latest
   ```

**éªŒè¯**:
```bash
# æ„å»ºé•œåƒ
./scripts/docker_build.sh

# è¿è¡Œè®­ç»ƒ
./scripts/docker_train.sh

# äº¤äº’å¼è¿›å…¥å®¹å™¨
docker run -it --gpus all apt-transformer:latest bash
```

**æ–‡ä»¶åˆ›å»º**:
- `Dockerfile` (æ–°å»º)
- `docker-compose.yml` (æ–°å»º)
- `.dockerignore` (æ–°å»º)
- `scripts/docker_build.sh` (æ–°å»º)
- `scripts/docker_train.sh` (æ–°å»º)

---

### H6: å®Œå–„requirements.txt ğŸŸ¡
**ä¼˜å…ˆçº§**: P1 - High
**å·¥ä½œé‡**: Small (2-3å°æ—¶)
**å½±å“**: ä¾èµ–å®‰è£…å›°éš¾ï¼Œç¯å¢ƒä¸ä¸€è‡´

**é—®é¢˜æè¿°**:
- ç°æœ‰`requirements.txt`å¯èƒ½ä¸å®Œæ•´
- ç¼ºå°‘ç‰ˆæœ¬é”å®š
- ç¼ºå°‘å¼€å‘ä¾èµ–

**æœŸæœ›å†…å®¹**:
```txt
# requirements.txt (ç”Ÿäº§ä¾èµ–)

# æ ¸å¿ƒä¾èµ–
torch>=2.0.0,<2.2.0
numpy>=1.24.0,<2.0.0
transformers>=4.30.0,<5.0.0

# æ•°æ®å¤„ç†
pandas>=2.0.0
datasets>=2.12.0

# è®­ç»ƒå·¥å…·
tqdm>=4.65.0
tensorboard>=2.13.0
wandb>=0.15.0  # å¯é€‰

# é…ç½®ç®¡ç†
pyyaml>=6.0
omegaconf>=2.3.0

# å·¥å…·
rich>=13.4.0  # è¿›åº¦æ¡ç¾åŒ–
loguru>=0.7.0  # æ—¥å¿—

# æ¨ç†
onnx>=1.14.0  # å¯é€‰
onnxruntime>=1.15.0  # å¯é€‰
```

```txt
# requirements-dev.txt (å¼€å‘ä¾èµ–)

# æµ‹è¯•
pytest>=7.4.0
pytest-cov>=4.1.0
pytest-mock>=3.11.0
pytest-asyncio>=0.21.0

# ä»£ç è´¨é‡
black>=23.3.0
flake8>=6.0.0
mypy>=1.4.0
isort>=5.12.0

# æ–‡æ¡£
sphinx>=7.0.0
sphinx-rtd-theme>=1.2.0
myst-parser>=2.0.0  # Markdownæ”¯æŒ

# æ€§èƒ½åˆ†æ
py-spy>=0.3.14
memory-profiler>=0.61.0
```

**ç‰ˆæœ¬é”å®š**:
```bash
# ç”Ÿæˆå®Œæ•´é”å®šç‰ˆæœ¬
pip freeze > requirements-lock.txt
```

**å®‰è£…è„šæœ¬**:
```bash
# scripts/install_deps.sh
#!/bin/bash

# ç”Ÿäº§ç¯å¢ƒ
pip install -r requirements.txt

# å¼€å‘ç¯å¢ƒ
if [ "$DEV" = "true" ]; then
    pip install -r requirements-dev.txt
fi
```

**æ–‡ä»¶ä¿®æ”¹/åˆ›å»º**:
- `requirements.txt` (ä¿®æ”¹)
- `requirements-dev.txt` (æ–°å»º)
- `requirements-lock.txt` (æ–°å»º)
- `scripts/install_deps.sh` (æ–°å»º)

---

## ğŸŸ¢ Medium - è®¡åˆ’ä¿®å¤ (8é¡¹)

### M1: å®ç°Flash Attentionä¼˜åŒ– ğŸŸ¢
**ä¼˜å…ˆçº§**: P2 - Medium
**å·¥ä½œé‡**: Medium (10-14å°æ—¶)
**å½±å“**: è®­ç»ƒé€Ÿåº¦æå‡2-3x

**å½“å‰å®ç°**:
```python
# apt_model/models/transformer.py
# ä½¿ç”¨æ ‡å‡†PyTorch attention
attn_weights = torch.softmax(scores, dim=-1)
output = torch.matmul(attn_weights, v)
```

**ä¼˜åŒ–å**:
```python
# ä½¿ç”¨Flash Attention 2
from flash_attn import flash_attn_func

output = flash_attn_func(q, k, v, causal=True)
# å†…å­˜ä½¿ç”¨: O(N) vs O(NÂ²)
# é€Ÿåº¦æå‡: 2-3x
```

**ä¾èµ–**:
```bash
pip install flash-attn --no-build-isolation
```

**å…¼å®¹æ€§**:
- ä»…æ”¯æŒCUDA
- éœ€è¦Ampereæ¶æ„ï¼ˆRTX 30ç³»åˆ—+ï¼‰æˆ–æ›´æ–°

**æ–‡ä»¶ä¿®æ”¹**:
- `apt_model/models/transformer.py`

---

### M2: æ·»åŠ æ··åˆç²¾åº¦è®­ç»ƒ ğŸŸ¢
**ä¼˜å…ˆçº§**: P2 - Medium
**å·¥ä½œé‡**: Small (4-6å°æ—¶)
**å½±å“**: å†…å­˜èŠ‚çœ~40%, é€Ÿåº¦æå‡~2x

**å®ç°**:
```python
# trainer.py
from torch.cuda.amp import autocast, GradScaler

scaler = GradScaler()

for batch in dataloader:
    optimizer.zero_grad()

    # æ··åˆç²¾åº¦å‰å‘ä¼ æ’­
    with autocast():
        output = model(batch)
        loss = criterion(output, target)

    # æ··åˆç²¾åº¦åå‘ä¼ æ’­
    scaler.scale(loss).backward()
    scaler.step(optimizer)
    scaler.update()
```

**é…ç½®**:
```yaml
# config.yaml
training:
  use_mixed_precision: true
  fp16: true  # æˆ– bf16 (æ›´ç¨³å®š)
```

**æ–‡ä»¶ä¿®æ”¹**:
- `apt_model/training/trainer.py`
- `apt_model/config/training_config.py`

---

### M3: å®ç°åˆ†å¸ƒå¼è®­ç»ƒï¼ˆDDPï¼‰ ğŸŸ¢
**ä¼˜å…ˆçº§**: P2 - Medium
**å·¥ä½œé‡**: Large (20-24å°æ—¶)
**å½±å“**: æ”¯æŒå¤šGPUè®­ç»ƒï¼Œé€Ÿåº¦çº¿æ€§æå‡

**å®ç°**:
```python
# trainer.py
import torch.distributed as dist
from torch.nn.parallel import DistributedDataParallel as DDP

def setup_ddp(rank, world_size):
    dist.init_process_group("nccl", rank=rank, world_size=world_size)

def train_ddp(rank, world_size, ...):
    setup_ddp(rank, world_size)

    model = model.to(rank)
    model = DDP(model, device_ids=[rank])

    # è®­ç»ƒå¾ªç¯
    for batch in dataloader:
        # ...
```

**å¯åŠ¨**:
```bash
# å•æœº4å¡
torchrun --nproc_per_node=4 -m apt_model.training.trainer

# å¤šæœºè®­ç»ƒ
torchrun --nproc_per_node=4 \
         --nnodes=2 \
         --node_rank=0 \
         --master_addr="192.168.1.1" \
         --master_port=29500 \
         -m apt_model.training.trainer
```

**æ–‡ä»¶ä¿®æ”¹**:
- `apt_model/training/trainer.py`
- `apt_model/training/distributed.py` (æ–°å»º)

---

### M4: æ·»åŠ æ¢¯åº¦ç´¯ç§¯ ğŸŸ¢
**ä¼˜å…ˆçº§**: P2 - Medium
**å·¥ä½œé‡**: Small (3-4å°æ—¶)
**å½±å“**: æ”¯æŒå¤§batchè®­ç»ƒï¼Œæå‡æ”¶æ•›é€Ÿåº¦

**å®ç°**:
```python
# trainer.py
accumulation_steps = 4  # ç´¯ç§¯4ä¸ªbatch

for batch_idx, batch in enumerate(dataloader):
    output = model(batch)
    loss = criterion(output, target) / accumulation_steps
    loss.backward()

    # æ¯Næ­¥æ›´æ–°ä¸€æ¬¡
    if (batch_idx + 1) % accumulation_steps == 0:
        optimizer.step()
        optimizer.zero_grad()
```

**é…ç½®**:
```yaml
training:
  gradient_accumulation_steps: 4
  effective_batch_size: 128  # = batch_size * accumulation_steps
```

**æ–‡ä»¶ä¿®æ”¹**:
- `apt_model/training/trainer.py`

---

### M5: å®ç°æ¨¡å‹é‡åŒ– ğŸŸ¢
**ä¼˜å…ˆçº§**: P2 - Medium
**å·¥ä½œé‡**: Medium (12-16å°æ—¶)
**å½±å“**: æ¨ç†é€Ÿåº¦æå‡2-4xï¼Œæ¨¡å‹å¤§å°å‡å°‘75%

**å®ç°æ–¹æ¡ˆ**:
1. **åŠ¨æ€é‡åŒ–**ï¼ˆæœ€ç®€å•ï¼‰:
   ```python
   import torch.quantization
   quantized_model = torch.quantization.quantize_dynamic(
       model, {torch.nn.Linear}, dtype=torch.qint8
   )
   ```

2. **é™æ€é‡åŒ–**:
   ```python
   # æ ¡å‡†
   model.qconfig = torch.quantization.get_default_qconfig('fbgemm')
   model_prepared = torch.quantization.prepare(model)
   # å–‚å…¥æ ¡å‡†æ•°æ®
   calibrate(model_prepared, calibration_data)
   # é‡åŒ–
   quantized_model = torch.quantization.convert(model_prepared)
   ```

3. **é‡åŒ–æ„ŸçŸ¥è®­ç»ƒ**ï¼ˆQATï¼‰:
   ```python
   model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')
   model_prepared = torch.quantization.prepare_qat(model)
   # æ­£å¸¸è®­ç»ƒ
   train(model_prepared)
   # é‡åŒ–
   quantized_model = torch.quantization.convert(model_prepared)
   ```

**æ–‡ä»¶åˆ›å»º**:
- `apt_model/quantization/quantize.py` (æ–°å»º)
- `scripts/quantize_model.sh` (æ–°å»º)

---

### M6: æ·»åŠ TensorBoard/wandbé›†æˆ ğŸŸ¢
**ä¼˜å…ˆçº§**: P2 - Medium
**å·¥ä½œé‡**: Small (4-6å°æ—¶)
**å½±å“**: å¯è§†åŒ–è®­ç»ƒè¿‡ç¨‹

**TensorBoardå®ç°**:
```python
# trainer.py
from torch.utils.tensorboard import SummaryWriter

writer = SummaryWriter(log_dir="runs/apt_training")

for epoch in range(epochs):
    for batch_idx, batch in enumerate(dataloader):
        # ...
        writer.add_scalar('Loss/train', loss.item(), global_step)
        writer.add_scalar('LR', lr, global_step)

    # epochç»“æŸè®°å½•
    writer.add_scalar('Loss/epoch', avg_loss, epoch)
    writer.add_histogram('Gradients/layer1', model.layer1.weight.grad, epoch)
```

**wandbå®ç°**:
```python
import wandb

wandb.init(project="apt-transformer", config=config)

for epoch in range(epochs):
    # ...
    wandb.log({
        "loss": loss.item(),
        "lr": lr,
        "epoch": epoch
    })
```

**æŸ¥çœ‹**:
```bash
# TensorBoard
tensorboard --logdir=runs/

# wandb
wandb login
# è®¿é—® https://wandb.ai/your-project
```

**æ–‡ä»¶ä¿®æ”¹**:
- `apt_model/training/callbacks.py` (æ·»åŠ TensorBoardCallback)

---

### M7: å®ç°æ¨¡å‹å¯¼å‡ºï¼ˆONNXï¼‰ ğŸŸ¢
**ä¼˜å…ˆçº§**: P2 - Medium
**å·¥ä½œé‡**: Medium (8-10å°æ—¶)
**å½±å“**: æ”¯æŒè·¨å¹³å°éƒ¨ç½²

**å®ç°**:
```python
# apt_model/export/onnx_export.py
import torch.onnx

def export_to_onnx(model, save_path, input_sample):
    """å¯¼å‡ºæ¨¡å‹ä¸ºONNXæ ¼å¼"""
    model.eval()

    torch.onnx.export(
        model,
        input_sample,
        save_path,
        export_params=True,
        opset_version=14,
        do_constant_folding=True,
        input_names=['input_ids'],
        output_names=['logits'],
        dynamic_axes={
            'input_ids': {0: 'batch_size', 1: 'sequence'},
            'logits': {0: 'batch_size', 1: 'sequence'}
        }
    )

# ä½¿ç”¨
model, tokenizer, config = load_model("best_model.pt")
input_sample = torch.randint(0, config.vocab_size, (1, 128))
export_to_onnx(model, "model.onnx", input_sample)
```

**æ¨ç†**:
```python
import onnxruntime as ort

session = ort.InferenceSession("model.onnx")
inputs = {"input_ids": input_ids.numpy()}
outputs = session.run(None, inputs)
```

**éªŒè¯**:
```python
# éªŒè¯ONNXè¾“å‡ºä¸PyTorchä¸€è‡´
torch_output = model(input_ids)
onnx_output = session.run(None, {"input_ids": input_ids.numpy()})[0]
assert np.allclose(torch_output.detach().numpy(), onnx_output, atol=1e-5)
```

**æ–‡ä»¶åˆ›å»º**:
- `apt_model/export/onnx_export.py` (æ–°å»º)
- `scripts/export_onnx.sh` (æ–°å»º)

---

### M8: æ·»åŠ æ€§èƒ½åŸºå‡†æµ‹è¯• ğŸŸ¢
**ä¼˜å…ˆçº§**: P2 - Medium
**å·¥ä½œé‡**: Medium (10-12å°æ—¶)
**å½±å“**: äº†è§£æ€§èƒ½ç“¶é¢ˆ

**å®ç°**:
```python
# benchmarks/benchmark_training.py
import time
import torch
from apt_model.training import train

def benchmark_training_speed():
    """æµ‹è¯•è®­ç»ƒé€Ÿåº¦"""
    start = time.time()

    metrics = train(
        config=config,
        train_dataloader=train_loader,
        epochs=5
    )

    elapsed = time.time() - start
    samples_per_sec = total_samples / elapsed

    print(f"è®­ç»ƒé€Ÿåº¦: {samples_per_sec:.2f} samples/sec")
    print(f"æ¯ä¸ªepoch: {elapsed/5:.2f}ç§’")

def benchmark_memory_usage():
    """æµ‹è¯•å†…å­˜ä½¿ç”¨"""
    torch.cuda.reset_peak_memory_stats()

    model = create_model(config)
    output = model(dummy_input)
    loss = output.sum()
    loss.backward()

    peak_memory = torch.cuda.max_memory_allocated() / 1024**3
    print(f"å³°å€¼GPUå†…å­˜: {peak_memory:.2f} GB")

def benchmark_inference_latency():
    """æµ‹è¯•æ¨ç†å»¶è¿Ÿ"""
    model.eval()
    with torch.no_grad():
        latencies = []
        for _ in range(100):
            start = time.time()
            output = model(input_ids)
            latencies.append(time.time() - start)

    print(f"å¹³å‡å»¶è¿Ÿ: {np.mean(latencies)*1000:.2f}ms")
    print(f"P95å»¶è¿Ÿ: {np.percentile(latencies, 95)*1000:.2f}ms")
```

**è¿è¡Œ**:
```bash
python benchmarks/benchmark_training.py
```

**è¾“å‡ºç¤ºä¾‹**:
```
è®­ç»ƒé€Ÿåº¦: 245.3 samples/sec
æ¯ä¸ªepoch: 122.5ç§’
å³°å€¼GPUå†…å­˜: 8.45 GB
---
å¹³å‡å»¶è¿Ÿ: 12.3ms
P95å»¶è¿Ÿ: 18.7ms
```

**æ–‡ä»¶åˆ›å»º**:
- `benchmarks/benchmark_training.py` (æ–°å»º)
- `benchmarks/benchmark_inference.py` (æ–°å»º)
- `benchmarks/benchmark_memory.py` (æ–°å»º)

---

## ğŸ”µ Low - é”¦ä¸Šæ·»èŠ± (5é¡¹)

### L1: åŠ¨æ€å‚æ•°è°ƒæ•´ï¼ˆEQI/COCï¼‰ ğŸ”µ
**ä¼˜å…ˆçº§**: P3 - Low
**å·¥ä½œé‡**: Medium (10-12å°æ—¶)
**å½±å“**: EQIç³»ç»Ÿæ›´çµæ´»

**å½“å‰é—®é¢˜**:
```python
# COCä¸­çš„Î±, Î²ç¡¬ç¼–ç 
Î± = 0.3  # å½“å‰å¤æ‚åº¦æƒé‡
Î² = 0.2  # å¤æ‚åº¦æ¼‚ç§»æƒé‡
```

**æœŸæœ›**:
```python
class COCAnalyzer:
    def __init__(self, alpha: float = 0.3, beta: float = 0.2):
        self.alpha = alpha
        self.beta = beta

    def set_weights(self, alpha: float, beta: float):
        """åŠ¨æ€è°ƒæ•´æƒé‡"""
        self.alpha = alpha
        self.beta = beta

# ä½¿ç”¨
analyzer = COCAnalyzer()
analyzer.set_weights(alpha=0.5, beta=0.1)  # é‡è§†å½“å‰å¤æ‚åº¦
```

**æ–‡ä»¶ä¿®æ”¹**:
- `apt_eqi_manager.py`

---

### L2: æ·»åŠ è‡ªå®šä¹‰EQIé—¨ç¦ ğŸ”µ
**ä¼˜å…ˆçº§**: P3 - Low
**å·¥ä½œé‡**: Small (6-8å°æ—¶)
**å½±å“**: EQIç³»ç»Ÿæ›´é€šç”¨

**å®ç°**:
```python
# apt_eqi_manager.py
class EQIGate:
    """å¯æ‰©å±•çš„é—¨ç¦åŸºç±»"""
    def __init__(self, name: str, threshold: float):
        self.name = name
        self.threshold = threshold

    def evaluate(self, evidence: Dict[str, Any]) -> Tuple[bool, float]:
        """å­ç±»å®ç°å…·ä½“è¯„ä¼°é€»è¾‘"""
        raise NotImplementedError

class CustomGate(EQIGate):
    """ç”¨æˆ·è‡ªå®šä¹‰é—¨ç¦"""
    def evaluate(self, evidence):
        # è‡ªå®šä¹‰é€»è¾‘
        score = custom_scoring(evidence)
        passed = score >= self.threshold
        return passed, score

# ä½¿ç”¨
manager = EQIManager()
manager.add_gate(CustomGate("custom_security", threshold=0.8))
```

**æ–‡ä»¶ä¿®æ”¹**:
- `apt_eqi_manager.py`

---

### L3: å®ç°LoRAå¾®è°ƒæ”¯æŒ ğŸ”µ
**ä¼˜å…ˆçº§**: P3 - Low
**å·¥ä½œé‡**: Large (20-24å°æ—¶)
**å½±å“**: å‚æ•°é«˜æ•ˆå¾®è°ƒ

**å®ç°**:
```python
# apt_model/lora/lora_layer.py
class LoRALinear(nn.Module):
    def __init__(self, in_features, out_features, rank=8):
        super().__init__()
        self.linear = nn.Linear(in_features, out_features)
        self.lora_A = nn.Parameter(torch.zeros(in_features, rank))
        self.lora_B = nn.Parameter(torch.zeros(rank, out_features))
        self.scaling = 0.01

        # å†»ç»“åŸå§‹æƒé‡
        self.linear.weight.requires_grad = False

    def forward(self, x):
        return self.linear(x) + (x @ self.lora_A @ self.lora_B) * self.scaling
```

**æ–‡ä»¶åˆ›å»º**:
- `apt_model/lora/` (æ–°å»ºç›®å½•)

---

### L4: æ·»åŠ æ—¥å¿—è½®è½¬ ğŸ”µ
**ä¼˜å…ˆçº§**: P3 - Low
**å·¥ä½œé‡**: Small (2-3å°æ—¶)
**å½±å“**: é˜²æ­¢æ—¥å¿—æ–‡ä»¶è¿‡å¤§

**å®ç°**:
```python
from logging.handlers import RotatingFileHandler

handler = RotatingFileHandler(
    "apt_training.log",
    maxBytes=10*1024*1024,  # 10MB
    backupCount=5
)
```

**æ–‡ä»¶ä¿®æ”¹**:
- `apt_model/infrastructure/logging.py`

---

### L5: æ·»åŠ é…ç½®ç‰ˆæœ¬æ§åˆ¶ ğŸ”µ
**ä¼˜å…ˆçº§**: P3 - Low
**å·¥ä½œé‡**: Small (4-5å°æ—¶)
**å½±å“**: å®éªŒå¯å¤ç°æ€§

**å®ç°**:
```python
# ä¿å­˜é…ç½®æ—¶æ·»åŠ ç‰ˆæœ¬å·å’Œhash
config_with_version = {
    "config_version": "1.0.0",
    "config_hash": hashlib.md5(json.dumps(config).encode()).hexdigest(),
    "timestamp": datetime.now().isoformat(),
    **config
}
```

**æ–‡ä»¶ä¿®æ”¹**:
- `apt_model/config/config.py`

---

## å·¥ä½œé‡æ±‡æ€»

### æŒ‰ä¼˜å…ˆçº§

| ä¼˜å…ˆçº§ | ä»»åŠ¡æ•° | æ€»å·¥ä½œé‡ | å¹³å‡å·¥ä½œé‡ |
|--------|--------|----------|------------|
| ğŸ”´ Critical | 3 | 19-22å°æ—¶ (~3å¤©) | 6-7å°æ—¶ |
| ğŸŸ¡ High | 6 | 76-96å°æ—¶ (~10-12å¤©) | 13-16å°æ—¶ |
| ğŸŸ¢ Medium | 8 | 87-107å°æ—¶ (~11-13å¤©) | 11-13å°æ—¶ |
| ğŸ”µ Low | 5 | 42-52å°æ—¶ (~5-7å¤©) | 8-10å°æ—¶ |
| **æ€»è®¡** | **22** | **224-277å°æ—¶ (~28-35å¤©)** | **10-13å°æ—¶** |

### æŒ‰ç±»å‹

| ç±»å‹ | ä»»åŠ¡æ•° | å·¥ä½œé‡ |
|------|--------|--------|
| è®­ç»ƒç³»ç»Ÿä¿®å¤ | 3 | 19-22å°æ—¶ |
| æµ‹è¯•è¡¥å…… | 2 | 32-40å°æ—¶ |
| æ–‡æ¡£ç¼–å†™ | 3 | 26-33å°æ—¶ |
| éƒ¨ç½²æ”¯æŒ | 2 | 10-15å°æ—¶ |
| æ€§èƒ½ä¼˜åŒ– | 6 | 67-82å°æ—¶ |
| åŠŸèƒ½å¢å¼º | 6 | 70-85å°æ—¶ |

---

## æ¨èæ‰§è¡Œé¡ºåº

### Sprint 1 (Week 1): Criticalä¿®å¤
**ç›®æ ‡**: è§£å†³é˜»å¡æ€§é—®é¢˜ï¼Œä½¿é¡¹ç›®åŸºæœ¬å¯ç”¨

1. **C1**: é›†æˆCheckpointManager (Day 1-2)
2. **C2**: ä¿®å¤è®­ç»ƒè¿ç§»é—®é¢˜ (Day 2)
3. **C3**: å®ç°tempæ–‡ä»¶å¤¹åŠŸèƒ½ (Day 3)
4. **éªŒè¯**: ç«¯åˆ°ç«¯è®­ç»ƒ+ä¸­æ–­æ¢å¤+è¿ç§»æµ‹è¯• (Day 3)

**éªŒæ”¶æ ‡å‡†**:
- âœ… è®­ç»ƒå¯ä»¥ä»ä»»æ„epochæ¢å¤
- âœ… checkpointå¯ä»¥æ‰“åŒ…è¿ç§»åˆ°å…¶ä»–æœºå™¨
- âœ… tempæ–‡ä»¶å¤¹æ­£å¸¸å·¥ä½œ

---

### Sprint 2 (Week 2): æµ‹è¯•å’Œæ–‡æ¡£
**ç›®æ ‡**: è¡¥å……æµ‹è¯•è¦†ç›–ç‡ï¼Œæå‡ä»£ç è´¨é‡

1. **H1**: è®­ç»ƒå™¨å•å…ƒæµ‹è¯• (Day 1-3)
2. **H2**: æ¨¡å‹æ¶æ„å•å…ƒæµ‹è¯• (Day 3-4)
3. **H3**: å¿«é€Ÿå¼€å§‹æ–‡æ¡£ (Day 4-5)
4. **H4**: APIæ–‡æ¡£ (Day 5-7)

**éªŒæ”¶æ ‡å‡†**:
- âœ… æµ‹è¯•è¦†ç›–ç‡ > 60%
- âœ… æ–°ç”¨æˆ·å¯ä»¥åœ¨30åˆ†é’Ÿå†…å®Œæˆè®­ç»ƒ

---

### Sprint 3 (Week 3): éƒ¨ç½²å’ŒåŸºç¡€è®¾æ–½
**ç›®æ ‡**: æ”¯æŒç”Ÿäº§éƒ¨ç½²

1. **H5**: Dockeré•œåƒ (Day 1-2)
2. **H6**: å®Œå–„requirements.txt (Day 2)
3. **M6**: TensorBoardé›†æˆ (Day 3)
4. **M8**: æ€§èƒ½åŸºå‡†æµ‹è¯• (Day 3-4)

**éªŒæ”¶æ ‡å‡†**:
- âœ… å¯ä»¥ä¸€é”®Dockeréƒ¨ç½²
- âœ… æœ‰æ€§èƒ½åŸºå‡†æ•°æ®

---

### Sprint 4 (Week 4): æ€§èƒ½ä¼˜åŒ–
**ç›®æ ‡**: æå‡è®­ç»ƒé€Ÿåº¦2-3x

1. **M2**: æ··åˆç²¾åº¦è®­ç»ƒ (Day 1)
2. **M4**: æ¢¯åº¦ç´¯ç§¯ (Day 1)
3. **M1**: Flash Attention (Day 2-3)
4. **M3**: åˆ†å¸ƒå¼è®­ç»ƒ (Day 3-5)

**éªŒæ”¶æ ‡å‡†**:
- âœ… è®­ç»ƒé€Ÿåº¦æå‡ > 2x
- âœ… æ”¯æŒå¤šGPUè®­ç»ƒ

---

### Sprint 5+ (Week 5+): å¢å¼ºåŠŸèƒ½
**ç›®æ ‡**: å®Œå–„é«˜çº§åŠŸèƒ½

- M5: æ¨¡å‹é‡åŒ–
- M7: ONNXå¯¼å‡º
- L1-L5: Lowä¼˜å…ˆçº§ä»»åŠ¡

---

## èµ„æºéœ€æ±‚

### äººåŠ›
- **1åå·¥ç¨‹å¸ˆ**: å…¨èŒ ~6-8å‘¨å®Œæˆæ‰€æœ‰ä»»åŠ¡
- **2åå·¥ç¨‹å¸ˆ**: å…¨èŒ ~3-4å‘¨å®Œæˆæ‰€æœ‰ä»»åŠ¡
- **ä¼˜å…ˆå®ŒæˆCritical+High**: ~2å‘¨ï¼ˆ1äººï¼‰æˆ– ~1å‘¨ï¼ˆ2äººï¼‰

### ç¡¬ä»¶
- GPU: è‡³å°‘1å¼ ç”¨äºæµ‹è¯•ï¼ˆRTX 3090æˆ–æ›´å¥½ï¼‰
- å­˜å‚¨: ~100GBï¼ˆæ•°æ®+checkpoint+Dockeré•œåƒï¼‰
- å†…å­˜: 32GB+ï¼ˆç”¨äºå¤§æ¨¡å‹æµ‹è¯•ï¼‰

### å·¥å…·
- pytest, pytest-cov (æµ‹è¯•)
- sphinx (æ–‡æ¡£)
- Docker (éƒ¨ç½²)
- TensorBoard/wandb (ç›‘æ§)

---

## é£é™©è¯„ä¼°

### é«˜é£é™©
1. **åˆ†å¸ƒå¼è®­ç»ƒï¼ˆM3ï¼‰**: å¤æ‚åº¦é«˜ï¼Œè°ƒè¯•å›°éš¾
   - ç¼“è§£: å…ˆå®ç°å•æœºå¤šå¡ï¼Œå†æ‰©å±•å¤šæœº
2. **Flash Attentionï¼ˆM1ï¼‰**: ä¾èµ–ç‰¹å®šç¡¬ä»¶
   - ç¼“è§£: æä¾›fallbackåˆ°æ ‡å‡†attention

### ä¸­é£é™©
3. **æµ‹è¯•è¦†ç›–ç‡**: è¡¥å……æµ‹è¯•å·¥ä½œé‡å¤§
   - ç¼“è§£: ä¼˜å…ˆæ ¸å¿ƒåŠŸèƒ½ï¼Œé€æ­¥è¦†ç›–
4. **Dockeré›†æˆ**: ä¾èµ–ç®¡ç†å¤æ‚
   - ç¼“è§£: ä½¿ç”¨å®˜æ–¹PyTorché•œåƒä½œä¸ºåŸºç¡€

### ä½é£é™©
5. **æ–‡æ¡£ç¼–å†™**: æ—¶é—´æŠ•å…¥ä½†é£é™©ä½
6. **æ··åˆç²¾åº¦**: æˆç†ŸæŠ€æœ¯ï¼Œå®ç°ç®€å•

---

## æ€»ç»“

**å½“å‰çŠ¶æ€**: 68%æˆç†Ÿåº¦ï¼Œæ ¸å¿ƒåŠŸèƒ½å®Œå¤‡ä½†å·¥ç¨‹å®è·µä¸è¶³

**Criticalé—®é¢˜**: 3ä¸ªï¼Œ~3å¤©å¯ä¿®å¤ï¼Œä¿®å¤åæˆç†Ÿåº¦æå‡è‡³80%+

**å®Œæ•´è·¯çº¿å›¾**: 22ä¸ªä»»åŠ¡ï¼Œ~28-35å¤©å®Œæˆï¼Œæœ€ç»ˆæˆç†Ÿåº¦å¯è¾¾90%+

**æŠ•å…¥äº§å‡ºæ¯”**:
- **æœ€å°å¯è¡Œç‰ˆæœ¬**ï¼ˆCriticalä¿®å¤ï¼‰: 3å¤© â†’ 80%æˆç†Ÿåº¦
- **ç”Ÿäº§å°±ç»ªç‰ˆæœ¬**ï¼ˆ+Highä»»åŠ¡ï¼‰: 2å‘¨ â†’ 85%æˆç†Ÿåº¦
- **å®Œæ•´ä¼˜åŒ–ç‰ˆæœ¬**ï¼ˆå…¨éƒ¨ä»»åŠ¡ï¼‰: 5-7å‘¨ â†’ 90%+æˆç†Ÿåº¦

**å»ºè®®**: ä¼˜å…ˆå®ŒæˆSprint 1ï¼ˆCriticalä¿®å¤ï¼‰ï¼Œç„¶åæ ¹æ®å®é™…éœ€æ±‚å†³å®šåç»­Sprintçš„ä¼˜å…ˆçº§ã€‚
